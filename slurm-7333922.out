NODELIST=lrdn0176
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
32

10

DEVICE SET
DEVICE SET
DEVICE SET
DEVICE SET
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
08/30/2024 19:13:25 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
08/30/2024 19:13:25 - INFO - __main__ -   ==================model_configs==================

08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_model_type : vast
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_frozen_vision : False
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_frozen_audio : False
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_checkpointing : True
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_max_caption_len : 40
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_contra_dim : 512
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_vision_resolution : 224
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_audio_melbins : 64
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_audio_target_length : 1024
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_beam_size : 3
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_captioner_mode : False
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_generate_nums : 1
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_max_vision_sample_num : 8
08/30/2024 19:13:25 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
08/30/2024 19:13:25 - INFO - __main__ -   ==================run_configs==================

08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_checkpoint : 
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast/downstream/retrieval-msrvtt
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_optim : adamw
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_weight_decay : 0.01
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_grad_norm : 2.0
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_resume : False
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_seed : 50
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_fp16 : True
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_bf16 : False
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_zero_shot : False
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_new_lr : 0
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_new_params_name : []
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_valid_freq : 10
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_dataset_mix_type : random
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_first_eval : False
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_num_train_steps : 0
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_save_best : True
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_pin_mem : True
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_vision_resolution : 224
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_use_ddp : False
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_mode : training
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_log_steps : 100
08/30/2024 19:13:25 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
08/30/2024 19:13:25 - INFO - __main__ -   ==================data_configs==================

08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_type : annoindexed
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_training : True
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_name : msrvtt_ret
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_txt : datasets/annotations/msrvtt/descs_ret_train.json
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision : ../MSRVTT/videos/videos
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_audio : ../MSRVTT/audios
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_transforms : crop_flip
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_format : video_rawvideo
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_sample_num : 8
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_audio_sample_num : 1
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_task : ret%tv%ta
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_epoch : 3.6
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_n_workers : 8
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_train_batch_size : 32
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 8
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tva
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
08/30/2024 19:13:25 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
08/30/2024 19:13:29 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
08/30/2024 19:13:29 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 158540 labels
ci sono 158540 labels
ci sono 158540 labels
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
ci sono 884 labelsci sono 884 labels

ci sono 884 labels
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
ci sono 158540 labels
08/30/2024 19:13:35 - INFO - __main__ -   Create Dataset msrvtt_ret Success
08/30/2024 19:13:35 - INFO - __main__ -    loader ret%tv%ta--msrvtt_ret , ratio 17834 , bs_pergpu 8, n_workers 8
08/30/2024 19:13:35 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
08/30/2024 19:13:35 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
08/30/2024 19:13:35 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
08/30/2024 19:13:36 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
08/30/2024 19:13:36 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 884 labels
08/30/2024 19:13:36 - INFO - __main__ -   Create Dataset msrvtt_ret Success
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
08/30/2024 19:13:38 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
08/30/2024 19:14:05 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
08/30/2024 19:14:06 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
08/30/2024 19:14:06 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
08/30/2024 19:14:07 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
08/30/2024 19:14:08 - INFO - root -   incompatible_keys.missing_keys: []
08/30/2024 19:14:08 - INFO - root -   incompatible_keys.missing_keys: []
08/30/2024 19:14:08 - INFO - root -   incompatible_keys.missing_keys: []
08/30/2024 19:14:08 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
08/30/2024 19:14:08 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
08/30/2024 19:14:08 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
08/30/2024 19:14:09 - INFO - root -   incompatible_keys.missing_keys: []
08/30/2024 19:14:09 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
08/30/2024 19:14:10 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
08/30/2024 19:14:10 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
08/30/2024 19:14:10 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
08/30/2024 19:14:11 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.8.crossattention.self.value.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'cls.predictions.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.value.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'cls.predictions.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.bias', 'cls.predictions.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
08/30/2024 19:14:16 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/ckpt/model_step_204994.pt
08/30/2024 19:14:16 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast
08/30/2024 19:14:17 - INFO - __main__ -   Unexpected keys ['vision_encoder.text.logit_scale']
08/30/2024 19:14:17 - INFO - __main__ -   missing_keys  ['vision_encoder.logit_scale']
08/30/2024 19:14:18 - INFO - __main__ -   ==================learning_rate_settings==================

08/30/2024 19:14:18 - INFO - __main__ -     basic_lr : 2e-05
08/30/2024 19:14:18 - INFO - __main__ -     clip_lr_visual : 5e-07
08/30/2024 19:14:18 - INFO - __main__ -     clip_lr_visual_len : 245
08/30/2024 19:14:18 - INFO - __main__ -     new_lr : 0
08/30/2024 19:14:18 - INFO - __main__ -     new_params_name: []
  0%|          | 0/17834 [00:00<?, ?it/s]  0%|          | 1/17834 [00:06<31:00:40,  6.26s/it]  0%|          | 2/17834 [00:07<17:07:43,  3.46s/it]  0%|          | 3/17834 [00:09<12:53:01,  2.60s/it]  0%|          | 4/17834 [00:10<10:51:38,  2.19s/it]/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 5/17834 [00:12<10:01:00,  2.02s/it]  0%|          | 6/17834 [00:14<9:24:33,  1.90s/it]   0%|          | 7/17834 [00:15<8:56:25,  1.81s/it]  0%|          | 8/17834 [00:17<8:41:42,  1.76s/it]  0%|          | 9/17834 [00:19<8:39:30,  1.75s/it]  0%|          | 10/17834 [00:21<8:37:38,  1.74s/it]  0%|          | 11/17834 [00:22<8:28:40,  1.71s/it]  0%|          | 12/17834 [00:24<8:18:37,  1.68s/it]  0%|          | 13/17834 [00:25<8:18:30,  1.68s/it]  0%|          | 14/17834 [00:27<8:13:21,  1.66s/it]  0%|          | 15/17834 [00:29<8:09:40,  1.65s/it]  0%|          | 16/17834 [00:30<8:23:22,  1.70s/it]  0%|          | 17/17834 [00:32<8:24:47,  1.70s/it]  0%|          | 18/17834 [00:34<8:13:45,  1.66s/it]  0%|          | 19/17834 [00:35<8:09:06,  1.65s/it]  0%|          | 20/17834 [00:37<8:09:07,  1.65s/it]  0%|          | 21/17834 [00:39<8:15:32,  1.67s/it]  0%|          | 22/17834 [00:40<8:20:15,  1.69s/it]  0%|          | 23/17834 [00:42<8:16:43,  1.67s/it]  0%|          | 24/17834 [00:44<8:16:04,  1.67s/it]  0%|          | 25/17834 [00:45<8:13:28,  1.66s/it]  0%|          | 26/17834 [00:47<8:14:12,  1.67s/it]  0%|          | 27/17834 [00:49<8:18:11,  1.68s/it]  0%|          | 28/17834 [00:50<8:16:57,  1.67s/it]  0%|          | 29/17834 [00:52<8:18:25,  1.68s/it]  0%|          | 30/17834 [00:54<8:14:10,  1.67s/it]  0%|          | 31/17834 [00:55<8:17:24,  1.68s/it]  0%|          | 32/17834 [00:57<8:17:54,  1.68s/it]  0%|          | 33/17834 [00:59<8:17:35,  1.68s/it]  0%|          | 34/17834 [01:01<8:17:19,  1.68s/it]  0%|          | 35/17834 [01:02<8:11:42,  1.66s/it]  0%|          | 36/17834 [01:04<8:06:17,  1.64s/it]  0%|          | 37/17834 [01:05<8:08:48,  1.65s/it]  0%|          | 38/17834 [01:07<8:13:35,  1.66s/it]  0%|          | 39/17834 [01:09<8:10:04,  1.65s/it]  0%|          | 40/17834 [01:10<8:14:17,  1.67s/it]  0%|          | 41/17834 [01:12<8:15:03,  1.67s/it]  0%|          | 42/17834 [01:14<8:10:23,  1.65s/it]  0%|          | 43/17834 [01:15<8:10:38,  1.65s/it]  0%|          | 44/17834 [01:17<8:10:54,  1.66s/it]  0%|          | 45/17834 [01:19<8:11:04,  1.66s/it]  0%|          | 46/17834 [01:20<8:05:44,  1.64s/it]  0%|          | 47/17834 [01:22<8:01:07,  1.62s/it]  0%|          | 48/17834 [01:24<8:09:58,  1.65s/it]  0%|          | 49/17834 [01:25<8:08:30,  1.65s/it]08/30/2024 19:15:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.636263847351074, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0631401389837265, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 3.5360641479492188, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 6.235467910766602}
  0%|          | 50/17834 [01:27<8:06:20,  1.64s/it]  0%|          | 51/17834 [01:29<8:06:10,  1.64s/it]  0%|          | 52/17834 [01:30<8:08:05,  1.65s/it]  0%|          | 53/17834 [01:32<8:09:39,  1.65s/it]  0%|          | 54/17834 [01:34<8:11:41,  1.66s/it]  0%|          | 55/17834 [01:35<8:16:24,  1.68s/it]  0%|          | 56/17834 [01:37<8:15:25,  1.67s/it]  0%|          | 57/17834 [01:39<8:10:47,  1.66s/it]  0%|          | 58/17834 [01:40<8:09:18,  1.65s/it]  0%|          | 59/17834 [01:42<8:04:35,  1.64s/it]  0%|          | 60/17834 [01:43<8:08:56,  1.65s/it]  0%|          | 61/17834 [01:45<8:08:51,  1.65s/it]  0%|          | 62/17834 [01:47<8:14:48,  1.67s/it]  0%|          | 63/17834 [01:48<8:16:04,  1.67s/it]  0%|          | 64/17834 [01:50<8:12:11,  1.66s/it]  0%|          | 65/17834 [01:52<8:14:52,  1.67s/it]  0%|          | 66/17834 [01:53<8:15:21,  1.67s/it]  0%|          | 67/17834 [01:55<8:18:00,  1.68s/it]  0%|          | 68/17834 [01:57<8:20:37,  1.69s/it]  0%|          | 69/17834 [01:59<8:16:32,  1.68s/it]  0%|          | 70/17834 [02:00<8:14:37,  1.67s/it]  0%|          | 71/17834 [02:02<8:14:35,  1.67s/it]  0%|          | 72/17834 [02:04<8:13:20,  1.67s/it]  0%|          | 73/17834 [02:05<8:13:24,  1.67s/it]  0%|          | 74/17834 [02:07<8:12:01,  1.66s/it]  0%|          | 75/17834 [02:08<8:10:51,  1.66s/it]  0%|          | 76/17834 [02:10<8:18:30,  1.68s/it]  0%|          | 77/17834 [02:12<8:16:17,  1.68s/it]  0%|          | 78/17834 [02:14<8:20:22,  1.69s/it]  0%|          | 79/17834 [02:15<8:17:18,  1.68s/it]  0%|          | 80/17834 [02:17<8:13:20,  1.67s/it]  0%|          | 81/17834 [02:19<8:19:00,  1.69s/it]  0%|          | 82/17834 [02:20<8:13:54,  1.67s/it]  0%|          | 83/17834 [02:22<8:14:20,  1.67s/it]  0%|          | 84/17834 [02:24<8:07:57,  1.65s/it]  0%|          | 85/17834 [02:25<8:05:23,  1.64s/it]  0%|          | 86/17834 [02:27<8:07:20,  1.65s/it]  0%|          | 87/17834 [02:29<8:12:30,  1.67s/it]  0%|          | 88/17834 [02:30<8:11:13,  1.66s/it]  0%|          | 89/17834 [02:32<8:08:23,  1.65s/it]  1%|          | 90/17834 [02:34<8:15:11,  1.67s/it]  1%|          | 91/17834 [02:35<8:19:17,  1.69s/it]  1%|          | 92/17834 [02:37<8:19:16,  1.69s/it]  1%|          | 93/17834 [02:39<8:17:55,  1.68s/it]  1%|          | 94/17834 [02:40<8:19:38,  1.69s/it]  1%|          | 95/17834 [02:42<8:10:44,  1.66s/it]  1%|          | 96/17834 [02:44<8:10:18,  1.66s/it]  1%|          | 97/17834 [02:45<8:14:13,  1.67s/it]  1%|          | 98/17834 [02:47<8:16:42,  1.68s/it]  1%|          | 99/17834 [02:49<8:13:03,  1.67s/it]08/30/2024 19:17:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.1179637908935547, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.07174930721521378, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 3.429680347442627, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 5.619393348693848}
  1%|          | 100/17834 [02:50<8:11:40,  1.66s/it]  1%|          | 101/17834 [02:52<8:12:16,  1.67s/it]  1%|          | 102/17834 [02:54<8:26:28,  1.71s/it]  1%|          | 103/17834 [02:55<8:24:20,  1.71s/it]  1%|          | 104/17834 [02:57<8:21:46,  1.70s/it]  1%|          | 105/17834 [02:59<8:22:48,  1.70s/it]  1%|          | 106/17834 [03:01<8:23:16,  1.70s/it]  1%|          | 107/17834 [03:02<8:24:48,  1.71s/it]  1%|          | 108/17834 [03:04<8:19:50,  1.69s/it]  1%|          | 109/17834 [03:06<8:14:45,  1.67s/it]  1%|          | 110/17834 [03:07<8:12:39,  1.67s/it]  1%|          | 111/17834 [03:09<8:10:22,  1.66s/it]  1%|          | 112/17834 [03:11<8:13:27,  1.67s/it]  1%|          | 113/17834 [03:12<8:27:24,  1.72s/it]  1%|          | 114/17834 [03:14<8:24:39,  1.71s/it]  1%|          | 115/17834 [03:16<8:17:19,  1.68s/it]  1%|          | 116/17834 [03:17<8:17:43,  1.69s/it]  1%|          | 117/17834 [03:19<8:19:01,  1.69s/it]  1%|          | 118/17834 [03:21<8:25:20,  1.71s/it]  1%|          | 119/17834 [03:23<8:23:27,  1.71s/it]  1%|          | 120/17834 [03:24<8:20:28,  1.70s/it]  1%|          | 121/17834 [03:26<8:14:17,  1.67s/it]  1%|          | 122/17834 [03:27<8:09:23,  1.66s/it]  1%|          | 123/17834 [03:29<8:12:52,  1.67s/it]  1%|          | 124/17834 [03:31<8:16:02,  1.68s/it]  1%|          | 125/17834 [03:33<8:17:31,  1.69s/it]  1%|          | 126/17834 [03:34<8:10:55,  1.66s/it]  1%|          | 127/17834 [03:36<8:05:12,  1.64s/it]  1%|          | 128/17834 [03:37<8:05:36,  1.65s/it]  1%|          | 129/17834 [03:39<8:12:25,  1.67s/it]  1%|          | 130/17834 [03:41<8:12:13,  1.67s/it]  1%|          | 131/17834 [03:42<8:13:53,  1.67s/it]  1%|          | 132/17834 [03:44<8:12:45,  1.67s/it]  1%|          | 133/17834 [03:46<8:14:48,  1.68s/it]  1%|          | 134/17834 [03:48<8:21:36,  1.70s/it]  1%|          | 135/17834 [03:49<8:10:33,  1.66s/it]  1%|          | 136/17834 [03:51<8:14:42,  1.68s/it]  1%|          | 137/17834 [03:53<8:13:09,  1.67s/it]  1%|          | 138/17834 [03:54<8:13:03,  1.67s/it]  1%|          | 139/17834 [03:56<8:12:20,  1.67s/it]  1%|          | 140/17834 [03:58<8:15:27,  1.68s/it]  1%|          | 141/17834 [03:59<8:11:18,  1.67s/it]  1%|          | 142/17834 [04:01<8:10:57,  1.66s/it]  1%|          | 143/17834 [04:03<8:13:29,  1.67s/it]  1%|          | 144/17834 [04:04<8:12:22,  1.67s/it]  1%|          | 145/17834 [04:06<8:14:15,  1.68s/it]  1%|          | 146/17834 [04:08<8:10:51,  1.67s/it]  1%|          | 147/17834 [04:09<8:13:43,  1.67s/it]  1%|          | 148/17834 [04:11<8:13:12,  1.67s/it]  1%|          | 149/17834 [04:13<8:12:58,  1.67s/it]08/30/2024 19:18:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.11649227142334, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04386556148529053, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.8081507682800293, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.968508720397949}
  1%|          | 150/17834 [04:14<8:11:09,  1.67s/it]  1%|          | 151/17834 [04:16<8:11:19,  1.67s/it]  1%|          | 152/17834 [04:18<8:12:07,  1.67s/it]  1%|          | 153/17834 [04:19<8:16:09,  1.68s/it]  1%|          | 154/17834 [04:21<8:18:18,  1.69s/it]  1%|          | 155/17834 [04:23<8:15:52,  1.68s/it]  1%|          | 156/17834 [04:24<8:12:07,  1.67s/it]  1%|          | 157/17834 [04:26<8:10:11,  1.66s/it]  1%|          | 158/17834 [04:28<8:12:22,  1.67s/it]  1%|          | 159/17834 [04:29<8:15:42,  1.68s/it]  1%|          | 160/17834 [04:31<8:15:21,  1.68s/it]  1%|          | 161/17834 [04:33<8:20:46,  1.70s/it]  1%|          | 162/17834 [04:34<8:17:00,  1.69s/it]  1%|          | 163/17834 [04:36<8:19:48,  1.70s/it]  1%|          | 164/17834 [04:38<8:14:44,  1.68s/it]  1%|          | 165/17834 [04:39<8:07:33,  1.66s/it]  1%|          | 166/17834 [04:41<8:07:09,  1.65s/it]  1%|          | 167/17834 [04:43<8:06:06,  1.65s/it]  1%|          | 168/17834 [04:44<8:03:09,  1.64s/it]  1%|          | 169/17834 [04:46<8:03:11,  1.64s/it]  1%|          | 170/17834 [04:48<8:07:05,  1.65s/it]  1%|          | 171/17834 [04:49<8:12:31,  1.67s/it]  1%|          | 172/17834 [04:51<8:19:35,  1.70s/it]  1%|          | 173/17834 [04:53<8:18:18,  1.69s/it]  1%|          | 174/17834 [04:54<8:15:56,  1.68s/it]  1%|          | 175/17834 [04:56<8:14:46,  1.68s/it]  1%|          | 176/17834 [04:58<8:18:54,  1.70s/it]  1%|          | 177/17834 [05:00<8:14:11,  1.68s/it]  1%|          | 178/17834 [05:01<8:12:39,  1.67s/it]  1%|          | 179/17834 [05:03<8:14:19,  1.68s/it]  1%|          | 180/17834 [05:05<8:11:00,  1.67s/it]  1%|          | 181/17834 [05:06<8:13:20,  1.68s/it]  1%|          | 182/17834 [05:08<8:08:14,  1.66s/it]  1%|          | 183/17834 [05:09<8:08:01,  1.66s/it]  1%|          | 184/17834 [05:11<8:11:36,  1.67s/it]  1%|          | 185/17834 [05:13<8:08:29,  1.66s/it]  1%|          | 186/17834 [05:15<8:12:13,  1.67s/it]  1%|          | 187/17834 [05:16<8:08:27,  1.66s/it]  1%|          | 188/17834 [05:18<8:16:43,  1.69s/it]  1%|          | 189/17834 [05:20<8:19:30,  1.70s/it]  1%|          | 190/17834 [05:21<8:22:09,  1.71s/it]  1%|          | 191/17834 [05:23<8:19:07,  1.70s/it]  1%|          | 192/17834 [05:25<8:22:06,  1.71s/it]  1%|          | 193/17834 [05:26<8:24:08,  1.71s/it]  1%|          | 194/17834 [05:28<8:30:13,  1.74s/it]  1%|          | 195/17834 [05:30<8:20:23,  1.70s/it]  1%|          | 196/17834 [05:32<8:27:04,  1.72s/it]  1%|          | 197/17834 [05:33<8:22:39,  1.71s/it]  1%|          | 198/17834 [05:35<8:16:08,  1.69s/it]  1%|          | 199/17834 [05:37<8:16:17,  1.69s/it]08/30/2024 19:19:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.8426462411880493, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04115348309278488, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 3.5870914459228516, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 5.470890998840332}
  1%|          | 200/17834 [05:38<8:10:16,  1.67s/it]  1%|          | 201/17834 [05:40<8:12:20,  1.68s/it]  1%|          | 202/17834 [05:42<8:15:09,  1.68s/it]  1%|          | 203/17834 [05:43<8:14:59,  1.68s/it]  1%|          | 204/17834 [05:45<8:08:41,  1.66s/it]  1%|          | 205/17834 [05:47<8:15:23,  1.69s/it]  1%|          | 206/17834 [05:48<8:15:03,  1.69s/it]  1%|          | 207/17834 [05:50<8:11:48,  1.67s/it]  1%|          | 208/17834 [05:52<8:14:48,  1.68s/it]  1%|          | 209/17834 [05:53<8:14:06,  1.68s/it]  1%|          | 210/17834 [05:55<8:12:29,  1.68s/it]  1%|          | 211/17834 [05:57<8:18:27,  1.70s/it]  1%|          | 212/17834 [05:59<8:14:52,  1.68s/it]  1%|          | 213/17834 [06:00<8:07:45,  1.66s/it]  1%|          | 214/17834 [06:02<8:05:19,  1.65s/it]  1%|          | 215/17834 [06:03<8:05:21,  1.65s/it]  1%|          | 216/17834 [06:05<8:10:56,  1.67s/it]  1%|          | 217/17834 [06:07<8:04:11,  1.65s/it]  1%|          | 218/17834 [06:08<8:07:20,  1.66s/it]  1%|          | 219/17834 [06:10<8:03:03,  1.65s/it]  1%|          | 220/17834 [06:12<8:06:22,  1.66s/it]  1%|          | 221/17834 [06:13<8:10:44,  1.67s/it]  1%|          | 222/17834 [06:15<8:03:05,  1.65s/it]  1%|▏         | 223/17834 [06:17<8:05:49,  1.66s/it]  1%|▏         | 224/17834 [06:18<8:08:21,  1.66s/it]  1%|▏         | 225/17834 [06:20<8:07:04,  1.66s/it]  1%|▏         | 226/17834 [06:22<8:16:39,  1.69s/it]  1%|▏         | 227/17834 [06:23<8:14:53,  1.69s/it]  1%|▏         | 228/17834 [06:25<8:11:47,  1.68s/it]  1%|▏         | 229/17834 [06:27<8:16:23,  1.69s/it]  1%|▏         | 230/17834 [06:29<8:18:07,  1.70s/it]  1%|▏         | 231/17834 [06:30<8:12:37,  1.68s/it]  1%|▏         | 232/17834 [06:32<8:10:22,  1.67s/it]  1%|▏         | 233/17834 [06:34<8:15:27,  1.69s/it]  1%|▏         | 234/17834 [06:35<8:17:22,  1.70s/it]  1%|▏         | 235/17834 [06:37<8:20:00,  1.70s/it]  1%|▏         | 236/17834 [06:39<8:14:44,  1.69s/it]  1%|▏         | 237/17834 [06:40<8:14:49,  1.69s/it]  1%|▏         | 238/17834 [06:42<8:21:13,  1.71s/it]  1%|▏         | 239/17834 [06:44<8:25:24,  1.72s/it]  1%|▏         | 240/17834 [06:46<8:28:56,  1.74s/it]  1%|▏         | 241/17834 [06:47<8:19:52,  1.70s/it]  1%|▏         | 242/17834 [06:49<8:15:51,  1.69s/it]  1%|▏         | 243/17834 [06:51<8:15:32,  1.69s/it]  1%|▏         | 244/17834 [06:52<8:12:35,  1.68s/it]  1%|▏         | 245/17834 [06:54<8:26:42,  1.73s/it]  1%|▏         | 246/17834 [06:56<8:20:33,  1.71s/it]  1%|▏         | 247/17834 [06:57<8:19:56,  1.71s/it]  1%|▏         | 248/17834 [06:59<8:14:41,  1.69s/it]  1%|▏         | 249/17834 [07:01<8:09:46,  1.67s/it]08/30/2024 19:21:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.337078809738159, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05442807078361511, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.6695139408111572, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 5.061020851135254}
  1%|▏         | 250/17834 [07:02<8:16:38,  1.69s/it]  1%|▏         | 251/17834 [07:04<8:22:21,  1.71s/it]  1%|▏         | 252/17834 [07:06<8:17:23,  1.70s/it]  1%|▏         | 253/17834 [07:08<8:14:15,  1.69s/it]  1%|▏         | 254/17834 [07:09<8:12:57,  1.68s/it]  1%|▏         | 255/17834 [07:11<8:18:19,  1.70s/it]  1%|▏         | 256/17834 [07:13<8:23:58,  1.72s/it]  1%|▏         | 257/17834 [07:14<8:20:39,  1.71s/it]  1%|▏         | 258/17834 [07:16<8:14:57,  1.69s/it]  1%|▏         | 259/17834 [07:18<8:14:24,  1.69s/it]  1%|▏         | 260/17834 [07:19<8:10:56,  1.68s/it]  1%|▏         | 261/17834 [07:21<8:07:43,  1.67s/it]  1%|▏         | 262/17834 [07:23<8:10:19,  1.67s/it]  1%|▏         | 263/17834 [07:24<8:10:41,  1.68s/it]  1%|▏         | 264/17834 [07:26<8:07:06,  1.66s/it]  1%|▏         | 265/17834 [07:28<8:09:51,  1.67s/it]  1%|▏         | 266/17834 [07:29<8:15:24,  1.69s/it]  1%|▏         | 267/17834 [07:31<8:15:00,  1.69s/it]  2%|▏         | 268/17834 [07:33<8:07:35,  1.67s/it]  2%|▏         | 269/17834 [07:34<8:07:41,  1.67s/it]  2%|▏         | 270/17834 [07:36<8:09:19,  1.67s/it]  2%|▏         | 271/17834 [07:38<8:09:31,  1.67s/it]  2%|▏         | 272/17834 [07:40<8:15:42,  1.69s/it]  2%|▏         | 273/17834 [07:41<8:11:09,  1.68s/it]  2%|▏         | 274/17834 [07:43<8:10:42,  1.68s/it]  2%|▏         | 275/17834 [07:45<8:08:40,  1.67s/it]  2%|▏         | 276/17834 [07:46<8:06:24,  1.66s/it]  2%|▏         | 277/17834 [07:48<8:07:55,  1.67s/it]  2%|▏         | 278/17834 [07:49<8:03:36,  1.65s/it]  2%|▏         | 279/17834 [07:51<8:10:38,  1.68s/it]  2%|▏         | 280/17834 [07:53<8:10:50,  1.68s/it]  2%|▏         | 281/17834 [07:55<8:07:15,  1.67s/it]  2%|▏         | 282/17834 [07:56<8:08:40,  1.67s/it]  2%|▏         | 283/17834 [07:58<8:11:05,  1.68s/it]  2%|▏         | 284/17834 [08:00<8:06:05,  1.66s/it]  2%|▏         | 285/17834 [08:01<8:06:09,  1.66s/it]  2%|▏         | 286/17834 [08:03<8:07:59,  1.67s/it]  2%|▏         | 287/17834 [08:04<8:01:48,  1.65s/it]  2%|▏         | 288/17834 [08:06<8:05:04,  1.66s/it]  2%|▏         | 289/17834 [08:08<8:06:47,  1.66s/it]  2%|▏         | 290/17834 [08:10<8:17:54,  1.70s/it]  2%|▏         | 291/17834 [08:11<8:15:39,  1.70s/it]  2%|▏         | 292/17834 [08:13<8:11:37,  1.68s/it]  2%|▏         | 293/17834 [08:15<8:09:05,  1.67s/it]  2%|▏         | 294/17834 [08:16<8:20:46,  1.71s/it]  2%|▏         | 295/17834 [08:18<8:11:04,  1.68s/it]  2%|▏         | 296/17834 [08:20<8:06:36,  1.66s/it]  2%|▏         | 297/17834 [08:21<8:08:58,  1.67s/it]  2%|▏         | 298/17834 [08:23<8:09:23,  1.67s/it]  2%|▏         | 299/17834 [08:25<8:09:22,  1.67s/it]08/30/2024 19:22:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.321706771850586, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05803820490837097, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5193161964416504, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.89906120300293}
  2%|▏         | 300/17834 [08:26<8:05:35,  1.66s/it]  2%|▏         | 301/17834 [08:28<8:09:00,  1.67s/it]  2%|▏         | 302/17834 [08:30<8:16:13,  1.70s/it]  2%|▏         | 303/17834 [08:31<8:15:06,  1.69s/it]  2%|▏         | 304/17834 [08:33<8:16:58,  1.70s/it]  2%|▏         | 305/17834 [08:35<8:11:43,  1.68s/it]  2%|▏         | 306/17834 [08:37<8:15:01,  1.69s/it]  2%|▏         | 307/17834 [08:38<8:12:06,  1.68s/it]  2%|▏         | 308/17834 [08:40<8:24:33,  1.73s/it]  2%|▏         | 309/17834 [08:42<8:24:00,  1.73s/it]  2%|▏         | 310/17834 [08:43<8:23:07,  1.72s/it]  2%|▏         | 311/17834 [08:45<8:15:24,  1.70s/it]  2%|▏         | 312/17834 [08:47<8:12:42,  1.69s/it]  2%|▏         | 313/17834 [08:48<8:09:11,  1.68s/it]  2%|▏         | 314/17834 [08:50<8:10:18,  1.68s/it]  2%|▏         | 315/17834 [08:52<8:14:42,  1.69s/it]  2%|▏         | 316/17834 [08:54<8:14:59,  1.70s/it]  2%|▏         | 317/17834 [08:55<8:12:40,  1.69s/it]  2%|▏         | 318/17834 [08:57<8:11:40,  1.68s/it]  2%|▏         | 319/17834 [08:59<8:15:58,  1.70s/it]  2%|▏         | 320/17834 [09:00<8:10:07,  1.68s/it]  2%|▏         | 321/17834 [09:02<8:07:13,  1.67s/it]  2%|▏         | 322/17834 [09:04<8:04:43,  1.66s/it]  2%|▏         | 323/17834 [09:05<8:09:10,  1.68s/it]  2%|▏         | 324/17834 [09:07<8:04:44,  1.66s/it]  2%|▏         | 325/17834 [09:08<8:02:30,  1.65s/it]  2%|▏         | 326/17834 [09:10<8:05:58,  1.67s/it]  2%|▏         | 327/17834 [09:12<8:08:56,  1.68s/it]  2%|▏         | 328/17834 [09:13<8:02:53,  1.66s/it]  2%|▏         | 329/17834 [09:15<8:08:38,  1.67s/it]  2%|▏         | 330/17834 [09:17<8:09:49,  1.68s/it]  2%|▏         | 331/17834 [09:19<8:08:28,  1.67s/it]  2%|▏         | 332/17834 [09:20<8:10:21,  1.68s/it]  2%|▏         | 333/17834 [09:22<8:18:44,  1.71s/it]  2%|▏         | 334/17834 [09:24<8:11:22,  1.68s/it]  2%|▏         | 335/17834 [09:25<8:15:50,  1.70s/it]  2%|▏         | 336/17834 [09:27<8:16:01,  1.70s/it]  2%|▏         | 337/17834 [09:29<8:12:48,  1.69s/it]  2%|▏         | 338/17834 [09:30<8:08:01,  1.67s/it]  2%|▏         | 339/17834 [09:32<8:10:36,  1.68s/it]  2%|▏         | 340/17834 [09:34<8:07:03,  1.67s/it]  2%|▏         | 341/17834 [09:35<8:03:54,  1.66s/it]  2%|▏         | 342/17834 [09:37<8:10:08,  1.68s/it]  2%|▏         | 343/17834 [09:39<8:09:39,  1.68s/it]  2%|▏         | 344/17834 [09:40<8:10:36,  1.68s/it]  2%|▏         | 345/17834 [09:42<8:09:59,  1.68s/it]  2%|▏         | 346/17834 [09:44<8:05:26,  1.67s/it]  2%|▏         | 347/17834 [09:45<8:06:14,  1.67s/it]  2%|▏         | 348/17834 [09:47<8:02:40,  1.66s/it]  2%|▏         | 349/17834 [09:49<8:09:57,  1.68s/it]08/30/2024 19:24:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.4896271228790283, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.057241566479206085, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 3.2346630096435547, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 5.781531810760498}
  2%|▏         | 350/17834 [09:51<8:11:54,  1.69s/it]  2%|▏         | 351/17834 [09:52<8:06:59,  1.67s/it]  2%|▏         | 352/17834 [09:54<8:08:52,  1.68s/it]  2%|▏         | 353/17834 [09:56<8:06:27,  1.67s/it]  2%|▏         | 354/17834 [09:57<8:08:27,  1.68s/it]  2%|▏         | 355/17834 [09:59<8:08:40,  1.68s/it]  2%|▏         | 356/17834 [10:01<8:13:37,  1.69s/it]  2%|▏         | 357/17834 [10:02<8:05:11,  1.67s/it]  2%|▏         | 358/17834 [10:04<8:13:42,  1.70s/it]  2%|▏         | 359/17834 [10:06<8:18:43,  1.71s/it]  2%|▏         | 360/17834 [10:07<8:15:02,  1.70s/it]  2%|▏         | 361/17834 [10:09<8:15:32,  1.70s/it]  2%|▏         | 362/17834 [10:11<8:12:34,  1.69s/it]  2%|▏         | 363/17834 [10:13<8:16:16,  1.70s/it]  2%|▏         | 364/17834 [10:14<8:16:51,  1.71s/it]  2%|▏         | 365/17834 [10:16<8:15:42,  1.70s/it]  2%|▏         | 366/17834 [10:18<8:15:12,  1.70s/it]  2%|▏         | 367/17834 [10:19<8:13:45,  1.70s/it]  2%|▏         | 368/17834 [10:21<8:13:02,  1.69s/it]  2%|▏         | 369/17834 [10:23<8:08:18,  1.68s/it]  2%|▏         | 370/17834 [10:24<8:04:44,  1.67s/it]  2%|▏         | 371/17834 [10:26<8:03:37,  1.66s/it]  2%|▏         | 372/17834 [10:28<8:20:50,  1.72s/it]  2%|▏         | 373/17834 [10:29<8:19:47,  1.72s/it]  2%|▏         | 374/17834 [10:31<8:13:28,  1.70s/it]  2%|▏         | 375/17834 [10:33<8:14:58,  1.70s/it]  2%|▏         | 376/17834 [10:34<8:10:59,  1.69s/it]  2%|▏         | 377/17834 [10:36<8:09:52,  1.68s/it]  2%|▏         | 378/17834 [10:38<8:12:11,  1.69s/it]  2%|▏         | 379/17834 [10:40<8:10:34,  1.69s/it]  2%|▏         | 380/17834 [10:41<8:11:24,  1.69s/it]  2%|▏         | 381/17834 [10:43<8:12:13,  1.69s/it]  2%|▏         | 382/17834 [10:45<8:13:58,  1.70s/it]  2%|▏         | 383/17834 [10:46<8:11:20,  1.69s/it]  2%|▏         | 384/17834 [10:48<8:11:27,  1.69s/it]  2%|▏         | 385/17834 [10:50<8:11:02,  1.69s/it]  2%|▏         | 386/17834 [10:51<8:14:00,  1.70s/it]  2%|▏         | 387/17834 [10:53<8:15:58,  1.71s/it]  2%|▏         | 388/17834 [10:55<8:12:06,  1.69s/it]  2%|▏         | 389/17834 [10:57<8:17:35,  1.71s/it]  2%|▏         | 390/17834 [10:58<8:14:33,  1.70s/it]  2%|▏         | 391/17834 [11:00<8:15:02,  1.70s/it]  2%|▏         | 392/17834 [11:02<8:10:20,  1.69s/it]  2%|▏         | 393/17834 [11:03<8:05:57,  1.67s/it]  2%|▏         | 394/17834 [11:05<8:07:17,  1.68s/it]  2%|▏         | 395/17834 [11:07<8:04:49,  1.67s/it]  2%|▏         | 396/17834 [11:08<8:07:36,  1.68s/it]  2%|▏         | 397/17834 [11:10<8:08:56,  1.68s/it]  2%|▏         | 398/17834 [11:12<8:07:11,  1.68s/it]  2%|▏         | 399/17834 [11:13<8:06:45,  1.68s/it]08/30/2024 19:25:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.813154697418213, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04925297945737839, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.9046554565429688, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.767063140869141}
  2%|▏         | 400/17834 [11:15<8:07:51,  1.68s/it]  2%|▏         | 401/17834 [11:17<8:11:00,  1.69s/it]  2%|▏         | 402/17834 [11:18<8:08:19,  1.68s/it]  2%|▏         | 403/17834 [11:20<8:06:12,  1.67s/it]  2%|▏         | 404/17834 [11:22<8:05:04,  1.67s/it]  2%|▏         | 405/17834 [11:23<8:02:13,  1.66s/it]  2%|▏         | 406/17834 [11:25<8:06:25,  1.67s/it]  2%|▏         | 407/17834 [11:27<8:05:22,  1.67s/it]  2%|▏         | 408/17834 [11:28<8:06:25,  1.67s/it]  2%|▏         | 409/17834 [11:30<8:03:34,  1.67s/it]  2%|▏         | 410/17834 [11:32<8:11:12,  1.69s/it]  2%|▏         | 411/17834 [11:33<8:11:43,  1.69s/it]  2%|▏         | 412/17834 [11:35<8:11:01,  1.69s/it]  2%|▏         | 413/17834 [11:37<8:15:05,  1.71s/it]  2%|▏         | 414/17834 [11:39<8:17:08,  1.71s/it]  2%|▏         | 415/17834 [11:40<8:12:36,  1.70s/it]  2%|▏         | 416/17834 [11:42<8:05:27,  1.67s/it]  2%|▏         | 417/17834 [11:44<8:02:06,  1.66s/it]  2%|▏         | 418/17834 [11:45<8:03:41,  1.67s/it]  2%|▏         | 419/17834 [11:47<8:00:49,  1.66s/it]  2%|▏         | 420/17834 [11:49<8:01:44,  1.66s/it]  2%|▏         | 421/17834 [11:50<8:03:26,  1.67s/it]  2%|▏         | 422/17834 [11:52<8:02:38,  1.66s/it]  2%|▏         | 423/17834 [11:54<8:04:55,  1.67s/it]  2%|▏         | 424/17834 [11:55<8:08:33,  1.68s/it]  2%|▏         | 425/17834 [11:57<8:08:01,  1.68s/it]  2%|▏         | 426/17834 [11:59<8:04:39,  1.67s/it]  2%|▏         | 427/17834 [12:00<8:03:47,  1.67s/it]  2%|▏         | 428/17834 [12:02<8:05:33,  1.67s/it]  2%|▏         | 429/17834 [12:04<8:10:12,  1.69s/it]  2%|▏         | 430/17834 [12:05<8:06:42,  1.68s/it]  2%|▏         | 431/17834 [12:07<8:10:30,  1.69s/it]  2%|▏         | 432/17834 [12:09<8:07:16,  1.68s/it]  2%|▏         | 433/17834 [12:10<8:07:56,  1.68s/it]  2%|▏         | 434/17834 [12:12<8:11:59,  1.70s/it]  2%|▏         | 435/17834 [12:14<8:13:50,  1.70s/it]  2%|▏         | 436/17834 [12:15<8:05:52,  1.68s/it]  2%|▏         | 437/17834 [12:17<8:07:50,  1.68s/it]  2%|▏         | 438/17834 [12:19<8:07:53,  1.68s/it]  2%|▏         | 439/17834 [12:20<8:03:26,  1.67s/it]  2%|▏         | 440/17834 [12:22<8:04:34,  1.67s/it]  2%|▏         | 441/17834 [12:24<8:03:16,  1.67s/it]  2%|▏         | 442/17834 [12:26<8:15:17,  1.71s/it]  2%|▏         | 443/17834 [12:27<8:08:18,  1.68s/it]  2%|▏         | 444/17834 [12:29<8:03:04,  1.67s/it]  2%|▏         | 445/17834 [12:31<8:06:31,  1.68s/it]  3%|▎         | 446/17834 [12:32<8:04:02,  1.67s/it]  3%|▎         | 447/17834 [12:34<8:05:11,  1.67s/it]  3%|▎         | 448/17834 [12:36<8:03:41,  1.67s/it]  3%|▎         | 449/17834 [12:37<8:07:23,  1.68s/it]08/30/2024 19:26:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.8074395656585693, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.042200759053230286, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.460029125213623, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.309669494628906}
  3%|▎         | 450/17834 [12:39<8:10:16,  1.69s/it]  3%|▎         | 451/17834 [12:41<8:09:03,  1.69s/it]  3%|▎         | 452/17834 [12:42<8:10:50,  1.69s/it]  3%|▎         | 453/17834 [12:44<8:09:08,  1.69s/it]  3%|▎         | 454/17834 [12:46<8:04:37,  1.67s/it]  3%|▎         | 455/17834 [12:47<7:59:24,  1.66s/it]  3%|▎         | 456/17834 [12:49<7:59:54,  1.66s/it]  3%|▎         | 457/17834 [12:51<7:59:59,  1.66s/it]  3%|▎         | 458/17834 [12:52<7:58:56,  1.65s/it]  3%|▎         | 459/17834 [12:54<8:01:58,  1.66s/it]  3%|▎         | 460/17834 [12:56<7:59:31,  1.66s/it]  3%|▎         | 461/17834 [12:57<8:04:29,  1.67s/it]  3%|▎         | 462/17834 [12:59<8:04:22,  1.67s/it]  3%|▎         | 463/17834 [13:01<7:58:21,  1.65s/it]  3%|▎         | 464/17834 [13:02<7:56:45,  1.65s/it]  3%|▎         | 465/17834 [13:04<7:55:40,  1.64s/it]  3%|▎         | 466/17834 [13:06<8:08:20,  1.69s/it]  3%|▎         | 467/17834 [13:07<8:10:10,  1.69s/it]  3%|▎         | 468/17834 [13:09<8:07:33,  1.68s/it]  3%|▎         | 469/17834 [13:11<8:10:53,  1.70s/it]  3%|▎         | 470/17834 [13:12<8:09:15,  1.69s/it]  3%|▎         | 471/17834 [13:14<8:07:12,  1.68s/it]  3%|▎         | 472/17834 [13:16<8:09:12,  1.69s/it]  3%|▎         | 473/17834 [13:18<8:14:18,  1.71s/it]  3%|▎         | 474/17834 [13:19<8:09:24,  1.69s/it]  3%|▎         | 475/17834 [13:21<8:05:45,  1.68s/it]  3%|▎         | 476/17834 [13:22<8:05:33,  1.68s/it]  3%|▎         | 477/17834 [13:24<8:05:52,  1.68s/it]  3%|▎         | 478/17834 [13:26<8:02:44,  1.67s/it]  3%|▎         | 479/17834 [13:28<8:11:04,  1.70s/it]  3%|▎         | 480/17834 [13:29<8:10:22,  1.70s/it]  3%|▎         | 481/17834 [13:31<8:05:50,  1.68s/it]  3%|▎         | 482/17834 [13:33<8:07:27,  1.69s/it]  3%|▎         | 483/17834 [13:34<8:10:42,  1.70s/it]  3%|▎         | 484/17834 [13:36<8:06:12,  1.68s/it]  3%|▎         | 485/17834 [13:38<8:08:32,  1.69s/it]  3%|▎         | 486/17834 [13:39<8:04:58,  1.68s/it]  3%|▎         | 487/17834 [13:41<8:09:37,  1.69s/it]  3%|▎         | 488/17834 [13:43<8:05:09,  1.68s/it]  3%|▎         | 489/17834 [13:44<8:03:04,  1.67s/it]  3%|▎         | 490/17834 [13:46<8:10:44,  1.70s/it]  3%|▎         | 491/17834 [13:48<8:11:32,  1.70s/it]  3%|▎         | 492/17834 [13:50<8:10:20,  1.70s/it]  3%|▎         | 493/17834 [13:51<8:10:41,  1.70s/it]  3%|▎         | 494/17834 [13:53<8:09:14,  1.69s/it]  3%|▎         | 495/17834 [13:55<8:02:51,  1.67s/it]  3%|▎         | 496/17834 [13:56<8:02:17,  1.67s/it]  3%|▎         | 497/17834 [13:58<8:05:06,  1.68s/it]  3%|▎         | 498/17834 [14:00<8:07:48,  1.69s/it]  3%|▎         | 499/17834 [14:01<8:05:59,  1.68s/it]08/30/2024 19:28:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.1906001567840576, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03657903894782066, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.546020984649658, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.773200035095215}
  3%|▎         | 500/17834 [14:03<8:06:35,  1.68s/it]  3%|▎         | 501/17834 [14:05<8:07:59,  1.69s/it]  3%|▎         | 502/17834 [14:06<8:05:55,  1.68s/it]  3%|▎         | 503/17834 [14:08<8:03:33,  1.67s/it]  3%|▎         | 504/17834 [14:10<8:04:06,  1.68s/it]  3%|▎         | 505/17834 [14:11<8:05:46,  1.68s/it]  3%|▎         | 506/17834 [14:13<8:04:27,  1.68s/it]  3%|▎         | 507/17834 [14:15<8:02:04,  1.67s/it]  3%|▎         | 508/17834 [14:16<8:06:21,  1.68s/it]  3%|▎         | 509/17834 [14:18<8:06:08,  1.68s/it]  3%|▎         | 510/17834 [14:20<8:06:06,  1.68s/it]  3%|▎         | 511/17834 [14:21<7:58:43,  1.66s/it]  3%|▎         | 512/17834 [14:23<8:01:56,  1.67s/it]  3%|▎         | 513/17834 [14:25<8:12:40,  1.71s/it]  3%|▎         | 514/17834 [14:27<8:13:36,  1.71s/it]  3%|▎         | 515/17834 [14:28<8:08:27,  1.69s/it]  3%|▎         | 516/17834 [14:30<8:00:44,  1.67s/it]  3%|▎         | 517/17834 [14:32<8:04:37,  1.68s/it]  3%|▎         | 518/17834 [14:33<8:04:14,  1.68s/it]  3%|▎         | 519/17834 [14:35<8:02:19,  1.67s/it]  3%|▎         | 520/17834 [14:36<7:57:24,  1.65s/it]  3%|▎         | 521/17834 [14:38<8:03:05,  1.67s/it]  3%|▎         | 522/17834 [14:40<7:56:51,  1.65s/it]  3%|▎         | 523/17834 [14:41<7:59:15,  1.66s/it]  3%|▎         | 524/17834 [14:43<7:58:12,  1.66s/it]  3%|▎         | 525/17834 [14:45<7:54:00,  1.64s/it]  3%|▎         | 526/17834 [14:46<7:55:15,  1.65s/it]  3%|▎         | 527/17834 [14:48<7:56:36,  1.65s/it]  3%|▎         | 528/17834 [14:50<7:56:56,  1.65s/it]  3%|▎         | 529/17834 [14:51<7:58:42,  1.66s/it]  3%|▎         | 530/17834 [14:53<8:04:57,  1.68s/it]  3%|▎         | 531/17834 [14:55<8:01:32,  1.67s/it]  3%|▎         | 532/17834 [14:56<8:00:50,  1.67s/it]  3%|▎         | 533/17834 [14:58<7:59:10,  1.66s/it]  3%|▎         | 534/17834 [15:00<7:57:40,  1.66s/it]  3%|▎         | 535/17834 [15:01<7:55:27,  1.65s/it]  3%|▎         | 536/17834 [15:03<7:59:17,  1.66s/it]  3%|▎         | 537/17834 [15:05<7:59:30,  1.66s/it]  3%|▎         | 538/17834 [15:06<8:04:47,  1.68s/it]  3%|▎         | 539/17834 [15:08<8:03:02,  1.68s/it]  3%|▎         | 540/17834 [15:10<8:03:45,  1.68s/it]  3%|▎         | 541/17834 [15:11<8:06:04,  1.69s/it]  3%|▎         | 542/17834 [15:13<8:06:33,  1.69s/it]  3%|▎         | 543/17834 [15:15<8:12:02,  1.71s/it]  3%|▎         | 544/17834 [15:17<8:05:50,  1.69s/it]  3%|▎         | 545/17834 [15:18<8:01:08,  1.67s/it]  3%|▎         | 546/17834 [15:20<8:02:23,  1.67s/it]  3%|▎         | 547/17834 [15:22<7:58:45,  1.66s/it]  3%|▎         | 548/17834 [15:23<8:02:17,  1.67s/it]  3%|▎         | 549/17834 [15:25<8:00:19,  1.67s/it]08/30/2024 19:29:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.215461254119873, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.043916091322898865, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.663883686065674, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.923260688781738}
  3%|▎         | 550/17834 [15:27<8:00:41,  1.67s/it]  3%|▎         | 551/17834 [15:28<8:06:50,  1.69s/it]  3%|▎         | 552/17834 [15:30<8:03:50,  1.68s/it]  3%|▎         | 553/17834 [15:32<8:04:34,  1.68s/it]  3%|▎         | 554/17834 [15:33<8:06:22,  1.69s/it]  3%|▎         | 555/17834 [15:35<8:03:42,  1.68s/it]  3%|▎         | 556/17834 [15:37<8:02:36,  1.68s/it]  3%|▎         | 557/17834 [15:38<7:59:51,  1.67s/it]  3%|▎         | 558/17834 [15:40<8:01:07,  1.67s/it]  3%|▎         | 559/17834 [15:42<8:05:19,  1.69s/it]  3%|▎         | 560/17834 [15:43<8:02:21,  1.68s/it]  3%|▎         | 561/17834 [15:45<8:04:36,  1.68s/it]  3%|▎         | 562/17834 [15:47<8:02:49,  1.68s/it]  3%|▎         | 563/17834 [15:48<8:05:32,  1.69s/it]  3%|▎         | 564/17834 [15:50<8:03:21,  1.68s/it]  3%|▎         | 565/17834 [15:52<8:02:55,  1.68s/it]  3%|▎         | 566/17834 [15:53<7:58:24,  1.66s/it]  3%|▎         | 567/17834 [15:55<7:57:51,  1.66s/it]  3%|▎         | 568/17834 [15:57<7:59:06,  1.66s/it]  3%|▎         | 569/17834 [15:58<8:00:32,  1.67s/it]  3%|▎         | 570/17834 [16:00<8:05:19,  1.69s/it]  3%|▎         | 571/17834 [16:02<8:02:26,  1.68s/it]  3%|▎         | 572/17834 [16:03<8:05:59,  1.69s/it]  3%|▎         | 573/17834 [16:05<8:06:02,  1.69s/it]  3%|▎         | 574/17834 [16:07<8:04:46,  1.69s/it]  3%|▎         | 575/17834 [16:09<8:01:30,  1.67s/it]  3%|▎         | 576/17834 [16:10<8:04:25,  1.68s/it]  3%|▎         | 577/17834 [16:12<8:05:14,  1.69s/it]  3%|▎         | 578/17834 [16:14<8:04:11,  1.68s/it]  3%|▎         | 579/17834 [16:15<8:07:29,  1.70s/it]  3%|▎         | 580/17834 [16:17<8:07:10,  1.69s/it]  3%|▎         | 581/17834 [16:19<8:03:31,  1.68s/it]  3%|▎         | 582/17834 [16:20<8:07:06,  1.69s/it]  3%|▎         | 583/17834 [16:22<8:05:03,  1.69s/it]  3%|▎         | 584/17834 [16:24<8:02:28,  1.68s/it]  3%|▎         | 585/17834 [16:25<8:05:58,  1.69s/it]  3%|▎         | 586/17834 [16:27<8:04:14,  1.68s/it]  3%|▎         | 587/17834 [16:29<8:02:15,  1.68s/it]  3%|▎         | 588/17834 [16:30<7:59:15,  1.67s/it]  3%|▎         | 589/17834 [16:32<8:05:52,  1.69s/it]  3%|▎         | 590/17834 [16:34<8:08:24,  1.70s/it]  3%|▎         | 591/17834 [16:35<8:01:35,  1.68s/it]  3%|▎         | 592/17834 [16:37<8:03:14,  1.68s/it]  3%|▎         | 593/17834 [16:39<8:02:26,  1.68s/it]  3%|▎         | 594/17834 [16:41<8:04:04,  1.68s/it]  3%|▎         | 595/17834 [16:42<8:07:22,  1.70s/it]  3%|▎         | 596/17834 [16:44<8:03:42,  1.68s/it]  3%|▎         | 597/17834 [16:46<8:01:03,  1.67s/it]  3%|▎         | 598/17834 [16:47<8:03:41,  1.68s/it]  3%|▎         | 599/17834 [16:49<7:59:57,  1.67s/it]08/30/2024 19:31:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.5120203495025635, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.051221176981925964, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.6226789951324463, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 5.185920715332031}
  3%|▎         | 600/17834 [16:51<8:11:59,  1.71s/it]  3%|▎         | 601/17834 [16:52<8:12:44,  1.72s/it]  3%|▎         | 602/17834 [16:54<8:07:56,  1.70s/it]  3%|▎         | 603/17834 [16:56<8:01:53,  1.68s/it]  3%|▎         | 604/17834 [16:57<8:02:44,  1.68s/it]  3%|▎         | 605/17834 [16:59<7:55:38,  1.66s/it]  3%|▎         | 606/17834 [17:01<7:55:27,  1.66s/it]  3%|▎         | 607/17834 [17:02<8:00:54,  1.67s/it]  3%|▎         | 608/17834 [17:04<8:02:48,  1.68s/it]  3%|▎         | 609/17834 [17:06<8:00:08,  1.67s/it]  3%|▎         | 610/17834 [17:07<8:05:00,  1.69s/it]  3%|▎         | 611/17834 [17:09<8:04:13,  1.69s/it]  3%|▎         | 612/17834 [17:11<8:02:22,  1.68s/it]  3%|▎         | 613/17834 [17:13<8:04:01,  1.69s/it]  3%|▎         | 614/17834 [17:14<8:02:53,  1.68s/it]  3%|▎         | 615/17834 [17:16<8:00:13,  1.67s/it]  3%|▎         | 616/17834 [17:18<8:02:55,  1.68s/it]  3%|▎         | 617/17834 [17:19<7:56:06,  1.66s/it]  3%|▎         | 618/17834 [17:21<7:59:49,  1.67s/it]  3%|▎         | 619/17834 [17:23<8:01:56,  1.68s/it]  3%|▎         | 620/17834 [17:24<7:57:44,  1.67s/it]  3%|▎         | 621/17834 [17:26<7:55:00,  1.66s/it]  3%|▎         | 622/17834 [17:27<7:54:44,  1.65s/it]  3%|▎         | 623/17834 [17:29<7:59:14,  1.67s/it]  3%|▎         | 624/17834 [17:31<8:04:30,  1.69s/it]  4%|▎         | 625/17834 [17:33<7:58:40,  1.67s/it]  4%|▎         | 626/17834 [17:34<8:05:41,  1.69s/it]  4%|▎         | 627/17834 [17:36<8:01:02,  1.68s/it]  4%|▎         | 628/17834 [17:38<8:05:58,  1.69s/it]  4%|▎         | 629/17834 [17:39<8:02:00,  1.68s/it]  4%|▎         | 630/17834 [17:41<8:01:58,  1.68s/it]  4%|▎         | 631/17834 [17:43<8:02:34,  1.68s/it]  4%|▎         | 632/17834 [17:44<7:56:56,  1.66s/it]  4%|▎         | 633/17834 [17:46<7:56:59,  1.66s/it]  4%|▎         | 634/17834 [17:48<8:01:34,  1.68s/it]  4%|▎         | 635/17834 [17:49<8:06:24,  1.70s/it]  4%|▎         | 636/17834 [17:51<8:11:09,  1.71s/it]  4%|▎         | 637/17834 [17:53<8:05:39,  1.69s/it]  4%|▎         | 638/17834 [17:54<8:02:39,  1.68s/it]  4%|▎         | 639/17834 [17:56<8:03:38,  1.69s/it]  4%|▎         | 640/17834 [17:58<8:12:11,  1.72s/it]  4%|▎         | 641/17834 [18:00<8:02:18,  1.68s/it]  4%|▎         | 642/17834 [18:01<8:03:14,  1.69s/it]  4%|▎         | 643/17834 [18:03<8:00:37,  1.68s/it]  4%|▎         | 644/17834 [18:05<7:57:33,  1.67s/it]  4%|▎         | 645/17834 [18:06<7:56:41,  1.66s/it]  4%|▎         | 646/17834 [18:08<7:55:40,  1.66s/it]  4%|▎         | 647/17834 [18:10<8:01:23,  1.68s/it]  4%|▎         | 648/17834 [18:11<7:59:10,  1.67s/it]  4%|▎         | 649/17834 [18:13<8:02:06,  1.68s/it]08/30/2024 19:32:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5376336574554443, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04351593926548958, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2564914226531982, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8376410007476807}
  4%|▎         | 650/17834 [18:15<7:55:50,  1.66s/it]  4%|▎         | 651/17834 [18:16<8:01:27,  1.68s/it]  4%|▎         | 652/17834 [18:18<8:02:14,  1.68s/it]  4%|▎         | 653/17834 [18:20<8:01:04,  1.68s/it]  4%|▎         | 654/17834 [18:21<7:59:09,  1.67s/it]  4%|▎         | 655/17834 [18:23<7:54:52,  1.66s/it]  4%|▎         | 656/17834 [18:25<7:56:47,  1.67s/it]  4%|▎         | 657/17834 [18:26<7:59:18,  1.67s/it]  4%|▎         | 658/17834 [18:28<8:00:40,  1.68s/it]  4%|▎         | 659/17834 [18:30<7:53:30,  1.65s/it]  4%|▎         | 660/17834 [18:31<7:58:48,  1.67s/it]  4%|▎         | 661/17834 [18:33<8:01:06,  1.68s/it]  4%|▎         | 662/17834 [18:35<7:57:31,  1.67s/it]  4%|▎         | 663/17834 [18:36<8:03:45,  1.69s/it]  4%|▎         | 664/17834 [18:38<8:00:12,  1.68s/it]  4%|▎         | 665/17834 [18:40<7:54:39,  1.66s/it]  4%|▎         | 666/17834 [18:41<7:56:07,  1.66s/it]  4%|▎         | 667/17834 [18:43<7:56:57,  1.67s/it]  4%|▎         | 668/17834 [18:45<7:53:27,  1.65s/it]  4%|▍         | 669/17834 [18:46<7:55:24,  1.66s/it]  4%|▍         | 670/17834 [18:48<7:58:19,  1.67s/it]  4%|▍         | 671/17834 [18:50<8:03:46,  1.69s/it]  4%|▍         | 672/17834 [18:51<8:04:37,  1.69s/it]  4%|▍         | 673/17834 [18:53<8:03:22,  1.69s/it]  4%|▍         | 674/17834 [18:55<8:12:47,  1.72s/it]  4%|▍         | 675/17834 [18:57<8:11:43,  1.72s/it]  4%|▍         | 676/17834 [18:58<8:09:27,  1.71s/it]  4%|▍         | 677/17834 [19:00<8:03:26,  1.69s/it]  4%|▍         | 678/17834 [19:02<8:11:16,  1.72s/it]  4%|▍         | 679/17834 [19:03<8:06:13,  1.70s/it]  4%|▍         | 680/17834 [19:05<8:06:42,  1.70s/it]  4%|▍         | 681/17834 [19:07<8:06:50,  1.70s/it]  4%|▍         | 682/17834 [19:09<8:03:26,  1.69s/it]  4%|▍         | 683/17834 [19:10<8:02:13,  1.69s/it]  4%|▍         | 684/17834 [19:12<8:06:59,  1.70s/it]  4%|▍         | 685/17834 [19:14<8:13:42,  1.73s/it]  4%|▍         | 686/17834 [19:15<8:01:57,  1.69s/it]  4%|▍         | 687/17834 [19:17<8:05:33,  1.70s/it]  4%|▍         | 688/17834 [19:19<8:08:21,  1.71s/it]  4%|▍         | 689/17834 [19:20<8:02:15,  1.69s/it]  4%|▍         | 690/17834 [19:22<8:07:41,  1.71s/it]  4%|▍         | 691/17834 [19:24<8:08:51,  1.71s/it]  4%|▍         | 692/17834 [19:26<8:02:39,  1.69s/it]  4%|▍         | 693/17834 [19:27<8:06:33,  1.70s/it]  4%|▍         | 694/17834 [19:29<8:05:36,  1.70s/it]  4%|▍         | 695/17834 [19:31<8:02:45,  1.69s/it]  4%|▍         | 696/17834 [19:32<7:56:06,  1.67s/it]  4%|▍         | 697/17834 [19:34<8:02:08,  1.69s/it]  4%|▍         | 698/17834 [19:36<8:03:49,  1.69s/it]  4%|▍         | 699/17834 [19:37<7:58:32,  1.68s/it]08/30/2024 19:33:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.1180567741394043, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045676007866859436, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5352649688720703, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.698997497558594}
  4%|▍         | 700/17834 [19:39<7:59:07,  1.68s/it]  4%|▍         | 701/17834 [19:41<7:55:25,  1.66s/it]  4%|▍         | 702/17834 [19:42<7:56:20,  1.67s/it]  4%|▍         | 703/17834 [19:44<7:58:37,  1.68s/it]  4%|▍         | 704/17834 [19:46<7:55:44,  1.67s/it]  4%|▍         | 705/17834 [19:47<7:56:03,  1.67s/it]  4%|▍         | 706/17834 [19:49<8:00:27,  1.68s/it]  4%|▍         | 707/17834 [19:51<8:04:36,  1.70s/it]  4%|▍         | 708/17834 [19:52<8:03:00,  1.69s/it]  4%|▍         | 709/17834 [19:54<8:05:20,  1.70s/it]  4%|▍         | 710/17834 [19:56<8:04:54,  1.70s/it]  4%|▍         | 711/17834 [19:57<8:00:17,  1.68s/it]  4%|▍         | 712/17834 [19:59<8:04:32,  1.70s/it]  4%|▍         | 713/17834 [20:01<7:59:46,  1.68s/it]  4%|▍         | 714/17834 [20:03<8:02:07,  1.69s/it]  4%|▍         | 715/17834 [20:04<8:03:45,  1.70s/it]  4%|▍         | 716/17834 [20:06<8:02:23,  1.69s/it]  4%|▍         | 717/17834 [20:08<7:55:14,  1.67s/it]  4%|▍         | 718/17834 [20:09<7:55:57,  1.67s/it]  4%|▍         | 719/17834 [20:11<7:54:03,  1.66s/it]  4%|▍         | 720/17834 [20:13<7:53:43,  1.66s/it]  4%|▍         | 721/17834 [20:14<7:53:19,  1.66s/it]  4%|▍         | 722/17834 [20:16<7:52:23,  1.66s/it]  4%|▍         | 723/17834 [20:18<7:54:09,  1.66s/it]  4%|▍         | 724/17834 [20:19<7:53:41,  1.66s/it]  4%|▍         | 725/17834 [20:21<7:53:57,  1.66s/it]  4%|▍         | 726/17834 [20:23<7:52:57,  1.66s/it]  4%|▍         | 727/17834 [20:24<7:51:11,  1.65s/it]  4%|▍         | 728/17834 [20:26<7:49:25,  1.65s/it]  4%|▍         | 729/17834 [20:27<7:50:57,  1.65s/it]  4%|▍         | 730/17834 [20:29<8:01:03,  1.69s/it]  4%|▍         | 731/17834 [20:31<7:57:30,  1.68s/it]  4%|▍         | 732/17834 [20:33<7:57:07,  1.67s/it]  4%|▍         | 733/17834 [20:34<7:56:23,  1.67s/it]  4%|▍         | 734/17834 [20:36<7:50:01,  1.65s/it]  4%|▍         | 735/17834 [20:37<7:52:25,  1.66s/it]  4%|▍         | 736/17834 [20:39<7:57:05,  1.67s/it]  4%|▍         | 737/17834 [20:41<8:02:27,  1.69s/it]  4%|▍         | 738/17834 [20:43<7:59:38,  1.68s/it]  4%|▍         | 739/17834 [20:44<7:58:58,  1.68s/it]  4%|▍         | 740/17834 [20:46<7:58:09,  1.68s/it]  4%|▍         | 741/17834 [20:48<7:59:39,  1.68s/it]  4%|▍         | 742/17834 [20:49<8:02:54,  1.70s/it]  4%|▍         | 743/17834 [20:51<8:00:08,  1.69s/it]  4%|▍         | 744/17834 [20:53<8:06:13,  1.71s/it]  4%|▍         | 745/17834 [20:54<8:03:08,  1.70s/it]  4%|▍         | 746/17834 [20:56<8:01:10,  1.69s/it]  4%|▍         | 747/17834 [20:58<8:05:31,  1.70s/it]  4%|▍         | 748/17834 [21:00<8:10:28,  1.72s/it]  4%|▍         | 749/17834 [21:01<8:05:16,  1.70s/it]08/30/2024 19:35:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.8100544214248657, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04415593296289444, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.479196310043335, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.333406448364258}
  4%|▍         | 750/17834 [21:03<7:57:25,  1.68s/it]  4%|▍         | 751/17834 [21:05<7:58:13,  1.68s/it]  4%|▍         | 752/17834 [21:06<8:04:53,  1.70s/it]  4%|▍         | 753/17834 [21:08<8:04:21,  1.70s/it]  4%|▍         | 754/17834 [21:10<8:02:26,  1.69s/it]  4%|▍         | 755/17834 [21:11<7:56:38,  1.67s/it]  4%|▍         | 756/17834 [21:13<8:00:47,  1.69s/it]  4%|▍         | 757/17834 [21:15<8:00:58,  1.69s/it]  4%|▍         | 758/17834 [21:16<7:58:15,  1.68s/it]  4%|▍         | 759/17834 [21:18<7:52:49,  1.66s/it]  4%|▍         | 760/17834 [21:20<7:55:18,  1.67s/it]  4%|▍         | 761/17834 [21:21<7:58:51,  1.68s/it]  4%|▍         | 762/17834 [21:23<7:56:31,  1.67s/it]  4%|▍         | 763/17834 [21:25<7:57:50,  1.68s/it]  4%|▍         | 764/17834 [21:26<7:55:28,  1.67s/it]  4%|▍         | 765/17834 [21:28<7:57:48,  1.68s/it]  4%|▍         | 766/17834 [21:30<7:54:23,  1.67s/it]  4%|▍         | 767/17834 [21:32<8:01:52,  1.69s/it]  4%|▍         | 768/17834 [21:33<8:01:28,  1.69s/it]  4%|▍         | 769/17834 [21:35<7:58:07,  1.68s/it]  4%|▍         | 770/17834 [21:37<7:54:36,  1.67s/it]  4%|▍         | 771/17834 [21:38<7:55:48,  1.67s/it]  4%|▍         | 772/17834 [21:40<7:53:39,  1.67s/it]  4%|▍         | 773/17834 [21:42<7:57:46,  1.68s/it]  4%|▍         | 774/17834 [21:43<7:54:56,  1.67s/it]  4%|▍         | 775/17834 [21:45<7:50:52,  1.66s/it]  4%|▍         | 776/17834 [21:46<7:50:32,  1.66s/it]  4%|▍         | 777/17834 [21:48<7:54:41,  1.67s/it]  4%|▍         | 778/17834 [21:50<7:54:20,  1.67s/it]  4%|▍         | 779/17834 [21:52<7:58:01,  1.68s/it]  4%|▍         | 780/17834 [21:53<7:59:31,  1.69s/it]  4%|▍         | 781/17834 [21:55<7:56:47,  1.68s/it]  4%|▍         | 782/17834 [21:57<7:53:24,  1.67s/it]  4%|▍         | 783/17834 [21:58<7:47:19,  1.64s/it]  4%|▍         | 784/17834 [22:00<7:45:28,  1.64s/it]  4%|▍         | 785/17834 [22:02<7:54:12,  1.67s/it]  4%|▍         | 786/17834 [22:03<7:55:20,  1.67s/it]  4%|▍         | 787/17834 [22:05<7:53:42,  1.67s/it]  4%|▍         | 788/17834 [22:07<7:56:12,  1.68s/it]  4%|▍         | 789/17834 [22:08<7:58:18,  1.68s/it]  4%|▍         | 790/17834 [22:10<7:51:27,  1.66s/it]  4%|▍         | 791/17834 [22:12<7:52:08,  1.66s/it]  4%|▍         | 792/17834 [22:13<8:00:44,  1.69s/it]  4%|▍         | 793/17834 [22:15<8:00:55,  1.69s/it]  4%|▍         | 794/17834 [22:17<8:02:08,  1.70s/it]  4%|▍         | 795/17834 [22:18<7:58:40,  1.69s/it]  4%|▍         | 796/17834 [22:20<7:53:41,  1.67s/it]  4%|▍         | 797/17834 [22:22<7:55:31,  1.67s/it]  4%|▍         | 798/17834 [22:23<7:55:28,  1.67s/it]  4%|▍         | 799/17834 [22:25<8:02:27,  1.70s/it]08/30/2024 19:36:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.652440071105957, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.043630290776491165, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.40670108795166, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.102771282196045}
  4%|▍         | 800/17834 [22:27<8:01:45,  1.70s/it]  4%|▍         | 801/17834 [22:28<7:54:35,  1.67s/it]  4%|▍         | 802/17834 [22:30<8:02:26,  1.70s/it]  5%|▍         | 803/17834 [22:32<8:00:29,  1.69s/it]  5%|▍         | 804/17834 [22:34<7:59:41,  1.69s/it]  5%|▍         | 805/17834 [22:35<7:54:07,  1.67s/it]  5%|▍         | 806/17834 [22:37<7:57:17,  1.68s/it]  5%|▍         | 807/17834 [22:38<7:51:41,  1.66s/it]  5%|▍         | 808/17834 [22:40<7:53:36,  1.67s/it]  5%|▍         | 809/17834 [22:42<7:57:29,  1.68s/it]  5%|▍         | 810/17834 [22:44<7:58:50,  1.69s/it]  5%|▍         | 811/17834 [22:45<8:01:39,  1.70s/it]  5%|▍         | 812/17834 [22:47<7:53:55,  1.67s/it]  5%|▍         | 813/17834 [22:49<7:51:55,  1.66s/it]  5%|▍         | 814/17834 [22:50<7:58:15,  1.69s/it]  5%|▍         | 815/17834 [22:52<7:58:45,  1.69s/it]  5%|▍         | 816/17834 [22:54<7:54:53,  1.67s/it]  5%|▍         | 817/17834 [22:55<7:51:22,  1.66s/it]  5%|▍         | 818/17834 [22:57<7:47:49,  1.65s/it]  5%|▍         | 819/17834 [22:59<7:46:42,  1.65s/it]  5%|▍         | 820/17834 [23:00<7:56:59,  1.68s/it]  5%|▍         | 821/17834 [23:02<7:58:11,  1.69s/it]  5%|▍         | 822/17834 [23:04<8:04:50,  1.71s/it]  5%|▍         | 823/17834 [23:05<7:59:37,  1.69s/it]  5%|▍         | 824/17834 [23:07<7:59:57,  1.69s/it]  5%|▍         | 825/17834 [23:09<8:02:19,  1.70s/it]  5%|▍         | 826/17834 [23:11<8:04:21,  1.71s/it]  5%|▍         | 827/17834 [23:12<8:08:49,  1.72s/it]  5%|▍         | 828/17834 [23:14<8:02:10,  1.70s/it]  5%|▍         | 829/17834 [23:16<7:54:40,  1.67s/it]  5%|▍         | 830/17834 [23:17<7:57:18,  1.68s/it]  5%|▍         | 831/17834 [23:19<7:50:45,  1.66s/it]  5%|▍         | 832/17834 [23:21<7:54:32,  1.67s/it]  5%|▍         | 833/17834 [23:22<8:08:46,  1.72s/it]  5%|▍         | 834/17834 [23:24<8:03:24,  1.71s/it]  5%|▍         | 835/17834 [23:26<8:00:39,  1.70s/it]  5%|▍         | 836/17834 [23:27<7:59:41,  1.69s/it]  5%|▍         | 837/17834 [23:29<7:57:52,  1.69s/it]  5%|▍         | 838/17834 [23:31<7:56:41,  1.68s/it]  5%|▍         | 839/17834 [23:32<7:55:55,  1.68s/it]  5%|▍         | 840/17834 [23:34<8:00:12,  1.70s/it]  5%|▍         | 841/17834 [23:36<8:01:52,  1.70s/it]  5%|▍         | 842/17834 [23:38<7:56:36,  1.68s/it]  5%|▍         | 843/17834 [23:39<7:51:25,  1.66s/it]  5%|▍         | 844/17834 [23:41<7:50:40,  1.66s/it]  5%|▍         | 845/17834 [23:43<7:53:02,  1.67s/it]  5%|▍         | 846/17834 [23:44<7:57:18,  1.69s/it]  5%|▍         | 847/17834 [23:46<7:58:10,  1.69s/it]  5%|▍         | 848/17834 [23:48<7:54:54,  1.68s/it]  5%|▍         | 849/17834 [23:49<7:52:32,  1.67s/it]08/30/2024 19:38:09 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.048614740371704, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.048150449991226196, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4986133575439453, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.595378875732422}
  5%|▍         | 850/17834 [23:51<7:54:25,  1.68s/it]  5%|▍         | 851/17834 [23:53<7:54:12,  1.68s/it]  5%|▍         | 852/17834 [23:54<7:54:11,  1.68s/it]  5%|▍         | 853/17834 [23:56<7:58:29,  1.69s/it]  5%|▍         | 854/17834 [23:58<7:52:38,  1.67s/it]  5%|▍         | 855/17834 [23:59<7:50:47,  1.66s/it]  5%|▍         | 856/17834 [24:01<7:44:40,  1.64s/it]  5%|▍         | 857/17834 [24:03<8:00:18,  1.70s/it]  5%|▍         | 858/17834 [24:04<7:58:49,  1.69s/it]  5%|▍         | 859/17834 [24:06<8:00:59,  1.70s/it]  5%|▍         | 860/17834 [24:08<8:06:07,  1.72s/it]  5%|▍         | 861/17834 [24:09<8:00:45,  1.70s/it]  5%|▍         | 862/17834 [24:11<8:05:49,  1.72s/it]  5%|▍         | 863/17834 [24:13<8:06:02,  1.72s/it]  5%|▍         | 864/17834 [24:15<8:07:16,  1.72s/it]  5%|▍         | 865/17834 [24:16<8:02:29,  1.71s/it]  5%|▍         | 866/17834 [24:18<8:00:36,  1.70s/it]  5%|▍         | 867/17834 [24:20<7:55:12,  1.68s/it]  5%|▍         | 868/17834 [24:21<7:59:53,  1.70s/it]  5%|▍         | 869/17834 [24:23<7:55:49,  1.68s/it]  5%|▍         | 870/17834 [24:25<7:48:38,  1.66s/it]  5%|▍         | 871/17834 [24:26<7:51:28,  1.67s/it]  5%|▍         | 872/17834 [24:28<7:54:10,  1.68s/it]  5%|▍         | 873/17834 [24:30<7:56:13,  1.68s/it]  5%|▍         | 874/17834 [24:31<7:53:49,  1.68s/it]  5%|▍         | 875/17834 [24:33<7:57:41,  1.69s/it]  5%|▍         | 876/17834 [24:35<8:01:46,  1.70s/it]  5%|▍         | 877/17834 [24:37<7:55:52,  1.68s/it]  5%|▍         | 878/17834 [24:38<7:56:33,  1.69s/it]  5%|▍         | 879/17834 [24:40<7:57:34,  1.69s/it]  5%|▍         | 880/17834 [24:42<7:52:52,  1.67s/it]  5%|▍         | 881/17834 [24:43<7:52:28,  1.67s/it]  5%|▍         | 882/17834 [24:45<7:50:25,  1.67s/it]  5%|▍         | 883/17834 [24:47<7:59:50,  1.70s/it]  5%|▍         | 884/17834 [24:48<7:58:19,  1.69s/it]  5%|▍         | 885/17834 [24:50<7:58:10,  1.69s/it]  5%|▍         | 886/17834 [24:52<7:54:09,  1.68s/it]  5%|▍         | 887/17834 [24:53<7:54:32,  1.68s/it]  5%|▍         | 888/17834 [24:55<7:53:11,  1.68s/it]  5%|▍         | 889/17834 [24:57<7:54:28,  1.68s/it]  5%|▍         | 890/17834 [24:58<7:56:53,  1.69s/it]  5%|▍         | 891/17834 [25:00<7:51:29,  1.67s/it]  5%|▌         | 892/17834 [25:02<7:51:39,  1.67s/it]  5%|▌         | 893/17834 [25:03<7:48:58,  1.66s/it]  5%|▌         | 894/17834 [25:05<7:53:02,  1.68s/it]  5%|▌         | 895/17834 [25:07<7:56:40,  1.69s/it]  5%|▌         | 896/17834 [25:08<7:52:10,  1.67s/it]  5%|▌         | 897/17834 [25:10<7:49:56,  1.66s/it]  5%|▌         | 898/17834 [25:12<7:55:39,  1.69s/it]  5%|▌         | 899/17834 [25:13<7:52:22,  1.67s/it]08/30/2024 19:39:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.0584053993225098, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05475576967000961, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5140204429626465, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.627181529998779}
  5%|▌         | 900/17834 [25:15<7:53:54,  1.68s/it]  5%|▌         | 901/17834 [25:17<7:51:14,  1.67s/it]  5%|▌         | 902/17834 [25:18<7:49:41,  1.66s/it]  5%|▌         | 903/17834 [25:20<7:47:47,  1.66s/it]  5%|▌         | 904/17834 [25:22<7:54:09,  1.68s/it]  5%|▌         | 905/17834 [25:24<7:55:36,  1.69s/it]  5%|▌         | 906/17834 [25:25<7:51:37,  1.67s/it]  5%|▌         | 907/17834 [25:27<7:47:21,  1.66s/it]  5%|▌         | 908/17834 [25:29<7:55:51,  1.69s/it]  5%|▌         | 909/17834 [25:30<7:51:56,  1.67s/it]  5%|▌         | 910/17834 [25:32<7:50:57,  1.67s/it]  5%|▌         | 911/17834 [25:33<7:49:41,  1.67s/it]  5%|▌         | 912/17834 [25:35<7:46:55,  1.66s/it]  5%|▌         | 913/17834 [25:37<7:50:05,  1.67s/it]  5%|▌         | 914/17834 [25:38<7:46:06,  1.65s/it]  5%|▌         | 915/17834 [25:40<7:50:05,  1.67s/it]  5%|▌         | 916/17834 [25:42<7:49:06,  1.66s/it]  5%|▌         | 917/17834 [25:43<7:51:06,  1.67s/it]  5%|▌         | 918/17834 [25:45<7:59:17,  1.70s/it]  5%|▌         | 919/17834 [25:47<7:58:32,  1.70s/it]  5%|▌         | 920/17834 [25:49<8:04:01,  1.72s/it]  5%|▌         | 921/17834 [25:50<7:57:15,  1.69s/it]  5%|▌         | 922/17834 [25:52<7:53:54,  1.68s/it]  5%|▌         | 923/17834 [25:54<7:56:42,  1.69s/it]  5%|▌         | 924/17834 [25:55<7:56:51,  1.69s/it]  5%|▌         | 925/17834 [25:57<7:54:18,  1.68s/it]  5%|▌         | 926/17834 [25:59<7:57:12,  1.69s/it]  5%|▌         | 927/17834 [26:00<7:59:45,  1.70s/it]  5%|▌         | 928/17834 [26:02<7:59:08,  1.70s/it]  5%|▌         | 929/17834 [26:04<7:50:15,  1.67s/it]  5%|▌         | 930/17834 [26:05<7:47:03,  1.66s/it]  5%|▌         | 931/17834 [26:07<7:52:22,  1.68s/it]  5%|▌         | 932/17834 [26:09<7:53:42,  1.68s/it]  5%|▌         | 933/17834 [26:11<7:52:37,  1.68s/it]  5%|▌         | 934/17834 [26:12<7:52:37,  1.68s/it]  5%|▌         | 935/17834 [26:14<7:53:37,  1.68s/it]  5%|▌         | 936/17834 [26:16<7:50:12,  1.67s/it]  5%|▌         | 937/17834 [26:17<8:01:21,  1.71s/it]  5%|▌         | 938/17834 [26:19<7:59:30,  1.70s/it]  5%|▌         | 939/17834 [26:21<7:55:12,  1.69s/it]  5%|▌         | 940/17834 [26:22<7:51:07,  1.67s/it]  5%|▌         | 941/17834 [26:24<7:51:08,  1.67s/it]  5%|▌         | 942/17834 [26:26<7:50:03,  1.67s/it]  5%|▌         | 943/17834 [26:27<7:55:50,  1.69s/it]  5%|▌         | 944/17834 [26:29<8:01:54,  1.71s/it]  5%|▌         | 945/17834 [26:31<8:00:32,  1.71s/it]  5%|▌         | 946/17834 [26:32<7:54:28,  1.69s/it]  5%|▌         | 947/17834 [26:34<7:52:40,  1.68s/it]  5%|▌         | 948/17834 [26:36<7:51:48,  1.68s/it]  5%|▌         | 949/17834 [26:37<7:51:33,  1.68s/it]08/30/2024 19:40:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.7901382446289062, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04821495711803436, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4397735595703125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.2781267166137695}
  5%|▌         | 950/17834 [26:39<7:48:31,  1.66s/it]  5%|▌         | 951/17834 [26:41<7:51:29,  1.68s/it]  5%|▌         | 952/17834 [26:42<7:51:03,  1.67s/it]  5%|▌         | 953/17834 [26:44<7:47:16,  1.66s/it]  5%|▌         | 954/17834 [26:46<7:47:41,  1.66s/it]  5%|▌         | 955/17834 [26:47<7:47:42,  1.66s/it]  5%|▌         | 956/17834 [26:49<7:49:04,  1.67s/it]  5%|▌         | 957/17834 [26:51<7:51:46,  1.68s/it]  5%|▌         | 958/17834 [26:52<7:51:39,  1.68s/it]  5%|▌         | 959/17834 [26:54<7:51:43,  1.68s/it]  5%|▌         | 960/17834 [26:56<7:47:02,  1.66s/it]  5%|▌         | 961/17834 [26:57<7:44:59,  1.65s/it]  5%|▌         | 962/17834 [26:59<7:42:15,  1.64s/it]  5%|▌         | 963/17834 [27:01<7:46:05,  1.66s/it]  5%|▌         | 964/17834 [27:02<7:44:01,  1.65s/it]  5%|▌         | 965/17834 [27:04<7:47:39,  1.66s/it]  5%|▌         | 966/17834 [27:06<7:40:14,  1.64s/it]  5%|▌         | 967/17834 [27:07<7:41:13,  1.64s/it]  5%|▌         | 968/17834 [27:09<7:47:19,  1.66s/it]  5%|▌         | 969/17834 [27:11<7:53:00,  1.68s/it]  5%|▌         | 970/17834 [27:12<7:52:26,  1.68s/it]  5%|▌         | 971/17834 [27:14<7:52:17,  1.68s/it]  5%|▌         | 972/17834 [27:16<7:48:30,  1.67s/it]  5%|▌         | 973/17834 [27:17<7:48:06,  1.67s/it]  5%|▌         | 974/17834 [27:19<7:47:22,  1.66s/it]  5%|▌         | 975/17834 [27:21<7:52:23,  1.68s/it]  5%|▌         | 976/17834 [27:23<7:56:38,  1.70s/it]  5%|▌         | 977/17834 [27:24<7:56:31,  1.70s/it]  5%|▌         | 978/17834 [27:26<7:50:42,  1.68s/it]  5%|▌         | 979/17834 [27:27<7:47:37,  1.66s/it]  5%|▌         | 980/17834 [27:29<7:47:36,  1.66s/it]  6%|▌         | 981/17834 [27:31<7:48:27,  1.67s/it]  6%|▌         | 982/17834 [27:32<7:44:23,  1.65s/it]  6%|▌         | 983/17834 [27:34<7:45:11,  1.66s/it]  6%|▌         | 984/17834 [27:36<7:44:17,  1.65s/it]  6%|▌         | 985/17834 [27:37<7:43:08,  1.65s/it]  6%|▌         | 986/17834 [27:39<7:38:43,  1.63s/it]  6%|▌         | 987/17834 [27:41<7:40:25,  1.64s/it]  6%|▌         | 988/17834 [27:42<7:41:45,  1.64s/it]  6%|▌         | 989/17834 [27:44<7:46:29,  1.66s/it]  6%|▌         | 990/17834 [27:46<7:43:08,  1.65s/it]  6%|▌         | 991/17834 [27:47<7:45:14,  1.66s/it]  6%|▌         | 992/17834 [27:49<7:43:44,  1.65s/it]  6%|▌         | 993/17834 [27:51<7:48:56,  1.67s/it]  6%|▌         | 994/17834 [27:52<7:53:21,  1.69s/it]  6%|▌         | 995/17834 [27:54<7:56:56,  1.70s/it]  6%|▌         | 996/17834 [27:56<7:58:42,  1.71s/it]  6%|▌         | 997/17834 [27:57<7:51:15,  1.68s/it]  6%|▌         | 998/17834 [27:59<7:55:33,  1.69s/it]  6%|▌         | 999/17834 [28:01<7:57:25,  1.70s/it]08/30/2024 19:42:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.954310417175293, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04720396548509598, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3266241550445557, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.32813835144043}
  6%|▌         | 1000/17834 [28:03<7:54:37,  1.69s/it]  6%|▌         | 1001/17834 [28:04<7:51:46,  1.68s/it]  6%|▌         | 1002/17834 [28:06<7:52:31,  1.68s/it]  6%|▌         | 1003/17834 [28:08<7:50:47,  1.68s/it]  6%|▌         | 1004/17834 [28:09<7:56:05,  1.70s/it]  6%|▌         | 1005/17834 [28:11<8:00:14,  1.71s/it]  6%|▌         | 1006/17834 [28:13<7:51:21,  1.68s/it]  6%|▌         | 1007/17834 [28:14<7:52:18,  1.68s/it]  6%|▌         | 1008/17834 [28:16<7:53:28,  1.69s/it]  6%|▌         | 1009/17834 [28:18<7:48:51,  1.67s/it]  6%|▌         | 1010/17834 [28:19<7:48:10,  1.67s/it]  6%|▌         | 1011/17834 [28:21<7:48:33,  1.67s/it]  6%|▌         | 1012/17834 [28:23<7:47:44,  1.67s/it]  6%|▌         | 1013/17834 [28:24<7:46:27,  1.66s/it]  6%|▌         | 1014/17834 [28:26<7:50:42,  1.68s/it]  6%|▌         | 1015/17834 [28:28<7:43:47,  1.65s/it]  6%|▌         | 1016/17834 [28:29<7:46:27,  1.66s/it]  6%|▌         | 1017/17834 [28:31<7:47:12,  1.67s/it]  6%|▌         | 1018/17834 [28:33<7:45:19,  1.66s/it]  6%|▌         | 1019/17834 [28:34<7:44:12,  1.66s/it]  6%|▌         | 1020/17834 [28:36<7:48:12,  1.67s/it]  6%|▌         | 1021/17834 [28:38<7:47:34,  1.67s/it]  6%|▌         | 1022/17834 [28:39<7:50:21,  1.68s/it]  6%|▌         | 1023/17834 [28:41<7:45:20,  1.66s/it]  6%|▌         | 1024/17834 [28:43<7:43:56,  1.66s/it]  6%|▌         | 1025/17834 [28:44<7:37:56,  1.63s/it]  6%|▌         | 1026/17834 [28:46<7:43:12,  1.65s/it]  6%|▌         | 1027/17834 [28:48<7:40:58,  1.65s/it]  6%|▌         | 1028/17834 [28:49<7:45:59,  1.66s/it]  6%|▌         | 1029/17834 [28:51<7:46:16,  1.66s/it]  6%|▌         | 1030/17834 [28:53<7:43:11,  1.65s/it]  6%|▌         | 1031/17834 [28:54<7:49:43,  1.68s/it]  6%|▌         | 1032/17834 [28:56<7:50:21,  1.68s/it]  6%|▌         | 1033/17834 [28:58<7:46:30,  1.67s/it]  6%|▌         | 1034/17834 [28:59<7:48:17,  1.67s/it]  6%|▌         | 1035/17834 [29:01<8:00:52,  1.72s/it]  6%|▌         | 1036/17834 [29:03<7:56:27,  1.70s/it]  6%|▌         | 1037/17834 [29:04<7:58:38,  1.71s/it]  6%|▌         | 1038/17834 [29:06<7:58:43,  1.71s/it]  6%|▌         | 1039/17834 [29:08<7:49:19,  1.68s/it]  6%|▌         | 1040/17834 [29:09<7:49:51,  1.68s/it]  6%|▌         | 1041/17834 [29:11<7:45:28,  1.66s/it]  6%|▌         | 1042/17834 [29:13<7:50:36,  1.68s/it]  6%|▌         | 1043/17834 [29:15<7:51:23,  1.68s/it]  6%|▌         | 1044/17834 [29:16<7:48:17,  1.67s/it]  6%|▌         | 1045/17834 [29:18<7:48:12,  1.67s/it]  6%|▌         | 1046/17834 [29:20<7:49:27,  1.68s/it]  6%|▌         | 1047/17834 [29:21<7:48:35,  1.67s/it]  6%|▌         | 1048/17834 [29:23<7:41:31,  1.65s/it]  6%|▌         | 1049/17834 [29:24<7:41:50,  1.65s/it]08/30/2024 19:43:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.8949816226959229, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.046050168573856354, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.305371046066284, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.246402740478516}
  6%|▌         | 1050/17834 [29:26<7:45:05,  1.66s/it]  6%|▌         | 1051/17834 [29:28<7:41:37,  1.65s/it]  6%|▌         | 1052/17834 [29:29<7:41:53,  1.65s/it]  6%|▌         | 1053/17834 [29:31<7:50:41,  1.68s/it]  6%|▌         | 1054/17834 [29:33<7:52:11,  1.69s/it]  6%|▌         | 1055/17834 [29:35<7:55:26,  1.70s/it]  6%|▌         | 1056/17834 [29:36<7:54:39,  1.70s/it]  6%|▌         | 1057/17834 [29:38<7:49:25,  1.68s/it]  6%|▌         | 1058/17834 [29:40<7:50:43,  1.68s/it]  6%|▌         | 1059/17834 [29:41<7:56:07,  1.70s/it]  6%|▌         | 1060/17834 [29:43<7:59:02,  1.71s/it]  6%|▌         | 1061/17834 [29:45<7:54:45,  1.70s/it]  6%|▌         | 1062/17834 [29:46<7:56:12,  1.70s/it]  6%|▌         | 1063/17834 [29:48<7:55:04,  1.70s/it]  6%|▌         | 1064/17834 [29:50<7:56:59,  1.71s/it]  6%|▌         | 1065/17834 [29:52<7:52:18,  1.69s/it]  6%|▌         | 1066/17834 [29:53<7:54:06,  1.70s/it]  6%|▌         | 1067/17834 [29:55<7:50:21,  1.68s/it]  6%|▌         | 1068/17834 [29:57<7:49:59,  1.68s/it]  6%|▌         | 1069/17834 [29:58<7:45:20,  1.67s/it]  6%|▌         | 1070/17834 [30:00<7:46:20,  1.67s/it]  6%|▌         | 1071/17834 [30:02<7:42:20,  1.65s/it]  6%|▌         | 1072/17834 [30:03<7:42:00,  1.65s/it]  6%|▌         | 1073/17834 [30:05<7:41:20,  1.65s/it]  6%|▌         | 1074/17834 [30:07<7:51:20,  1.69s/it]  6%|▌         | 1075/17834 [30:08<7:48:03,  1.68s/it]  6%|▌         | 1076/17834 [30:10<7:51:27,  1.69s/it]  6%|▌         | 1077/17834 [30:12<7:50:44,  1.69s/it]  6%|▌         | 1078/17834 [30:13<7:43:12,  1.66s/it]  6%|▌         | 1079/17834 [30:15<7:47:10,  1.67s/it]  6%|▌         | 1080/17834 [30:17<7:46:47,  1.67s/it]  6%|▌         | 1081/17834 [30:18<7:44:29,  1.66s/it]  6%|▌         | 1082/17834 [30:20<7:47:06,  1.67s/it]  6%|▌         | 1083/17834 [30:22<7:42:36,  1.66s/it]  6%|▌         | 1084/17834 [30:23<7:47:04,  1.67s/it]  6%|▌         | 1085/17834 [30:25<7:52:43,  1.69s/it]  6%|▌         | 1086/17834 [30:27<7:47:45,  1.68s/it]  6%|▌         | 1087/17834 [30:28<7:45:22,  1.67s/it]  6%|▌         | 1088/17834 [30:30<7:55:31,  1.70s/it]  6%|▌         | 1089/17834 [30:32<7:46:17,  1.67s/it]  6%|▌         | 1090/17834 [30:33<7:44:16,  1.66s/it]  6%|▌         | 1091/17834 [30:35<7:49:11,  1.68s/it]  6%|▌         | 1092/17834 [30:37<7:51:38,  1.69s/it]  6%|▌         | 1093/17834 [30:38<7:47:09,  1.67s/it]  6%|▌         | 1094/17834 [30:40<7:47:02,  1.67s/it]  6%|▌         | 1095/17834 [30:42<7:44:32,  1.67s/it]  6%|▌         | 1096/17834 [30:43<7:50:40,  1.69s/it]  6%|▌         | 1097/17834 [30:45<7:44:32,  1.67s/it]  6%|▌         | 1098/17834 [30:47<7:45:54,  1.67s/it]  6%|▌         | 1099/17834 [30:48<7:46:43,  1.67s/it]08/30/2024 19:45:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4407310485839844, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03941182792186737, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.316929340362549, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.797072172164917}
  6%|▌         | 1100/17834 [30:50<7:46:18,  1.67s/it]  6%|▌         | 1101/17834 [30:52<7:45:52,  1.67s/it]  6%|▌         | 1102/17834 [30:53<7:48:33,  1.68s/it]  6%|▌         | 1103/17834 [30:55<7:50:21,  1.69s/it]  6%|▌         | 1104/17834 [30:57<7:46:31,  1.67s/it]  6%|▌         | 1105/17834 [30:59<7:48:36,  1.68s/it]  6%|▌         | 1106/17834 [31:00<7:49:30,  1.68s/it]  6%|▌         | 1107/17834 [31:02<7:54:30,  1.70s/it]  6%|▌         | 1108/17834 [31:04<7:46:46,  1.67s/it]  6%|▌         | 1109/17834 [31:05<7:44:45,  1.67s/it]  6%|▌         | 1110/17834 [31:07<7:50:18,  1.69s/it]  6%|▌         | 1111/17834 [31:09<7:49:32,  1.68s/it]  6%|▌         | 1112/17834 [31:10<7:49:54,  1.69s/it]  6%|▌         | 1113/17834 [31:12<7:42:59,  1.66s/it]  6%|▌         | 1114/17834 [31:14<7:47:09,  1.68s/it]  6%|▋         | 1115/17834 [31:15<7:44:29,  1.67s/it]  6%|▋         | 1116/17834 [31:17<7:42:16,  1.66s/it]  6%|▋         | 1117/17834 [31:19<7:47:46,  1.68s/it]  6%|▋         | 1118/17834 [31:20<7:51:24,  1.69s/it]  6%|▋         | 1119/17834 [31:22<7:51:37,  1.69s/it]  6%|▋         | 1120/17834 [31:24<7:52:49,  1.70s/it]  6%|▋         | 1121/17834 [31:25<7:45:48,  1.67s/it]  6%|▋         | 1122/17834 [31:27<7:44:12,  1.67s/it]  6%|▋         | 1123/17834 [31:29<7:46:56,  1.68s/it]  6%|▋         | 1124/17834 [31:30<7:44:10,  1.67s/it]  6%|▋         | 1125/17834 [31:32<7:46:26,  1.67s/it]  6%|▋         | 1126/17834 [31:34<7:43:07,  1.66s/it]  6%|▋         | 1127/17834 [31:35<7:53:21,  1.70s/it]  6%|▋         | 1128/17834 [31:37<7:52:16,  1.70s/it]  6%|▋         | 1129/17834 [31:39<7:43:25,  1.66s/it]  6%|▋         | 1130/17834 [31:40<7:42:53,  1.66s/it]  6%|▋         | 1131/17834 [31:42<7:46:50,  1.68s/it]  6%|▋         | 1132/17834 [31:44<7:45:46,  1.67s/it]  6%|▋         | 1133/17834 [31:46<7:51:20,  1.69s/it]  6%|▋         | 1134/17834 [31:47<7:50:45,  1.69s/it]  6%|▋         | 1135/17834 [31:49<7:56:45,  1.71s/it]  6%|▋         | 1136/17834 [31:51<8:07:59,  1.75s/it]  6%|▋         | 1137/17834 [31:52<7:57:17,  1.72s/it]  6%|▋         | 1138/17834 [31:54<7:50:18,  1.69s/it]  6%|▋         | 1139/17834 [31:56<7:49:20,  1.69s/it]  6%|▋         | 1140/17834 [31:57<7:46:09,  1.68s/it]  6%|▋         | 1141/17834 [31:59<7:50:25,  1.69s/it]  6%|▋         | 1142/17834 [32:01<7:49:48,  1.69s/it]  6%|▋         | 1143/17834 [32:02<7:45:35,  1.67s/it]  6%|▋         | 1144/17834 [32:04<7:44:12,  1.67s/it]  6%|▋         | 1145/17834 [32:06<7:46:44,  1.68s/it]  6%|▋         | 1146/17834 [32:07<7:44:34,  1.67s/it]  6%|▋         | 1147/17834 [32:09<7:44:05,  1.67s/it]  6%|▋         | 1148/17834 [32:11<7:49:26,  1.69s/it]  6%|▋         | 1149/17834 [32:13<7:46:25,  1.68s/it]08/30/2024 19:46:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.767818570137024, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.039592765271663666, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.47670316696167, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.284114360809326}
  6%|▋         | 1150/17834 [32:14<7:52:10,  1.70s/it]  6%|▋         | 1151/17834 [32:16<7:47:48,  1.68s/it]  6%|▋         | 1152/17834 [32:18<7:47:11,  1.68s/it]  6%|▋         | 1153/17834 [32:19<7:45:52,  1.68s/it]  6%|▋         | 1154/17834 [32:21<7:41:27,  1.66s/it]  6%|▋         | 1155/17834 [32:23<7:46:26,  1.68s/it]  6%|▋         | 1156/17834 [32:24<7:49:41,  1.69s/it]  6%|▋         | 1157/17834 [32:26<7:46:39,  1.68s/it]  6%|▋         | 1158/17834 [32:28<7:45:20,  1.67s/it]  6%|▋         | 1159/17834 [32:29<7:47:01,  1.68s/it]  7%|▋         | 1160/17834 [32:31<7:50:46,  1.69s/it]  7%|▋         | 1161/17834 [32:33<7:49:32,  1.69s/it]  7%|▋         | 1162/17834 [32:34<7:47:46,  1.68s/it]  7%|▋         | 1163/17834 [32:36<7:42:05,  1.66s/it]  7%|▋         | 1164/17834 [32:38<7:44:40,  1.67s/it]  7%|▋         | 1165/17834 [32:39<7:47:49,  1.68s/it]  7%|▋         | 1166/17834 [32:41<7:47:22,  1.68s/it]  7%|▋         | 1167/17834 [32:43<7:46:02,  1.68s/it]  7%|▋         | 1168/17834 [32:44<7:43:18,  1.67s/it]  7%|▋         | 1169/17834 [32:46<7:40:54,  1.66s/it]  7%|▋         | 1170/17834 [32:48<7:38:25,  1.65s/it]  7%|▋         | 1171/17834 [32:49<7:37:40,  1.65s/it]  7%|▋         | 1172/17834 [32:51<7:45:22,  1.68s/it]  7%|▋         | 1173/17834 [32:53<7:44:14,  1.67s/it]  7%|▋         | 1174/17834 [32:54<7:45:12,  1.68s/it]  7%|▋         | 1175/17834 [32:56<7:41:33,  1.66s/it]  7%|▋         | 1176/17834 [32:58<7:45:07,  1.68s/it]  7%|▋         | 1177/17834 [32:59<7:48:42,  1.69s/it]  7%|▋         | 1178/17834 [33:01<7:47:25,  1.68s/it]  7%|▋         | 1179/17834 [33:03<7:53:57,  1.71s/it]  7%|▋         | 1180/17834 [33:05<7:48:10,  1.69s/it]  7%|▋         | 1181/17834 [33:06<7:54:09,  1.71s/it]  7%|▋         | 1182/17834 [33:08<7:45:24,  1.68s/it]  7%|▋         | 1183/17834 [33:10<7:39:43,  1.66s/it]  7%|▋         | 1184/17834 [33:11<7:46:00,  1.68s/it]  7%|▋         | 1185/17834 [33:13<7:40:28,  1.66s/it]  7%|▋         | 1186/17834 [33:15<7:46:13,  1.68s/it]  7%|▋         | 1187/17834 [33:16<7:40:37,  1.66s/it]  7%|▋         | 1188/17834 [33:18<7:41:44,  1.66s/it]  7%|▋         | 1189/17834 [33:20<7:42:58,  1.67s/it]  7%|▋         | 1190/17834 [33:21<7:39:39,  1.66s/it]  7%|▋         | 1191/17834 [33:23<7:39:24,  1.66s/it]  7%|▋         | 1192/17834 [33:25<7:48:39,  1.69s/it]  7%|▋         | 1193/17834 [33:26<7:46:40,  1.68s/it]  7%|▋         | 1194/17834 [33:28<7:46:25,  1.68s/it]  7%|▋         | 1195/17834 [33:30<7:42:39,  1.67s/it]  7%|▋         | 1196/17834 [33:31<7:40:54,  1.66s/it]  7%|▋         | 1197/17834 [33:33<7:38:49,  1.65s/it]  7%|▋         | 1198/17834 [33:35<7:37:55,  1.65s/it]  7%|▋         | 1199/17834 [33:36<7:40:01,  1.66s/it]08/30/2024 19:47:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.0356476306915283, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.053037818521261215, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5002927780151367, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.588978290557861}
  7%|▋         | 1200/17834 [33:38<7:42:06,  1.67s/it]  7%|▋         | 1201/17834 [33:40<7:42:31,  1.67s/it]  7%|▋         | 1202/17834 [33:41<7:40:04,  1.66s/it]  7%|▋         | 1203/17834 [33:43<7:46:21,  1.68s/it]  7%|▋         | 1204/17834 [33:45<7:48:50,  1.69s/it]  7%|▋         | 1205/17834 [33:46<7:42:11,  1.67s/it]  7%|▋         | 1206/17834 [33:48<7:37:02,  1.65s/it]  7%|▋         | 1207/17834 [33:50<7:37:41,  1.65s/it]  7%|▋         | 1208/17834 [33:51<7:33:47,  1.64s/it]  7%|▋         | 1209/17834 [33:53<7:41:42,  1.67s/it]  7%|▋         | 1210/17834 [33:55<7:44:12,  1.68s/it]  7%|▋         | 1211/17834 [33:56<7:42:15,  1.67s/it]  7%|▋         | 1212/17834 [33:58<7:40:09,  1.66s/it]  7%|▋         | 1213/17834 [34:00<7:40:51,  1.66s/it]  7%|▋         | 1214/17834 [34:01<7:44:23,  1.68s/it]  7%|▋         | 1215/17834 [34:03<7:51:45,  1.70s/it]  7%|▋         | 1216/17834 [34:05<7:45:00,  1.68s/it]  7%|▋         | 1217/17834 [34:06<7:44:13,  1.68s/it]  7%|▋         | 1218/17834 [34:08<7:45:55,  1.68s/it]  7%|▋         | 1219/17834 [34:10<7:44:52,  1.68s/it]  7%|▋         | 1220/17834 [34:11<7:43:25,  1.67s/it]  7%|▋         | 1221/17834 [34:13<7:47:22,  1.69s/it]  7%|▋         | 1222/17834 [34:15<7:47:03,  1.69s/it]  7%|▋         | 1223/17834 [34:16<7:48:13,  1.69s/it]  7%|▋         | 1224/17834 [34:18<7:52:35,  1.71s/it]  7%|▋         | 1225/17834 [34:20<7:49:15,  1.70s/it]  7%|▋         | 1226/17834 [34:22<7:47:13,  1.69s/it]  7%|▋         | 1227/17834 [34:23<7:42:26,  1.67s/it]  7%|▋         | 1228/17834 [34:25<7:38:09,  1.66s/it]  7%|▋         | 1229/17834 [34:26<7:39:18,  1.66s/it]  7%|▋         | 1230/17834 [34:28<7:42:32,  1.67s/it]  7%|▋         | 1231/17834 [34:30<7:48:33,  1.69s/it]  7%|▋         | 1232/17834 [34:31<7:43:06,  1.67s/it]  7%|▋         | 1233/17834 [34:33<7:44:37,  1.68s/it]  7%|▋         | 1234/17834 [34:35<7:44:54,  1.68s/it]  7%|▋         | 1235/17834 [34:36<7:38:59,  1.66s/it]  7%|▋         | 1236/17834 [34:38<7:37:21,  1.65s/it]  7%|▋         | 1237/17834 [34:40<7:33:27,  1.64s/it]  7%|▋         | 1238/17834 [34:41<7:41:38,  1.67s/it]  7%|▋         | 1239/17834 [34:43<7:36:23,  1.65s/it]  7%|▋         | 1240/17834 [34:45<7:36:26,  1.65s/it]  7%|▋         | 1241/17834 [34:46<7:42:50,  1.67s/it]  7%|▋         | 1242/17834 [34:48<7:46:17,  1.69s/it]  7%|▋         | 1243/17834 [34:50<7:41:33,  1.67s/it]  7%|▋         | 1244/17834 [34:51<7:41:26,  1.67s/it]  7%|▋         | 1245/17834 [34:53<7:44:09,  1.68s/it]  7%|▋         | 1246/17834 [34:55<7:41:08,  1.67s/it]  7%|▋         | 1247/17834 [34:56<7:42:25,  1.67s/it]  7%|▋         | 1248/17834 [34:58<7:42:46,  1.67s/it]  7%|▋         | 1249/17834 [35:00<7:47:44,  1.69s/it]08/30/2024 19:49:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1941677331924438, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029973888769745827, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1900675296783447, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4142091274261475}
  7%|▋         | 1250/17834 [35:02<7:52:05,  1.71s/it]  7%|▋         | 1251/17834 [35:03<7:49:56,  1.70s/it]  7%|▋         | 1252/17834 [35:05<7:43:02,  1.68s/it]  7%|▋         | 1253/17834 [35:07<7:46:10,  1.69s/it]  7%|▋         | 1254/17834 [35:08<7:41:24,  1.67s/it]  7%|▋         | 1255/17834 [35:10<7:38:29,  1.66s/it]  7%|▋         | 1256/17834 [35:12<7:37:49,  1.66s/it]  7%|▋         | 1257/17834 [35:13<7:31:13,  1.63s/it]  7%|▋         | 1258/17834 [35:15<7:36:25,  1.65s/it]  7%|▋         | 1259/17834 [35:17<7:43:28,  1.68s/it]  7%|▋         | 1260/17834 [35:18<7:45:17,  1.68s/it]  7%|▋         | 1261/17834 [35:20<7:47:07,  1.69s/it]  7%|▋         | 1262/17834 [35:22<7:39:39,  1.66s/it]  7%|▋         | 1263/17834 [35:23<7:35:41,  1.65s/it]  7%|▋         | 1264/17834 [35:25<7:34:03,  1.64s/it]  7%|▋         | 1265/17834 [35:27<7:37:25,  1.66s/it]  7%|▋         | 1266/17834 [35:28<7:37:38,  1.66s/it]  7%|▋         | 1267/17834 [35:30<7:41:19,  1.67s/it]  7%|▋         | 1268/17834 [35:32<7:43:50,  1.68s/it]  7%|▋         | 1269/17834 [35:33<7:43:29,  1.68s/it]  7%|▋         | 1270/17834 [35:35<7:44:24,  1.68s/it]  7%|▋         | 1271/17834 [35:37<7:43:40,  1.68s/it]  7%|▋         | 1272/17834 [35:38<7:41:59,  1.67s/it]  7%|▋         | 1273/17834 [35:40<7:42:52,  1.68s/it]  7%|▋         | 1274/17834 [35:42<7:49:28,  1.70s/it]  7%|▋         | 1275/17834 [35:43<7:52:40,  1.71s/it]  7%|▋         | 1276/17834 [35:45<7:47:27,  1.69s/it]  7%|▋         | 1277/17834 [35:47<7:44:28,  1.68s/it]  7%|▋         | 1278/17834 [35:48<7:42:22,  1.68s/it]  7%|▋         | 1279/17834 [35:50<7:39:05,  1.66s/it]  7%|▋         | 1280/17834 [35:52<7:48:11,  1.70s/it]  7%|▋         | 1281/17834 [35:54<7:47:56,  1.70s/it]  7%|▋         | 1282/17834 [35:55<7:46:57,  1.69s/it]  7%|▋         | 1283/17834 [35:57<7:42:36,  1.68s/it]  7%|▋         | 1284/17834 [35:59<7:40:53,  1.67s/it]  7%|▋         | 1285/17834 [36:00<7:39:55,  1.67s/it]  7%|▋         | 1286/17834 [36:02<7:41:02,  1.67s/it]  7%|▋         | 1287/17834 [36:04<7:38:22,  1.66s/it]  7%|▋         | 1288/17834 [36:05<7:42:25,  1.68s/it]  7%|▋         | 1289/17834 [36:07<7:44:48,  1.69s/it]  7%|▋         | 1290/17834 [36:09<7:41:25,  1.67s/it]  7%|▋         | 1291/17834 [36:10<7:40:32,  1.67s/it]  7%|▋         | 1292/17834 [36:12<7:37:25,  1.66s/it]  7%|▋         | 1293/17834 [36:14<7:38:11,  1.66s/it]  7%|▋         | 1294/17834 [36:15<7:38:39,  1.66s/it]  7%|▋         | 1295/17834 [36:17<7:39:25,  1.67s/it]  7%|▋         | 1296/17834 [36:19<7:47:42,  1.70s/it]  7%|▋         | 1297/17834 [36:20<7:47:55,  1.70s/it]  7%|▋         | 1298/17834 [36:22<7:50:46,  1.71s/it]  7%|▋         | 1299/17834 [36:24<7:47:21,  1.70s/it]08/30/2024 19:50:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.089831829071045, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.054589226841926575, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3898489475250244, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.534270286560059}
  7%|▋         | 1300/17834 [36:26<7:52:47,  1.72s/it]  7%|▋         | 1301/17834 [36:27<7:50:29,  1.71s/it]  7%|▋         | 1302/17834 [36:29<7:56:43,  1.73s/it]  7%|▋         | 1303/17834 [36:31<7:50:31,  1.71s/it]  7%|▋         | 1304/17834 [36:32<7:44:33,  1.69s/it]  7%|▋         | 1305/17834 [36:34<7:39:17,  1.67s/it]  7%|▋         | 1306/17834 [36:36<7:43:57,  1.68s/it]  7%|▋         | 1307/17834 [36:37<7:41:43,  1.68s/it]  7%|▋         | 1308/17834 [36:39<7:41:26,  1.68s/it]  7%|▋         | 1309/17834 [36:41<7:37:12,  1.66s/it]  7%|▋         | 1310/17834 [36:42<7:33:46,  1.65s/it]  7%|▋         | 1311/17834 [36:44<7:32:41,  1.64s/it]  7%|▋         | 1312/17834 [36:45<7:32:51,  1.64s/it]  7%|▋         | 1313/17834 [36:47<7:37:31,  1.66s/it]  7%|▋         | 1314/17834 [36:49<7:40:38,  1.67s/it]  7%|▋         | 1315/17834 [36:51<7:42:55,  1.68s/it]  7%|▋         | 1316/17834 [36:52<7:44:49,  1.69s/it]  7%|▋         | 1317/17834 [36:54<7:45:13,  1.69s/it]  7%|▋         | 1318/17834 [36:56<7:42:25,  1.68s/it]  7%|▋         | 1319/17834 [36:57<7:43:25,  1.68s/it]  7%|▋         | 1320/17834 [36:59<7:42:34,  1.68s/it]  7%|▋         | 1321/17834 [37:01<7:39:16,  1.67s/it]  7%|▋         | 1322/17834 [37:02<7:37:40,  1.66s/it]  7%|▋         | 1323/17834 [37:04<7:35:47,  1.66s/it]  7%|▋         | 1324/17834 [37:06<7:38:38,  1.67s/it]  7%|▋         | 1325/17834 [37:07<7:41:04,  1.68s/it]  7%|▋         | 1326/17834 [37:09<7:38:02,  1.66s/it]  7%|▋         | 1327/17834 [37:11<7:38:33,  1.67s/it]  7%|▋         | 1328/17834 [37:12<7:31:57,  1.64s/it]  7%|▋         | 1329/17834 [37:14<7:31:18,  1.64s/it]  7%|▋         | 1330/17834 [37:16<7:35:08,  1.65s/it]  7%|▋         | 1331/17834 [37:17<7:33:28,  1.65s/it]  7%|▋         | 1332/17834 [37:19<7:34:04,  1.65s/it]  7%|▋         | 1333/17834 [37:21<7:36:56,  1.66s/it]  7%|▋         | 1334/17834 [37:22<7:41:54,  1.68s/it]  7%|▋         | 1335/17834 [37:24<7:44:33,  1.69s/it]  7%|▋         | 1336/17834 [37:26<7:50:05,  1.71s/it]  7%|▋         | 1337/17834 [37:27<7:55:19,  1.73s/it]  8%|▊         | 1338/17834 [37:29<7:49:01,  1.71s/it]  8%|▊         | 1339/17834 [37:31<7:44:56,  1.69s/it]  8%|▊         | 1340/17834 [37:32<7:45:14,  1.69s/it]  8%|▊         | 1341/17834 [37:34<7:44:55,  1.69s/it]  8%|▊         | 1342/17834 [37:36<7:45:29,  1.69s/it]  8%|▊         | 1343/17834 [37:38<7:43:35,  1.69s/it]  8%|▊         | 1344/17834 [37:39<7:46:29,  1.70s/it]  8%|▊         | 1345/17834 [37:41<7:43:56,  1.69s/it]  8%|▊         | 1346/17834 [37:43<7:47:13,  1.70s/it]  8%|▊         | 1347/17834 [37:44<7:42:47,  1.68s/it]  8%|▊         | 1348/17834 [37:46<7:42:59,  1.69s/it]  8%|▊         | 1349/17834 [37:48<7:42:12,  1.68s/it]08/30/2024 19:52:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4759036302566528, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04013853892683983, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2213025093078613, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.737344741821289}
  8%|▊         | 1350/17834 [37:49<7:42:52,  1.68s/it]  8%|▊         | 1351/17834 [37:51<7:43:17,  1.69s/it]  8%|▊         | 1352/17834 [37:53<7:45:41,  1.70s/it]  8%|▊         | 1353/17834 [37:54<7:40:20,  1.68s/it]  8%|▊         | 1354/17834 [37:56<7:37:37,  1.67s/it]  8%|▊         | 1355/17834 [37:58<7:48:42,  1.71s/it]  8%|▊         | 1356/17834 [38:00<7:48:18,  1.71s/it]  8%|▊         | 1357/17834 [38:01<7:42:17,  1.68s/it]  8%|▊         | 1358/17834 [38:03<7:36:58,  1.66s/it]  8%|▊         | 1359/17834 [38:04<7:39:44,  1.67s/it]  8%|▊         | 1360/17834 [38:06<7:35:41,  1.66s/it]  8%|▊         | 1361/17834 [38:08<7:37:13,  1.67s/it]  8%|▊         | 1362/17834 [38:09<7:35:39,  1.66s/it]  8%|▊         | 1363/17834 [38:11<7:34:22,  1.66s/it]  8%|▊         | 1364/17834 [38:13<7:33:43,  1.65s/it]  8%|▊         | 1365/17834 [38:14<7:34:19,  1.66s/it]  8%|▊         | 1366/17834 [38:16<7:36:53,  1.66s/it]  8%|▊         | 1367/17834 [38:18<7:41:09,  1.68s/it]  8%|▊         | 1368/17834 [38:20<7:45:21,  1.70s/it]  8%|▊         | 1369/17834 [38:21<7:40:55,  1.68s/it]  8%|▊         | 1370/17834 [38:23<7:40:29,  1.68s/it]  8%|▊         | 1371/17834 [38:25<7:43:21,  1.69s/it]  8%|▊         | 1372/17834 [38:26<7:45:18,  1.70s/it]  8%|▊         | 1373/17834 [38:28<7:37:41,  1.67s/it]  8%|▊         | 1374/17834 [38:30<7:36:06,  1.66s/it]  8%|▊         | 1375/17834 [38:31<7:40:06,  1.68s/it]  8%|▊         | 1376/17834 [38:33<7:35:52,  1.66s/it]  8%|▊         | 1377/17834 [38:35<7:38:40,  1.67s/it]  8%|▊         | 1378/17834 [38:36<7:39:01,  1.67s/it]  8%|▊         | 1379/17834 [38:38<7:35:06,  1.66s/it]  8%|▊         | 1380/17834 [38:40<7:38:19,  1.67s/it]  8%|▊         | 1381/17834 [38:41<7:39:14,  1.67s/it]  8%|▊         | 1382/17834 [38:43<7:41:02,  1.68s/it]  8%|▊         | 1383/17834 [38:45<7:36:29,  1.66s/it]  8%|▊         | 1384/17834 [38:46<7:43:45,  1.69s/it]  8%|▊         | 1385/17834 [38:48<7:43:34,  1.69s/it]  8%|▊         | 1386/17834 [38:50<7:45:13,  1.70s/it]  8%|▊         | 1387/17834 [38:51<7:38:25,  1.67s/it]  8%|▊         | 1388/17834 [38:53<7:45:33,  1.70s/it]  8%|▊         | 1389/17834 [38:55<7:41:31,  1.68s/it]  8%|▊         | 1390/17834 [38:56<7:38:39,  1.67s/it]  8%|▊         | 1391/17834 [38:58<7:37:34,  1.67s/it]  8%|▊         | 1392/17834 [39:00<7:32:18,  1.65s/it]  8%|▊         | 1393/17834 [39:01<7:34:14,  1.66s/it]  8%|▊         | 1394/17834 [39:03<7:29:12,  1.64s/it]  8%|▊         | 1395/17834 [39:05<7:39:45,  1.68s/it]  8%|▊         | 1396/17834 [39:06<7:42:59,  1.69s/it]  8%|▊         | 1397/17834 [39:08<7:40:03,  1.68s/it]  8%|▊         | 1398/17834 [39:10<7:45:32,  1.70s/it]  8%|▊         | 1399/17834 [39:12<7:44:48,  1.70s/it]08/30/2024 19:53:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2733831405639648, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.033137671649456024, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.203533411026001, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.510054111480713}
  8%|▊         | 1400/17834 [39:13<7:38:12,  1.67s/it]  8%|▊         | 1401/17834 [39:15<7:48:04,  1.71s/it]  8%|▊         | 1402/17834 [39:17<7:40:15,  1.68s/it]  8%|▊         | 1403/17834 [39:18<7:37:33,  1.67s/it]  8%|▊         | 1404/17834 [39:20<7:48:46,  1.71s/it]  8%|▊         | 1405/17834 [39:22<7:49:45,  1.72s/it]  8%|▊         | 1406/17834 [39:23<7:45:30,  1.70s/it]  8%|▊         | 1407/17834 [39:25<7:46:48,  1.71s/it]  8%|▊         | 1408/17834 [39:27<7:50:40,  1.72s/it]  8%|▊         | 1409/17834 [39:28<7:42:54,  1.69s/it]  8%|▊         | 1410/17834 [39:30<7:47:14,  1.71s/it]  8%|▊         | 1411/17834 [39:32<7:42:49,  1.69s/it]  8%|▊         | 1412/17834 [39:34<7:42:45,  1.69s/it]  8%|▊         | 1413/17834 [39:35<7:37:32,  1.67s/it]  8%|▊         | 1414/17834 [39:37<7:38:21,  1.67s/it]  8%|▊         | 1415/17834 [39:39<7:41:38,  1.69s/it]  8%|▊         | 1416/17834 [39:40<7:42:03,  1.69s/it]  8%|▊         | 1417/17834 [39:42<7:40:06,  1.68s/it]  8%|▊         | 1418/17834 [39:44<7:38:46,  1.68s/it]  8%|▊         | 1419/17834 [39:45<7:43:11,  1.69s/it]  8%|▊         | 1420/17834 [39:47<7:40:48,  1.68s/it]  8%|▊         | 1421/17834 [39:49<7:34:43,  1.66s/it]  8%|▊         | 1422/17834 [39:50<7:36:15,  1.67s/it]  8%|▊         | 1423/17834 [39:52<7:39:17,  1.68s/it]  8%|▊         | 1424/17834 [39:54<7:44:33,  1.70s/it]  8%|▊         | 1425/17834 [39:55<7:47:26,  1.71s/it]  8%|▊         | 1426/17834 [39:57<7:44:55,  1.70s/it]  8%|▊         | 1427/17834 [39:59<7:42:09,  1.69s/it]  8%|▊         | 1428/17834 [40:01<7:41:06,  1.69s/it]  8%|▊         | 1429/17834 [40:02<7:50:00,  1.72s/it]  8%|▊         | 1430/17834 [40:04<7:49:47,  1.72s/it]  8%|▊         | 1431/17834 [40:06<7:46:27,  1.71s/it]  8%|▊         | 1432/17834 [40:07<7:44:02,  1.70s/it]  8%|▊         | 1433/17834 [40:09<7:44:42,  1.70s/it]  8%|▊         | 1434/17834 [40:11<7:41:12,  1.69s/it]  8%|▊         | 1435/17834 [40:12<7:42:50,  1.69s/it]  8%|▊         | 1436/17834 [40:14<7:35:53,  1.67s/it]  8%|▊         | 1437/17834 [40:16<7:40:35,  1.69s/it]  8%|▊         | 1438/17834 [40:17<7:37:59,  1.68s/it]  8%|▊         | 1439/17834 [40:19<7:33:48,  1.66s/it]  8%|▊         | 1440/17834 [40:21<7:38:38,  1.68s/it]  8%|▊         | 1441/17834 [40:22<7:37:02,  1.67s/it]  8%|▊         | 1442/17834 [40:24<7:36:20,  1.67s/it]  8%|▊         | 1443/17834 [40:26<7:33:06,  1.66s/it]  8%|▊         | 1444/17834 [40:27<7:33:55,  1.66s/it]  8%|▊         | 1445/17834 [40:29<7:33:53,  1.66s/it]  8%|▊         | 1446/17834 [40:31<7:31:52,  1.65s/it]  8%|▊         | 1447/17834 [40:32<7:33:18,  1.66s/it]  8%|▊         | 1448/17834 [40:34<7:36:06,  1.67s/it]  8%|▊         | 1449/17834 [40:36<7:37:04,  1.67s/it]08/30/2024 19:54:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6041778326034546, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04062410816550255, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.370630979537964, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.015432834625244}
  8%|▊         | 1450/17834 [40:37<7:38:33,  1.68s/it]  8%|▊         | 1451/17834 [40:39<7:39:02,  1.68s/it]  8%|▊         | 1452/17834 [40:41<7:38:11,  1.68s/it]  8%|▊         | 1453/17834 [40:42<7:36:20,  1.67s/it]  8%|▊         | 1454/17834 [40:44<7:39:03,  1.68s/it]  8%|▊         | 1455/17834 [40:46<7:36:48,  1.67s/it]  8%|▊         | 1456/17834 [40:48<7:38:25,  1.68s/it]  8%|▊         | 1457/17834 [40:49<7:39:52,  1.68s/it]  8%|▊         | 1458/17834 [40:51<7:38:37,  1.68s/it]  8%|▊         | 1459/17834 [40:53<7:41:40,  1.69s/it]  8%|▊         | 1460/17834 [40:54<7:35:46,  1.67s/it]  8%|▊         | 1461/17834 [40:56<7:34:14,  1.66s/it]  8%|▊         | 1462/17834 [40:58<7:33:08,  1.66s/it]  8%|▊         | 1463/17834 [40:59<7:31:36,  1.66s/it]  8%|▊         | 1464/17834 [41:01<7:40:45,  1.69s/it]  8%|▊         | 1465/17834 [41:03<7:41:16,  1.69s/it]  8%|▊         | 1466/17834 [41:04<7:45:55,  1.71s/it]  8%|▊         | 1467/17834 [41:06<7:40:03,  1.69s/it]  8%|▊         | 1468/17834 [41:08<7:36:29,  1.67s/it]  8%|▊         | 1469/17834 [41:09<7:39:38,  1.69s/it]  8%|▊         | 1470/17834 [41:11<7:39:57,  1.69s/it]  8%|▊         | 1471/17834 [41:13<7:42:59,  1.70s/it]  8%|▊         | 1472/17834 [41:14<7:41:56,  1.69s/it]  8%|▊         | 1473/17834 [41:16<7:38:59,  1.68s/it]  8%|▊         | 1474/17834 [41:18<7:41:26,  1.69s/it]  8%|▊         | 1475/17834 [41:20<7:42:36,  1.70s/it]  8%|▊         | 1476/17834 [41:21<7:42:13,  1.70s/it]  8%|▊         | 1477/17834 [41:23<7:41:17,  1.69s/it]  8%|▊         | 1478/17834 [41:25<7:36:03,  1.67s/it]  8%|▊         | 1479/17834 [41:26<7:39:24,  1.69s/it]  8%|▊         | 1480/17834 [41:28<7:41:05,  1.69s/it]  8%|▊         | 1481/17834 [41:30<7:36:44,  1.68s/it]  8%|▊         | 1482/17834 [41:31<7:46:48,  1.71s/it]  8%|▊         | 1483/17834 [41:33<7:40:44,  1.69s/it]  8%|▊         | 1484/17834 [41:35<7:44:26,  1.70s/it]  8%|▊         | 1485/17834 [41:37<7:48:11,  1.72s/it]  8%|▊         | 1486/17834 [41:38<7:41:17,  1.69s/it]  8%|▊         | 1487/17834 [41:40<7:42:04,  1.70s/it]  8%|▊         | 1488/17834 [41:42<7:41:02,  1.69s/it]  8%|▊         | 1489/17834 [41:43<7:39:46,  1.69s/it]  8%|▊         | 1490/17834 [41:45<7:38:04,  1.68s/it]  8%|▊         | 1491/17834 [41:47<7:40:01,  1.69s/it]  8%|▊         | 1492/17834 [41:48<7:42:21,  1.70s/it]  8%|▊         | 1493/17834 [41:50<7:39:06,  1.69s/it]  8%|▊         | 1494/17834 [41:52<7:35:06,  1.67s/it]  8%|▊         | 1495/17834 [41:53<7:33:19,  1.66s/it]  8%|▊         | 1496/17834 [41:55<7:37:18,  1.68s/it]  8%|▊         | 1497/17834 [41:57<7:39:27,  1.69s/it]  8%|▊         | 1498/17834 [41:58<7:45:29,  1.71s/it]  8%|▊         | 1499/17834 [42:00<7:43:38,  1.70s/it]08/30/2024 19:56:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.098062038421631, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05899340659379959, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.7807416915893555, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.9377970695495605}
  8%|▊         | 1500/17834 [42:02<7:36:30,  1.68s/it]  8%|▊         | 1501/17834 [42:04<7:49:39,  1.73s/it]  8%|▊         | 1502/17834 [42:05<7:41:08,  1.69s/it]  8%|▊         | 1503/17834 [42:07<7:37:09,  1.68s/it]  8%|▊         | 1504/17834 [42:09<7:37:39,  1.68s/it]  8%|▊         | 1505/17834 [42:10<7:47:50,  1.72s/it]  8%|▊         | 1506/17834 [42:12<7:45:17,  1.71s/it]  8%|▊         | 1507/17834 [42:14<7:37:04,  1.68s/it]  8%|▊         | 1508/17834 [42:15<7:35:20,  1.67s/it]  8%|▊         | 1509/17834 [42:17<7:37:47,  1.68s/it]  8%|▊         | 1510/17834 [42:19<7:31:37,  1.66s/it]  8%|▊         | 1511/17834 [42:20<7:28:57,  1.65s/it]  8%|▊         | 1512/17834 [42:22<7:32:04,  1.66s/it]  8%|▊         | 1513/17834 [42:24<7:35:33,  1.67s/it]  8%|▊         | 1514/17834 [42:25<7:32:40,  1.66s/it]  8%|▊         | 1515/17834 [42:27<7:40:18,  1.69s/it]  9%|▊         | 1516/17834 [42:29<7:36:33,  1.68s/it]  9%|▊         | 1517/17834 [42:30<7:34:06,  1.67s/it]  9%|▊         | 1518/17834 [42:32<7:32:20,  1.66s/it]  9%|▊         | 1519/17834 [42:34<7:29:28,  1.65s/it]  9%|▊         | 1520/17834 [42:35<7:32:00,  1.66s/it]  9%|▊         | 1521/17834 [42:37<7:29:40,  1.65s/it]  9%|▊         | 1522/17834 [42:39<7:33:54,  1.67s/it]  9%|▊         | 1523/17834 [42:40<7:31:18,  1.66s/it]  9%|▊         | 1524/17834 [42:42<7:28:49,  1.65s/it]  9%|▊         | 1525/17834 [42:44<7:31:01,  1.66s/it]  9%|▊         | 1526/17834 [42:45<7:34:31,  1.67s/it]  9%|▊         | 1527/17834 [42:47<7:32:49,  1.67s/it]  9%|▊         | 1528/17834 [42:49<7:38:12,  1.69s/it]  9%|▊         | 1529/17834 [42:50<7:38:24,  1.69s/it]  9%|▊         | 1530/17834 [42:52<7:36:38,  1.68s/it]  9%|▊         | 1531/17834 [42:54<7:32:06,  1.66s/it]  9%|▊         | 1532/17834 [42:55<7:38:24,  1.69s/it]  9%|▊         | 1533/17834 [42:57<7:32:02,  1.66s/it]  9%|▊         | 1534/17834 [42:59<7:43:36,  1.71s/it]  9%|▊         | 1535/17834 [43:00<7:42:42,  1.70s/it]  9%|▊         | 1536/17834 [43:02<7:39:49,  1.69s/it]  9%|▊         | 1537/17834 [43:04<7:41:09,  1.70s/it]  9%|▊         | 1538/17834 [43:06<7:38:44,  1.69s/it]  9%|▊         | 1539/17834 [43:07<7:36:09,  1.68s/it]  9%|▊         | 1540/17834 [43:09<7:40:56,  1.70s/it]  9%|▊         | 1541/17834 [43:11<7:46:15,  1.72s/it]  9%|▊         | 1542/17834 [43:12<7:41:00,  1.70s/it]  9%|▊         | 1543/17834 [43:14<7:38:51,  1.69s/it]  9%|▊         | 1544/17834 [43:16<7:40:14,  1.70s/it]  9%|▊         | 1545/17834 [43:17<7:38:18,  1.69s/it]  9%|▊         | 1546/17834 [43:19<7:36:18,  1.68s/it]  9%|▊         | 1547/17834 [43:21<7:32:22,  1.67s/it]  9%|▊         | 1548/17834 [43:22<7:38:03,  1.69s/it]  9%|▊         | 1549/17834 [43:24<7:34:25,  1.67s/it]08/30/2024 19:57:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5450105667114258, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.041772082448005676, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.178849697113037, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.765632390975952}
  9%|▊         | 1550/17834 [43:26<7:34:37,  1.68s/it]  9%|▊         | 1551/17834 [43:27<7:35:20,  1.68s/it]  9%|▊         | 1552/17834 [43:29<7:29:35,  1.66s/it]  9%|▊         | 1553/17834 [43:31<7:30:35,  1.66s/it]  9%|▊         | 1554/17834 [43:32<7:34:37,  1.68s/it]  9%|▊         | 1555/17834 [43:34<7:34:13,  1.67s/it]  9%|▊         | 1556/17834 [43:36<7:43:10,  1.71s/it]  9%|▊         | 1557/17834 [43:37<7:33:47,  1.67s/it]  9%|▊         | 1558/17834 [43:39<7:34:35,  1.68s/it]  9%|▊         | 1559/17834 [43:41<7:32:26,  1.67s/it]  9%|▊         | 1560/17834 [43:43<7:44:09,  1.71s/it]  9%|▉         | 1561/17834 [43:44<7:38:42,  1.69s/it]  9%|▉         | 1562/17834 [43:46<7:37:06,  1.69s/it]  9%|▉         | 1563/17834 [43:48<7:36:15,  1.68s/it]  9%|▉         | 1564/17834 [43:49<7:40:37,  1.70s/it]  9%|▉         | 1565/17834 [43:51<7:39:52,  1.70s/it]  9%|▉         | 1566/17834 [43:53<7:35:23,  1.68s/it]  9%|▉         | 1567/17834 [43:54<7:33:24,  1.67s/it]  9%|▉         | 1568/17834 [43:56<7:33:06,  1.67s/it]  9%|▉         | 1569/17834 [43:58<7:29:29,  1.66s/it]  9%|▉         | 1570/17834 [43:59<7:27:59,  1.65s/it]  9%|▉         | 1571/17834 [44:01<7:25:46,  1.64s/it]  9%|▉         | 1572/17834 [44:03<7:30:15,  1.66s/it]  9%|▉         | 1573/17834 [44:04<7:26:26,  1.65s/it]  9%|▉         | 1574/17834 [44:06<7:28:29,  1.65s/it]  9%|▉         | 1575/17834 [44:08<7:30:40,  1.66s/it]  9%|▉         | 1576/17834 [44:09<7:31:28,  1.67s/it]  9%|▉         | 1577/17834 [44:11<7:38:18,  1.69s/it]  9%|▉         | 1578/17834 [44:13<7:34:37,  1.68s/it]  9%|▉         | 1579/17834 [44:14<7:35:59,  1.68s/it]  9%|▉         | 1580/17834 [44:16<7:36:54,  1.69s/it]  9%|▉         | 1581/17834 [44:18<7:34:28,  1.68s/it]  9%|▉         | 1582/17834 [44:19<7:40:37,  1.70s/it]  9%|▉         | 1583/17834 [44:21<7:37:19,  1.69s/it]  9%|▉         | 1584/17834 [44:23<7:40:49,  1.70s/it]  9%|▉         | 1585/17834 [44:24<7:34:52,  1.68s/it]  9%|▉         | 1586/17834 [44:26<7:37:57,  1.69s/it]  9%|▉         | 1587/17834 [44:28<7:37:54,  1.69s/it]  9%|▉         | 1588/17834 [44:29<7:30:14,  1.66s/it]  9%|▉         | 1589/17834 [44:31<7:26:53,  1.65s/it]  9%|▉         | 1590/17834 [44:33<7:31:36,  1.67s/it]  9%|▉         | 1591/17834 [44:35<7:38:21,  1.69s/it]  9%|▉         | 1592/17834 [44:36<7:35:55,  1.68s/it]  9%|▉         | 1593/17834 [44:38<7:35:49,  1.68s/it]  9%|▉         | 1594/17834 [44:40<7:35:11,  1.68s/it]  9%|▉         | 1595/17834 [44:41<7:33:25,  1.68s/it]  9%|▉         | 1596/17834 [44:43<7:35:52,  1.68s/it]  9%|▉         | 1597/17834 [44:45<7:41:24,  1.71s/it]  9%|▉         | 1598/17834 [44:46<7:33:49,  1.68s/it]  9%|▉         | 1599/17834 [44:48<7:35:35,  1.68s/it]08/30/2024 19:59:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.385497808456421, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04624410346150398, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3257362842559814, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7574782371520996}
  9%|▉         | 1600/17834 [44:50<7:37:13,  1.69s/it]  9%|▉         | 1601/17834 [44:51<7:37:19,  1.69s/it]  9%|▉         | 1602/17834 [44:53<7:37:07,  1.69s/it]  9%|▉         | 1603/17834 [44:55<7:33:30,  1.68s/it]  9%|▉         | 1604/17834 [44:56<7:39:03,  1.70s/it]  9%|▉         | 1605/17834 [44:58<7:37:20,  1.69s/it]  9%|▉         | 1606/17834 [45:00<7:38:44,  1.70s/it]  9%|▉         | 1607/17834 [45:01<7:32:48,  1.67s/it]  9%|▉         | 1608/17834 [45:03<7:37:19,  1.69s/it]  9%|▉         | 1609/17834 [45:05<7:34:04,  1.68s/it]  9%|▉         | 1610/17834 [45:07<7:34:08,  1.68s/it]  9%|▉         | 1611/17834 [45:08<7:42:06,  1.71s/it]  9%|▉         | 1612/17834 [45:10<7:36:46,  1.69s/it]  9%|▉         | 1613/17834 [45:12<7:39:41,  1.70s/it]  9%|▉         | 1614/17834 [45:13<7:38:50,  1.70s/it]  9%|▉         | 1615/17834 [45:15<7:35:25,  1.68s/it]  9%|▉         | 1616/17834 [45:17<7:31:08,  1.67s/it]  9%|▉         | 1617/17834 [45:18<7:37:35,  1.69s/it]  9%|▉         | 1618/17834 [45:20<7:34:07,  1.68s/it]  9%|▉         | 1619/17834 [45:22<7:32:42,  1.68s/it]  9%|▉         | 1620/17834 [45:23<7:35:49,  1.69s/it]  9%|▉         | 1621/17834 [45:25<7:37:34,  1.69s/it]  9%|▉         | 1622/17834 [45:27<7:35:14,  1.68s/it]  9%|▉         | 1623/17834 [45:29<7:40:26,  1.70s/it]  9%|▉         | 1624/17834 [45:30<7:38:16,  1.70s/it]  9%|▉         | 1625/17834 [45:32<7:41:36,  1.71s/it]  9%|▉         | 1626/17834 [45:34<7:43:36,  1.72s/it]  9%|▉         | 1627/17834 [45:35<7:40:55,  1.71s/it]  9%|▉         | 1628/17834 [45:37<7:37:51,  1.70s/it]  9%|▉         | 1629/17834 [45:39<7:36:59,  1.69s/it]  9%|▉         | 1630/17834 [45:40<7:38:54,  1.70s/it]  9%|▉         | 1631/17834 [45:42<7:34:37,  1.68s/it]  9%|▉         | 1632/17834 [45:44<7:31:36,  1.67s/it]  9%|▉         | 1633/17834 [45:46<7:43:53,  1.72s/it]  9%|▉         | 1634/17834 [45:47<7:36:29,  1.69s/it]  9%|▉         | 1635/17834 [45:49<7:33:15,  1.68s/it]  9%|▉         | 1636/17834 [45:51<7:38:30,  1.70s/it]  9%|▉         | 1637/17834 [45:52<7:39:11,  1.70s/it]  9%|▉         | 1638/17834 [45:54<7:39:00,  1.70s/it]  9%|▉         | 1639/17834 [45:56<7:35:50,  1.69s/it]  9%|▉         | 1640/17834 [45:57<7:34:01,  1.68s/it]  9%|▉         | 1641/17834 [45:59<7:33:22,  1.68s/it]  9%|▉         | 1642/17834 [46:01<7:31:16,  1.67s/it]  9%|▉         | 1643/17834 [46:02<7:32:03,  1.68s/it]  9%|▉         | 1644/17834 [46:04<7:34:34,  1.68s/it]  9%|▉         | 1645/17834 [46:06<7:37:40,  1.70s/it]  9%|▉         | 1646/17834 [46:07<7:35:43,  1.69s/it]  9%|▉         | 1647/17834 [46:09<7:34:12,  1.68s/it]  9%|▉         | 1648/17834 [46:11<7:34:07,  1.68s/it]  9%|▉         | 1649/17834 [46:13<7:40:07,  1.71s/it]08/30/2024 20:00:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.1168055534362793, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.061841271817684174, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.567519426345825, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.746166229248047}
  9%|▉         | 1650/17834 [46:14<7:46:00,  1.73s/it]  9%|▉         | 1651/17834 [46:16<7:46:55,  1.73s/it]  9%|▉         | 1652/17834 [46:18<7:41:25,  1.71s/it]  9%|▉         | 1653/17834 [46:19<7:39:20,  1.70s/it]  9%|▉         | 1654/17834 [46:21<7:36:34,  1.69s/it]  9%|▉         | 1655/17834 [46:23<7:39:09,  1.70s/it]  9%|▉         | 1656/17834 [46:25<7:41:36,  1.71s/it]  9%|▉         | 1657/17834 [46:26<7:34:42,  1.69s/it]  9%|▉         | 1658/17834 [46:28<7:34:17,  1.69s/it]  9%|▉         | 1659/17834 [46:30<7:34:05,  1.68s/it]  9%|▉         | 1660/17834 [46:31<7:33:34,  1.68s/it]  9%|▉         | 1661/17834 [46:33<7:34:19,  1.69s/it]  9%|▉         | 1662/17834 [46:35<7:33:46,  1.68s/it]  9%|▉         | 1663/17834 [46:36<7:29:19,  1.67s/it]  9%|▉         | 1664/17834 [46:38<7:31:12,  1.67s/it]  9%|▉         | 1665/17834 [46:40<7:39:15,  1.70s/it]  9%|▉         | 1666/17834 [46:41<7:36:59,  1.70s/it]  9%|▉         | 1667/17834 [46:43<7:40:38,  1.71s/it]  9%|▉         | 1668/17834 [46:45<7:41:43,  1.71s/it]  9%|▉         | 1669/17834 [46:47<7:40:06,  1.71s/it]  9%|▉         | 1670/17834 [46:48<7:38:39,  1.70s/it]  9%|▉         | 1671/17834 [46:50<7:30:47,  1.67s/it]  9%|▉         | 1672/17834 [46:52<7:35:52,  1.69s/it]  9%|▉         | 1673/17834 [46:53<7:33:46,  1.68s/it]  9%|▉         | 1674/17834 [46:55<7:32:22,  1.68s/it]  9%|▉         | 1675/17834 [46:57<7:36:52,  1.70s/it]  9%|▉         | 1676/17834 [46:58<7:32:16,  1.68s/it]  9%|▉         | 1677/17834 [47:00<7:35:39,  1.69s/it]  9%|▉         | 1678/17834 [47:02<7:33:21,  1.68s/it]  9%|▉         | 1679/17834 [47:03<7:27:26,  1.66s/it]  9%|▉         | 1680/17834 [47:05<7:25:05,  1.65s/it]  9%|▉         | 1681/17834 [47:07<7:26:11,  1.66s/it]  9%|▉         | 1682/17834 [47:08<7:28:53,  1.67s/it]  9%|▉         | 1683/17834 [47:10<7:25:17,  1.65s/it]  9%|▉         | 1684/17834 [47:12<7:22:12,  1.64s/it]  9%|▉         | 1685/17834 [47:13<7:22:59,  1.65s/it]  9%|▉         | 1686/17834 [47:15<7:26:26,  1.66s/it]  9%|▉         | 1687/17834 [47:17<7:33:28,  1.69s/it]  9%|▉         | 1688/17834 [47:18<7:35:57,  1.69s/it]  9%|▉         | 1689/17834 [47:20<7:36:32,  1.70s/it]  9%|▉         | 1690/17834 [47:22<7:38:44,  1.70s/it]  9%|▉         | 1691/17834 [47:23<7:39:26,  1.71s/it]  9%|▉         | 1692/17834 [47:25<7:38:15,  1.70s/it]  9%|▉         | 1693/17834 [47:27<7:38:33,  1.70s/it]  9%|▉         | 1694/17834 [47:29<7:34:19,  1.69s/it] 10%|▉         | 1695/17834 [47:30<7:31:49,  1.68s/it] 10%|▉         | 1696/17834 [47:32<7:29:57,  1.67s/it] 10%|▉         | 1697/17834 [47:33<7:28:12,  1.67s/it] 10%|▉         | 1698/17834 [47:35<7:26:23,  1.66s/it] 10%|▉         | 1699/17834 [47:37<7:34:19,  1.69s/it]08/30/2024 20:01:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6498539447784424, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04428809881210327, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.273240566253662, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9673826694488525}
 10%|▉         | 1700/17834 [47:38<7:27:57,  1.67s/it] 10%|▉         | 1701/17834 [47:40<7:25:21,  1.66s/it] 10%|▉         | 1702/17834 [47:42<7:29:58,  1.67s/it] 10%|▉         | 1703/17834 [47:44<7:30:56,  1.68s/it] 10%|▉         | 1704/17834 [47:45<7:32:01,  1.68s/it] 10%|▉         | 1705/17834 [47:47<7:29:29,  1.67s/it] 10%|▉         | 1706/17834 [47:49<7:27:27,  1.66s/it] 10%|▉         | 1707/17834 [47:50<7:37:41,  1.70s/it] 10%|▉         | 1708/17834 [47:52<7:38:01,  1.70s/it] 10%|▉         | 1709/17834 [47:54<7:32:53,  1.69s/it] 10%|▉         | 1710/17834 [47:55<7:31:44,  1.68s/it] 10%|▉         | 1711/17834 [47:57<7:26:01,  1.66s/it] 10%|▉         | 1712/17834 [47:59<7:24:57,  1.66s/it] 10%|▉         | 1713/17834 [48:00<7:29:00,  1.67s/it] 10%|▉         | 1714/17834 [48:02<7:31:07,  1.68s/it] 10%|▉         | 1715/17834 [48:04<7:33:47,  1.69s/it] 10%|▉         | 1716/17834 [48:05<7:31:20,  1.68s/it] 10%|▉         | 1717/17834 [48:07<7:31:24,  1.68s/it] 10%|▉         | 1718/17834 [48:09<7:32:04,  1.68s/it] 10%|▉         | 1719/17834 [48:10<7:38:07,  1.71s/it] 10%|▉         | 1720/17834 [48:12<7:33:59,  1.69s/it] 10%|▉         | 1721/17834 [48:14<7:32:29,  1.68s/it] 10%|▉         | 1722/17834 [48:16<7:32:54,  1.69s/it] 10%|▉         | 1723/17834 [48:17<7:29:19,  1.67s/it] 10%|▉         | 1724/17834 [48:19<7:28:16,  1.67s/it] 10%|▉         | 1725/17834 [48:20<7:22:37,  1.65s/it] 10%|▉         | 1726/17834 [48:22<7:23:02,  1.65s/it] 10%|▉         | 1727/17834 [48:24<7:26:51,  1.66s/it] 10%|▉         | 1728/17834 [48:25<7:23:11,  1.65s/it] 10%|▉         | 1729/17834 [48:27<7:21:26,  1.64s/it] 10%|▉         | 1730/17834 [48:29<7:32:27,  1.69s/it] 10%|▉         | 1731/17834 [48:30<7:31:44,  1.68s/it] 10%|▉         | 1732/17834 [48:32<7:31:01,  1.68s/it] 10%|▉         | 1733/17834 [48:34<7:33:50,  1.69s/it] 10%|▉         | 1734/17834 [48:35<7:27:54,  1.67s/it] 10%|▉         | 1735/17834 [48:37<7:22:51,  1.65s/it] 10%|▉         | 1736/17834 [48:39<7:23:04,  1.65s/it] 10%|▉         | 1737/17834 [48:40<7:20:36,  1.64s/it] 10%|▉         | 1738/17834 [48:42<7:26:07,  1.66s/it] 10%|▉         | 1739/17834 [48:44<7:31:07,  1.68s/it] 10%|▉         | 1740/17834 [48:45<7:29:20,  1.68s/it] 10%|▉         | 1741/17834 [48:47<7:27:25,  1.67s/it] 10%|▉         | 1742/17834 [48:49<7:30:00,  1.68s/it] 10%|▉         | 1743/17834 [48:51<7:32:36,  1.69s/it] 10%|▉         | 1744/17834 [48:52<7:29:05,  1.67s/it] 10%|▉         | 1745/17834 [48:54<7:37:21,  1.71s/it] 10%|▉         | 1746/17834 [48:56<7:37:02,  1.70s/it] 10%|▉         | 1747/17834 [48:57<7:32:31,  1.69s/it] 10%|▉         | 1748/17834 [48:59<7:34:22,  1.69s/it] 10%|▉         | 1749/17834 [49:01<7:34:02,  1.69s/it]08/30/2024 20:03:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.617254376411438, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04436618089675903, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3499221801757812, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.011542797088623}
 10%|▉         | 1750/17834 [49:02<7:37:12,  1.71s/it] 10%|▉         | 1751/17834 [49:04<7:35:03,  1.70s/it] 10%|▉         | 1752/17834 [49:06<7:31:26,  1.68s/it] 10%|▉         | 1753/17834 [49:07<7:33:59,  1.69s/it] 10%|▉         | 1754/17834 [49:09<7:32:35,  1.69s/it] 10%|▉         | 1755/17834 [49:11<7:27:17,  1.67s/it] 10%|▉         | 1756/17834 [49:12<7:26:47,  1.67s/it] 10%|▉         | 1757/17834 [49:14<7:29:28,  1.68s/it] 10%|▉         | 1758/17834 [49:16<7:29:40,  1.68s/it] 10%|▉         | 1759/17834 [49:18<7:31:04,  1.68s/it] 10%|▉         | 1760/17834 [49:19<7:33:49,  1.69s/it] 10%|▉         | 1761/17834 [49:21<7:28:31,  1.67s/it] 10%|▉         | 1762/17834 [49:22<7:22:53,  1.65s/it] 10%|▉         | 1763/17834 [49:24<7:28:07,  1.67s/it] 10%|▉         | 1764/17834 [49:26<7:28:40,  1.68s/it] 10%|▉         | 1765/17834 [49:28<7:28:46,  1.68s/it] 10%|▉         | 1766/17834 [49:29<7:31:43,  1.69s/it] 10%|▉         | 1767/17834 [49:31<7:30:07,  1.68s/it] 10%|▉         | 1768/17834 [49:33<7:32:07,  1.69s/it] 10%|▉         | 1769/17834 [49:34<7:30:20,  1.68s/it] 10%|▉         | 1770/17834 [49:36<7:30:36,  1.68s/it] 10%|▉         | 1771/17834 [49:38<7:35:24,  1.70s/it] 10%|▉         | 1772/17834 [49:39<7:33:31,  1.69s/it] 10%|▉         | 1773/17834 [49:41<7:29:39,  1.68s/it] 10%|▉         | 1774/17834 [49:43<7:29:05,  1.68s/it] 10%|▉         | 1775/17834 [49:44<7:25:26,  1.66s/it] 10%|▉         | 1776/17834 [49:46<7:37:19,  1.71s/it] 10%|▉         | 1777/17834 [49:48<7:30:44,  1.68s/it] 10%|▉         | 1778/17834 [49:50<7:39:29,  1.72s/it] 10%|▉         | 1779/17834 [49:51<7:35:31,  1.70s/it] 10%|▉         | 1780/17834 [49:53<7:32:38,  1.69s/it] 10%|▉         | 1781/17834 [49:55<7:31:45,  1.69s/it]08/30/2024 20:04:13 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/30/2024 20:04:13 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:41,  5.31it/s][A
  1%|▏         | 3/221 [00:00<01:02,  3.48it/s][A
  2%|▏         | 4/221 [00:00<00:47,  4.55it/s][A
  2%|▏         | 5/221 [00:01<00:52,  4.14it/s][A
  3%|▎         | 6/221 [00:01<00:50,  4.27it/s][A
  3%|▎         | 7/221 [00:01<00:55,  3.85it/s][A
  4%|▎         | 8/221 [00:02<00:57,  3.73it/s][A
  4%|▍         | 9/221 [00:02<01:09,  3.06it/s][A
  5%|▍         | 10/221 [00:02<01:06,  3.19it/s][A
  5%|▍         | 11/221 [00:03<01:07,  3.13it/s][A
  5%|▌         | 12/221 [00:03<00:56,  3.67it/s][A
  6%|▌         | 13/221 [00:03<01:17,  2.69it/s][A
  6%|▋         | 14/221 [00:04<01:04,  3.23it/s][A
  7%|▋         | 15/221 [00:04<01:10,  2.93it/s][A
  7%|▋         | 16/221 [00:04<01:09,  2.95it/s][A
  8%|▊         | 17/221 [00:05<01:07,  3.04it/s][A
  8%|▊         | 18/221 [00:05<00:57,  3.50it/s][A
  9%|▊         | 19/221 [00:05<00:50,  4.02it/s][A
  9%|▉         | 20/221 [00:05<00:43,  4.63it/s][A
 10%|▉         | 21/221 [00:05<00:42,  4.74it/s][A
 10%|▉         | 22/221 [00:06<00:51,  3.88it/s][A
 10%|█         | 23/221 [00:06<00:45,  4.36it/s][A
 11%|█         | 24/221 [00:06<00:38,  5.12it/s][A
 11%|█▏        | 25/221 [00:06<00:36,  5.32it/s][A
 12%|█▏        | 26/221 [00:06<00:41,  4.66it/s][A
 12%|█▏        | 27/221 [00:07<00:58,  3.30it/s][A
 13%|█▎        | 28/221 [00:07<00:53,  3.58it/s][A
 13%|█▎        | 29/221 [00:07<00:52,  3.64it/s][A
 14%|█▎        | 30/221 [00:08<00:52,  3.65it/s][A
 14%|█▍        | 31/221 [00:08<00:54,  3.49it/s][A
 14%|█▍        | 32/221 [00:08<00:58,  3.22it/s][A
 15%|█▍        | 33/221 [00:09<00:59,  3.15it/s][A
 15%|█▌        | 34/221 [00:09<01:18,  2.38it/s][A
 16%|█▌        | 35/221 [00:10<01:08,  2.73it/s][A
 16%|█▋        | 36/221 [00:10<01:09,  2.67it/s][A
 17%|█▋        | 37/221 [00:10<01:00,  3.05it/s][A
 17%|█▋        | 38/221 [00:10<00:59,  3.08it/s][A
 18%|█▊        | 39/221 [00:11<00:49,  3.70it/s][A
 18%|█▊        | 40/221 [00:11<00:47,  3.77it/s][A
 19%|█▊        | 41/221 [00:11<00:51,  3.49it/s][A
 19%|█▉        | 42/221 [00:11<00:41,  4.30it/s][A
 19%|█▉        | 43/221 [00:11<00:38,  4.59it/s][A
 20%|█▉        | 44/221 [00:12<00:48,  3.68it/s][A
 20%|██        | 45/221 [00:12<00:55,  3.18it/s][A
 21%|██        | 46/221 [00:12<00:49,  3.57it/s][A
 21%|██▏       | 47/221 [00:13<00:40,  4.33it/s][A
 22%|██▏       | 49/221 [00:13<00:33,  5.16it/s][A
 23%|██▎       | 50/221 [00:13<00:45,  3.72it/s][A
 23%|██▎       | 51/221 [00:14<00:44,  3.86it/s][A
 24%|██▍       | 53/221 [00:14<00:35,  4.79it/s][A
 24%|██▍       | 54/221 [00:14<00:42,  3.96it/s][A
 25%|██▍       | 55/221 [00:15<00:41,  3.95it/s][A
 25%|██▌       | 56/221 [00:15<00:43,  3.80it/s][A
 26%|██▌       | 57/221 [00:15<00:42,  3.81it/s][A
 26%|██▌       | 58/221 [00:15<00:38,  4.27it/s][A
 27%|██▋       | 59/221 [00:15<00:34,  4.67it/s][A
 27%|██▋       | 60/221 [00:16<00:34,  4.71it/s][A
 28%|██▊       | 61/221 [00:16<00:37,  4.24it/s][A
 28%|██▊       | 62/221 [00:16<00:37,  4.25it/s][A
 29%|██▊       | 63/221 [00:17<00:40,  3.87it/s][A
 29%|██▉       | 64/221 [00:17<00:40,  3.85it/s][A
 29%|██▉       | 65/221 [00:17<00:42,  3.66it/s][A
 30%|██▉       | 66/221 [00:18<00:54,  2.83it/s][A
 30%|███       | 67/221 [00:18<01:03,  2.41it/s][A
 31%|███       | 68/221 [00:18<00:50,  3.00it/s][A
 31%|███       | 69/221 [00:19<00:56,  2.70it/s][A
 32%|███▏      | 70/221 [00:19<00:53,  2.85it/s][A
 32%|███▏      | 71/221 [00:20<00:57,  2.63it/s][A
 33%|███▎      | 72/221 [00:20<00:50,  2.93it/s][A
 33%|███▎      | 73/221 [00:20<00:46,  3.19it/s][A
 34%|███▍      | 75/221 [00:20<00:37,  3.87it/s][A
 34%|███▍      | 76/221 [00:21<00:42,  3.40it/s][A
 35%|███▍      | 77/221 [00:21<00:40,  3.57it/s][A
 35%|███▌      | 78/221 [00:21<00:42,  3.35it/s][A
 36%|███▌      | 79/221 [00:22<00:45,  3.12it/s][A
 36%|███▌      | 80/221 [00:22<00:41,  3.40it/s][A
 37%|███▋      | 81/221 [00:22<00:36,  3.87it/s][A
 37%|███▋      | 82/221 [00:23<00:40,  3.42it/s][A
 38%|███▊      | 83/221 [00:23<00:51,  2.66it/s][A
 38%|███▊      | 84/221 [00:23<00:44,  3.07it/s][A
 38%|███▊      | 85/221 [00:24<00:38,  3.57it/s][A
 39%|███▉      | 86/221 [00:24<00:44,  3.02it/s][A
 39%|███▉      | 87/221 [00:24<00:45,  2.93it/s][A
 40%|███▉      | 88/221 [00:25<00:41,  3.19it/s][A
 40%|████      | 89/221 [00:25<00:43,  3.03it/s][A
 41%|████      | 90/221 [00:25<00:40,  3.25it/s][A
 41%|████      | 91/221 [00:25<00:33,  3.85it/s][A
 42%|████▏     | 92/221 [00:25<00:29,  4.45it/s][A
 42%|████▏     | 93/221 [00:26<00:46,  2.77it/s][A
 43%|████▎     | 94/221 [00:26<00:41,  3.04it/s][A
 43%|████▎     | 95/221 [00:27<00:48,  2.62it/s][A
 43%|████▎     | 96/221 [00:27<00:40,  3.12it/s][A
 44%|████▍     | 97/221 [00:27<00:40,  3.08it/s][A
 44%|████▍     | 98/221 [00:28<00:39,  3.14it/s][A
 45%|████▍     | 99/221 [00:28<00:38,  3.14it/s][A
 45%|████▌     | 100/221 [00:28<00:36,  3.34it/s][A
 46%|████▌     | 101/221 [00:29<00:33,  3.63it/s][A
 46%|████▌     | 102/221 [00:29<00:39,  2.98it/s][A
 47%|████▋     | 103/221 [00:29<00:42,  2.75it/s][A
 47%|████▋     | 104/221 [00:30<00:44,  2.63it/s][A
 48%|████▊     | 105/221 [00:30<00:45,  2.56it/s][A
 48%|████▊     | 106/221 [00:30<00:38,  3.03it/s][A
 48%|████▊     | 107/221 [00:31<00:32,  3.51it/s][A
 49%|████▉     | 108/221 [00:31<00:29,  3.88it/s][A
 49%|████▉     | 109/221 [00:31<00:27,  4.14it/s][A
 50%|████▉     | 110/221 [00:32<00:36,  3.03it/s][A
 50%|█████     | 111/221 [00:32<00:35,  3.06it/s][A
 51%|█████     | 112/221 [00:32<00:36,  2.99it/s][A
 51%|█████     | 113/221 [00:32<00:29,  3.61it/s][A
 52%|█████▏    | 114/221 [00:33<00:25,  4.19it/s][A
 52%|█████▏    | 115/221 [00:33<00:31,  3.41it/s][A
 52%|█████▏    | 116/221 [00:33<00:29,  3.57it/s][A
 53%|█████▎    | 117/221 [00:33<00:28,  3.66it/s][A
 53%|█████▎    | 118/221 [00:34<00:28,  3.61it/s][A
 54%|█████▍    | 119/221 [00:34<00:28,  3.55it/s][A
 54%|█████▍    | 120/221 [00:34<00:29,  3.40it/s][A
 55%|█████▍    | 121/221 [00:35<00:27,  3.64it/s][A
 55%|█████▌    | 122/221 [00:35<00:31,  3.10it/s][A
 56%|█████▌    | 123/221 [00:35<00:26,  3.64it/s][A
 56%|█████▌    | 124/221 [00:35<00:25,  3.81it/s][A
 57%|█████▋    | 125/221 [00:36<00:31,  3.02it/s][A
 57%|█████▋    | 126/221 [00:36<00:25,  3.66it/s][A
 57%|█████▋    | 127/221 [00:36<00:29,  3.16it/s][A
 58%|█████▊    | 128/221 [00:37<00:26,  3.45it/s][A
 58%|█████▊    | 129/221 [00:37<00:22,  4.08it/s][A
 59%|█████▉    | 130/221 [00:37<00:22,  4.06it/s][A
 59%|█████▉    | 131/221 [00:37<00:23,  3.78it/s][A
 60%|█████▉    | 132/221 [00:38<00:25,  3.46it/s][A
 60%|██████    | 133/221 [00:38<00:26,  3.26it/s][A
 61%|██████    | 134/221 [00:39<00:31,  2.76it/s][A
 61%|██████    | 135/221 [00:39<00:29,  2.93it/s][A
 62%|██████▏   | 136/221 [00:39<00:27,  3.15it/s][A
 62%|██████▏   | 137/221 [00:39<00:23,  3.51it/s][A
 62%|██████▏   | 138/221 [00:40<00:22,  3.69it/s][A
 63%|██████▎   | 139/221 [00:40<00:20,  4.08it/s][A
 63%|██████▎   | 140/221 [00:40<00:19,  4.14it/s][A
 64%|██████▍   | 141/221 [00:40<00:22,  3.58it/s][A
 64%|██████▍   | 142/221 [00:41<00:19,  4.04it/s][A
 65%|██████▍   | 143/221 [00:41<00:22,  3.49it/s][A
 65%|██████▌   | 144/221 [00:41<00:24,  3.12it/s][A
 66%|██████▌   | 145/221 [00:42<00:25,  2.97it/s][A
 66%|██████▌   | 146/221 [00:42<00:22,  3.27it/s][A
 67%|██████▋   | 147/221 [00:42<00:20,  3.53it/s][A
 67%|██████▋   | 148/221 [00:42<00:20,  3.61it/s][A
 67%|██████▋   | 149/221 [00:43<00:21,  3.41it/s][A
 68%|██████▊   | 150/221 [00:43<00:18,  3.84it/s][A
 68%|██████▊   | 151/221 [00:43<00:24,  2.92it/s][A
 69%|██████▉   | 152/221 [00:44<00:31,  2.20it/s][A
 69%|██████▉   | 153/221 [00:44<00:24,  2.83it/s][A
 70%|██████▉   | 154/221 [00:45<00:21,  3.16it/s][A
 70%|███████   | 155/221 [00:45<00:23,  2.81it/s][A
 71%|███████   | 156/221 [00:45<00:20,  3.23it/s][A
 71%|███████   | 157/221 [00:45<00:17,  3.66it/s][A
 71%|███████▏  | 158/221 [00:46<00:18,  3.39it/s][A
 72%|███████▏  | 159/221 [00:46<00:15,  3.98it/s][A
 72%|███████▏  | 160/221 [00:46<00:16,  3.61it/s][A
 73%|███████▎  | 161/221 [00:47<00:27,  2.19it/s][A
 73%|███████▎  | 162/221 [00:47<00:21,  2.75it/s][A
 74%|███████▍  | 163/221 [00:48<00:21,  2.73it/s][A
 74%|███████▍  | 164/221 [00:48<00:16,  3.35it/s][A
 75%|███████▍  | 165/221 [00:48<00:16,  3.49it/s][A
 75%|███████▌  | 166/221 [00:48<00:14,  3.68it/s][A
 76%|███████▌  | 167/221 [00:48<00:13,  4.00it/s][A
 76%|███████▌  | 168/221 [00:49<00:14,  3.56it/s][A
 76%|███████▋  | 169/221 [00:49<00:14,  3.56it/s][A
 77%|███████▋  | 170/221 [00:50<00:18,  2.70it/s][A
 77%|███████▋  | 171/221 [00:50<00:19,  2.58it/s][A
 78%|███████▊  | 172/221 [00:50<00:16,  2.97it/s][A
 78%|███████▊  | 173/221 [00:51<00:16,  2.89it/s][A
 79%|███████▊  | 174/221 [00:51<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:51<00:15,  3.03it/s][A
 80%|███████▉  | 176/221 [00:51<00:12,  3.62it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.13it/s][A
 81%|████████  | 178/221 [00:52<00:13,  3.08it/s][A
 81%|████████  | 179/221 [00:52<00:12,  3.27it/s][A
 81%|████████▏ | 180/221 [00:53<00:12,  3.24it/s][A
 82%|████████▏ | 181/221 [00:53<00:12,  3.21it/s][A
 82%|████████▏ | 182/221 [00:53<00:11,  3.38it/s][A
 83%|████████▎ | 183/221 [00:54<00:12,  3.15it/s][A
 83%|████████▎ | 184/221 [00:54<00:12,  2.87it/s][A
 84%|████████▎ | 185/221 [00:54<00:11,  3.10it/s][A
 84%|████████▍ | 186/221 [00:55<00:12,  2.77it/s][A
 85%|████████▍ | 187/221 [00:55<00:12,  2.77it/s][A
 85%|████████▌ | 188/221 [00:56<00:15,  2.16it/s][A
 86%|████████▌ | 189/221 [00:56<00:11,  2.73it/s][A
 86%|████████▌ | 190/221 [00:56<00:11,  2.69it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.19it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.50it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.81it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.59it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.57it/s][A
 89%|████████▊ | 196/221 [00:58<00:08,  2.87it/s][A
 89%|████████▉ | 197/221 [00:58<00:08,  2.71it/s][A
 90%|████████▉ | 198/221 [00:59<00:09,  2.49it/s][A
 90%|█████████ | 199/221 [00:59<00:07,  3.03it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.16it/s][A
 91%|█████████ | 201/221 [01:00<00:05,  3.34it/s][A
 91%|█████████▏| 202/221 [01:00<00:06,  3.08it/s][A
 92%|█████████▏| 203/221 [01:00<00:05,  3.10it/s][A
 92%|█████████▏| 204/221 [01:01<00:06,  2.65it/s][A
 93%|█████████▎| 205/221 [01:01<00:05,  3.09it/s][A
 93%|█████████▎| 206/221 [01:01<00:04,  3.53it/s][A
 94%|█████████▎| 207/221 [01:01<00:03,  3.79it/s][A
 94%|█████████▍| 208/221 [01:02<00:03,  3.71it/s][A
 95%|█████████▍| 209/221 [01:02<00:02,  4.48it/s][A
 95%|█████████▌| 211/221 [01:02<00:01,  5.62it/s][A
 96%|█████████▌| 212/221 [01:02<00:01,  4.89it/s][A
 96%|█████████▋| 213/221 [01:03<00:01,  4.60it/s][A
 97%|█████████▋| 214/221 [01:03<00:01,  3.77it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  4.01it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  4.26it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.80it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.39it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  3.08it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.51it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.86it/s][A100%|██████████| 221/221 [01:05<00:00,  3.38it/s]
08/30/2024 20:06:40 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 1781--===========

08/30/2024 20:06:40 - INFO - __main__ -   {'area_r1': 4.3, 'area_recall': '4.3/12.8/21.0', 'area_ravg': 12.7}
08/30/2024 20:06:40 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 1781--===========

08/30/2024 20:06:40 - INFO - __main__ -   {'area_r1': 41.6, 'area_recall': '41.6/72.6/81.8', 'area_ravg': 65.3}
08/30/2024 20:06:40 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 1781--===========

08/30/2024 20:06:40 - INFO - __main__ -   {'video_r1': 36.7, 'video_recall': '36.7/68.3/77.7', 'video_ravg': 60.9}
08/30/2024 20:06:40 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 1781=======

08/30/2024 20:06:40 - INFO - __main__ -   {'video_r1': 36.7, 'video_recall': '36.7/68.3/77.7', 'video_ravg': 60.9}
08/30/2024 20:06:40 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 1781--===========

08/30/2024 20:06:40 - INFO - __main__ -   {'video_r1': 56.2, 'video_recall': '56.2/77.7/85.6', 'video_ravg': 73.2}
08/30/2024 20:06:40 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 1781=======

08/30/2024 20:06:40 - INFO - __main__ -   {'video_r1': 56.2, 'video_recall': '56.2/77.7/85.6', 'video_ravg': 73.2}
 10%|▉         | 1782/17834 [52:51<241:11:07, 54.09s/it] 10%|▉         | 1783/17834 [52:53<171:05:07, 38.37s/it] 10%|█         | 1784/17834 [52:54<122:05:11, 27.38s/it] 10%|█         | 1785/17834 [52:56<87:47:21, 19.69s/it]  10%|█         | 1786/17834 [52:58<63:51:39, 14.33s/it] 10%|█         | 1787/17834 [53:00<47:01:38, 10.55s/it] 10%|█         | 1788/17834 [53:01<35:18:43,  7.92s/it] 10%|█         | 1789/17834 [53:03<27:14:08,  6.11s/it] 10%|█         | 1790/17834 [53:05<21:18:36,  4.78s/it] 10%|█         | 1791/17834 [53:07<17:15:54,  3.87s/it] 10%|█         | 1792/17834 [53:09<14:27:24,  3.24s/it] 10%|█         | 1793/17834 [53:10<12:27:16,  2.80s/it] 10%|█         | 1794/17834 [53:12<11:18:00,  2.54s/it] 10%|█         | 1795/17834 [53:14<10:09:47,  2.28s/it] 10%|█         | 1796/17834 [53:16<9:22:21,  2.10s/it]  10%|█         | 1797/17834 [53:17<8:56:54,  2.01s/it] 10%|█         | 1798/17834 [53:19<8:36:31,  1.93s/it] 10%|█         | 1799/17834 [53:21<8:21:14,  1.88s/it]08/30/2024 20:07:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5780586004257202, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0458436980843544, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3777856826782227, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.001688003540039}
 10%|█         | 1800/17834 [53:23<8:05:55,  1.82s/it] 10%|█         | 1801/17834 [53:24<7:58:43,  1.79s/it] 10%|█         | 1802/17834 [53:26<7:56:27,  1.78s/it] 10%|█         | 1803/17834 [53:28<7:48:57,  1.76s/it] 10%|█         | 1804/17834 [53:30<7:49:13,  1.76s/it] 10%|█         | 1805/17834 [53:31<7:54:16,  1.78s/it] 10%|█         | 1806/17834 [53:33<7:45:51,  1.74s/it] 10%|█         | 1807/17834 [53:35<7:42:57,  1.73s/it] 10%|█         | 1808/17834 [53:37<7:45:45,  1.74s/it] 10%|█         | 1809/17834 [53:38<7:45:19,  1.74s/it] 10%|█         | 1810/17834 [53:40<7:48:27,  1.75s/it] 10%|█         | 1811/17834 [53:42<7:48:57,  1.76s/it] 10%|█         | 1812/17834 [53:44<7:47:48,  1.75s/it] 10%|█         | 1813/17834 [53:45<7:46:31,  1.75s/it] 10%|█         | 1814/17834 [53:47<7:50:12,  1.76s/it] 10%|█         | 1815/17834 [53:49<7:48:50,  1.76s/it] 10%|█         | 1816/17834 [53:51<7:47:21,  1.75s/it] 10%|█         | 1817/17834 [53:52<7:50:08,  1.76s/it] 10%|█         | 1818/17834 [53:54<7:54:33,  1.78s/it] 10%|█         | 1819/17834 [53:56<7:51:24,  1.77s/it] 10%|█         | 1820/17834 [53:58<7:49:24,  1.76s/it] 10%|█         | 1821/17834 [53:59<7:41:16,  1.73s/it] 10%|█         | 1822/17834 [54:01<7:47:07,  1.75s/it] 10%|█         | 1823/17834 [54:03<7:44:43,  1.74s/it] 10%|█         | 1824/17834 [54:05<7:53:41,  1.78s/it] 10%|█         | 1825/17834 [54:06<7:51:35,  1.77s/it] 10%|█         | 1826/17834 [54:08<7:52:05,  1.77s/it] 10%|█         | 1827/17834 [54:10<7:50:08,  1.76s/it] 10%|█         | 1828/17834 [54:12<7:45:47,  1.75s/it] 10%|█         | 1829/17834 [54:13<7:48:18,  1.76s/it] 10%|█         | 1830/17834 [54:15<7:48:42,  1.76s/it] 10%|█         | 1831/17834 [54:17<7:48:16,  1.76s/it] 10%|█         | 1832/17834 [54:19<7:46:08,  1.75s/it] 10%|█         | 1833/17834 [54:20<7:52:14,  1.77s/it] 10%|█         | 1834/17834 [54:22<7:47:56,  1.75s/it] 10%|█         | 1835/17834 [54:24<7:53:40,  1.78s/it] 10%|█         | 1836/17834 [54:26<7:50:46,  1.77s/it] 10%|█         | 1837/17834 [54:28<7:52:04,  1.77s/it] 10%|█         | 1838/17834 [54:29<7:52:44,  1.77s/it] 10%|█         | 1839/17834 [54:31<7:45:54,  1.75s/it] 10%|█         | 1840/17834 [54:33<7:46:50,  1.75s/it] 10%|█         | 1841/17834 [54:35<7:53:35,  1.78s/it] 10%|█         | 1842/17834 [54:36<7:47:26,  1.75s/it] 10%|█         | 1843/17834 [54:38<7:52:38,  1.77s/it] 10%|█         | 1844/17834 [54:40<7:51:25,  1.77s/it] 10%|█         | 1845/17834 [54:42<7:45:08,  1.75s/it] 10%|█         | 1846/17834 [54:43<7:47:49,  1.76s/it] 10%|█         | 1847/17834 [54:45<7:52:05,  1.77s/it] 10%|█         | 1848/17834 [54:47<7:52:34,  1.77s/it] 10%|█         | 1849/17834 [54:49<7:54:16,  1.78s/it]08/30/2024 20:09:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2584965229034424, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03111858293414116, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2315456867218018, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.521160840988159}
 10%|█         | 1850/17834 [54:51<7:55:51,  1.79s/it] 10%|█         | 1851/17834 [54:52<7:48:35,  1.76s/it] 10%|█         | 1852/17834 [54:54<7:46:37,  1.75s/it] 10%|█         | 1853/17834 [54:56<7:45:56,  1.75s/it] 10%|█         | 1854/17834 [54:57<7:42:43,  1.74s/it] 10%|█         | 1855/17834 [54:59<7:43:51,  1.74s/it] 10%|█         | 1856/17834 [55:01<7:40:13,  1.73s/it] 10%|█         | 1857/17834 [55:03<7:41:28,  1.73s/it] 10%|█         | 1858/17834 [55:04<7:37:09,  1.72s/it] 10%|█         | 1859/17834 [55:06<7:36:30,  1.71s/it] 10%|█         | 1860/17834 [55:08<7:40:47,  1.73s/it] 10%|█         | 1861/17834 [55:10<7:47:44,  1.76s/it] 10%|█         | 1862/17834 [55:11<7:44:27,  1.74s/it] 10%|█         | 1863/17834 [55:13<7:47:25,  1.76s/it] 10%|█         | 1864/17834 [55:15<7:47:40,  1.76s/it] 10%|█         | 1865/17834 [55:17<7:45:20,  1.75s/it] 10%|█         | 1866/17834 [55:18<7:48:42,  1.76s/it] 10%|█         | 1867/17834 [55:20<7:45:30,  1.75s/it] 10%|█         | 1868/17834 [55:22<7:45:40,  1.75s/it] 10%|█         | 1869/17834 [55:24<7:52:28,  1.78s/it] 10%|█         | 1870/17834 [55:25<7:47:56,  1.76s/it] 10%|█         | 1871/17834 [55:27<7:46:58,  1.76s/it] 10%|█         | 1872/17834 [55:29<7:51:40,  1.77s/it] 11%|█         | 1873/17834 [55:31<7:48:34,  1.76s/it] 11%|█         | 1874/17834 [55:32<7:48:41,  1.76s/it] 11%|█         | 1875/17834 [55:34<7:47:47,  1.76s/it] 11%|█         | 1876/17834 [55:36<7:52:14,  1.78s/it] 11%|█         | 1877/17834 [55:38<7:48:44,  1.76s/it] 11%|█         | 1878/17834 [55:39<7:46:21,  1.75s/it] 11%|█         | 1879/17834 [55:41<7:49:06,  1.76s/it] 11%|█         | 1880/17834 [55:43<7:42:41,  1.74s/it] 11%|█         | 1881/17834 [55:45<7:46:16,  1.75s/it] 11%|█         | 1882/17834 [55:47<7:50:26,  1.77s/it] 11%|█         | 1883/17834 [55:48<7:46:34,  1.76s/it] 11%|█         | 1884/17834 [55:50<7:44:49,  1.75s/it] 11%|█         | 1885/17834 [55:52<7:42:57,  1.74s/it] 11%|█         | 1886/17834 [55:53<7:44:06,  1.75s/it] 11%|█         | 1887/17834 [55:55<7:43:35,  1.74s/it] 11%|█         | 1888/17834 [55:57<7:48:25,  1.76s/it] 11%|█         | 1889/17834 [55:59<7:55:25,  1.79s/it] 11%|█         | 1890/17834 [56:01<7:57:15,  1.80s/it] 11%|█         | 1891/17834 [56:02<7:52:54,  1.78s/it] 11%|█         | 1892/17834 [56:04<7:48:20,  1.76s/it] 11%|█         | 1893/17834 [56:06<7:42:20,  1.74s/it] 11%|█         | 1894/17834 [56:08<7:45:02,  1.75s/it] 11%|█         | 1895/17834 [56:09<7:51:23,  1.77s/it] 11%|█         | 1896/17834 [56:11<7:48:57,  1.77s/it] 11%|█         | 1897/17834 [56:13<7:47:21,  1.76s/it] 11%|█         | 1898/17834 [56:15<7:43:47,  1.75s/it] 11%|█         | 1899/17834 [56:16<7:42:55,  1.74s/it]08/30/2024 20:10:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5875481367111206, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.050085730850696564, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2124719619750977, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8501057624816895}
 11%|█         | 1900/17834 [56:18<7:44:38,  1.75s/it] 11%|█         | 1901/17834 [56:20<7:46:27,  1.76s/it] 11%|█         | 1902/17834 [56:22<7:50:06,  1.77s/it] 11%|█         | 1903/17834 [56:23<7:43:27,  1.75s/it] 11%|█         | 1904/17834 [56:25<7:41:28,  1.74s/it] 11%|█         | 1905/17834 [56:27<7:52:23,  1.78s/it] 11%|█         | 1906/17834 [56:29<7:41:54,  1.74s/it] 11%|█         | 1907/17834 [56:30<7:46:49,  1.76s/it] 11%|█         | 1908/17834 [56:32<7:50:45,  1.77s/it] 11%|█         | 1909/17834 [56:34<7:52:59,  1.78s/it] 11%|█         | 1910/17834 [56:36<7:45:06,  1.75s/it] 11%|█         | 1911/17834 [56:38<7:46:24,  1.76s/it] 11%|█         | 1912/17834 [56:39<7:47:55,  1.76s/it] 11%|█         | 1913/17834 [56:41<7:48:13,  1.76s/it] 11%|█         | 1914/17834 [56:43<7:47:43,  1.76s/it] 11%|█         | 1915/17834 [56:45<7:50:42,  1.77s/it] 11%|█         | 1916/17834 [56:46<7:50:45,  1.77s/it] 11%|█         | 1917/17834 [56:48<7:54:46,  1.79s/it] 11%|█         | 1918/17834 [56:50<7:47:23,  1.76s/it] 11%|█         | 1919/17834 [56:52<7:44:12,  1.75s/it] 11%|█         | 1920/17834 [56:53<7:45:05,  1.75s/it] 11%|█         | 1921/17834 [56:55<7:47:28,  1.76s/it] 11%|█         | 1922/17834 [56:57<7:46:27,  1.76s/it] 11%|█         | 1923/17834 [56:59<7:43:39,  1.75s/it] 11%|█         | 1924/17834 [57:01<7:49:11,  1.77s/it] 11%|█         | 1925/17834 [57:02<7:50:21,  1.77s/it] 11%|█         | 1926/17834 [57:04<7:44:41,  1.75s/it] 11%|█         | 1927/17834 [57:06<7:49:22,  1.77s/it] 11%|█         | 1928/17834 [57:08<7:45:53,  1.76s/it] 11%|█         | 1929/17834 [57:09<7:43:27,  1.75s/it] 11%|█         | 1930/17834 [57:11<7:44:46,  1.75s/it] 11%|█         | 1931/17834 [57:13<7:42:26,  1.74s/it] 11%|█         | 1932/17834 [57:15<7:45:25,  1.76s/it] 11%|█         | 1933/17834 [57:16<7:42:21,  1.74s/it] 11%|█         | 1934/17834 [57:18<7:42:47,  1.75s/it] 11%|█         | 1935/17834 [57:20<7:50:25,  1.78s/it] 11%|█         | 1936/17834 [57:22<7:48:27,  1.77s/it] 11%|█         | 1937/17834 [57:23<7:57:39,  1.80s/it] 11%|█         | 1938/17834 [57:25<7:56:08,  1.80s/it] 11%|█         | 1939/17834 [57:27<7:52:59,  1.79s/it] 11%|█         | 1940/17834 [57:29<7:56:14,  1.80s/it] 11%|█         | 1941/17834 [57:31<7:54:30,  1.79s/it] 11%|█         | 1942/17834 [57:32<7:50:47,  1.78s/it] 11%|█         | 1943/17834 [57:34<7:45:48,  1.76s/it] 11%|█         | 1944/17834 [57:36<7:45:15,  1.76s/it] 11%|█         | 1945/17834 [57:38<7:46:23,  1.76s/it] 11%|█         | 1946/17834 [57:39<7:44:28,  1.75s/it] 11%|█         | 1947/17834 [57:41<7:41:13,  1.74s/it] 11%|█         | 1948/17834 [57:43<7:38:24,  1.73s/it] 11%|█         | 1949/17834 [57:44<7:38:39,  1.73s/it]08/30/2024 20:12:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.9455757141113281, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.06038978695869446, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5589723587036133, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.564937591552734}
 11%|█         | 1950/17834 [57:46<7:41:45,  1.74s/it] 11%|█         | 1951/17834 [57:48<7:44:11,  1.75s/it] 11%|█         | 1952/17834 [57:50<7:38:13,  1.73s/it] 11%|█         | 1953/17834 [57:51<7:41:42,  1.74s/it] 11%|█         | 1954/17834 [57:53<7:44:57,  1.76s/it] 11%|█         | 1955/17834 [57:55<7:45:32,  1.76s/it] 11%|█         | 1956/17834 [57:57<7:50:04,  1.78s/it] 11%|█         | 1957/17834 [57:59<7:40:09,  1.74s/it] 11%|█         | 1958/17834 [58:00<7:38:14,  1.73s/it] 11%|█         | 1959/17834 [58:02<7:39:59,  1.74s/it] 11%|█         | 1960/17834 [58:04<7:40:38,  1.74s/it] 11%|█         | 1961/17834 [58:05<7:37:57,  1.73s/it] 11%|█         | 1962/17834 [58:07<7:35:03,  1.72s/it] 11%|█         | 1963/17834 [58:09<7:43:04,  1.75s/it] 11%|█         | 1964/17834 [58:11<7:41:31,  1.74s/it] 11%|█         | 1965/17834 [58:12<7:40:16,  1.74s/it] 11%|█         | 1966/17834 [58:14<7:48:02,  1.77s/it] 11%|█         | 1967/17834 [58:16<7:51:00,  1.78s/it] 11%|█         | 1968/17834 [58:18<7:51:51,  1.78s/it] 11%|█         | 1969/17834 [58:20<7:48:38,  1.77s/it] 11%|█         | 1970/17834 [58:21<7:46:02,  1.76s/it] 11%|█         | 1971/17834 [58:23<7:41:10,  1.74s/it] 11%|█         | 1972/17834 [58:25<7:43:34,  1.75s/it] 11%|█         | 1973/17834 [58:27<7:43:58,  1.76s/it] 11%|█         | 1974/17834 [58:28<7:42:31,  1.75s/it] 11%|█         | 1975/17834 [58:30<7:43:35,  1.75s/it] 11%|█         | 1976/17834 [58:32<7:46:11,  1.76s/it] 11%|█         | 1977/17834 [58:34<7:41:11,  1.75s/it] 11%|█         | 1978/17834 [58:35<7:33:46,  1.72s/it] 11%|█         | 1979/17834 [58:37<7:41:06,  1.74s/it] 11%|█         | 1980/17834 [58:39<7:38:45,  1.74s/it] 11%|█         | 1981/17834 [58:40<7:39:05,  1.74s/it] 11%|█         | 1982/17834 [58:42<7:37:20,  1.73s/it] 11%|█         | 1983/17834 [58:44<7:32:29,  1.71s/it] 11%|█         | 1984/17834 [58:46<7:38:12,  1.73s/it] 11%|█         | 1985/17834 [58:47<7:39:52,  1.74s/it] 11%|█         | 1986/17834 [58:49<7:41:02,  1.75s/it] 11%|█         | 1987/17834 [58:51<7:39:40,  1.74s/it] 11%|█         | 1988/17834 [58:53<7:44:17,  1.76s/it] 11%|█         | 1989/17834 [58:55<7:48:27,  1.77s/it] 11%|█         | 1990/17834 [58:56<7:42:30,  1.75s/it] 11%|█         | 1991/17834 [58:58<7:44:27,  1.76s/it] 11%|█         | 1992/17834 [59:00<7:37:44,  1.73s/it] 11%|█         | 1993/17834 [59:01<7:41:27,  1.75s/it] 11%|█         | 1994/17834 [59:03<7:38:20,  1.74s/it] 11%|█         | 1995/17834 [59:05<7:38:51,  1.74s/it] 11%|█         | 1996/17834 [59:07<7:36:34,  1.73s/it] 11%|█         | 1997/17834 [59:08<7:38:57,  1.74s/it] 11%|█         | 1998/17834 [59:10<7:43:34,  1.76s/it] 11%|█         | 1999/17834 [59:12<7:41:01,  1.75s/it]08/30/2024 20:13:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.420685887336731, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038798414170742035, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3045196533203125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7640039920806885}
 11%|█         | 2000/17834 [59:14<7:42:42,  1.75s/it] 11%|█         | 2001/17834 [59:15<7:45:26,  1.76s/it] 11%|█         | 2002/17834 [59:17<7:42:17,  1.75s/it] 11%|█         | 2003/17834 [59:19<7:43:40,  1.76s/it] 11%|█         | 2004/17834 [59:21<7:43:14,  1.76s/it] 11%|█         | 2005/17834 [59:22<7:41:51,  1.75s/it] 11%|█         | 2006/17834 [59:24<7:39:58,  1.74s/it] 11%|█▏        | 2007/17834 [59:26<7:39:25,  1.74s/it] 11%|█▏        | 2008/17834 [59:28<7:37:29,  1.73s/it] 11%|█▏        | 2009/17834 [59:29<7:42:50,  1.75s/it] 11%|█▏        | 2010/17834 [59:31<7:41:40,  1.75s/it] 11%|█▏        | 2011/17834 [59:33<7:42:40,  1.75s/it] 11%|█▏        | 2012/17834 [59:35<7:35:43,  1.73s/it] 11%|█▏        | 2013/17834 [59:36<7:34:02,  1.72s/it] 11%|█▏        | 2014/17834 [59:38<7:38:20,  1.74s/it] 11%|█▏        | 2015/17834 [59:40<7:42:15,  1.75s/it] 11%|█▏        | 2016/17834 [59:42<7:36:21,  1.73s/it] 11%|█▏        | 2017/17834 [59:43<7:35:18,  1.73s/it] 11%|█▏        | 2018/17834 [59:45<7:33:28,  1.72s/it] 11%|█▏        | 2019/17834 [59:47<7:32:53,  1.72s/it] 11%|█▏        | 2020/17834 [59:48<7:33:50,  1.72s/it] 11%|█▏        | 2021/17834 [59:50<7:37:45,  1.74s/it] 11%|█▏        | 2022/17834 [59:52<7:45:59,  1.77s/it] 11%|█▏        | 2023/17834 [59:54<7:45:03,  1.76s/it] 11%|█▏        | 2024/17834 [59:56<7:48:06,  1.78s/it] 11%|█▏        | 2025/17834 [59:57<7:41:14,  1.75s/it] 11%|█▏        | 2026/17834 [59:59<7:50:18,  1.79s/it] 11%|█▏        | 2027/17834 [1:00:01<7:46:11,  1.77s/it] 11%|█▏        | 2028/17834 [1:00:03<7:48:32,  1.78s/it] 11%|█▏        | 2029/17834 [1:00:04<7:46:35,  1.77s/it] 11%|█▏        | 2030/17834 [1:00:06<7:43:04,  1.76s/it] 11%|█▏        | 2031/17834 [1:00:08<7:43:31,  1.76s/it] 11%|█▏        | 2032/17834 [1:00:10<7:44:30,  1.76s/it] 11%|█▏        | 2033/17834 [1:00:11<7:47:19,  1.77s/it] 11%|█▏        | 2034/17834 [1:00:13<7:38:08,  1.74s/it] 11%|█▏        | 2035/17834 [1:00:15<7:40:50,  1.75s/it] 11%|█▏        | 2036/17834 [1:00:17<7:39:43,  1.75s/it] 11%|█▏        | 2037/17834 [1:00:18<7:34:48,  1.73s/it] 11%|█▏        | 2038/17834 [1:00:20<7:37:12,  1.74s/it] 11%|█▏        | 2039/17834 [1:00:22<7:40:46,  1.75s/it] 11%|█▏        | 2040/17834 [1:00:24<7:39:15,  1.74s/it] 11%|█▏        | 2041/17834 [1:00:25<7:45:53,  1.77s/it] 11%|█▏        | 2042/17834 [1:00:27<7:45:50,  1.77s/it] 11%|█▏        | 2043/17834 [1:00:29<7:43:26,  1.76s/it] 11%|█▏        | 2044/17834 [1:00:31<7:39:37,  1.75s/it] 11%|█▏        | 2045/17834 [1:00:33<7:48:33,  1.78s/it] 11%|█▏        | 2046/17834 [1:00:34<7:57:36,  1.82s/it] 11%|█▏        | 2047/17834 [1:00:36<7:52:42,  1.80s/it] 11%|█▏        | 2048/17834 [1:00:38<7:43:59,  1.76s/it] 11%|█▏        | 2049/17834 [1:00:40<7:39:06,  1.75s/it]08/30/2024 20:14:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2510673999786377, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028994783759117126, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1530685424804688, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4331307411193848}
 11%|█▏        | 2050/17834 [1:00:41<7:42:12,  1.76s/it] 12%|█▏        | 2051/17834 [1:00:43<7:45:36,  1.77s/it] 12%|█▏        | 2052/17834 [1:00:45<7:46:41,  1.77s/it] 12%|█▏        | 2053/17834 [1:00:47<7:43:26,  1.76s/it] 12%|█▏        | 2054/17834 [1:00:48<7:40:06,  1.75s/it] 12%|█▏        | 2055/17834 [1:00:50<7:37:57,  1.74s/it] 12%|█▏        | 2056/17834 [1:00:52<7:41:33,  1.76s/it] 12%|█▏        | 2057/17834 [1:00:54<7:42:29,  1.76s/it] 12%|█▏        | 2058/17834 [1:00:55<7:40:41,  1.75s/it] 12%|█▏        | 2059/17834 [1:00:57<7:42:51,  1.76s/it] 12%|█▏        | 2060/17834 [1:00:59<7:42:42,  1.76s/it] 12%|█▏        | 2061/17834 [1:01:01<7:42:53,  1.76s/it] 12%|█▏        | 2062/17834 [1:01:03<7:49:49,  1.79s/it] 12%|█▏        | 2063/17834 [1:01:04<7:47:27,  1.78s/it] 12%|█▏        | 2064/17834 [1:01:06<7:42:06,  1.76s/it] 12%|█▏        | 2065/17834 [1:01:08<7:50:00,  1.79s/it] 12%|█▏        | 2066/17834 [1:01:10<7:47:48,  1.78s/it] 12%|█▏        | 2067/17834 [1:01:11<7:51:24,  1.79s/it] 12%|█▏        | 2068/17834 [1:01:13<7:48:55,  1.78s/it] 12%|█▏        | 2069/17834 [1:01:15<7:44:49,  1.77s/it] 12%|█▏        | 2070/17834 [1:01:17<7:40:23,  1.75s/it] 12%|█▏        | 2071/17834 [1:01:18<7:39:49,  1.75s/it] 12%|█▏        | 2072/17834 [1:01:20<7:37:24,  1.74s/it] 12%|█▏        | 2073/17834 [1:01:22<7:45:58,  1.77s/it] 12%|█▏        | 2074/17834 [1:01:24<7:42:38,  1.76s/it] 12%|█▏        | 2075/17834 [1:01:25<7:41:37,  1.76s/it] 12%|█▏        | 2076/17834 [1:01:27<7:37:43,  1.74s/it] 12%|█▏        | 2077/17834 [1:01:29<7:38:19,  1.75s/it] 12%|█▏        | 2078/17834 [1:01:31<7:38:05,  1.74s/it] 12%|█▏        | 2079/17834 [1:01:32<7:38:39,  1.75s/it] 12%|█▏        | 2080/17834 [1:01:34<7:36:53,  1.74s/it] 12%|█▏        | 2081/17834 [1:01:36<7:37:16,  1.74s/it] 12%|█▏        | 2082/17834 [1:01:38<7:36:37,  1.74s/it] 12%|█▏        | 2083/17834 [1:01:39<7:31:49,  1.72s/it] 12%|█▏        | 2084/17834 [1:01:41<7:31:04,  1.72s/it] 12%|█▏        | 2085/17834 [1:01:43<7:32:51,  1.73s/it] 12%|█▏        | 2086/17834 [1:01:45<7:36:37,  1.74s/it] 12%|█▏        | 2087/17834 [1:01:46<7:34:44,  1.73s/it] 12%|█▏        | 2088/17834 [1:01:48<7:35:02,  1.73s/it] 12%|█▏        | 2089/17834 [1:01:50<7:32:14,  1.72s/it] 12%|█▏        | 2090/17834 [1:01:52<7:47:15,  1.78s/it] 12%|█▏        | 2091/17834 [1:01:53<7:42:50,  1.76s/it] 12%|█▏        | 2092/17834 [1:01:55<7:36:48,  1.74s/it] 12%|█▏        | 2093/17834 [1:01:57<7:34:12,  1.73s/it] 12%|█▏        | 2094/17834 [1:01:59<7:40:50,  1.76s/it] 12%|█▏        | 2095/17834 [1:02:00<7:35:16,  1.74s/it] 12%|█▏        | 2096/17834 [1:02:02<7:35:11,  1.74s/it] 12%|█▏        | 2097/17834 [1:02:04<7:49:03,  1.79s/it] 12%|█▏        | 2098/17834 [1:02:06<7:39:57,  1.75s/it] 12%|█▏        | 2099/17834 [1:02:07<7:37:07,  1.74s/it]08/30/2024 20:16:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.752617597579956, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045382022857666016, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2444615364074707, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.042461395263672}
 12%|█▏        | 2100/17834 [1:02:09<7:35:30,  1.74s/it] 12%|█▏        | 2101/17834 [1:02:11<7:46:58,  1.78s/it] 12%|█▏        | 2102/17834 [1:02:13<7:41:08,  1.76s/it] 12%|█▏        | 2103/17834 [1:02:14<7:40:55,  1.76s/it] 12%|█▏        | 2104/17834 [1:02:16<7:35:46,  1.74s/it] 12%|█▏        | 2105/17834 [1:02:18<7:40:14,  1.76s/it] 12%|█▏        | 2106/17834 [1:02:19<7:33:51,  1.73s/it] 12%|█▏        | 2107/17834 [1:02:21<7:33:20,  1.73s/it] 12%|█▏        | 2108/17834 [1:02:23<7:31:17,  1.72s/it] 12%|█▏        | 2109/17834 [1:02:25<7:36:43,  1.74s/it] 12%|█▏        | 2110/17834 [1:02:26<7:39:36,  1.75s/it] 12%|█▏        | 2111/17834 [1:02:28<7:37:08,  1.74s/it] 12%|█▏        | 2112/17834 [1:02:30<7:39:08,  1.75s/it] 12%|█▏        | 2113/17834 [1:02:32<7:37:50,  1.75s/it] 12%|█▏        | 2114/17834 [1:02:34<7:41:46,  1.76s/it] 12%|█▏        | 2115/17834 [1:02:35<7:46:04,  1.78s/it] 12%|█▏        | 2116/17834 [1:02:37<7:48:50,  1.79s/it] 12%|█▏        | 2117/17834 [1:02:39<7:43:10,  1.77s/it] 12%|█▏        | 2118/17834 [1:02:41<7:39:02,  1.75s/it] 12%|█▏        | 2119/17834 [1:02:42<7:39:49,  1.76s/it] 12%|█▏        | 2120/17834 [1:02:44<7:37:29,  1.75s/it] 12%|█▏        | 2121/17834 [1:02:46<7:39:35,  1.75s/it] 12%|█▏        | 2122/17834 [1:02:48<7:35:03,  1.74s/it] 12%|█▏        | 2123/17834 [1:02:49<7:31:32,  1.72s/it] 12%|█▏        | 2124/17834 [1:02:51<7:39:29,  1.75s/it] 12%|█▏        | 2125/17834 [1:02:53<7:41:07,  1.76s/it] 12%|█▏        | 2126/17834 [1:02:55<7:41:15,  1.76s/it] 12%|█▏        | 2127/17834 [1:02:56<7:38:35,  1.75s/it] 12%|█▏        | 2128/17834 [1:02:58<7:43:54,  1.77s/it] 12%|█▏        | 2129/17834 [1:03:00<7:38:39,  1.75s/it] 12%|█▏        | 2130/17834 [1:03:02<7:39:26,  1.76s/it] 12%|█▏        | 2131/17834 [1:03:03<7:41:36,  1.76s/it] 12%|█▏        | 2132/17834 [1:03:05<7:34:40,  1.74s/it] 12%|█▏        | 2133/17834 [1:03:07<7:30:03,  1.72s/it] 12%|█▏        | 2134/17834 [1:03:08<7:27:36,  1.71s/it] 12%|█▏        | 2135/17834 [1:03:10<7:26:51,  1.71s/it] 12%|█▏        | 2136/17834 [1:03:12<7:27:05,  1.71s/it] 12%|█▏        | 2137/17834 [1:03:14<7:30:44,  1.72s/it] 12%|█▏        | 2138/17834 [1:03:15<7:26:48,  1.71s/it] 12%|█▏        | 2139/17834 [1:03:17<7:31:27,  1.73s/it] 12%|█▏        | 2140/17834 [1:03:19<7:39:45,  1.76s/it] 12%|█▏        | 2141/17834 [1:03:21<7:39:00,  1.75s/it] 12%|█▏        | 2142/17834 [1:03:22<7:40:46,  1.76s/it] 12%|█▏        | 2143/17834 [1:03:24<7:38:04,  1.75s/it] 12%|█▏        | 2144/17834 [1:03:26<7:37:17,  1.75s/it] 12%|█▏        | 2145/17834 [1:03:28<7:32:15,  1.73s/it] 12%|█▏        | 2146/17834 [1:03:29<7:36:16,  1.75s/it] 12%|█▏        | 2147/17834 [1:03:31<7:33:21,  1.73s/it] 12%|█▏        | 2148/17834 [1:03:33<7:34:21,  1.74s/it] 12%|█▏        | 2149/17834 [1:03:34<7:29:32,  1.72s/it]08/30/2024 20:17:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.3022379875183105, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05833790451288223, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.446181297302246, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.806756973266602}
 12%|█▏        | 2150/17834 [1:03:36<7:29:52,  1.72s/it] 12%|█▏        | 2151/17834 [1:03:38<7:36:20,  1.75s/it] 12%|█▏        | 2152/17834 [1:03:40<7:33:43,  1.74s/it] 12%|█▏        | 2153/17834 [1:03:42<7:40:38,  1.76s/it] 12%|█▏        | 2154/17834 [1:03:43<7:36:14,  1.75s/it] 12%|█▏        | 2155/17834 [1:03:45<7:36:03,  1.75s/it] 12%|█▏        | 2156/17834 [1:03:47<7:39:36,  1.76s/it] 12%|█▏        | 2157/17834 [1:03:49<7:39:17,  1.76s/it] 12%|█▏        | 2158/17834 [1:03:50<7:38:38,  1.76s/it] 12%|█▏        | 2159/17834 [1:03:52<7:37:02,  1.75s/it] 12%|█▏        | 2160/17834 [1:03:54<7:35:44,  1.74s/it] 12%|█▏        | 2161/17834 [1:03:55<7:32:14,  1.73s/it] 12%|█▏        | 2162/17834 [1:03:57<7:31:07,  1.73s/it] 12%|█▏        | 2163/17834 [1:03:59<7:30:53,  1.73s/it] 12%|█▏        | 2164/17834 [1:04:01<7:31:58,  1.73s/it] 12%|█▏        | 2165/17834 [1:04:02<7:36:24,  1.75s/it] 12%|█▏        | 2166/17834 [1:04:04<7:35:16,  1.74s/it] 12%|█▏        | 2167/17834 [1:04:06<7:40:49,  1.76s/it] 12%|█▏        | 2168/17834 [1:04:08<7:40:33,  1.76s/it] 12%|█▏        | 2169/17834 [1:04:09<7:35:12,  1.74s/it] 12%|█▏        | 2170/17834 [1:04:11<7:39:58,  1.76s/it] 12%|█▏        | 2171/17834 [1:04:13<7:36:19,  1.75s/it] 12%|█▏        | 2172/17834 [1:04:15<7:36:58,  1.75s/it] 12%|█▏        | 2173/17834 [1:04:16<7:34:17,  1.74s/it] 12%|█▏        | 2174/17834 [1:04:18<7:41:58,  1.77s/it] 12%|█▏        | 2175/17834 [1:04:20<7:38:47,  1.76s/it] 12%|█▏        | 2176/17834 [1:04:22<7:42:26,  1.77s/it] 12%|█▏        | 2177/17834 [1:04:24<7:38:13,  1.76s/it] 12%|█▏        | 2178/17834 [1:04:25<7:39:46,  1.76s/it] 12%|█▏        | 2179/17834 [1:04:27<7:46:12,  1.79s/it] 12%|█▏        | 2180/17834 [1:04:29<7:42:09,  1.77s/it] 12%|█▏        | 2181/17834 [1:04:31<7:44:00,  1.78s/it] 12%|█▏        | 2182/17834 [1:04:32<7:45:26,  1.78s/it] 12%|█▏        | 2183/17834 [1:04:34<7:38:40,  1.76s/it] 12%|█▏        | 2184/17834 [1:04:36<7:42:51,  1.77s/it] 12%|█▏        | 2185/17834 [1:04:38<7:40:20,  1.77s/it] 12%|█▏        | 2186/17834 [1:04:40<7:46:17,  1.79s/it] 12%|█▏        | 2187/17834 [1:04:41<7:49:05,  1.80s/it] 12%|█▏        | 2188/17834 [1:04:43<7:48:57,  1.80s/it] 12%|█▏        | 2189/17834 [1:04:45<7:40:35,  1.77s/it] 12%|█▏        | 2190/17834 [1:04:47<7:38:29,  1.76s/it] 12%|█▏        | 2191/17834 [1:04:48<7:33:59,  1.74s/it] 12%|█▏        | 2192/17834 [1:04:50<7:31:37,  1.73s/it] 12%|█▏        | 2193/17834 [1:04:52<7:36:37,  1.75s/it] 12%|█▏        | 2194/17834 [1:04:54<7:37:28,  1.76s/it] 12%|█▏        | 2195/17834 [1:04:55<7:33:20,  1.74s/it] 12%|█▏        | 2196/17834 [1:04:57<7:32:36,  1.74s/it] 12%|█▏        | 2197/17834 [1:04:59<7:37:05,  1.75s/it] 12%|█▏        | 2198/17834 [1:05:01<7:45:37,  1.79s/it] 12%|█▏        | 2199/17834 [1:05:02<7:45:04,  1.78s/it]08/30/2024 20:19:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.436920642852783, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.059889715164899826, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.8031530380249023, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 5.299963474273682}
 12%|█▏        | 2200/17834 [1:05:04<7:43:20,  1.78s/it] 12%|█▏        | 2201/17834 [1:05:06<7:38:52,  1.76s/it] 12%|█▏        | 2202/17834 [1:05:08<7:36:17,  1.75s/it] 12%|█▏        | 2203/17834 [1:05:09<7:32:43,  1.74s/it] 12%|█▏        | 2204/17834 [1:05:11<7:32:43,  1.74s/it] 12%|█▏        | 2205/17834 [1:05:13<7:32:16,  1.74s/it] 12%|█▏        | 2206/17834 [1:05:15<7:34:42,  1.75s/it] 12%|█▏        | 2207/17834 [1:05:16<7:40:39,  1.77s/it] 12%|█▏        | 2208/17834 [1:05:18<7:34:40,  1.75s/it] 12%|█▏        | 2209/17834 [1:05:20<7:32:47,  1.74s/it] 12%|█▏        | 2210/17834 [1:05:22<7:32:28,  1.74s/it] 12%|█▏        | 2211/17834 [1:05:23<7:38:25,  1.76s/it] 12%|█▏        | 2212/17834 [1:05:25<7:34:51,  1.75s/it] 12%|█▏        | 2213/17834 [1:05:27<7:38:48,  1.76s/it] 12%|█▏        | 2214/17834 [1:05:29<7:40:37,  1.77s/it] 12%|█▏        | 2215/17834 [1:05:30<7:34:42,  1.75s/it] 12%|█▏        | 2216/17834 [1:05:32<7:34:55,  1.75s/it] 12%|█▏        | 2217/17834 [1:05:34<7:35:49,  1.75s/it] 12%|█▏        | 2218/17834 [1:05:36<7:36:21,  1.75s/it] 12%|█▏        | 2219/17834 [1:05:37<7:38:18,  1.76s/it] 12%|█▏        | 2220/17834 [1:05:39<7:39:13,  1.76s/it] 12%|█▏        | 2221/17834 [1:05:41<7:37:52,  1.76s/it] 12%|█▏        | 2222/17834 [1:05:43<7:32:45,  1.74s/it] 12%|█▏        | 2223/17834 [1:05:44<7:37:59,  1.76s/it] 12%|█▏        | 2224/17834 [1:05:46<7:39:18,  1.77s/it] 12%|█▏        | 2225/17834 [1:05:48<7:38:56,  1.76s/it] 12%|█▏        | 2226/17834 [1:05:50<7:38:40,  1.76s/it] 12%|█▏        | 2227/17834 [1:05:51<7:36:24,  1.75s/it] 12%|█▏        | 2228/17834 [1:05:53<7:31:16,  1.73s/it] 12%|█▏        | 2229/17834 [1:05:55<7:37:49,  1.76s/it] 13%|█▎        | 2230/17834 [1:05:57<7:39:27,  1.77s/it] 13%|█▎        | 2231/17834 [1:05:59<7:38:57,  1.76s/it] 13%|█▎        | 2232/17834 [1:06:00<7:35:45,  1.75s/it] 13%|█▎        | 2233/17834 [1:06:02<7:31:42,  1.74s/it] 13%|█▎        | 2234/17834 [1:06:04<7:47:19,  1.80s/it] 13%|█▎        | 2235/17834 [1:06:06<7:48:03,  1.80s/it] 13%|█▎        | 2236/17834 [1:06:07<7:41:44,  1.78s/it] 13%|█▎        | 2237/17834 [1:06:09<7:40:51,  1.77s/it] 13%|█▎        | 2238/17834 [1:06:11<7:36:48,  1.76s/it] 13%|█▎        | 2239/17834 [1:06:13<7:43:13,  1.78s/it] 13%|█▎        | 2240/17834 [1:06:15<7:41:13,  1.77s/it] 13%|█▎        | 2241/17834 [1:06:16<7:31:56,  1.74s/it] 13%|█▎        | 2242/17834 [1:06:18<7:33:17,  1.74s/it] 13%|█▎        | 2243/17834 [1:06:20<7:32:00,  1.74s/it] 13%|█▎        | 2244/17834 [1:06:21<7:35:48,  1.75s/it] 13%|█▎        | 2245/17834 [1:06:23<7:35:32,  1.75s/it] 13%|█▎        | 2246/17834 [1:06:25<7:28:59,  1.73s/it] 13%|█▎        | 2247/17834 [1:06:27<7:28:51,  1.73s/it] 13%|█▎        | 2248/17834 [1:06:28<7:25:57,  1.72s/it] 13%|█▎        | 2249/17834 [1:06:30<7:25:33,  1.72s/it]08/30/2024 20:20:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.248234510421753, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.035618603229522705, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2361602783203125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5200133323669434}
 13%|█▎        | 2250/17834 [1:06:32<7:23:52,  1.71s/it] 13%|█▎        | 2251/17834 [1:06:33<7:28:03,  1.73s/it] 13%|█▎        | 2252/17834 [1:06:35<7:35:08,  1.75s/it] 13%|█▎        | 2253/17834 [1:06:37<7:33:32,  1.75s/it] 13%|█▎        | 2254/17834 [1:06:39<7:33:23,  1.75s/it] 13%|█▎        | 2255/17834 [1:06:41<7:34:48,  1.75s/it] 13%|█▎        | 2256/17834 [1:06:42<7:32:17,  1.74s/it] 13%|█▎        | 2257/17834 [1:06:44<7:34:11,  1.75s/it] 13%|█▎        | 2258/17834 [1:06:46<7:35:07,  1.75s/it] 13%|█▎        | 2259/17834 [1:06:47<7:31:20,  1.74s/it] 13%|█▎        | 2260/17834 [1:06:49<7:29:19,  1.73s/it] 13%|█▎        | 2261/17834 [1:06:51<7:36:45,  1.76s/it] 13%|█▎        | 2262/17834 [1:06:53<7:36:26,  1.76s/it] 13%|█▎        | 2263/17834 [1:06:55<7:35:17,  1.75s/it] 13%|█▎        | 2264/17834 [1:06:56<7:31:10,  1.74s/it] 13%|█▎        | 2265/17834 [1:06:58<7:33:28,  1.75s/it] 13%|█▎        | 2266/17834 [1:07:00<7:29:33,  1.73s/it] 13%|█▎        | 2267/17834 [1:07:01<7:33:49,  1.75s/it] 13%|█▎        | 2268/17834 [1:07:03<7:28:10,  1.73s/it] 13%|█▎        | 2269/17834 [1:07:05<7:36:37,  1.76s/it] 13%|█▎        | 2270/17834 [1:07:07<7:38:18,  1.77s/it] 13%|█▎        | 2271/17834 [1:07:09<7:41:47,  1.78s/it] 13%|█▎        | 2272/17834 [1:07:10<7:39:06,  1.77s/it] 13%|█▎        | 2273/17834 [1:07:12<7:38:24,  1.77s/it] 13%|█▎        | 2274/17834 [1:07:14<7:38:12,  1.77s/it] 13%|█▎        | 2275/17834 [1:07:16<7:35:51,  1.76s/it] 13%|█▎        | 2276/17834 [1:07:17<7:45:28,  1.80s/it] 13%|█▎        | 2277/17834 [1:07:19<7:38:44,  1.77s/it] 13%|█▎        | 2278/17834 [1:07:21<7:41:08,  1.78s/it] 13%|█▎        | 2279/17834 [1:07:23<7:37:06,  1.76s/it] 13%|█▎        | 2280/17834 [1:07:24<7:33:45,  1.75s/it] 13%|█▎        | 2281/17834 [1:07:26<7:33:49,  1.75s/it] 13%|█▎        | 2282/17834 [1:07:28<7:33:13,  1.75s/it] 13%|█▎        | 2283/17834 [1:07:30<7:32:00,  1.74s/it] 13%|█▎        | 2284/17834 [1:07:31<7:29:26,  1.73s/it] 13%|█▎        | 2285/17834 [1:07:33<7:30:13,  1.74s/it] 13%|█▎        | 2286/17834 [1:07:35<7:26:44,  1.72s/it] 13%|█▎        | 2287/17834 [1:07:37<7:27:19,  1.73s/it] 13%|█▎        | 2288/17834 [1:07:38<7:27:45,  1.73s/it] 13%|█▎        | 2289/17834 [1:07:40<7:28:07,  1.73s/it] 13%|█▎        | 2290/17834 [1:07:42<7:32:45,  1.75s/it] 13%|█▎        | 2291/17834 [1:07:44<7:34:06,  1.75s/it] 13%|█▎        | 2292/17834 [1:07:45<7:31:01,  1.74s/it] 13%|█▎        | 2293/17834 [1:07:47<7:27:07,  1.73s/it] 13%|█▎        | 2294/17834 [1:07:49<7:28:09,  1.73s/it] 13%|█▎        | 2295/17834 [1:07:51<7:36:59,  1.76s/it] 13%|█▎        | 2296/17834 [1:07:52<7:37:27,  1.77s/it] 13%|█▎        | 2297/17834 [1:07:54<7:41:14,  1.78s/it] 13%|█▎        | 2298/17834 [1:07:56<7:39:06,  1.77s/it] 13%|█▎        | 2299/17834 [1:07:58<7:38:55,  1.77s/it]08/30/2024 20:22:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6183804273605347, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0306842140853405, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4041383266448975, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.053203105926514}
 13%|█▎        | 2300/17834 [1:07:59<7:30:12,  1.74s/it] 13%|█▎        | 2301/17834 [1:08:01<7:26:17,  1.72s/it] 13%|█▎        | 2302/17834 [1:08:03<7:31:08,  1.74s/it] 13%|█▎        | 2303/17834 [1:08:05<7:35:26,  1.76s/it] 13%|█▎        | 2304/17834 [1:08:06<7:30:17,  1.74s/it] 13%|█▎        | 2305/17834 [1:08:08<7:39:32,  1.78s/it] 13%|█▎        | 2306/17834 [1:08:10<7:32:32,  1.75s/it] 13%|█▎        | 2307/17834 [1:08:12<7:31:53,  1.75s/it] 13%|█▎        | 2308/17834 [1:08:13<7:27:39,  1.73s/it] 13%|█▎        | 2309/17834 [1:08:15<7:31:09,  1.74s/it] 13%|█▎        | 2310/17834 [1:08:17<7:38:30,  1.77s/it] 13%|█▎        | 2311/17834 [1:08:19<7:35:45,  1.76s/it] 13%|█▎        | 2312/17834 [1:08:20<7:40:39,  1.78s/it] 13%|█▎        | 2313/17834 [1:08:22<7:44:01,  1.79s/it] 13%|█▎        | 2314/17834 [1:08:24<7:35:10,  1.76s/it] 13%|█▎        | 2315/17834 [1:08:26<7:33:31,  1.75s/it] 13%|█▎        | 2316/17834 [1:08:27<7:35:52,  1.76s/it] 13%|█▎        | 2317/17834 [1:08:29<7:30:23,  1.74s/it] 13%|█▎        | 2318/17834 [1:08:31<7:27:19,  1.73s/it] 13%|█▎        | 2319/17834 [1:08:33<7:29:51,  1.74s/it] 13%|█▎        | 2320/17834 [1:08:34<7:26:43,  1.73s/it] 13%|█▎        | 2321/17834 [1:08:36<7:27:15,  1.73s/it] 13%|█▎        | 2322/17834 [1:08:38<7:33:50,  1.76s/it] 13%|█▎        | 2323/17834 [1:08:40<7:40:00,  1.78s/it] 13%|█▎        | 2324/17834 [1:08:41<7:37:20,  1.77s/it] 13%|█▎        | 2325/17834 [1:08:43<7:29:11,  1.74s/it] 13%|█▎        | 2326/17834 [1:08:45<7:26:45,  1.73s/it] 13%|█▎        | 2327/17834 [1:08:47<7:26:59,  1.73s/it] 13%|█▎        | 2328/17834 [1:08:48<7:31:29,  1.75s/it] 13%|█▎        | 2329/17834 [1:08:50<7:35:34,  1.76s/it] 13%|█▎        | 2330/17834 [1:08:52<7:38:44,  1.78s/it] 13%|█▎        | 2331/17834 [1:08:54<7:45:36,  1.80s/it] 13%|█▎        | 2332/17834 [1:08:56<7:38:34,  1.77s/it] 13%|█▎        | 2333/17834 [1:08:57<7:42:17,  1.79s/it] 13%|█▎        | 2334/17834 [1:08:59<7:43:24,  1.79s/it] 13%|█▎        | 2335/17834 [1:09:01<7:34:30,  1.76s/it] 13%|█▎        | 2336/17834 [1:09:03<7:29:29,  1.74s/it] 13%|█▎        | 2337/17834 [1:09:04<7:35:46,  1.76s/it] 13%|█▎        | 2338/17834 [1:09:06<7:34:13,  1.76s/it] 13%|█▎        | 2339/17834 [1:09:08<7:31:59,  1.75s/it] 13%|█▎        | 2340/17834 [1:09:10<7:30:40,  1.75s/it] 13%|█▎        | 2341/17834 [1:09:11<7:29:29,  1.74s/it] 13%|█▎        | 2342/17834 [1:09:13<7:32:21,  1.75s/it] 13%|█▎        | 2343/17834 [1:09:15<7:26:33,  1.73s/it] 13%|█▎        | 2344/17834 [1:09:16<7:25:39,  1.73s/it] 13%|█▎        | 2345/17834 [1:09:18<7:28:58,  1.74s/it] 13%|█▎        | 2346/17834 [1:09:20<7:27:10,  1.73s/it] 13%|█▎        | 2347/17834 [1:09:22<7:32:26,  1.75s/it] 13%|█▎        | 2348/17834 [1:09:24<7:34:58,  1.76s/it] 13%|█▎        | 2349/17834 [1:09:25<7:37:31,  1.77s/it]08/30/2024 20:23:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6438655853271484, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045212522149086, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.343783378601074, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.032861709594727}
 13%|█▎        | 2350/17834 [1:09:27<7:34:25,  1.76s/it] 13%|█▎        | 2351/17834 [1:09:29<7:31:27,  1.75s/it] 13%|█▎        | 2352/17834 [1:09:31<7:30:41,  1.75s/it] 13%|█▎        | 2353/17834 [1:09:32<7:28:31,  1.74s/it] 13%|█▎        | 2354/17834 [1:09:34<7:33:09,  1.76s/it] 13%|█▎        | 2355/17834 [1:09:36<7:34:17,  1.76s/it] 13%|█▎        | 2356/17834 [1:09:38<7:32:10,  1.75s/it] 13%|█▎        | 2357/17834 [1:09:39<7:28:50,  1.74s/it] 13%|█▎        | 2358/17834 [1:09:41<7:29:34,  1.74s/it] 13%|█▎        | 2359/17834 [1:09:43<7:31:31,  1.75s/it] 13%|█▎        | 2360/17834 [1:09:45<7:34:09,  1.76s/it] 13%|█▎        | 2361/17834 [1:09:46<7:36:13,  1.77s/it] 13%|█▎        | 2362/17834 [1:09:48<7:32:32,  1.75s/it] 13%|█▎        | 2363/17834 [1:09:50<7:31:30,  1.75s/it] 13%|█▎        | 2364/17834 [1:09:52<7:31:20,  1.75s/it] 13%|█▎        | 2365/17834 [1:09:53<7:27:57,  1.74s/it] 13%|█▎        | 2366/17834 [1:09:55<7:31:53,  1.75s/it] 13%|█▎        | 2367/17834 [1:09:57<7:32:57,  1.76s/it] 13%|█▎        | 2368/17834 [1:09:59<7:29:22,  1.74s/it] 13%|█▎        | 2369/17834 [1:10:00<7:36:23,  1.77s/it] 13%|█▎        | 2370/17834 [1:10:02<7:38:31,  1.78s/it] 13%|█▎        | 2371/17834 [1:10:04<7:30:33,  1.75s/it] 13%|█▎        | 2372/17834 [1:10:06<7:31:12,  1.75s/it] 13%|█▎        | 2373/17834 [1:10:07<7:32:09,  1.75s/it] 13%|█▎        | 2374/17834 [1:10:09<7:37:06,  1.77s/it] 13%|█▎        | 2375/17834 [1:10:11<7:35:47,  1.77s/it] 13%|█▎        | 2376/17834 [1:10:13<7:42:10,  1.79s/it] 13%|█▎        | 2377/17834 [1:10:14<7:34:28,  1.76s/it] 13%|█▎        | 2378/17834 [1:10:16<7:35:07,  1.77s/it] 13%|█▎        | 2379/17834 [1:10:18<7:31:09,  1.75s/it] 13%|█▎        | 2380/17834 [1:10:20<7:29:33,  1.75s/it] 13%|█▎        | 2381/17834 [1:10:22<7:36:21,  1.77s/it] 13%|█▎        | 2382/17834 [1:10:23<7:32:21,  1.76s/it] 13%|█▎        | 2383/17834 [1:10:25<7:30:16,  1.75s/it] 13%|█▎        | 2384/17834 [1:10:27<7:27:42,  1.74s/it] 13%|█▎        | 2385/17834 [1:10:28<7:27:11,  1.74s/it] 13%|█▎        | 2386/17834 [1:10:30<7:27:25,  1.74s/it] 13%|█▎        | 2387/17834 [1:10:32<7:27:28,  1.74s/it] 13%|█▎        | 2388/17834 [1:10:34<7:31:01,  1.75s/it] 13%|█▎        | 2389/17834 [1:10:35<7:31:12,  1.75s/it] 13%|█▎        | 2390/17834 [1:10:37<7:33:05,  1.76s/it] 13%|█▎        | 2391/17834 [1:10:39<7:32:21,  1.76s/it] 13%|█▎        | 2392/17834 [1:10:41<7:41:55,  1.79s/it] 13%|█▎        | 2393/17834 [1:10:43<7:36:08,  1.77s/it] 13%|█▎        | 2394/17834 [1:10:44<7:38:42,  1.78s/it] 13%|█▎        | 2395/17834 [1:10:46<7:34:12,  1.77s/it] 13%|█▎        | 2396/17834 [1:10:48<7:33:25,  1.76s/it] 13%|█▎        | 2397/17834 [1:10:50<7:32:01,  1.76s/it] 13%|█▎        | 2398/17834 [1:10:51<7:36:18,  1.77s/it] 13%|█▎        | 2399/17834 [1:10:53<7:36:47,  1.78s/it]08/30/2024 20:25:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.7058296203613281, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04166223853826523, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.270057439804077, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.017549514770508}
 13%|█▎        | 2400/17834 [1:10:55<7:35:46,  1.77s/it] 13%|█▎        | 2401/17834 [1:10:57<7:32:24,  1.76s/it] 13%|█▎        | 2402/17834 [1:10:59<7:37:39,  1.78s/it] 13%|█▎        | 2403/17834 [1:11:00<7:31:45,  1.76s/it] 13%|█▎        | 2404/17834 [1:11:02<7:30:07,  1.75s/it] 13%|█▎        | 2405/17834 [1:11:04<7:33:28,  1.76s/it] 13%|█▎        | 2406/17834 [1:11:06<7:37:18,  1.78s/it] 13%|█▎        | 2407/17834 [1:11:07<7:36:54,  1.78s/it] 14%|█▎        | 2408/17834 [1:11:09<7:40:43,  1.79s/it] 14%|█▎        | 2409/17834 [1:11:11<7:35:18,  1.77s/it] 14%|█▎        | 2410/17834 [1:11:13<7:33:26,  1.76s/it] 14%|█▎        | 2411/17834 [1:11:15<7:42:08,  1.80s/it] 14%|█▎        | 2412/17834 [1:11:16<7:32:41,  1.76s/it] 14%|█▎        | 2413/17834 [1:11:18<7:29:47,  1.75s/it] 14%|█▎        | 2414/17834 [1:11:20<7:36:44,  1.78s/it] 14%|█▎        | 2415/17834 [1:11:22<7:37:57,  1.78s/it] 14%|█▎        | 2416/17834 [1:11:23<7:33:46,  1.77s/it] 14%|█▎        | 2417/17834 [1:11:25<7:32:01,  1.76s/it] 14%|█▎        | 2418/17834 [1:11:27<7:25:41,  1.73s/it] 14%|█▎        | 2419/17834 [1:11:29<7:31:28,  1.76s/it] 14%|█▎        | 2420/17834 [1:11:30<7:31:43,  1.76s/it] 14%|█▎        | 2421/17834 [1:11:32<7:35:12,  1.77s/it] 14%|█▎        | 2422/17834 [1:11:34<7:27:29,  1.74s/it] 14%|█▎        | 2423/17834 [1:11:36<7:30:52,  1.76s/it] 14%|█▎        | 2424/17834 [1:11:37<7:32:00,  1.76s/it] 14%|█▎        | 2425/17834 [1:11:39<7:25:47,  1.74s/it] 14%|█▎        | 2426/17834 [1:11:41<7:23:52,  1.73s/it] 14%|█▎        | 2427/17834 [1:11:42<7:25:22,  1.73s/it] 14%|█▎        | 2428/17834 [1:11:44<7:30:02,  1.75s/it] 14%|█▎        | 2429/17834 [1:11:46<7:26:01,  1.74s/it] 14%|█▎        | 2430/17834 [1:11:48<7:32:26,  1.76s/it] 14%|█▎        | 2431/17834 [1:11:49<7:29:05,  1.75s/it] 14%|█▎        | 2432/17834 [1:11:51<7:36:40,  1.78s/it] 14%|█▎        | 2433/17834 [1:11:53<7:31:50,  1.76s/it] 14%|█▎        | 2434/17834 [1:11:55<7:37:17,  1.78s/it] 14%|█▎        | 2435/17834 [1:11:57<7:31:27,  1.76s/it] 14%|█▎        | 2436/17834 [1:11:58<7:26:05,  1.74s/it] 14%|█▎        | 2437/17834 [1:12:00<7:21:55,  1.72s/it] 14%|█▎        | 2438/17834 [1:12:02<7:24:16,  1.73s/it] 14%|█▎        | 2439/17834 [1:12:03<7:24:18,  1.73s/it] 14%|█▎        | 2440/17834 [1:12:05<7:27:53,  1.75s/it] 14%|█▎        | 2441/17834 [1:12:07<7:28:17,  1.75s/it] 14%|█▎        | 2442/17834 [1:12:09<7:23:43,  1.73s/it] 14%|█▎        | 2443/17834 [1:12:10<7:27:15,  1.74s/it] 14%|█▎        | 2444/17834 [1:12:12<7:26:02,  1.74s/it] 14%|█▎        | 2445/17834 [1:12:14<7:30:27,  1.76s/it] 14%|█▎        | 2446/17834 [1:12:16<7:30:46,  1.76s/it] 14%|█▎        | 2447/17834 [1:12:18<7:31:45,  1.76s/it] 14%|█▎        | 2448/17834 [1:12:19<7:28:42,  1.75s/it] 14%|█▎        | 2449/17834 [1:12:21<7:33:04,  1.77s/it]08/30/2024 20:26:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6086390018463135, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038660719990730286, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.45505428314209, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.102354049682617}
 14%|█▎        | 2450/17834 [1:12:23<7:27:56,  1.75s/it] 14%|█▎        | 2451/17834 [1:12:24<7:27:20,  1.74s/it] 14%|█▎        | 2452/17834 [1:12:26<7:30:59,  1.76s/it] 14%|█▍        | 2453/17834 [1:12:28<7:29:20,  1.75s/it] 14%|█▍        | 2454/17834 [1:12:30<7:34:12,  1.77s/it] 14%|█▍        | 2455/17834 [1:12:32<7:27:56,  1.75s/it] 14%|█▍        | 2456/17834 [1:12:33<7:37:20,  1.78s/it] 14%|█▍        | 2457/17834 [1:12:35<7:27:54,  1.75s/it] 14%|█▍        | 2458/17834 [1:12:37<7:26:32,  1.74s/it] 14%|█▍        | 2459/17834 [1:12:38<7:21:17,  1.72s/it] 14%|█▍        | 2460/17834 [1:12:40<7:20:52,  1.72s/it] 14%|█▍        | 2461/17834 [1:12:42<7:22:48,  1.73s/it] 14%|█▍        | 2462/17834 [1:12:44<7:24:19,  1.73s/it] 14%|█▍        | 2463/17834 [1:12:45<7:27:33,  1.75s/it] 14%|█▍        | 2464/17834 [1:12:47<7:25:59,  1.74s/it] 14%|█▍        | 2465/17834 [1:12:49<7:24:14,  1.73s/it] 14%|█▍        | 2466/17834 [1:12:51<7:28:49,  1.75s/it] 14%|█▍        | 2467/17834 [1:12:52<7:26:33,  1.74s/it] 14%|█▍        | 2468/17834 [1:12:54<7:26:35,  1.74s/it] 14%|█▍        | 2469/17834 [1:12:56<7:27:33,  1.75s/it] 14%|█▍        | 2470/17834 [1:12:58<7:23:12,  1.73s/it] 14%|█▍        | 2471/17834 [1:12:59<7:20:14,  1.72s/it] 14%|█▍        | 2472/17834 [1:13:01<7:20:38,  1.72s/it] 14%|█▍        | 2473/17834 [1:13:03<7:21:07,  1.72s/it] 14%|█▍        | 2474/17834 [1:13:05<7:29:47,  1.76s/it] 14%|█▍        | 2475/17834 [1:13:06<7:29:01,  1.75s/it] 14%|█▍        | 2476/17834 [1:13:08<7:29:23,  1.76s/it] 14%|█▍        | 2477/17834 [1:13:10<7:24:49,  1.74s/it] 14%|█▍        | 2478/17834 [1:13:11<7:23:36,  1.73s/it] 14%|█▍        | 2479/17834 [1:13:13<7:26:32,  1.74s/it] 14%|█▍        | 2480/17834 [1:13:15<7:23:52,  1.73s/it] 14%|█▍        | 2481/17834 [1:13:17<7:30:49,  1.76s/it] 14%|█▍        | 2482/17834 [1:13:19<7:26:12,  1.74s/it] 14%|█▍        | 2483/17834 [1:13:20<7:23:44,  1.73s/it] 14%|█▍        | 2484/17834 [1:13:22<7:28:21,  1.75s/it] 14%|█▍        | 2485/17834 [1:13:24<7:30:37,  1.76s/it] 14%|█▍        | 2486/17834 [1:13:26<7:35:35,  1.78s/it] 14%|█▍        | 2487/17834 [1:13:27<7:34:55,  1.78s/it] 14%|█▍        | 2488/17834 [1:13:29<7:30:32,  1.76s/it] 14%|█▍        | 2489/17834 [1:13:31<7:26:30,  1.75s/it] 14%|█▍        | 2490/17834 [1:13:33<7:25:00,  1.74s/it] 14%|█▍        | 2491/17834 [1:13:34<7:31:25,  1.77s/it] 14%|█▍        | 2492/17834 [1:13:36<7:27:35,  1.75s/it] 14%|█▍        | 2493/17834 [1:13:38<7:33:19,  1.77s/it] 14%|█▍        | 2494/17834 [1:13:40<7:31:00,  1.76s/it] 14%|█▍        | 2495/17834 [1:13:42<7:37:06,  1.79s/it] 14%|█▍        | 2496/17834 [1:13:43<7:31:29,  1.77s/it] 14%|█▍        | 2497/17834 [1:13:45<7:34:43,  1.78s/it] 14%|█▍        | 2498/17834 [1:13:47<7:26:16,  1.75s/it] 14%|█▍        | 2499/17834 [1:13:48<7:27:40,  1.75s/it]08/30/2024 20:28:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3712371587753296, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03783822059631348, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1962621212005615, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.605337619781494}
 14%|█▍        | 2500/17834 [1:13:50<7:23:37,  1.74s/it] 14%|█▍        | 2501/17834 [1:13:52<7:26:32,  1.75s/it] 14%|█▍        | 2502/17834 [1:13:54<7:31:08,  1.77s/it] 14%|█▍        | 2503/17834 [1:13:56<7:36:24,  1.79s/it] 14%|█▍        | 2504/17834 [1:13:57<7:31:31,  1.77s/it] 14%|█▍        | 2505/17834 [1:13:59<7:28:20,  1.75s/it] 14%|█▍        | 2506/17834 [1:14:01<7:26:27,  1.75s/it] 14%|█▍        | 2507/17834 [1:14:02<7:23:17,  1.74s/it] 14%|█▍        | 2508/17834 [1:14:04<7:22:41,  1.73s/it] 14%|█▍        | 2509/17834 [1:14:06<7:26:00,  1.75s/it] 14%|█▍        | 2510/17834 [1:14:08<7:24:41,  1.74s/it] 14%|█▍        | 2511/17834 [1:14:09<7:28:33,  1.76s/it] 14%|█▍        | 2512/17834 [1:14:11<7:24:15,  1.74s/it] 14%|█▍        | 2513/17834 [1:14:13<7:28:40,  1.76s/it] 14%|█▍        | 2514/17834 [1:14:15<7:22:34,  1.73s/it] 14%|█▍        | 2515/17834 [1:14:16<7:20:44,  1.73s/it] 14%|█▍        | 2516/17834 [1:14:18<7:21:12,  1.73s/it] 14%|█▍        | 2517/17834 [1:14:20<7:19:05,  1.72s/it] 14%|█▍        | 2518/17834 [1:14:22<7:18:52,  1.72s/it] 14%|█▍        | 2519/17834 [1:14:23<7:20:41,  1.73s/it] 14%|█▍        | 2520/17834 [1:14:25<7:28:21,  1.76s/it] 14%|█▍        | 2521/17834 [1:14:27<7:24:59,  1.74s/it] 14%|█▍        | 2522/17834 [1:14:29<7:23:46,  1.74s/it] 14%|█▍        | 2523/17834 [1:14:30<7:20:10,  1.72s/it] 14%|█▍        | 2524/17834 [1:14:32<7:19:54,  1.72s/it] 14%|█▍        | 2525/17834 [1:14:34<7:23:38,  1.74s/it] 14%|█▍        | 2526/17834 [1:14:35<7:22:11,  1.73s/it] 14%|█▍        | 2527/17834 [1:14:37<7:27:09,  1.75s/it] 14%|█▍        | 2528/17834 [1:14:39<7:20:20,  1.73s/it] 14%|█▍        | 2529/17834 [1:14:41<7:19:25,  1.72s/it] 14%|█▍        | 2530/17834 [1:14:42<7:18:23,  1.72s/it] 14%|█▍        | 2531/17834 [1:14:44<7:22:11,  1.73s/it] 14%|█▍        | 2532/17834 [1:14:46<7:28:42,  1.76s/it] 14%|█▍        | 2533/17834 [1:14:48<7:26:31,  1.75s/it] 14%|█▍        | 2534/17834 [1:14:49<7:29:05,  1.76s/it] 14%|█▍        | 2535/17834 [1:14:51<7:22:48,  1.74s/it] 14%|█▍        | 2536/17834 [1:14:53<7:22:57,  1.74s/it] 14%|█▍        | 2537/17834 [1:14:55<7:26:48,  1.75s/it] 14%|█▍        | 2538/17834 [1:14:56<7:22:51,  1.74s/it] 14%|█▍        | 2539/17834 [1:14:58<7:22:26,  1.74s/it] 14%|█▍        | 2540/17834 [1:15:00<7:29:58,  1.77s/it] 14%|█▍        | 2541/17834 [1:15:02<7:29:34,  1.76s/it] 14%|█▍        | 2542/17834 [1:15:03<7:31:06,  1.77s/it] 14%|█▍        | 2543/17834 [1:15:05<7:30:35,  1.77s/it] 14%|█▍        | 2544/17834 [1:15:07<7:28:54,  1.76s/it] 14%|█▍        | 2545/17834 [1:15:09<7:27:21,  1.76s/it] 14%|█▍        | 2546/17834 [1:15:10<7:27:53,  1.76s/it] 14%|█▍        | 2547/17834 [1:15:12<7:26:35,  1.75s/it] 14%|█▍        | 2548/17834 [1:15:14<7:24:59,  1.75s/it] 14%|█▍        | 2549/17834 [1:15:16<7:27:42,  1.76s/it]08/30/2024 20:29:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3830854892730713, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03965187072753906, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.269174098968506, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.691911458969116}
 14%|█▍        | 2550/17834 [1:15:17<7:27:15,  1.76s/it] 14%|█▍        | 2551/17834 [1:15:19<7:29:53,  1.77s/it] 14%|█▍        | 2552/17834 [1:15:21<7:28:49,  1.76s/it] 14%|█▍        | 2553/17834 [1:15:23<7:27:02,  1.76s/it] 14%|█▍        | 2554/17834 [1:15:24<7:24:36,  1.75s/it] 14%|█▍        | 2555/17834 [1:15:26<7:30:24,  1.77s/it] 14%|█▍        | 2556/17834 [1:15:28<7:35:46,  1.79s/it] 14%|█▍        | 2557/17834 [1:15:30<7:29:49,  1.77s/it] 14%|█▍        | 2558/17834 [1:15:32<7:28:08,  1.76s/it] 14%|█▍        | 2559/17834 [1:15:33<7:25:28,  1.75s/it] 14%|█▍        | 2560/17834 [1:15:35<7:21:17,  1.73s/it] 14%|█▍        | 2561/17834 [1:15:37<7:25:49,  1.75s/it] 14%|█▍        | 2562/17834 [1:15:39<7:23:52,  1.74s/it] 14%|█▍        | 2563/17834 [1:15:40<7:26:30,  1.75s/it] 14%|█▍        | 2564/17834 [1:15:42<7:30:52,  1.77s/it] 14%|█▍        | 2565/17834 [1:15:44<7:31:55,  1.78s/it] 14%|█▍        | 2566/17834 [1:15:46<7:30:52,  1.77s/it] 14%|█▍        | 2567/17834 [1:15:47<7:30:07,  1.77s/it] 14%|█▍        | 2568/17834 [1:15:49<7:34:31,  1.79s/it] 14%|█▍        | 2569/17834 [1:15:51<7:31:34,  1.77s/it] 14%|█▍        | 2570/17834 [1:15:53<7:24:24,  1.75s/it] 14%|█▍        | 2571/17834 [1:15:54<7:24:29,  1.75s/it] 14%|█▍        | 2572/17834 [1:15:56<7:24:35,  1.75s/it] 14%|█▍        | 2573/17834 [1:15:58<7:23:45,  1.74s/it] 14%|█▍        | 2574/17834 [1:16:00<7:27:50,  1.76s/it] 14%|█▍        | 2575/17834 [1:16:02<7:27:51,  1.76s/it] 14%|█▍        | 2576/17834 [1:16:03<7:30:03,  1.77s/it] 14%|█▍        | 2577/17834 [1:16:05<7:26:40,  1.76s/it] 14%|█▍        | 2578/17834 [1:16:07<7:27:51,  1.76s/it] 14%|█▍        | 2579/17834 [1:16:09<7:32:03,  1.78s/it] 14%|█▍        | 2580/17834 [1:16:10<7:27:47,  1.76s/it] 14%|█▍        | 2581/17834 [1:16:12<7:22:44,  1.74s/it] 14%|█▍        | 2582/17834 [1:16:14<7:25:35,  1.75s/it] 14%|█▍        | 2583/17834 [1:16:16<7:25:34,  1.75s/it] 14%|█▍        | 2584/17834 [1:16:17<7:24:57,  1.75s/it] 14%|█▍        | 2585/17834 [1:16:19<7:31:29,  1.78s/it] 15%|█▍        | 2586/17834 [1:16:21<7:24:35,  1.75s/it] 15%|█▍        | 2587/17834 [1:16:23<7:26:08,  1.76s/it] 15%|█▍        | 2588/17834 [1:16:24<7:29:43,  1.77s/it] 15%|█▍        | 2589/17834 [1:16:26<7:26:15,  1.76s/it] 15%|█▍        | 2590/17834 [1:16:28<7:24:48,  1.75s/it] 15%|█▍        | 2591/17834 [1:16:30<7:21:28,  1.74s/it] 15%|█▍        | 2592/17834 [1:16:31<7:25:31,  1.75s/it] 15%|█▍        | 2593/17834 [1:16:33<7:23:08,  1.74s/it] 15%|█▍        | 2594/17834 [1:16:35<7:18:48,  1.73s/it] 15%|█▍        | 2595/17834 [1:16:36<7:18:50,  1.73s/it] 15%|█▍        | 2596/17834 [1:16:38<7:13:25,  1.71s/it] 15%|█▍        | 2597/17834 [1:16:40<7:20:32,  1.73s/it] 15%|█▍        | 2598/17834 [1:16:42<7:15:51,  1.72s/it] 15%|█▍        | 2599/17834 [1:16:43<7:13:48,  1.71s/it]08/30/2024 20:31:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.155346393585205, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03456435352563858, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1467678546905518, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3366785049438477}
 15%|█▍        | 2600/17834 [1:16:45<7:15:57,  1.72s/it] 15%|█▍        | 2601/17834 [1:16:47<7:14:20,  1.71s/it] 15%|█▍        | 2602/17834 [1:16:48<7:15:27,  1.72s/it] 15%|█▍        | 2603/17834 [1:16:50<7:20:56,  1.74s/it] 15%|█▍        | 2604/17834 [1:16:52<7:20:20,  1.73s/it] 15%|█▍        | 2605/17834 [1:16:54<7:18:36,  1.73s/it] 15%|█▍        | 2606/17834 [1:16:56<7:24:21,  1.75s/it] 15%|█▍        | 2607/17834 [1:16:57<7:20:34,  1.74s/it] 15%|█▍        | 2608/17834 [1:16:59<7:21:20,  1.74s/it] 15%|█▍        | 2609/17834 [1:17:01<7:22:31,  1.74s/it] 15%|█▍        | 2610/17834 [1:17:03<7:28:56,  1.77s/it] 15%|█▍        | 2611/17834 [1:17:04<7:27:57,  1.77s/it] 15%|█▍        | 2612/17834 [1:17:06<7:31:07,  1.78s/it] 15%|█▍        | 2613/17834 [1:17:08<7:24:06,  1.75s/it] 15%|█▍        | 2614/17834 [1:17:10<7:29:37,  1.77s/it] 15%|█▍        | 2615/17834 [1:17:11<7:26:02,  1.76s/it] 15%|█▍        | 2616/17834 [1:17:13<7:23:58,  1.75s/it] 15%|█▍        | 2617/17834 [1:17:15<7:29:48,  1.77s/it] 15%|█▍        | 2618/17834 [1:17:17<7:26:20,  1.76s/it] 15%|█▍        | 2619/17834 [1:17:18<7:22:58,  1.75s/it] 15%|█▍        | 2620/17834 [1:17:20<7:24:14,  1.75s/it] 15%|█▍        | 2621/17834 [1:17:22<7:18:26,  1.73s/it] 15%|█▍        | 2622/17834 [1:17:23<7:14:13,  1.71s/it] 15%|█▍        | 2623/17834 [1:17:25<7:18:15,  1.73s/it] 15%|█▍        | 2624/17834 [1:17:27<7:13:04,  1.71s/it] 15%|█▍        | 2625/17834 [1:17:29<7:17:10,  1.72s/it] 15%|█▍        | 2626/17834 [1:17:30<7:19:09,  1.73s/it] 15%|█▍        | 2627/17834 [1:17:32<7:18:35,  1.73s/it] 15%|█▍        | 2628/17834 [1:17:34<7:26:24,  1.76s/it] 15%|█▍        | 2629/17834 [1:17:36<7:21:47,  1.74s/it] 15%|█▍        | 2630/17834 [1:17:37<7:19:04,  1.73s/it] 15%|█▍        | 2631/17834 [1:17:39<7:20:52,  1.74s/it] 15%|█▍        | 2632/17834 [1:17:41<7:25:02,  1.76s/it] 15%|█▍        | 2633/17834 [1:17:43<7:23:34,  1.75s/it] 15%|█▍        | 2634/17834 [1:17:44<7:23:47,  1.75s/it] 15%|█▍        | 2635/17834 [1:17:46<7:23:02,  1.75s/it] 15%|█▍        | 2636/17834 [1:17:48<7:18:18,  1.73s/it] 15%|█▍        | 2637/17834 [1:17:50<7:17:39,  1.73s/it] 15%|█▍        | 2638/17834 [1:17:51<7:18:36,  1.73s/it] 15%|█▍        | 2639/17834 [1:17:53<7:17:21,  1.73s/it] 15%|█▍        | 2640/17834 [1:17:55<7:21:38,  1.74s/it] 15%|█▍        | 2641/17834 [1:17:57<7:22:12,  1.75s/it] 15%|█▍        | 2642/17834 [1:17:58<7:23:27,  1.75s/it] 15%|█▍        | 2643/17834 [1:18:00<7:20:07,  1.74s/it] 15%|█▍        | 2644/17834 [1:18:02<7:23:43,  1.75s/it] 15%|█▍        | 2645/17834 [1:18:04<7:22:46,  1.75s/it] 15%|█▍        | 2646/17834 [1:18:05<7:24:25,  1.76s/it] 15%|█▍        | 2647/17834 [1:18:07<7:19:34,  1.74s/it] 15%|█▍        | 2648/17834 [1:18:09<7:18:49,  1.73s/it] 15%|█▍        | 2649/17834 [1:18:10<7:18:23,  1.73s/it]08/30/2024 20:32:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5416873693466187, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04049336165189743, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4108164310455322, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.992997169494629}
 15%|█▍        | 2650/17834 [1:18:12<7:21:16,  1.74s/it] 15%|█▍        | 2651/17834 [1:18:14<7:22:10,  1.75s/it] 15%|█▍        | 2652/17834 [1:18:16<7:24:15,  1.76s/it] 15%|█▍        | 2653/17834 [1:18:18<7:22:13,  1.75s/it] 15%|█▍        | 2654/17834 [1:18:19<7:22:19,  1.75s/it] 15%|█▍        | 2655/17834 [1:18:21<7:20:07,  1.74s/it] 15%|█▍        | 2656/17834 [1:18:23<7:22:07,  1.75s/it] 15%|█▍        | 2657/17834 [1:18:24<7:16:28,  1.73s/it] 15%|█▍        | 2658/17834 [1:18:26<7:17:40,  1.73s/it] 15%|█▍        | 2659/17834 [1:18:28<7:18:51,  1.74s/it] 15%|█▍        | 2660/17834 [1:18:30<7:15:19,  1.72s/it] 15%|█▍        | 2661/17834 [1:18:31<7:17:09,  1.73s/it] 15%|█▍        | 2662/17834 [1:18:33<7:20:14,  1.74s/it] 15%|█▍        | 2663/17834 [1:18:35<7:21:16,  1.75s/it] 15%|█▍        | 2664/17834 [1:18:37<7:24:22,  1.76s/it] 15%|█▍        | 2665/17834 [1:18:38<7:16:44,  1.73s/it] 15%|█▍        | 2666/17834 [1:18:40<7:21:47,  1.75s/it] 15%|█▍        | 2667/17834 [1:18:42<7:20:39,  1.74s/it] 15%|█▍        | 2668/17834 [1:18:44<7:17:37,  1.73s/it] 15%|█▍        | 2669/17834 [1:18:45<7:15:49,  1.72s/it] 15%|█▍        | 2670/17834 [1:18:47<7:16:05,  1.73s/it] 15%|█▍        | 2671/17834 [1:18:49<7:17:46,  1.73s/it] 15%|█▍        | 2672/17834 [1:18:50<7:19:12,  1.74s/it] 15%|█▍        | 2673/17834 [1:18:52<7:18:43,  1.74s/it] 15%|█▍        | 2674/17834 [1:18:54<7:18:28,  1.74s/it] 15%|█▍        | 2675/17834 [1:18:56<7:22:16,  1.75s/it] 15%|█▌        | 2676/17834 [1:18:57<7:20:08,  1.74s/it] 15%|█▌        | 2677/17834 [1:18:59<7:18:22,  1.74s/it] 15%|█▌        | 2678/17834 [1:19:01<7:19:58,  1.74s/it] 15%|█▌        | 2679/17834 [1:19:03<7:26:55,  1.77s/it] 15%|█▌        | 2680/17834 [1:19:04<7:20:35,  1.74s/it] 15%|█▌        | 2681/17834 [1:19:06<7:20:35,  1.74s/it] 15%|█▌        | 2682/17834 [1:19:08<7:26:39,  1.77s/it] 15%|█▌        | 2683/17834 [1:19:10<7:23:22,  1.76s/it] 15%|█▌        | 2684/17834 [1:19:12<7:25:01,  1.76s/it] 15%|█▌        | 2685/17834 [1:19:13<7:23:58,  1.76s/it] 15%|█▌        | 2686/17834 [1:19:15<7:19:01,  1.74s/it] 15%|█▌        | 2687/17834 [1:19:17<7:20:16,  1.74s/it] 15%|█▌        | 2688/17834 [1:19:18<7:17:23,  1.73s/it] 15%|█▌        | 2689/17834 [1:19:20<7:19:41,  1.74s/it] 15%|█▌        | 2690/17834 [1:19:22<7:17:33,  1.73s/it] 15%|█▌        | 2691/17834 [1:19:24<7:19:14,  1.74s/it] 15%|█▌        | 2692/17834 [1:19:25<7:15:53,  1.73s/it] 15%|█▌        | 2693/17834 [1:19:27<7:26:45,  1.77s/it] 15%|█▌        | 2694/17834 [1:19:29<7:21:02,  1.75s/it] 15%|█▌        | 2695/17834 [1:19:31<7:20:44,  1.75s/it] 15%|█▌        | 2696/17834 [1:19:32<7:22:34,  1.75s/it] 15%|█▌        | 2697/17834 [1:19:34<7:22:15,  1.75s/it] 15%|█▌        | 2698/17834 [1:19:36<7:18:56,  1.74s/it] 15%|█▌        | 2699/17834 [1:19:38<7:19:20,  1.74s/it]08/30/2024 20:33:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.579754114151001, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05119075998663902, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.284467935562134, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9154129028320312}
 15%|█▌        | 2700/17834 [1:19:39<7:14:41,  1.72s/it] 15%|█▌        | 2701/17834 [1:19:41<7:16:57,  1.73s/it] 15%|█▌        | 2702/17834 [1:19:43<7:18:12,  1.74s/it] 15%|█▌        | 2703/17834 [1:19:45<7:13:53,  1.72s/it] 15%|█▌        | 2704/17834 [1:19:46<7:18:14,  1.74s/it] 15%|█▌        | 2705/17834 [1:19:48<7:18:58,  1.74s/it] 15%|█▌        | 2706/17834 [1:19:50<7:21:55,  1.75s/it] 15%|█▌        | 2707/17834 [1:19:52<7:17:22,  1.73s/it] 15%|█▌        | 2708/17834 [1:19:53<7:14:18,  1.72s/it] 15%|█▌        | 2709/17834 [1:19:55<7:16:03,  1.73s/it] 15%|█▌        | 2710/17834 [1:19:57<7:16:46,  1.73s/it] 15%|█▌        | 2711/17834 [1:19:59<7:27:08,  1.77s/it] 15%|█▌        | 2712/17834 [1:20:00<7:21:01,  1.75s/it] 15%|█▌        | 2713/17834 [1:20:02<7:23:22,  1.76s/it] 15%|█▌        | 2714/17834 [1:20:04<7:24:08,  1.76s/it] 15%|█▌        | 2715/17834 [1:20:05<7:18:33,  1.74s/it] 15%|█▌        | 2716/17834 [1:20:07<7:16:14,  1.73s/it] 15%|█▌        | 2717/17834 [1:20:09<7:23:02,  1.76s/it] 15%|█▌        | 2718/17834 [1:20:11<7:19:42,  1.75s/it] 15%|█▌        | 2719/17834 [1:20:13<7:25:15,  1.77s/it] 15%|█▌        | 2720/17834 [1:20:14<7:22:42,  1.76s/it] 15%|█▌        | 2721/17834 [1:20:16<7:23:28,  1.76s/it] 15%|█▌        | 2722/17834 [1:20:18<7:18:59,  1.74s/it] 15%|█▌        | 2723/17834 [1:20:19<7:18:18,  1.74s/it] 15%|█▌        | 2724/17834 [1:20:21<7:22:33,  1.76s/it] 15%|█▌        | 2725/17834 [1:20:23<7:17:46,  1.74s/it] 15%|█▌        | 2726/17834 [1:20:25<7:14:49,  1.73s/it] 15%|█▌        | 2727/17834 [1:20:26<7:14:20,  1.73s/it] 15%|█▌        | 2728/17834 [1:20:28<7:17:15,  1.74s/it] 15%|█▌        | 2729/17834 [1:20:30<7:16:24,  1.73s/it] 15%|█▌        | 2730/17834 [1:20:32<7:16:37,  1.73s/it] 15%|█▌        | 2731/17834 [1:20:33<7:19:34,  1.75s/it] 15%|█▌        | 2732/17834 [1:20:35<7:22:41,  1.76s/it] 15%|█▌        | 2733/17834 [1:20:37<7:40:29,  1.83s/it] 15%|█▌        | 2734/17834 [1:20:39<7:26:14,  1.77s/it] 15%|█▌        | 2735/17834 [1:20:41<7:22:29,  1.76s/it] 15%|█▌        | 2736/17834 [1:20:42<7:24:50,  1.77s/it] 15%|█▌        | 2737/17834 [1:20:44<7:18:33,  1.74s/it] 15%|█▌        | 2738/17834 [1:20:46<7:20:15,  1.75s/it] 15%|█▌        | 2739/17834 [1:20:48<7:18:22,  1.74s/it] 15%|█▌        | 2740/17834 [1:20:49<7:21:17,  1.75s/it] 15%|█▌        | 2741/17834 [1:20:51<7:22:05,  1.76s/it] 15%|█▌        | 2742/17834 [1:20:53<7:18:48,  1.74s/it] 15%|█▌        | 2743/17834 [1:20:54<7:16:36,  1.74s/it] 15%|█▌        | 2744/17834 [1:20:56<7:19:29,  1.75s/it] 15%|█▌        | 2745/17834 [1:20:58<7:16:19,  1.73s/it] 15%|█▌        | 2746/17834 [1:21:00<7:19:14,  1.75s/it] 15%|█▌        | 2747/17834 [1:21:01<7:15:17,  1.73s/it] 15%|█▌        | 2748/17834 [1:21:03<7:16:44,  1.74s/it] 15%|█▌        | 2749/17834 [1:21:05<7:21:59,  1.76s/it]08/30/2024 20:35:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4986298084259033, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0436384342610836, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2070863246917725, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.749354600906372}
 15%|█▌        | 2750/17834 [1:21:07<7:19:16,  1.75s/it] 15%|█▌        | 2751/17834 [1:21:08<7:15:49,  1.73s/it] 15%|█▌        | 2752/17834 [1:21:10<7:15:50,  1.73s/it] 15%|█▌        | 2753/17834 [1:21:12<7:19:15,  1.75s/it] 15%|█▌        | 2754/17834 [1:21:14<7:18:02,  1.74s/it] 15%|█▌        | 2755/17834 [1:21:15<7:18:41,  1.75s/it] 15%|█▌        | 2756/17834 [1:21:17<7:20:54,  1.75s/it] 15%|█▌        | 2757/17834 [1:21:19<7:24:23,  1.77s/it] 15%|█▌        | 2758/17834 [1:21:21<7:20:40,  1.75s/it] 15%|█▌        | 2759/17834 [1:21:22<7:21:09,  1.76s/it] 15%|█▌        | 2760/17834 [1:21:24<7:22:20,  1.76s/it] 15%|█▌        | 2761/17834 [1:21:26<7:16:29,  1.74s/it] 15%|█▌        | 2762/17834 [1:21:28<7:13:07,  1.72s/it] 15%|█▌        | 2763/17834 [1:21:29<7:15:55,  1.74s/it] 15%|█▌        | 2764/17834 [1:21:31<7:11:09,  1.72s/it] 16%|█▌        | 2765/17834 [1:21:33<7:20:34,  1.75s/it] 16%|█▌        | 2766/17834 [1:21:35<7:13:57,  1.73s/it] 16%|█▌        | 2767/17834 [1:21:36<7:11:23,  1.72s/it] 16%|█▌        | 2768/17834 [1:21:38<7:13:17,  1.73s/it] 16%|█▌        | 2769/17834 [1:21:40<7:19:16,  1.75s/it] 16%|█▌        | 2770/17834 [1:21:42<7:15:26,  1.73s/it] 16%|█▌        | 2771/17834 [1:21:43<7:17:11,  1.74s/it] 16%|█▌        | 2772/17834 [1:21:45<7:15:16,  1.73s/it] 16%|█▌        | 2773/17834 [1:21:47<7:19:20,  1.75s/it] 16%|█▌        | 2774/17834 [1:21:48<7:15:20,  1.73s/it] 16%|█▌        | 2775/17834 [1:21:50<7:11:25,  1.72s/it] 16%|█▌        | 2776/17834 [1:21:52<7:11:38,  1.72s/it] 16%|█▌        | 2777/17834 [1:21:54<7:21:17,  1.76s/it] 16%|█▌        | 2778/17834 [1:21:55<7:17:26,  1.74s/it] 16%|█▌        | 2779/17834 [1:21:57<7:23:15,  1.77s/it] 16%|█▌        | 2780/17834 [1:21:59<7:24:15,  1.77s/it] 16%|█▌        | 2781/17834 [1:22:01<7:24:27,  1.77s/it] 16%|█▌        | 2782/17834 [1:22:03<7:24:26,  1.77s/it] 16%|█▌        | 2783/17834 [1:22:04<7:18:13,  1.75s/it] 16%|█▌        | 2784/17834 [1:22:06<7:16:46,  1.74s/it] 16%|█▌        | 2785/17834 [1:22:08<7:22:14,  1.76s/it] 16%|█▌        | 2786/17834 [1:22:10<7:19:20,  1.75s/it] 16%|█▌        | 2787/17834 [1:22:11<7:19:33,  1.75s/it] 16%|█▌        | 2788/17834 [1:22:13<7:29:01,  1.79s/it] 16%|█▌        | 2789/17834 [1:22:15<7:21:33,  1.76s/it] 16%|█▌        | 2790/17834 [1:22:17<7:33:23,  1.81s/it] 16%|█▌        | 2791/17834 [1:22:18<7:25:47,  1.78s/it] 16%|█▌        | 2792/17834 [1:22:20<7:20:40,  1.76s/it] 16%|█▌        | 2793/17834 [1:22:22<7:19:17,  1.75s/it] 16%|█▌        | 2794/17834 [1:22:24<7:14:44,  1.73s/it] 16%|█▌        | 2795/17834 [1:22:25<7:15:39,  1.74s/it] 16%|█▌        | 2796/17834 [1:22:27<7:21:29,  1.76s/it] 16%|█▌        | 2797/17834 [1:22:29<7:18:23,  1.75s/it] 16%|█▌        | 2798/17834 [1:22:31<7:12:29,  1.73s/it] 16%|█▌        | 2799/17834 [1:22:32<7:18:55,  1.75s/it]08/30/2024 20:36:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.8996257781982422, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05234822630882263, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.6329431533813477, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.584917068481445}
 16%|█▌        | 2800/17834 [1:22:34<7:23:24,  1.77s/it] 16%|█▌        | 2801/17834 [1:22:36<7:20:00,  1.76s/it] 16%|█▌        | 2802/17834 [1:22:38<7:17:28,  1.75s/it] 16%|█▌        | 2803/17834 [1:22:39<7:14:08,  1.73s/it] 16%|█▌        | 2804/17834 [1:22:41<7:16:24,  1.74s/it] 16%|█▌        | 2805/17834 [1:22:43<7:16:52,  1.74s/it] 16%|█▌        | 2806/17834 [1:22:45<7:17:21,  1.75s/it] 16%|█▌        | 2807/17834 [1:22:46<7:17:59,  1.75s/it] 16%|█▌        | 2808/17834 [1:22:48<7:22:40,  1.77s/it] 16%|█▌        | 2809/17834 [1:22:50<7:28:04,  1.79s/it] 16%|█▌        | 2810/17834 [1:22:52<7:21:47,  1.76s/it] 16%|█▌        | 2811/17834 [1:22:53<7:14:16,  1.73s/it] 16%|█▌        | 2812/17834 [1:22:55<7:14:50,  1.74s/it] 16%|█▌        | 2813/17834 [1:22:57<7:19:08,  1.75s/it] 16%|█▌        | 2814/17834 [1:22:59<7:19:58,  1.76s/it] 16%|█▌        | 2815/17834 [1:23:00<7:15:09,  1.74s/it] 16%|█▌        | 2816/17834 [1:23:02<7:24:12,  1.77s/it] 16%|█▌        | 2817/17834 [1:23:04<7:23:56,  1.77s/it] 16%|█▌        | 2818/17834 [1:23:06<7:20:46,  1.76s/it] 16%|█▌        | 2819/17834 [1:23:08<7:23:49,  1.77s/it] 16%|█▌        | 2820/17834 [1:23:09<7:27:41,  1.79s/it] 16%|█▌        | 2821/17834 [1:23:11<7:28:04,  1.79s/it] 16%|█▌        | 2822/17834 [1:23:13<7:21:57,  1.77s/it] 16%|█▌        | 2823/17834 [1:23:15<7:18:13,  1.75s/it] 16%|█▌        | 2824/17834 [1:23:16<7:20:03,  1.76s/it] 16%|█▌        | 2825/17834 [1:23:18<7:19:15,  1.76s/it] 16%|█▌        | 2826/17834 [1:23:20<7:16:28,  1.74s/it] 16%|█▌        | 2827/17834 [1:23:22<7:10:56,  1.72s/it] 16%|█▌        | 2828/17834 [1:23:23<7:12:29,  1.73s/it] 16%|█▌        | 2829/17834 [1:23:25<7:14:45,  1.74s/it] 16%|█▌        | 2830/17834 [1:23:27<7:30:43,  1.80s/it] 16%|█▌        | 2831/17834 [1:23:29<7:24:58,  1.78s/it] 16%|█▌        | 2832/17834 [1:23:30<7:20:34,  1.76s/it] 16%|█▌        | 2833/17834 [1:23:32<7:34:11,  1.82s/it] 16%|█▌        | 2834/17834 [1:23:34<7:24:53,  1.78s/it] 16%|█▌        | 2835/17834 [1:23:36<7:19:05,  1.76s/it] 16%|█▌        | 2836/17834 [1:23:37<7:16:05,  1.74s/it] 16%|█▌        | 2837/17834 [1:23:39<7:19:20,  1.76s/it] 16%|█▌        | 2838/17834 [1:23:41<7:24:46,  1.78s/it] 16%|█▌        | 2839/17834 [1:23:43<7:24:12,  1.78s/it] 16%|█▌        | 2840/17834 [1:23:45<7:21:31,  1.77s/it] 16%|█▌        | 2841/17834 [1:23:46<7:16:55,  1.75s/it] 16%|█▌        | 2842/17834 [1:23:48<7:12:40,  1.73s/it] 16%|█▌        | 2843/17834 [1:23:50<7:15:43,  1.74s/it] 16%|█▌        | 2844/17834 [1:23:52<7:23:24,  1.77s/it] 16%|█▌        | 2845/17834 [1:23:53<7:24:39,  1.78s/it] 16%|█▌        | 2846/17834 [1:23:55<7:28:24,  1.80s/it] 16%|█▌        | 2847/17834 [1:23:57<7:18:44,  1.76s/it] 16%|█▌        | 2848/17834 [1:23:59<7:21:42,  1.77s/it] 16%|█▌        | 2849/17834 [1:24:00<7:20:57,  1.77s/it]08/30/2024 20:38:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4868659973144531, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04405999183654785, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3835041522979736, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9144301414489746}
 16%|█▌        | 2850/17834 [1:24:02<7:20:26,  1.76s/it] 16%|█▌        | 2851/17834 [1:24:04<7:19:27,  1.76s/it] 16%|█▌        | 2852/17834 [1:24:06<7:20:18,  1.76s/it] 16%|█▌        | 2853/17834 [1:24:08<7:19:20,  1.76s/it] 16%|█▌        | 2854/17834 [1:24:09<7:20:17,  1.76s/it] 16%|█▌        | 2855/17834 [1:24:11<7:20:47,  1.77s/it] 16%|█▌        | 2856/17834 [1:24:13<7:23:18,  1.78s/it] 16%|█▌        | 2857/17834 [1:24:15<7:21:48,  1.77s/it] 16%|█▌        | 2858/17834 [1:24:16<7:18:43,  1.76s/it] 16%|█▌        | 2859/17834 [1:24:18<7:20:39,  1.77s/it] 16%|█▌        | 2860/17834 [1:24:20<7:13:03,  1.74s/it] 16%|█▌        | 2861/17834 [1:24:21<7:10:30,  1.73s/it] 16%|█▌        | 2862/17834 [1:24:23<7:10:07,  1.72s/it] 16%|█▌        | 2863/17834 [1:24:25<7:20:09,  1.76s/it] 16%|█▌        | 2864/17834 [1:24:27<7:16:04,  1.75s/it] 16%|█▌        | 2865/17834 [1:24:29<7:14:20,  1.74s/it] 16%|█▌        | 2866/17834 [1:24:30<7:14:45,  1.74s/it] 16%|█▌        | 2867/17834 [1:24:32<7:11:32,  1.73s/it] 16%|█▌        | 2868/17834 [1:24:34<7:14:18,  1.74s/it] 16%|█▌        | 2869/17834 [1:24:35<7:13:37,  1.74s/it] 16%|█▌        | 2870/17834 [1:24:37<7:19:48,  1.76s/it] 16%|█▌        | 2871/17834 [1:24:39<7:17:16,  1.75s/it] 16%|█▌        | 2872/17834 [1:24:41<7:28:30,  1.80s/it] 16%|█▌        | 2873/17834 [1:24:43<7:24:57,  1.78s/it] 16%|█▌        | 2874/17834 [1:24:44<7:22:28,  1.77s/it] 16%|█▌        | 2875/17834 [1:24:46<7:19:44,  1.76s/it] 16%|█▌        | 2876/17834 [1:24:48<7:15:20,  1.75s/it] 16%|█▌        | 2877/17834 [1:24:50<7:15:26,  1.75s/it] 16%|█▌        | 2878/17834 [1:24:51<7:15:29,  1.75s/it] 16%|█▌        | 2879/17834 [1:24:53<7:16:30,  1.75s/it] 16%|█▌        | 2880/17834 [1:24:55<7:21:38,  1.77s/it] 16%|█▌        | 2881/17834 [1:24:57<7:14:24,  1.74s/it] 16%|█▌        | 2882/17834 [1:24:58<7:13:22,  1.74s/it] 16%|█▌        | 2883/17834 [1:25:00<7:15:25,  1.75s/it] 16%|█▌        | 2884/17834 [1:25:02<7:16:37,  1.75s/it] 16%|█▌        | 2885/17834 [1:25:04<7:18:48,  1.76s/it] 16%|█▌        | 2886/17834 [1:25:05<7:14:31,  1.74s/it] 16%|█▌        | 2887/17834 [1:25:07<7:16:06,  1.75s/it] 16%|█▌        | 2888/17834 [1:25:09<7:17:08,  1.75s/it] 16%|█▌        | 2889/17834 [1:25:11<7:13:51,  1.74s/it] 16%|█▌        | 2890/17834 [1:25:12<7:18:28,  1.76s/it] 16%|█▌        | 2891/17834 [1:25:14<7:15:30,  1.75s/it] 16%|█▌        | 2892/17834 [1:25:16<7:11:39,  1.73s/it] 16%|█▌        | 2893/17834 [1:25:18<7:16:40,  1.75s/it] 16%|█▌        | 2894/17834 [1:25:19<7:14:25,  1.74s/it] 16%|█▌        | 2895/17834 [1:25:21<7:19:41,  1.77s/it] 16%|█▌        | 2896/17834 [1:25:23<7:13:57,  1.74s/it] 16%|█▌        | 2897/17834 [1:25:25<7:11:44,  1.73s/it] 16%|█▌        | 2898/17834 [1:25:26<7:18:23,  1.76s/it] 16%|█▋        | 2899/17834 [1:25:28<7:11:09,  1.73s/it]08/30/2024 20:39:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3806942701339722, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04587730020284653, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2560155391693115, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.682587146759033}
 16%|█▋        | 2900/17834 [1:25:30<7:11:22,  1.73s/it] 16%|█▋        | 2901/17834 [1:25:32<7:10:33,  1.73s/it] 16%|█▋        | 2902/17834 [1:25:33<7:10:14,  1.73s/it] 16%|█▋        | 2903/17834 [1:25:35<7:11:30,  1.73s/it] 16%|█▋        | 2904/17834 [1:25:37<7:16:35,  1.75s/it] 16%|█▋        | 2905/17834 [1:25:39<7:20:27,  1.77s/it] 16%|█▋        | 2906/17834 [1:25:40<7:20:52,  1.77s/it] 16%|█▋        | 2907/17834 [1:25:42<7:19:15,  1.77s/it] 16%|█▋        | 2908/17834 [1:25:44<7:21:29,  1.77s/it] 16%|█▋        | 2909/17834 [1:25:46<7:17:45,  1.76s/it] 16%|█▋        | 2910/17834 [1:25:47<7:13:36,  1.74s/it] 16%|█▋        | 2911/17834 [1:25:49<7:15:37,  1.75s/it] 16%|█▋        | 2912/17834 [1:25:51<7:18:07,  1.76s/it] 16%|█▋        | 2913/17834 [1:25:53<7:18:24,  1.76s/it] 16%|█▋        | 2914/17834 [1:25:54<7:21:00,  1.77s/it] 16%|█▋        | 2915/17834 [1:25:56<7:16:14,  1.75s/it] 16%|█▋        | 2916/17834 [1:25:58<7:18:43,  1.76s/it] 16%|█▋        | 2917/17834 [1:26:00<7:18:30,  1.76s/it] 16%|█▋        | 2918/17834 [1:26:01<7:17:25,  1.76s/it] 16%|█▋        | 2919/17834 [1:26:03<7:10:29,  1.73s/it] 16%|█▋        | 2920/17834 [1:26:05<7:14:03,  1.75s/it] 16%|█▋        | 2921/17834 [1:26:07<7:14:36,  1.75s/it] 16%|█▋        | 2922/17834 [1:26:08<7:14:41,  1.75s/it] 16%|█▋        | 2923/17834 [1:26:10<7:14:03,  1.75s/it] 16%|█▋        | 2924/17834 [1:26:12<7:15:18,  1.75s/it] 16%|█▋        | 2925/17834 [1:26:14<7:10:09,  1.73s/it] 16%|█▋        | 2926/17834 [1:26:15<7:10:07,  1.73s/it] 16%|█▋        | 2927/17834 [1:26:17<7:09:26,  1.73s/it] 16%|█▋        | 2928/17834 [1:26:19<7:08:53,  1.73s/it] 16%|█▋        | 2929/17834 [1:26:20<7:06:10,  1.72s/it] 16%|█▋        | 2930/17834 [1:26:22<7:04:41,  1.71s/it] 16%|█▋        | 2931/17834 [1:26:24<7:04:46,  1.71s/it] 16%|█▋        | 2932/17834 [1:26:26<7:06:29,  1.72s/it] 16%|█▋        | 2933/17834 [1:26:27<7:04:07,  1.71s/it] 16%|█▋        | 2934/17834 [1:26:29<7:17:02,  1.76s/it] 16%|█▋        | 2935/17834 [1:26:31<7:17:31,  1.76s/it] 16%|█▋        | 2936/17834 [1:26:33<7:20:01,  1.77s/it] 16%|█▋        | 2937/17834 [1:26:35<7:22:17,  1.78s/it] 16%|█▋        | 2938/17834 [1:26:36<7:16:39,  1.76s/it] 16%|█▋        | 2939/17834 [1:26:38<7:16:21,  1.76s/it] 16%|█▋        | 2940/17834 [1:26:40<7:13:22,  1.75s/it] 16%|█▋        | 2941/17834 [1:26:41<7:13:53,  1.75s/it] 16%|█▋        | 2942/17834 [1:26:43<7:10:35,  1.73s/it] 17%|█▋        | 2943/17834 [1:26:45<7:12:29,  1.74s/it] 17%|█▋        | 2944/17834 [1:26:47<7:10:01,  1.73s/it] 17%|█▋        | 2945/17834 [1:26:48<7:07:47,  1.72s/it] 17%|█▋        | 2946/17834 [1:26:50<7:13:44,  1.75s/it] 17%|█▋        | 2947/17834 [1:26:52<7:17:41,  1.76s/it] 17%|█▋        | 2948/17834 [1:26:54<7:14:22,  1.75s/it] 17%|█▋        | 2949/17834 [1:26:55<7:14:41,  1.75s/it]08/30/2024 20:41:15 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3581873178482056, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.054835278540849686, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.164219856262207, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.577242374420166}
 17%|█▋        | 2950/17834 [1:26:57<7:11:07,  1.74s/it] 17%|█▋        | 2951/17834 [1:26:59<7:10:04,  1.73s/it] 17%|█▋        | 2952/17834 [1:27:01<7:12:47,  1.74s/it] 17%|█▋        | 2953/17834 [1:27:02<7:18:17,  1.77s/it] 17%|█▋        | 2954/17834 [1:27:04<7:15:39,  1.76s/it] 17%|█▋        | 2955/17834 [1:27:06<7:20:16,  1.78s/it] 17%|█▋        | 2956/17834 [1:27:08<7:17:46,  1.77s/it] 17%|█▋        | 2957/17834 [1:27:09<7:15:49,  1.76s/it] 17%|█▋        | 2958/17834 [1:27:11<7:17:19,  1.76s/it] 17%|█▋        | 2959/17834 [1:27:13<7:14:26,  1.75s/it] 17%|█▋        | 2960/17834 [1:27:15<7:19:11,  1.77s/it] 17%|█▋        | 2961/17834 [1:27:17<7:14:20,  1.75s/it] 17%|█▋        | 2962/17834 [1:27:18<7:12:09,  1.74s/it] 17%|█▋        | 2963/17834 [1:27:20<7:11:49,  1.74s/it] 17%|█▋        | 2964/17834 [1:27:22<7:09:19,  1.73s/it] 17%|█▋        | 2965/17834 [1:27:23<7:11:52,  1.74s/it] 17%|█▋        | 2966/17834 [1:27:25<7:13:06,  1.75s/it] 17%|█▋        | 2967/17834 [1:27:27<7:17:48,  1.77s/it] 17%|█▋        | 2968/17834 [1:27:29<7:18:14,  1.77s/it] 17%|█▋        | 2969/17834 [1:27:31<7:21:09,  1.78s/it] 17%|█▋        | 2970/17834 [1:27:32<7:16:17,  1.76s/it] 17%|█▋        | 2971/17834 [1:27:34<7:22:02,  1.78s/it] 17%|█▋        | 2972/17834 [1:27:36<7:23:13,  1.79s/it] 17%|█▋        | 2973/17834 [1:27:38<7:20:23,  1.78s/it] 17%|█▋        | 2974/17834 [1:27:39<7:19:50,  1.78s/it] 17%|█▋        | 2975/17834 [1:27:41<7:16:15,  1.76s/it] 17%|█▋        | 2976/17834 [1:27:43<7:15:06,  1.76s/it] 17%|█▋        | 2977/17834 [1:27:45<7:17:53,  1.77s/it] 17%|█▋        | 2978/17834 [1:27:47<7:17:17,  1.77s/it] 17%|█▋        | 2979/17834 [1:27:48<7:10:40,  1.74s/it] 17%|█▋        | 2980/17834 [1:27:50<7:06:29,  1.72s/it] 17%|█▋        | 2981/17834 [1:27:52<7:06:45,  1.72s/it] 17%|█▋        | 2982/17834 [1:27:53<7:06:07,  1.72s/it] 17%|█▋        | 2983/17834 [1:27:55<7:08:41,  1.73s/it] 17%|█▋        | 2984/17834 [1:27:57<7:15:12,  1.76s/it] 17%|█▋        | 2985/17834 [1:27:59<7:11:30,  1.74s/it] 17%|█▋        | 2986/17834 [1:28:00<7:10:08,  1.74s/it] 17%|█▋        | 2987/17834 [1:28:02<7:07:40,  1.73s/it] 17%|█▋        | 2988/17834 [1:28:04<7:04:02,  1.71s/it] 17%|█▋        | 2989/17834 [1:28:05<7:02:02,  1.71s/it] 17%|█▋        | 2990/17834 [1:28:07<7:09:59,  1.74s/it] 17%|█▋        | 2991/17834 [1:28:09<7:05:59,  1.72s/it] 17%|█▋        | 2992/17834 [1:28:11<7:11:16,  1.74s/it] 17%|█▋        | 2993/17834 [1:28:12<7:08:06,  1.73s/it] 17%|█▋        | 2994/17834 [1:28:14<7:14:44,  1.76s/it] 17%|█▋        | 2995/17834 [1:28:16<7:12:45,  1.75s/it] 17%|█▋        | 2996/17834 [1:28:18<7:07:06,  1.73s/it] 17%|█▋        | 2997/17834 [1:28:19<7:06:43,  1.73s/it] 17%|█▋        | 2998/17834 [1:28:21<7:08:22,  1.73s/it] 17%|█▋        | 2999/17834 [1:28:23<7:06:28,  1.72s/it]08/30/2024 20:42:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.7027432918548584, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04163796454668045, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5855350494384766, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.329916477203369}
 17%|█▋        | 3000/17834 [1:28:25<7:07:21,  1.73s/it] 17%|█▋        | 3001/17834 [1:28:26<7:10:39,  1.74s/it] 17%|█▋        | 3002/17834 [1:28:28<7:24:47,  1.80s/it] 17%|█▋        | 3003/17834 [1:28:30<7:15:20,  1.76s/it] 17%|█▋        | 3004/17834 [1:28:32<7:11:42,  1.75s/it] 17%|█▋        | 3005/17834 [1:28:33<7:15:09,  1.76s/it] 17%|█▋        | 3006/17834 [1:28:35<7:14:53,  1.76s/it] 17%|█▋        | 3007/17834 [1:28:37<7:11:51,  1.75s/it] 17%|█▋        | 3008/17834 [1:28:39<7:12:44,  1.75s/it] 17%|█▋        | 3009/17834 [1:28:40<7:09:29,  1.74s/it] 17%|█▋        | 3010/17834 [1:28:42<7:13:26,  1.75s/it] 17%|█▋        | 3011/17834 [1:28:44<7:15:28,  1.76s/it] 17%|█▋        | 3012/17834 [1:28:46<7:12:23,  1.75s/it] 17%|█▋        | 3013/17834 [1:28:47<7:15:46,  1.76s/it] 17%|█▋        | 3014/17834 [1:28:49<7:12:23,  1.75s/it] 17%|█▋        | 3015/17834 [1:28:51<7:11:41,  1.75s/it] 17%|█▋        | 3016/17834 [1:28:53<7:05:07,  1.72s/it] 17%|█▋        | 3017/17834 [1:28:54<7:07:01,  1.73s/it] 17%|█▋        | 3018/17834 [1:28:56<7:11:56,  1.75s/it] 17%|█▋        | 3019/17834 [1:28:58<7:08:56,  1.74s/it] 17%|█▋        | 3020/17834 [1:29:00<7:12:37,  1.75s/it] 17%|█▋        | 3021/17834 [1:29:01<7:12:24,  1.75s/it] 17%|█▋        | 3022/17834 [1:29:03<7:18:16,  1.78s/it] 17%|█▋        | 3023/17834 [1:29:05<7:18:26,  1.78s/it] 17%|█▋        | 3024/17834 [1:29:07<7:08:37,  1.74s/it] 17%|█▋        | 3025/17834 [1:29:09<7:18:52,  1.78s/it] 17%|█▋        | 3026/17834 [1:29:10<7:14:23,  1.76s/it] 17%|█▋        | 3027/17834 [1:29:12<7:17:15,  1.77s/it] 17%|█▋        | 3028/17834 [1:29:14<7:09:56,  1.74s/it] 17%|█▋        | 3029/17834 [1:29:15<7:11:27,  1.75s/it] 17%|█▋        | 3030/17834 [1:29:17<7:10:55,  1.75s/it] 17%|█▋        | 3031/17834 [1:29:19<7:11:09,  1.75s/it] 17%|█▋        | 3032/17834 [1:29:21<7:08:29,  1.74s/it] 17%|█▋        | 3033/17834 [1:29:22<7:08:56,  1.74s/it] 17%|█▋        | 3034/17834 [1:29:24<7:12:17,  1.75s/it] 17%|█▋        | 3035/17834 [1:29:26<7:16:30,  1.77s/it] 17%|█▋        | 3036/17834 [1:29:28<7:10:52,  1.75s/it] 17%|█▋        | 3037/17834 [1:29:29<7:11:39,  1.75s/it] 17%|█▋        | 3038/17834 [1:29:31<7:09:45,  1.74s/it] 17%|█▋        | 3039/17834 [1:29:33<7:08:29,  1.74s/it] 17%|█▋        | 3040/17834 [1:29:35<7:06:14,  1.73s/it] 17%|█▋        | 3041/17834 [1:29:36<7:05:55,  1.73s/it] 17%|█▋        | 3042/17834 [1:29:38<7:10:06,  1.74s/it] 17%|█▋        | 3043/17834 [1:29:40<7:09:08,  1.74s/it] 17%|█▋        | 3044/17834 [1:29:42<7:10:55,  1.75s/it] 17%|█▋        | 3045/17834 [1:29:43<7:20:09,  1.79s/it] 17%|█▋        | 3046/17834 [1:29:45<7:15:46,  1.77s/it] 17%|█▋        | 3047/17834 [1:29:47<7:11:04,  1.75s/it] 17%|█▋        | 3048/17834 [1:29:49<7:12:56,  1.76s/it] 17%|█▋        | 3049/17834 [1:29:50<7:10:19,  1.75s/it]08/30/2024 20:44:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.688750982284546, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05002214014530182, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4126362800598145, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.151409149169922}
 17%|█▋        | 3050/17834 [1:29:52<7:10:37,  1.75s/it] 17%|█▋        | 3051/17834 [1:29:54<7:11:30,  1.75s/it] 17%|█▋        | 3052/17834 [1:29:56<7:14:51,  1.77s/it] 17%|█▋        | 3053/17834 [1:29:57<7:11:26,  1.75s/it] 17%|█▋        | 3054/17834 [1:29:59<7:07:33,  1.74s/it] 17%|█▋        | 3055/17834 [1:30:01<7:16:01,  1.77s/it] 17%|█▋        | 3056/17834 [1:30:03<7:14:52,  1.77s/it] 17%|█▋        | 3057/17834 [1:30:05<7:14:03,  1.76s/it] 17%|█▋        | 3058/17834 [1:30:06<7:13:29,  1.76s/it] 17%|█▋        | 3059/17834 [1:30:08<7:20:47,  1.79s/it] 17%|█▋        | 3060/17834 [1:30:10<7:18:37,  1.78s/it] 17%|█▋        | 3061/17834 [1:30:12<7:11:57,  1.75s/it] 17%|█▋        | 3062/17834 [1:30:13<7:16:24,  1.77s/it] 17%|█▋        | 3063/17834 [1:30:15<7:14:17,  1.76s/it] 17%|█▋        | 3064/17834 [1:30:17<7:18:30,  1.78s/it] 17%|█▋        | 3065/17834 [1:30:19<7:16:32,  1.77s/it] 17%|█▋        | 3066/17834 [1:30:21<7:20:38,  1.79s/it] 17%|█▋        | 3067/17834 [1:30:22<7:10:36,  1.75s/it] 17%|█▋        | 3068/17834 [1:30:24<7:11:08,  1.75s/it] 17%|█▋        | 3069/17834 [1:30:26<7:13:37,  1.76s/it] 17%|█▋        | 3070/17834 [1:30:27<7:09:45,  1.75s/it] 17%|█▋        | 3071/17834 [1:30:29<7:15:05,  1.77s/it] 17%|█▋        | 3072/17834 [1:30:31<7:11:41,  1.75s/it] 17%|█▋        | 3073/17834 [1:30:33<7:11:32,  1.75s/it] 17%|█▋        | 3074/17834 [1:30:35<7:13:03,  1.76s/it] 17%|█▋        | 3075/17834 [1:30:36<7:12:32,  1.76s/it] 17%|█▋        | 3076/17834 [1:30:38<7:10:41,  1.75s/it] 17%|█▋        | 3077/17834 [1:30:40<7:13:14,  1.76s/it] 17%|█▋        | 3078/17834 [1:30:42<7:09:36,  1.75s/it] 17%|█▋        | 3079/17834 [1:30:43<7:07:20,  1.74s/it] 17%|█▋        | 3080/17834 [1:30:45<7:07:58,  1.74s/it] 17%|█▋        | 3081/17834 [1:30:47<7:10:33,  1.75s/it] 17%|█▋        | 3082/17834 [1:30:48<7:06:58,  1.74s/it] 17%|█▋        | 3083/17834 [1:30:50<7:04:36,  1.73s/it] 17%|█▋        | 3084/17834 [1:30:52<7:03:07,  1.72s/it] 17%|█▋        | 3085/17834 [1:30:54<7:06:07,  1.73s/it] 17%|█▋        | 3086/17834 [1:30:55<7:04:54,  1.73s/it] 17%|█▋        | 3087/17834 [1:30:57<7:06:25,  1.73s/it] 17%|█▋        | 3088/17834 [1:30:59<7:05:19,  1.73s/it] 17%|█▋        | 3089/17834 [1:31:01<7:06:23,  1.74s/it] 17%|█▋        | 3090/17834 [1:31:02<7:12:07,  1.76s/it] 17%|█▋        | 3091/17834 [1:31:04<7:05:20,  1.73s/it] 17%|█▋        | 3092/17834 [1:31:06<7:02:48,  1.72s/it] 17%|█▋        | 3093/17834 [1:31:07<7:04:18,  1.73s/it] 17%|█▋        | 3094/17834 [1:31:09<7:11:50,  1.76s/it] 17%|█▋        | 3095/17834 [1:31:11<7:08:47,  1.75s/it] 17%|█▋        | 3096/17834 [1:31:13<7:08:23,  1.74s/it] 17%|█▋        | 3097/17834 [1:31:14<7:03:58,  1.73s/it] 17%|█▋        | 3098/17834 [1:31:16<7:11:41,  1.76s/it] 17%|█▋        | 3099/17834 [1:31:18<7:14:18,  1.77s/it]08/30/2024 20:45:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.160642385482788, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03559478372335434, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.177004098892212, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.373241424560547}
 17%|█▋        | 3100/17834 [1:31:20<7:10:49,  1.75s/it] 17%|█▋        | 3101/17834 [1:31:22<7:19:58,  1.79s/it] 17%|█▋        | 3102/17834 [1:31:23<7:10:59,  1.76s/it] 17%|█▋        | 3103/17834 [1:31:25<7:09:43,  1.75s/it] 17%|█▋        | 3104/17834 [1:31:27<7:07:05,  1.74s/it] 17%|█▋        | 3105/17834 [1:31:29<7:08:05,  1.74s/it] 17%|█▋        | 3106/17834 [1:31:30<7:06:53,  1.74s/it] 17%|█▋        | 3107/17834 [1:31:32<7:09:36,  1.75s/it] 17%|█▋        | 3108/17834 [1:31:34<7:07:43,  1.74s/it] 17%|█▋        | 3109/17834 [1:31:36<7:09:30,  1.75s/it] 17%|█▋        | 3110/17834 [1:31:37<7:06:20,  1.74s/it] 17%|█▋        | 3111/17834 [1:31:39<7:05:15,  1.73s/it] 17%|█▋        | 3112/17834 [1:31:41<7:09:44,  1.75s/it] 17%|█▋        | 3113/17834 [1:31:42<7:06:25,  1.74s/it] 17%|█▋        | 3114/17834 [1:31:44<7:06:09,  1.74s/it] 17%|█▋        | 3115/17834 [1:31:46<7:05:20,  1.73s/it] 17%|█▋        | 3116/17834 [1:31:48<7:09:47,  1.75s/it] 17%|█▋        | 3117/17834 [1:31:49<7:10:35,  1.76s/it] 17%|█▋        | 3118/17834 [1:31:51<7:07:51,  1.74s/it] 17%|█▋        | 3119/17834 [1:31:53<7:06:30,  1.74s/it] 17%|█▋        | 3120/17834 [1:31:55<7:04:05,  1.73s/it] 18%|█▊        | 3121/17834 [1:31:56<7:06:26,  1.74s/it] 18%|█▊        | 3122/17834 [1:31:58<7:12:01,  1.76s/it] 18%|█▊        | 3123/17834 [1:32:00<7:08:20,  1.75s/it] 18%|█▊        | 3124/17834 [1:32:02<7:04:42,  1.73s/it] 18%|█▊        | 3125/17834 [1:32:03<7:05:14,  1.73s/it] 18%|█▊        | 3126/17834 [1:32:05<7:03:27,  1.73s/it] 18%|█▊        | 3127/17834 [1:32:07<7:01:56,  1.72s/it] 18%|█▊        | 3128/17834 [1:32:08<7:00:53,  1.72s/it] 18%|█▊        | 3129/17834 [1:32:10<7:05:08,  1.73s/it] 18%|█▊        | 3130/17834 [1:32:12<7:04:24,  1.73s/it] 18%|█▊        | 3131/17834 [1:32:14<7:04:43,  1.73s/it] 18%|█▊        | 3132/17834 [1:32:16<7:08:39,  1.75s/it] 18%|█▊        | 3133/17834 [1:32:17<7:14:30,  1.77s/it] 18%|█▊        | 3134/17834 [1:32:19<7:13:05,  1.77s/it] 18%|█▊        | 3135/17834 [1:32:21<7:06:32,  1.74s/it] 18%|█▊        | 3136/17834 [1:32:22<7:04:22,  1.73s/it] 18%|█▊        | 3137/17834 [1:32:24<7:08:52,  1.75s/it] 18%|█▊        | 3138/17834 [1:32:26<7:07:38,  1.75s/it] 18%|█▊        | 3139/17834 [1:32:28<7:06:13,  1.74s/it] 18%|█▊        | 3140/17834 [1:32:30<7:16:44,  1.78s/it] 18%|█▊        | 3141/17834 [1:32:31<7:12:24,  1.77s/it] 18%|█▊        | 3142/17834 [1:32:33<7:12:01,  1.76s/it] 18%|█▊        | 3143/17834 [1:32:35<7:05:27,  1.74s/it] 18%|█▊        | 3144/17834 [1:32:36<7:02:52,  1.73s/it] 18%|█▊        | 3145/17834 [1:32:38<7:03:12,  1.73s/it] 18%|█▊        | 3146/17834 [1:32:40<7:00:01,  1.72s/it] 18%|█▊        | 3147/17834 [1:32:42<7:01:06,  1.72s/it] 18%|█▊        | 3148/17834 [1:32:43<7:02:00,  1.72s/it] 18%|█▊        | 3149/17834 [1:32:45<7:06:08,  1.74s/it]08/30/2024 20:47:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.965897560119629, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.044340625405311584, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3512730598449707, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.36151123046875}
 18%|█▊        | 3150/17834 [1:32:47<7:02:17,  1.73s/it] 18%|█▊        | 3151/17834 [1:32:49<7:00:22,  1.72s/it] 18%|█▊        | 3152/17834 [1:32:50<7:05:14,  1.74s/it] 18%|█▊        | 3153/17834 [1:32:52<7:05:38,  1.74s/it] 18%|█▊        | 3154/17834 [1:32:54<7:08:21,  1.75s/it] 18%|█▊        | 3155/17834 [1:32:56<7:07:33,  1.75s/it] 18%|█▊        | 3156/17834 [1:32:57<7:05:17,  1.74s/it] 18%|█▊        | 3157/17834 [1:32:59<7:01:25,  1.72s/it] 18%|█▊        | 3158/17834 [1:33:01<7:03:22,  1.73s/it] 18%|█▊        | 3159/17834 [1:33:03<7:07:06,  1.75s/it] 18%|█▊        | 3160/17834 [1:33:04<7:11:27,  1.76s/it] 18%|█▊        | 3161/17834 [1:33:06<7:03:31,  1.73s/it] 18%|█▊        | 3162/17834 [1:33:08<7:06:28,  1.74s/it] 18%|█▊        | 3163/17834 [1:33:10<7:11:06,  1.76s/it] 18%|█▊        | 3164/17834 [1:33:11<7:14:46,  1.78s/it] 18%|█▊        | 3165/17834 [1:33:13<7:08:50,  1.75s/it] 18%|█▊        | 3166/17834 [1:33:15<7:02:22,  1.73s/it] 18%|█▊        | 3167/17834 [1:33:17<7:05:28,  1.74s/it] 18%|█▊        | 3168/17834 [1:33:18<7:04:05,  1.74s/it] 18%|█▊        | 3169/17834 [1:33:20<7:01:45,  1.73s/it] 18%|█▊        | 3170/17834 [1:33:22<7:06:15,  1.74s/it] 18%|█▊        | 3171/17834 [1:33:23<7:07:10,  1.75s/it] 18%|█▊        | 3172/17834 [1:33:25<7:07:01,  1.75s/it] 18%|█▊        | 3173/17834 [1:33:27<7:05:23,  1.74s/it] 18%|█▊        | 3174/17834 [1:33:29<7:08:32,  1.75s/it] 18%|█▊        | 3175/17834 [1:33:30<7:02:10,  1.73s/it] 18%|█▊        | 3176/17834 [1:33:32<7:01:22,  1.72s/it] 18%|█▊        | 3177/17834 [1:33:34<7:02:30,  1.73s/it] 18%|█▊        | 3178/17834 [1:33:36<7:01:50,  1.73s/it] 18%|█▊        | 3179/17834 [1:33:37<7:03:55,  1.74s/it] 18%|█▊        | 3180/17834 [1:33:39<7:10:05,  1.76s/it] 18%|█▊        | 3181/17834 [1:33:41<7:13:03,  1.77s/it] 18%|█▊        | 3182/17834 [1:33:43<7:08:46,  1.76s/it] 18%|█▊        | 3183/17834 [1:33:44<7:03:13,  1.73s/it] 18%|█▊        | 3184/17834 [1:33:46<7:00:22,  1.72s/it] 18%|█▊        | 3185/17834 [1:33:48<7:05:31,  1.74s/it] 18%|█▊        | 3186/17834 [1:33:50<7:02:35,  1.73s/it] 18%|█▊        | 3187/17834 [1:33:51<7:07:08,  1.75s/it] 18%|█▊        | 3188/17834 [1:33:53<7:06:13,  1.75s/it] 18%|█▊        | 3189/17834 [1:33:55<7:02:50,  1.73s/it] 18%|█▊        | 3190/17834 [1:33:57<7:04:08,  1.74s/it] 18%|█▊        | 3191/17834 [1:33:58<7:01:36,  1.73s/it] 18%|█▊        | 3192/17834 [1:34:00<7:06:33,  1.75s/it] 18%|█▊        | 3193/17834 [1:34:02<7:04:44,  1.74s/it] 18%|█▊        | 3194/17834 [1:34:03<7:03:34,  1.74s/it] 18%|█▊        | 3195/17834 [1:34:05<7:05:30,  1.74s/it] 18%|█▊        | 3196/17834 [1:34:07<7:11:07,  1.77s/it] 18%|█▊        | 3197/17834 [1:34:09<7:14:22,  1.78s/it] 18%|█▊        | 3198/17834 [1:34:11<7:08:24,  1.76s/it] 18%|█▊        | 3199/17834 [1:34:12<7:11:43,  1.77s/it]08/30/2024 20:48:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3546793460845947, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03544583171606064, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2621402740478516, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6522655487060547}
 18%|█▊        | 3200/17834 [1:34:14<7:09:19,  1.76s/it] 18%|█▊        | 3201/17834 [1:34:16<7:07:41,  1.75s/it] 18%|█▊        | 3202/17834 [1:34:18<7:13:32,  1.78s/it] 18%|█▊        | 3203/17834 [1:34:19<7:11:14,  1.77s/it] 18%|█▊        | 3204/17834 [1:34:21<7:10:30,  1.77s/it] 18%|█▊        | 3205/17834 [1:34:23<7:04:16,  1.74s/it] 18%|█▊        | 3206/17834 [1:34:25<7:02:06,  1.73s/it] 18%|█▊        | 3207/17834 [1:34:26<7:07:31,  1.75s/it] 18%|█▊        | 3208/17834 [1:34:28<7:11:24,  1.77s/it] 18%|█▊        | 3209/17834 [1:34:30<7:11:36,  1.77s/it] 18%|█▊        | 3210/17834 [1:34:32<7:06:39,  1.75s/it] 18%|█▊        | 3211/17834 [1:34:33<7:09:18,  1.76s/it] 18%|█▊        | 3212/17834 [1:34:35<7:07:12,  1.75s/it] 18%|█▊        | 3213/17834 [1:34:37<6:59:10,  1.72s/it] 18%|█▊        | 3214/17834 [1:34:39<7:02:06,  1.73s/it] 18%|█▊        | 3215/17834 [1:34:40<6:59:31,  1.72s/it] 18%|█▊        | 3216/17834 [1:34:42<7:01:59,  1.73s/it] 18%|█▊        | 3217/17834 [1:34:44<7:07:00,  1.75s/it] 18%|█▊        | 3218/17834 [1:34:46<7:04:50,  1.74s/it] 18%|█▊        | 3219/17834 [1:34:47<7:06:07,  1.75s/it] 18%|█▊        | 3220/17834 [1:34:49<7:00:41,  1.73s/it] 18%|█▊        | 3221/17834 [1:34:51<7:04:54,  1.74s/it] 18%|█▊        | 3222/17834 [1:34:52<6:59:14,  1.72s/it] 18%|█▊        | 3223/17834 [1:34:54<7:02:40,  1.74s/it] 18%|█▊        | 3224/17834 [1:34:56<7:02:52,  1.74s/it] 18%|█▊        | 3225/17834 [1:34:58<7:01:21,  1.73s/it] 18%|█▊        | 3226/17834 [1:35:00<7:10:50,  1.77s/it] 18%|█▊        | 3227/17834 [1:35:01<7:10:12,  1.77s/it] 18%|█▊        | 3228/17834 [1:35:03<7:09:50,  1.77s/it] 18%|█▊        | 3229/17834 [1:35:05<7:05:14,  1.75s/it] 18%|█▊        | 3230/17834 [1:35:07<7:05:14,  1.75s/it] 18%|█▊        | 3231/17834 [1:35:08<7:03:38,  1.74s/it] 18%|█▊        | 3232/17834 [1:35:10<7:00:39,  1.73s/it] 18%|█▊        | 3233/17834 [1:35:12<7:01:32,  1.73s/it] 18%|█▊        | 3234/17834 [1:35:13<7:05:23,  1.75s/it] 18%|█▊        | 3235/17834 [1:35:15<7:03:07,  1.74s/it] 18%|█▊        | 3236/17834 [1:35:17<7:03:53,  1.74s/it] 18%|█▊        | 3237/17834 [1:35:19<7:05:44,  1.75s/it] 18%|█▊        | 3238/17834 [1:35:21<7:13:32,  1.78s/it] 18%|█▊        | 3239/17834 [1:35:22<7:06:30,  1.75s/it] 18%|█▊        | 3240/17834 [1:35:24<7:05:23,  1.75s/it] 18%|█▊        | 3241/17834 [1:35:26<7:06:39,  1.75s/it] 18%|█▊        | 3242/17834 [1:35:28<7:11:36,  1.77s/it] 18%|█▊        | 3243/17834 [1:35:29<7:05:21,  1.75s/it] 18%|█▊        | 3244/17834 [1:35:31<7:04:00,  1.74s/it] 18%|█▊        | 3245/17834 [1:35:33<6:59:11,  1.72s/it] 18%|█▊        | 3246/17834 [1:35:35<7:07:51,  1.76s/it] 18%|█▊        | 3247/17834 [1:35:36<7:00:45,  1.73s/it] 18%|█▊        | 3248/17834 [1:35:38<7:01:24,  1.73s/it] 18%|█▊        | 3249/17834 [1:35:40<7:02:23,  1.74s/it]08/30/2024 20:49:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1638647317886353, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03712824732065201, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2370119094848633, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.438004970550537}
 18%|█▊        | 3250/17834 [1:35:41<7:04:36,  1.75s/it] 18%|█▊        | 3251/17834 [1:35:43<7:04:11,  1.75s/it] 18%|█▊        | 3252/17834 [1:35:45<7:04:12,  1.75s/it] 18%|█▊        | 3253/17834 [1:35:47<7:00:54,  1.73s/it] 18%|█▊        | 3254/17834 [1:35:48<6:59:36,  1.73s/it] 18%|█▊        | 3255/17834 [1:35:50<7:03:00,  1.74s/it] 18%|█▊        | 3256/17834 [1:35:52<7:03:32,  1.74s/it] 18%|█▊        | 3257/17834 [1:35:54<6:58:20,  1.72s/it] 18%|█▊        | 3258/17834 [1:35:55<6:54:56,  1.71s/it] 18%|█▊        | 3259/17834 [1:35:57<6:57:04,  1.72s/it] 18%|█▊        | 3260/17834 [1:35:59<6:58:21,  1.72s/it] 18%|█▊        | 3261/17834 [1:36:00<6:59:29,  1.73s/it] 18%|█▊        | 3262/17834 [1:36:02<7:03:45,  1.74s/it] 18%|█▊        | 3263/17834 [1:36:04<7:08:42,  1.77s/it] 18%|█▊        | 3264/17834 [1:36:06<7:11:12,  1.78s/it] 18%|█▊        | 3265/17834 [1:36:08<7:05:08,  1.75s/it] 18%|█▊        | 3266/17834 [1:36:09<7:06:29,  1.76s/it] 18%|█▊        | 3267/17834 [1:36:11<7:01:05,  1.73s/it] 18%|█▊        | 3268/17834 [1:36:13<7:00:41,  1.73s/it] 18%|█▊        | 3269/17834 [1:36:14<7:01:12,  1.74s/it] 18%|█▊        | 3270/17834 [1:36:16<7:00:07,  1.73s/it] 18%|█▊        | 3271/17834 [1:36:18<7:03:04,  1.74s/it] 18%|█▊        | 3272/17834 [1:36:20<7:04:27,  1.75s/it] 18%|█▊        | 3273/17834 [1:36:21<7:05:08,  1.75s/it] 18%|█▊        | 3274/17834 [1:36:23<7:06:39,  1.76s/it] 18%|█▊        | 3275/17834 [1:36:25<7:04:22,  1.75s/it] 18%|█▊        | 3276/17834 [1:36:27<7:07:00,  1.76s/it] 18%|█▊        | 3277/17834 [1:36:29<7:06:08,  1.76s/it] 18%|█▊        | 3278/17834 [1:36:30<7:05:30,  1.75s/it] 18%|█▊        | 3279/17834 [1:36:32<7:04:32,  1.75s/it] 18%|█▊        | 3280/17834 [1:36:34<7:04:47,  1.75s/it] 18%|█▊        | 3281/17834 [1:36:36<7:13:24,  1.79s/it] 18%|█▊        | 3282/17834 [1:36:37<7:08:07,  1.77s/it] 18%|█▊        | 3283/17834 [1:36:39<7:07:24,  1.76s/it] 18%|█▊        | 3284/17834 [1:36:41<7:11:56,  1.78s/it] 18%|█▊        | 3285/17834 [1:36:43<7:20:58,  1.82s/it] 18%|█▊        | 3286/17834 [1:36:45<7:11:18,  1.78s/it] 18%|█▊        | 3287/17834 [1:36:46<7:19:28,  1.81s/it] 18%|█▊        | 3288/17834 [1:36:48<7:10:38,  1.78s/it] 18%|█▊        | 3289/17834 [1:36:50<7:04:23,  1.75s/it] 18%|█▊        | 3290/17834 [1:36:52<7:03:16,  1.75s/it] 18%|█▊        | 3291/17834 [1:36:53<7:03:54,  1.75s/it] 18%|█▊        | 3292/17834 [1:36:55<7:05:14,  1.75s/it] 18%|█▊        | 3293/17834 [1:36:57<7:03:10,  1.75s/it] 18%|█▊        | 3294/17834 [1:36:59<7:08:03,  1.77s/it] 18%|█▊        | 3295/17834 [1:37:00<7:00:02,  1.73s/it] 18%|█▊        | 3296/17834 [1:37:02<6:57:58,  1.73s/it] 18%|█▊        | 3297/17834 [1:37:04<7:05:32,  1.76s/it] 18%|█▊        | 3298/17834 [1:37:05<7:03:05,  1.75s/it] 18%|█▊        | 3299/17834 [1:37:07<7:10:39,  1.78s/it]08/30/2024 20:51:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.0115718841552734, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.054092902690172195, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4994211196899414, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.5650858879089355}
 19%|█▊        | 3300/17834 [1:37:09<7:05:04,  1.75s/it] 19%|█▊        | 3301/17834 [1:37:11<7:06:18,  1.76s/it] 19%|█▊        | 3302/17834 [1:37:13<7:07:14,  1.76s/it] 19%|█▊        | 3303/17834 [1:37:14<7:06:27,  1.76s/it] 19%|█▊        | 3304/17834 [1:37:16<6:59:49,  1.73s/it] 19%|█▊        | 3305/17834 [1:37:18<7:03:55,  1.75s/it] 19%|█▊        | 3306/17834 [1:37:20<7:02:12,  1.74s/it] 19%|█▊        | 3307/17834 [1:37:21<7:03:35,  1.75s/it] 19%|█▊        | 3308/17834 [1:37:23<6:57:55,  1.73s/it] 19%|█▊        | 3309/17834 [1:37:25<6:54:37,  1.71s/it] 19%|█▊        | 3310/17834 [1:37:26<7:00:46,  1.74s/it] 19%|█▊        | 3311/17834 [1:37:28<7:01:02,  1.74s/it] 19%|█▊        | 3312/17834 [1:37:30<7:13:36,  1.79s/it] 19%|█▊        | 3313/17834 [1:37:32<7:06:42,  1.76s/it] 19%|█▊        | 3314/17834 [1:37:34<7:07:39,  1.77s/it] 19%|█▊        | 3315/17834 [1:37:35<7:08:45,  1.77s/it] 19%|█▊        | 3316/17834 [1:37:37<7:09:15,  1.77s/it] 19%|█▊        | 3317/17834 [1:37:39<7:03:19,  1.75s/it] 19%|█▊        | 3318/17834 [1:37:41<7:04:25,  1.75s/it] 19%|█▊        | 3319/17834 [1:37:42<7:05:16,  1.76s/it] 19%|█▊        | 3320/17834 [1:37:44<7:14:15,  1.80s/it] 19%|█▊        | 3321/17834 [1:37:46<7:07:10,  1.77s/it] 19%|█▊        | 3322/17834 [1:37:48<7:08:51,  1.77s/it] 19%|█▊        | 3323/17834 [1:37:49<7:04:13,  1.75s/it] 19%|█▊        | 3324/17834 [1:37:51<7:01:32,  1.74s/it] 19%|█▊        | 3325/17834 [1:37:53<7:00:04,  1.74s/it] 19%|█▊        | 3326/17834 [1:37:55<6:57:58,  1.73s/it] 19%|█▊        | 3327/17834 [1:37:56<7:05:04,  1.76s/it] 19%|█▊        | 3328/17834 [1:37:58<6:56:27,  1.72s/it] 19%|█▊        | 3329/17834 [1:38:00<7:01:36,  1.74s/it] 19%|█▊        | 3330/17834 [1:38:02<7:10:52,  1.78s/it] 19%|█▊        | 3331/17834 [1:38:03<7:03:02,  1.75s/it] 19%|█▊        | 3332/17834 [1:38:05<7:02:33,  1.75s/it] 19%|█▊        | 3333/17834 [1:38:07<6:59:55,  1.74s/it] 19%|█▊        | 3334/17834 [1:38:09<6:59:31,  1.74s/it] 19%|█▊        | 3335/17834 [1:38:10<6:58:22,  1.73s/it] 19%|█▊        | 3336/17834 [1:38:12<7:00:49,  1.74s/it] 19%|█▊        | 3337/17834 [1:38:14<6:59:20,  1.74s/it] 19%|█▊        | 3338/17834 [1:38:16<6:59:11,  1.74s/it] 19%|█▊        | 3339/17834 [1:38:17<7:00:32,  1.74s/it] 19%|█▊        | 3340/17834 [1:38:19<7:05:06,  1.76s/it] 19%|█▊        | 3341/17834 [1:38:21<7:01:59,  1.75s/it] 19%|█▊        | 3342/17834 [1:38:23<7:06:05,  1.76s/it] 19%|█▊        | 3343/17834 [1:38:24<7:09:02,  1.78s/it] 19%|█▉        | 3344/17834 [1:38:26<7:12:17,  1.79s/it] 19%|█▉        | 3345/17834 [1:38:28<7:10:56,  1.78s/it] 19%|█▉        | 3346/17834 [1:38:30<7:05:19,  1.76s/it] 19%|█▉        | 3347/17834 [1:38:32<7:08:12,  1.77s/it] 19%|█▉        | 3348/17834 [1:38:33<7:05:55,  1.76s/it] 19%|█▉        | 3349/17834 [1:38:35<7:02:00,  1.75s/it]08/30/2024 20:52:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4964048862457275, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045765433460474014, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2541685104370117, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7963387966156006}
 19%|█▉        | 3350/17834 [1:38:37<7:01:18,  1.75s/it] 19%|█▉        | 3351/17834 [1:38:38<6:58:41,  1.73s/it] 19%|█▉        | 3352/17834 [1:38:40<6:57:18,  1.73s/it] 19%|█▉        | 3353/17834 [1:38:42<6:55:08,  1.72s/it] 19%|█▉        | 3354/17834 [1:38:44<7:01:22,  1.75s/it] 19%|█▉        | 3355/17834 [1:38:45<7:06:02,  1.77s/it] 19%|█▉        | 3356/17834 [1:38:47<7:03:36,  1.76s/it] 19%|█▉        | 3357/17834 [1:38:49<7:01:51,  1.75s/it] 19%|█▉        | 3358/17834 [1:38:51<7:05:00,  1.76s/it] 19%|█▉        | 3359/17834 [1:38:53<7:09:29,  1.78s/it] 19%|█▉        | 3360/17834 [1:38:54<7:05:04,  1.76s/it] 19%|█▉        | 3361/17834 [1:38:56<7:12:24,  1.79s/it] 19%|█▉        | 3362/17834 [1:38:58<7:02:50,  1.75s/it] 19%|█▉        | 3363/17834 [1:39:00<7:04:35,  1.76s/it] 19%|█▉        | 3364/17834 [1:39:01<7:02:29,  1.75s/it] 19%|█▉        | 3365/17834 [1:39:03<6:58:30,  1.74s/it] 19%|█▉        | 3366/17834 [1:39:05<7:01:27,  1.75s/it] 19%|█▉        | 3367/17834 [1:39:07<7:01:33,  1.75s/it] 19%|█▉        | 3368/17834 [1:39:08<7:00:07,  1.74s/it] 19%|█▉        | 3369/17834 [1:39:10<7:00:12,  1.74s/it] 19%|█▉        | 3370/17834 [1:39:12<6:56:22,  1.73s/it] 19%|█▉        | 3371/17834 [1:39:13<6:57:37,  1.73s/it] 19%|█▉        | 3372/17834 [1:39:15<6:58:58,  1.74s/it] 19%|█▉        | 3373/17834 [1:39:17<7:06:47,  1.77s/it] 19%|█▉        | 3374/17834 [1:39:19<7:02:23,  1.75s/it] 19%|█▉        | 3375/17834 [1:39:20<7:04:13,  1.76s/it] 19%|█▉        | 3376/17834 [1:39:22<6:59:46,  1.74s/it] 19%|█▉        | 3377/17834 [1:39:24<7:01:02,  1.75s/it] 19%|█▉        | 3378/17834 [1:39:26<6:52:47,  1.71s/it] 19%|█▉        | 3379/17834 [1:39:27<6:54:38,  1.72s/it] 19%|█▉        | 3380/17834 [1:39:29<6:53:13,  1.72s/it] 19%|█▉        | 3381/17834 [1:39:31<7:02:22,  1.75s/it] 19%|█▉        | 3382/17834 [1:39:33<7:06:06,  1.77s/it] 19%|█▉        | 3383/17834 [1:39:34<7:04:00,  1.76s/it] 19%|█▉        | 3384/17834 [1:39:36<7:02:04,  1.75s/it] 19%|█▉        | 3385/17834 [1:39:38<7:02:53,  1.76s/it] 19%|█▉        | 3386/17834 [1:39:40<6:58:10,  1.74s/it] 19%|█▉        | 3387/17834 [1:39:41<6:57:29,  1.73s/it] 19%|█▉        | 3388/17834 [1:39:43<7:01:18,  1.75s/it] 19%|█▉        | 3389/17834 [1:39:45<7:02:20,  1.75s/it] 19%|█▉        | 3390/17834 [1:39:47<7:05:59,  1.77s/it] 19%|█▉        | 3391/17834 [1:39:48<7:07:31,  1.78s/it] 19%|█▉        | 3392/17834 [1:39:50<7:02:40,  1.76s/it] 19%|█▉        | 3393/17834 [1:39:52<7:01:27,  1.75s/it] 19%|█▉        | 3394/17834 [1:39:54<6:59:29,  1.74s/it] 19%|█▉        | 3395/17834 [1:39:55<6:58:29,  1.74s/it] 19%|█▉        | 3396/17834 [1:39:57<6:58:45,  1.74s/it] 19%|█▉        | 3397/17834 [1:39:59<6:58:14,  1.74s/it] 19%|█▉        | 3398/17834 [1:40:01<6:58:15,  1.74s/it] 19%|█▉        | 3399/17834 [1:40:02<7:03:19,  1.76s/it]08/30/2024 20:54:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2930405139923096, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.050714798271656036, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.243398666381836, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.587153911590576}
 19%|█▉        | 3400/17834 [1:40:04<7:03:52,  1.76s/it] 19%|█▉        | 3401/17834 [1:40:06<7:01:31,  1.75s/it] 19%|█▉        | 3402/17834 [1:40:08<7:09:41,  1.79s/it] 19%|█▉        | 3403/17834 [1:40:09<7:00:57,  1.75s/it] 19%|█▉        | 3404/17834 [1:40:11<7:01:19,  1.75s/it] 19%|█▉        | 3405/17834 [1:40:13<7:00:01,  1.75s/it] 19%|█▉        | 3406/17834 [1:40:15<7:05:47,  1.77s/it] 19%|█▉        | 3407/17834 [1:40:16<7:01:51,  1.75s/it] 19%|█▉        | 3408/17834 [1:40:18<7:11:43,  1.80s/it] 19%|█▉        | 3409/17834 [1:40:20<7:02:49,  1.76s/it] 19%|█▉        | 3410/17834 [1:40:22<6:58:30,  1.74s/it] 19%|█▉        | 3411/17834 [1:40:24<7:00:45,  1.75s/it] 19%|█▉        | 3412/17834 [1:40:25<7:02:43,  1.76s/it] 19%|█▉        | 3413/17834 [1:40:27<7:01:34,  1.75s/it] 19%|█▉        | 3414/17834 [1:40:29<6:59:45,  1.75s/it] 19%|█▉        | 3415/17834 [1:40:30<6:55:28,  1.73s/it] 19%|█▉        | 3416/17834 [1:40:32<6:56:31,  1.73s/it] 19%|█▉        | 3417/17834 [1:40:34<6:59:25,  1.75s/it] 19%|█▉        | 3418/17834 [1:40:36<6:59:34,  1.75s/it] 19%|█▉        | 3419/17834 [1:40:37<6:59:01,  1.74s/it] 19%|█▉        | 3420/17834 [1:40:39<6:54:29,  1.73s/it] 19%|█▉        | 3421/17834 [1:40:41<6:52:39,  1.72s/it] 19%|█▉        | 3422/17834 [1:40:43<6:57:50,  1.74s/it] 19%|█▉        | 3423/17834 [1:40:44<6:58:42,  1.74s/it] 19%|█▉        | 3424/17834 [1:40:46<6:51:48,  1.71s/it] 19%|█▉        | 3425/17834 [1:40:48<6:57:21,  1.74s/it] 19%|█▉        | 3426/17834 [1:40:50<7:00:46,  1.75s/it] 19%|█▉        | 3427/17834 [1:40:51<6:56:14,  1.73s/it] 19%|█▉        | 3428/17834 [1:40:53<6:57:35,  1.74s/it] 19%|█▉        | 3429/17834 [1:40:55<7:00:37,  1.75s/it] 19%|█▉        | 3430/17834 [1:40:57<6:58:02,  1.74s/it] 19%|█▉        | 3431/17834 [1:40:58<7:06:55,  1.78s/it] 19%|█▉        | 3432/17834 [1:41:00<7:03:35,  1.76s/it] 19%|█▉        | 3433/17834 [1:41:02<7:02:01,  1.76s/it] 19%|█▉        | 3434/17834 [1:41:04<7:00:59,  1.75s/it] 19%|█▉        | 3435/17834 [1:41:05<7:06:43,  1.78s/it] 19%|█▉        | 3436/17834 [1:41:07<7:01:19,  1.76s/it] 19%|█▉        | 3437/17834 [1:41:09<6:57:22,  1.74s/it] 19%|█▉        | 3438/17834 [1:41:11<6:59:04,  1.75s/it] 19%|█▉        | 3439/17834 [1:41:12<6:58:55,  1.75s/it] 19%|█▉        | 3440/17834 [1:41:14<7:03:47,  1.77s/it] 19%|█▉        | 3441/17834 [1:41:16<7:06:08,  1.78s/it] 19%|█▉        | 3442/17834 [1:41:18<7:02:39,  1.76s/it] 19%|█▉        | 3443/17834 [1:41:20<7:05:55,  1.78s/it] 19%|█▉        | 3444/17834 [1:41:21<6:59:31,  1.75s/it] 19%|█▉        | 3445/17834 [1:41:23<6:57:47,  1.74s/it] 19%|█▉        | 3446/17834 [1:41:25<6:59:35,  1.75s/it] 19%|█▉        | 3447/17834 [1:41:27<7:03:51,  1.77s/it] 19%|█▉        | 3448/17834 [1:41:28<7:04:46,  1.77s/it] 19%|█▉        | 3449/17834 [1:41:30<7:01:07,  1.76s/it]08/30/2024 20:55:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6409887075424194, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04518017917871475, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3280282020568848, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.01419734954834}
 19%|█▉        | 3450/17834 [1:41:32<6:56:54,  1.74s/it] 19%|█▉        | 3451/17834 [1:41:34<7:01:36,  1.76s/it] 19%|█▉        | 3452/17834 [1:41:35<7:01:23,  1.76s/it] 19%|█▉        | 3453/17834 [1:41:37<6:58:26,  1.75s/it] 19%|█▉        | 3454/17834 [1:41:39<7:00:20,  1.75s/it] 19%|█▉        | 3455/17834 [1:41:41<6:59:40,  1.75s/it] 19%|█▉        | 3456/17834 [1:41:42<7:01:22,  1.76s/it] 19%|█▉        | 3457/17834 [1:41:44<6:58:14,  1.75s/it] 19%|█▉        | 3458/17834 [1:41:46<6:56:09,  1.74s/it] 19%|█▉        | 3459/17834 [1:41:47<6:56:33,  1.74s/it] 19%|█▉        | 3460/17834 [1:41:49<6:59:18,  1.75s/it] 19%|█▉        | 3461/17834 [1:41:51<7:02:28,  1.76s/it] 19%|█▉        | 3462/17834 [1:41:53<7:00:01,  1.75s/it] 19%|█▉        | 3463/17834 [1:41:55<7:02:30,  1.76s/it] 19%|█▉        | 3464/17834 [1:41:56<6:56:33,  1.74s/it] 19%|█▉        | 3465/17834 [1:41:58<6:53:17,  1.73s/it] 19%|█▉        | 3466/17834 [1:42:00<6:50:38,  1.71s/it] 19%|█▉        | 3467/17834 [1:42:01<6:51:06,  1.72s/it] 19%|█▉        | 3468/17834 [1:42:03<6:49:51,  1.71s/it] 19%|█▉        | 3469/17834 [1:42:05<6:57:55,  1.75s/it] 19%|█▉        | 3470/17834 [1:42:07<6:56:08,  1.74s/it] 19%|█▉        | 3471/17834 [1:42:08<6:56:42,  1.74s/it] 19%|█▉        | 3472/17834 [1:42:10<6:57:35,  1.74s/it] 19%|█▉        | 3473/17834 [1:42:12<7:00:40,  1.76s/it] 19%|█▉        | 3474/17834 [1:42:14<7:03:14,  1.77s/it] 19%|█▉        | 3475/17834 [1:42:15<7:01:28,  1.76s/it] 19%|█▉        | 3476/17834 [1:42:17<7:06:06,  1.78s/it] 19%|█▉        | 3477/17834 [1:42:19<7:08:06,  1.79s/it] 20%|█▉        | 3478/17834 [1:42:21<7:07:13,  1.79s/it] 20%|█▉        | 3479/17834 [1:42:23<7:05:33,  1.78s/it] 20%|█▉        | 3480/17834 [1:42:24<7:06:23,  1.78s/it] 20%|█▉        | 3481/17834 [1:42:26<7:03:51,  1.77s/it] 20%|█▉        | 3482/17834 [1:42:28<7:03:28,  1.77s/it] 20%|█▉        | 3483/17834 [1:42:30<7:02:41,  1.77s/it] 20%|█▉        | 3484/17834 [1:42:31<7:05:08,  1.78s/it] 20%|█▉        | 3485/17834 [1:42:33<7:07:29,  1.79s/it] 20%|█▉        | 3486/17834 [1:42:35<7:04:54,  1.78s/it] 20%|█▉        | 3487/17834 [1:42:37<7:08:56,  1.79s/it] 20%|█▉        | 3488/17834 [1:42:39<7:05:32,  1.78s/it] 20%|█▉        | 3489/17834 [1:42:40<7:00:01,  1.76s/it] 20%|█▉        | 3490/17834 [1:42:42<6:57:31,  1.75s/it] 20%|█▉        | 3491/17834 [1:42:44<6:55:10,  1.74s/it] 20%|█▉        | 3492/17834 [1:42:45<6:54:08,  1.73s/it] 20%|█▉        | 3493/17834 [1:42:47<6:53:23,  1.73s/it] 20%|█▉        | 3494/17834 [1:42:49<6:56:08,  1.74s/it] 20%|█▉        | 3495/17834 [1:42:51<6:58:09,  1.75s/it] 20%|█▉        | 3496/17834 [1:42:52<6:54:42,  1.74s/it] 20%|█▉        | 3497/17834 [1:42:54<6:54:03,  1.73s/it] 20%|█▉        | 3498/17834 [1:42:56<6:57:31,  1.75s/it] 20%|█▉        | 3499/17834 [1:42:58<6:53:56,  1.73s/it]08/30/2024 20:57:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.100497245788574, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.06327009201049805, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.6037139892578125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.767481327056885}
 20%|█▉        | 3500/17834 [1:42:59<6:52:02,  1.72s/it] 20%|█▉        | 3501/17834 [1:43:01<6:54:43,  1.74s/it] 20%|█▉        | 3502/17834 [1:43:03<6:56:50,  1.75s/it] 20%|█▉        | 3503/17834 [1:43:05<6:58:22,  1.75s/it] 20%|█▉        | 3504/17834 [1:43:06<7:02:39,  1.77s/it] 20%|█▉        | 3505/17834 [1:43:08<6:57:44,  1.75s/it] 20%|█▉        | 3506/17834 [1:43:10<6:57:55,  1.75s/it] 20%|█▉        | 3507/17834 [1:43:12<7:03:28,  1.77s/it] 20%|█▉        | 3508/17834 [1:43:13<7:00:20,  1.76s/it] 20%|█▉        | 3509/17834 [1:43:15<7:00:44,  1.76s/it] 20%|█▉        | 3510/17834 [1:43:17<7:03:05,  1.77s/it] 20%|█▉        | 3511/17834 [1:43:19<7:09:32,  1.80s/it] 20%|█▉        | 3512/17834 [1:43:21<7:07:22,  1.79s/it] 20%|█▉        | 3513/17834 [1:43:22<7:06:51,  1.79s/it] 20%|█▉        | 3514/17834 [1:43:24<6:57:34,  1.75s/it] 20%|█▉        | 3515/17834 [1:43:26<6:57:28,  1.75s/it] 20%|█▉        | 3516/17834 [1:43:28<7:02:22,  1.77s/it] 20%|█▉        | 3517/17834 [1:43:29<6:59:58,  1.76s/it] 20%|█▉        | 3518/17834 [1:43:31<6:56:27,  1.75s/it] 20%|█▉        | 3519/17834 [1:43:33<7:04:38,  1.78s/it] 20%|█▉        | 3520/17834 [1:43:35<6:59:47,  1.76s/it] 20%|█▉        | 3521/17834 [1:43:36<6:58:39,  1.75s/it] 20%|█▉        | 3522/17834 [1:43:38<6:58:31,  1.75s/it] 20%|█▉        | 3523/17834 [1:43:40<7:00:05,  1.76s/it] 20%|█▉        | 3524/17834 [1:43:42<6:55:20,  1.74s/it] 20%|█▉        | 3525/17834 [1:43:43<6:57:47,  1.75s/it] 20%|█▉        | 3526/17834 [1:43:45<6:57:56,  1.75s/it] 20%|█▉        | 3527/17834 [1:43:47<6:59:44,  1.76s/it] 20%|█▉        | 3528/17834 [1:43:49<7:00:24,  1.76s/it] 20%|█▉        | 3529/17834 [1:43:51<7:05:50,  1.79s/it] 20%|█▉        | 3530/17834 [1:43:52<7:03:49,  1.78s/it] 20%|█▉        | 3531/17834 [1:43:54<7:01:43,  1.77s/it] 20%|█▉        | 3532/17834 [1:43:56<7:00:57,  1.77s/it] 20%|█▉        | 3533/17834 [1:43:58<6:59:59,  1.76s/it] 20%|█▉        | 3534/17834 [1:43:59<6:57:26,  1.75s/it] 20%|█▉        | 3535/17834 [1:44:01<7:12:28,  1.81s/it] 20%|█▉        | 3536/17834 [1:44:03<7:08:21,  1.80s/it] 20%|█▉        | 3537/17834 [1:44:05<7:06:03,  1.79s/it] 20%|█▉        | 3538/17834 [1:44:07<7:03:24,  1.78s/it] 20%|█▉        | 3539/17834 [1:44:08<6:57:05,  1.75s/it] 20%|█▉        | 3540/17834 [1:44:10<6:58:35,  1.76s/it] 20%|█▉        | 3541/17834 [1:44:12<6:52:51,  1.73s/it] 20%|█▉        | 3542/17834 [1:44:14<6:59:14,  1.76s/it] 20%|█▉        | 3543/17834 [1:44:15<6:58:11,  1.76s/it] 20%|█▉        | 3544/17834 [1:44:17<6:55:42,  1.75s/it] 20%|█▉        | 3545/17834 [1:44:19<6:54:27,  1.74s/it] 20%|█▉        | 3546/17834 [1:44:20<6:56:55,  1.75s/it] 20%|█▉        | 3547/17834 [1:44:22<6:55:54,  1.75s/it] 20%|█▉        | 3548/17834 [1:44:24<6:52:39,  1.73s/it] 20%|█▉        | 3549/17834 [1:44:26<6:50:56,  1.73s/it]08/30/2024 20:58:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.597289800643921, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.060269467532634735, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.355194568634033, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.012753963470459}
 20%|█▉        | 3550/17834 [1:44:27<6:49:51,  1.72s/it] 20%|█▉        | 3551/17834 [1:44:29<6:51:24,  1.73s/it] 20%|█▉        | 3552/17834 [1:44:31<6:57:18,  1.75s/it] 20%|█▉        | 3553/17834 [1:44:33<6:50:55,  1.73s/it] 20%|█▉        | 3554/17834 [1:44:34<6:53:50,  1.74s/it] 20%|█▉        | 3555/17834 [1:44:36<6:55:41,  1.75s/it] 20%|█▉        | 3556/17834 [1:44:38<6:57:28,  1.75s/it] 20%|█▉        | 3557/17834 [1:44:40<6:56:49,  1.75s/it] 20%|█▉        | 3558/17834 [1:44:41<6:53:13,  1.74s/it] 20%|█▉        | 3559/17834 [1:44:43<6:51:30,  1.73s/it] 20%|█▉        | 3560/17834 [1:44:45<6:51:35,  1.73s/it] 20%|█▉        | 3561/17834 [1:44:47<6:53:52,  1.74s/it] 20%|█▉        | 3562/17834 [1:44:48<7:00:03,  1.77s/it] 20%|█▉        | 3563/17834 [1:44:50<7:01:14,  1.77s/it]08/30/2024 20:59:09 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/30/2024 20:59:09 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:25,  8.47it/s][A
  1%|▏         | 3/221 [00:00<00:49,  4.43it/s][A
  2%|▏         | 4/221 [00:00<00:40,  5.37it/s][A
  2%|▏         | 5/221 [00:00<00:45,  4.70it/s][A
  3%|▎         | 6/221 [00:01<00:46,  4.65it/s][A
  3%|▎         | 7/221 [00:01<00:53,  4.01it/s][A
  4%|▎         | 8/221 [00:01<00:52,  4.05it/s][A
  4%|▍         | 9/221 [00:02<01:06,  3.18it/s][A
  5%|▍         | 10/221 [00:02<01:15,  2.79it/s][A
  5%|▍         | 11/221 [00:02<01:03,  3.30it/s][A
  5%|▌         | 12/221 [00:03<00:56,  3.70it/s][A
  6%|▌         | 13/221 [00:03<01:00,  3.44it/s][A
  6%|▋         | 14/221 [00:03<00:59,  3.51it/s][A
  7%|▋         | 15/221 [00:04<01:01,  3.34it/s][A
  7%|▋         | 16/221 [00:04<01:00,  3.39it/s][A
  8%|▊         | 17/221 [00:04<01:03,  3.22it/s][A
  8%|▊         | 18/221 [00:04<00:54,  3.75it/s][A
  9%|▊         | 19/221 [00:05<00:55,  3.67it/s][A
  9%|▉         | 20/221 [00:05<00:50,  3.99it/s][A
 10%|▉         | 21/221 [00:05<00:47,  4.20it/s][A
 10%|▉         | 22/221 [00:05<00:48,  4.08it/s][A
 10%|█         | 23/221 [00:06<00:52,  3.76it/s][A
 11%|█         | 24/221 [00:06<00:49,  3.95it/s][A
 11%|█▏        | 25/221 [00:06<00:48,  4.00it/s][A
 12%|█▏        | 26/221 [00:06<00:41,  4.65it/s][A
 12%|█▏        | 27/221 [00:07<01:05,  2.98it/s][A
 13%|█▎        | 28/221 [00:07<01:08,  2.80it/s][A
 13%|█▎        | 29/221 [00:08<01:17,  2.48it/s][A
 14%|█▎        | 30/221 [00:08<01:09,  2.75it/s][A
 14%|█▍        | 31/221 [00:08<01:03,  2.97it/s][A
 14%|█▍        | 32/221 [00:08<00:56,  3.32it/s][A
 15%|█▍        | 33/221 [00:09<00:58,  3.20it/s][A
 15%|█▌        | 34/221 [00:09<00:55,  3.38it/s][A
 16%|█▌        | 35/221 [00:09<01:00,  3.05it/s][A
 16%|█▋        | 36/221 [00:10<01:01,  3.02it/s][A
 17%|█▋        | 37/221 [00:10<00:53,  3.45it/s][A
 17%|█▋        | 38/221 [00:10<00:54,  3.37it/s][A
 18%|█▊        | 39/221 [00:11<00:47,  3.80it/s][A
 18%|█▊        | 40/221 [00:11<00:48,  3.76it/s][A
 19%|█▊        | 41/221 [00:11<00:50,  3.60it/s][A
 19%|█▉        | 42/221 [00:11<00:44,  4.00it/s][A
 19%|█▉        | 43/221 [00:11<00:38,  4.68it/s][A
 20%|█▉        | 44/221 [00:12<00:46,  3.79it/s][A
 20%|██        | 45/221 [00:12<00:55,  3.20it/s][A
 21%|██        | 46/221 [00:12<00:46,  3.73it/s][A
 21%|██▏       | 47/221 [00:13<00:42,  4.07it/s][A
 22%|██▏       | 48/221 [00:13<00:38,  4.46it/s][A
 22%|██▏       | 49/221 [00:13<00:33,  5.19it/s][A
 23%|██▎       | 50/221 [00:13<00:47,  3.60it/s][A
 23%|██▎       | 51/221 [00:14<00:45,  3.72it/s][A
 24%|██▎       | 52/221 [00:14<00:44,  3.78it/s][A
 24%|██▍       | 53/221 [00:14<00:36,  4.58it/s][A
 24%|██▍       | 54/221 [00:14<00:40,  4.16it/s][A
 25%|██▍       | 55/221 [00:15<00:44,  3.75it/s][A
 25%|██▌       | 56/221 [00:15<00:47,  3.46it/s][A
 26%|██▌       | 57/221 [00:15<00:54,  3.02it/s][A
 26%|██▌       | 58/221 [00:16<00:47,  3.47it/s][A
 27%|██▋       | 59/221 [00:16<00:42,  3.82it/s][A
 27%|██▋       | 60/221 [00:16<00:35,  4.50it/s][A
 28%|██▊       | 61/221 [00:16<00:39,  4.04it/s][A
 28%|██▊       | 62/221 [00:16<00:41,  3.86it/s][A
 29%|██▊       | 63/221 [00:17<00:45,  3.49it/s][A
 29%|██▉       | 64/221 [00:17<00:43,  3.59it/s][A
 29%|██▉       | 65/221 [00:17<00:45,  3.45it/s][A
 30%|██▉       | 66/221 [00:18<00:51,  3.04it/s][A
 30%|███       | 67/221 [00:18<00:51,  3.00it/s][A
 31%|███       | 68/221 [00:18<00:41,  3.67it/s][A
 31%|███       | 69/221 [00:19<00:56,  2.70it/s][A
 32%|███▏      | 70/221 [00:19<00:56,  2.65it/s][A
 32%|███▏      | 71/221 [00:19<00:49,  3.00it/s][A
 33%|███▎      | 72/221 [00:20<00:48,  3.06it/s][A
 33%|███▎      | 73/221 [00:20<00:44,  3.34it/s][A
 33%|███▎      | 74/221 [00:20<00:36,  3.98it/s][A
 34%|███▍      | 75/221 [00:20<00:39,  3.69it/s][A
 34%|███▍      | 76/221 [00:21<00:47,  3.02it/s][A
 35%|███▍      | 77/221 [00:21<00:50,  2.83it/s][A
 35%|███▌      | 78/221 [00:22<00:53,  2.68it/s][A
 36%|███▌      | 79/221 [00:22<00:50,  2.81it/s][A
 36%|███▌      | 80/221 [00:22<00:41,  3.43it/s][A
 37%|███▋      | 81/221 [00:22<00:36,  3.83it/s][A
 37%|███▋      | 82/221 [00:23<00:37,  3.74it/s][A
 38%|███▊      | 83/221 [00:23<00:45,  3.01it/s][A
 38%|███▊      | 84/221 [00:23<00:39,  3.43it/s][A
 38%|███▊      | 85/221 [00:24<00:37,  3.63it/s][A
 39%|███▉      | 86/221 [00:24<00:43,  3.09it/s][A
 39%|███▉      | 87/221 [00:25<00:52,  2.57it/s][A
 40%|███▉      | 88/221 [00:25<00:46,  2.87it/s][A
 40%|████      | 89/221 [00:25<00:40,  3.28it/s][A
 41%|████      | 90/221 [00:25<00:37,  3.45it/s][A
 41%|████      | 91/221 [00:26<00:33,  3.88it/s][A
 42%|████▏     | 92/221 [00:26<00:30,  4.19it/s][A
 42%|████▏     | 93/221 [00:26<00:41,  3.05it/s][A
 43%|████▎     | 94/221 [00:26<00:36,  3.44it/s][A
 43%|████▎     | 95/221 [00:27<00:39,  3.19it/s][A
 43%|████▎     | 96/221 [00:27<00:37,  3.35it/s][A
 44%|████▍     | 97/221 [00:27<00:38,  3.20it/s][A
 44%|████▍     | 98/221 [00:28<00:39,  3.09it/s][A
 45%|████▍     | 99/221 [00:28<00:37,  3.28it/s][A
 45%|████▌     | 100/221 [00:28<00:37,  3.19it/s][A
 46%|████▌     | 101/221 [00:29<00:39,  3.04it/s][A
 46%|████▌     | 102/221 [00:29<00:36,  3.30it/s][A
 47%|████▋     | 103/221 [00:29<00:38,  3.06it/s][A
 47%|████▋     | 104/221 [00:30<00:42,  2.73it/s][A
 48%|████▊     | 105/221 [00:30<00:41,  2.81it/s][A
 48%|████▊     | 106/221 [00:30<00:36,  3.12it/s][A
 48%|████▊     | 107/221 [00:31<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:31<00:31,  3.59it/s][A
 49%|████▉     | 109/221 [00:31<00:30,  3.69it/s][A
 50%|████▉     | 110/221 [00:31<00:30,  3.64it/s][A
 50%|█████     | 111/221 [00:32<00:29,  3.73it/s][A
 51%|█████     | 112/221 [00:32<00:33,  3.23it/s][A
 51%|█████     | 113/221 [00:32<00:30,  3.50it/s][A
 52%|█████▏    | 114/221 [00:32<00:27,  3.94it/s][A
 52%|█████▏    | 115/221 [00:33<00:31,  3.42it/s][A
 52%|█████▏    | 116/221 [00:33<00:27,  3.87it/s][A
 53%|█████▎    | 117/221 [00:33<00:25,  4.15it/s][A
 53%|█████▎    | 118/221 [00:34<00:28,  3.57it/s][A
 54%|█████▍    | 119/221 [00:34<00:34,  2.99it/s][A
 54%|█████▍    | 120/221 [00:34<00:36,  2.78it/s][A
 55%|█████▍    | 121/221 [00:35<00:30,  3.27it/s][A
 55%|█████▌    | 122/221 [00:35<00:33,  2.99it/s][A
 56%|█████▌    | 123/221 [00:35<00:27,  3.58it/s][A
 56%|█████▌    | 124/221 [00:36<00:29,  3.34it/s][A
 57%|█████▋    | 125/221 [00:36<00:32,  2.99it/s][A
 57%|█████▋    | 126/221 [00:36<00:26,  3.64it/s][A
 57%|█████▋    | 127/221 [00:37<00:29,  3.21it/s][A
 58%|█████▊    | 128/221 [00:37<00:28,  3.23it/s][A
 58%|█████▊    | 129/221 [00:37<00:26,  3.52it/s][A
 59%|█████▉    | 130/221 [00:37<00:27,  3.37it/s][A
 59%|█████▉    | 131/221 [00:38<00:24,  3.65it/s][A
 60%|█████▉    | 132/221 [00:38<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:38<00:32,  2.73it/s][A
 61%|██████    | 134/221 [00:39<00:34,  2.53it/s][A
 61%|██████    | 135/221 [00:39<00:33,  2.56it/s][A
 62%|██████▏   | 136/221 [00:40<00:29,  2.90it/s][A
 62%|██████▏   | 137/221 [00:40<00:24,  3.50it/s][A
 62%|██████▏   | 138/221 [00:40<00:22,  3.72it/s][A
 63%|██████▎   | 139/221 [00:40<00:22,  3.65it/s][A
 63%|██████▎   | 140/221 [00:40<00:21,  3.75it/s][A
 64%|██████▍   | 141/221 [00:41<00:23,  3.37it/s][A
 64%|██████▍   | 142/221 [00:41<00:19,  4.12it/s][A
 65%|██████▍   | 143/221 [00:41<00:21,  3.55it/s][A
 65%|██████▌   | 144/221 [00:42<00:23,  3.30it/s][A
 66%|██████▌   | 145/221 [00:42<00:25,  2.97it/s][A
 66%|██████▌   | 146/221 [00:42<00:22,  3.38it/s][A
 67%|██████▋   | 147/221 [00:43<00:22,  3.28it/s][A
 67%|██████▋   | 148/221 [00:43<00:20,  3.53it/s][A
 67%|██████▋   | 149/221 [00:43<00:21,  3.37it/s][A
 68%|██████▊   | 151/221 [00:44<00:18,  3.69it/s][A
 69%|██████▉   | 152/221 [00:44<00:24,  2.76it/s][A
 69%|██████▉   | 153/221 [00:44<00:20,  3.28it/s][A
 70%|██████▉   | 154/221 [00:45<00:20,  3.32it/s][A
 70%|███████   | 155/221 [00:45<00:23,  2.85it/s][A
 71%|███████   | 156/221 [00:45<00:20,  3.23it/s][A
 71%|███████   | 157/221 [00:46<00:17,  3.57it/s][A
 71%|███████▏  | 158/221 [00:46<00:19,  3.29it/s][A
 72%|███████▏  | 159/221 [00:46<00:17,  3.64it/s][A
 72%|███████▏  | 160/221 [00:47<00:18,  3.29it/s][A
 73%|███████▎  | 161/221 [00:47<00:18,  3.28it/s][A
 73%|███████▎  | 162/221 [00:47<00:16,  3.66it/s][A
 74%|███████▍  | 163/221 [00:47<00:15,  3.72it/s][A
 75%|███████▍  | 165/221 [00:48<00:12,  4.42it/s][A
 75%|███████▌  | 166/221 [00:48<00:12,  4.39it/s][A
 76%|███████▌  | 167/221 [00:48<00:12,  4.43it/s][A
 76%|███████▌  | 168/221 [00:48<00:12,  4.13it/s][A
 76%|███████▋  | 169/221 [00:49<00:11,  4.50it/s][A
 77%|███████▋  | 170/221 [00:49<00:14,  3.43it/s][A
 77%|███████▋  | 171/221 [00:49<00:14,  3.46it/s][A
 78%|███████▊  | 172/221 [00:49<00:11,  4.18it/s][A
 78%|███████▊  | 173/221 [00:50<00:12,  3.77it/s][A
 79%|███████▊  | 174/221 [00:50<00:14,  3.30it/s][A
 79%|███████▉  | 175/221 [00:51<00:16,  2.78it/s][A
 80%|███████▉  | 176/221 [00:51<00:12,  3.47it/s][A
 80%|████████  | 177/221 [00:51<00:10,  4.15it/s][A
 81%|████████  | 178/221 [00:51<00:12,  3.52it/s][A
 81%|████████  | 179/221 [00:52<00:12,  3.49it/s][A
 81%|████████▏ | 180/221 [00:52<00:12,  3.33it/s][A
 82%|████████▏ | 181/221 [00:52<00:14,  2.77it/s][A
 82%|████████▏ | 182/221 [00:53<00:12,  3.05it/s][A
 83%|████████▎ | 183/221 [00:53<00:14,  2.60it/s][A
 83%|████████▎ | 184/221 [00:54<00:13,  2.74it/s][A
 84%|████████▎ | 185/221 [00:54<00:11,  3.11it/s][A
 84%|████████▍ | 186/221 [00:54<00:11,  3.07it/s][A
 85%|████████▍ | 187/221 [00:54<00:10,  3.18it/s][A
 85%|████████▌ | 188/221 [00:55<00:10,  3.17it/s][A
 86%|████████▌ | 189/221 [00:55<00:08,  3.90it/s][A
 86%|████████▌ | 190/221 [00:55<00:08,  3.60it/s][A
 86%|████████▋ | 191/221 [00:55<00:07,  3.75it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.59it/s][A
 87%|████████▋ | 193/221 [00:56<00:08,  3.35it/s][A
 88%|████████▊ | 194/221 [00:56<00:07,  3.50it/s][A
 88%|████████▊ | 195/221 [00:57<00:07,  3.29it/s][A
 89%|████████▊ | 196/221 [00:57<00:09,  2.65it/s][A
 89%|████████▉ | 197/221 [00:57<00:08,  2.77it/s][A
 90%|████████▉ | 198/221 [00:58<00:07,  3.04it/s][A
 90%|█████████ | 200/221 [00:58<00:05,  4.12it/s][A
 91%|█████████ | 201/221 [00:58<00:04,  4.17it/s][A
 91%|█████████▏| 202/221 [00:58<00:04,  4.42it/s][A
 92%|█████████▏| 203/221 [00:59<00:03,  4.97it/s][A
 92%|█████████▏| 204/221 [00:59<00:04,  3.62it/s][A
 93%|█████████▎| 205/221 [00:59<00:03,  4.04it/s][A
 93%|█████████▎| 206/221 [00:59<00:03,  4.07it/s][A
 94%|█████████▎| 207/221 [01:00<00:03,  4.13it/s][A
 94%|█████████▍| 208/221 [01:00<00:02,  4.95it/s][A
 95%|█████████▍| 209/221 [01:00<00:02,  4.63it/s][A
 95%|█████████▌| 210/221 [01:00<00:02,  5.22it/s][A
 95%|█████████▌| 211/221 [01:00<00:02,  4.63it/s][A
 96%|█████████▌| 212/221 [01:01<00:02,  4.35it/s][A
 96%|█████████▋| 213/221 [01:01<00:01,  4.25it/s][A
 97%|█████████▋| 214/221 [01:01<00:02,  3.11it/s][A
 97%|█████████▋| 215/221 [01:02<00:01,  3.73it/s][A
 98%|█████████▊| 216/221 [01:02<00:01,  3.67it/s][A
 98%|█████████▊| 217/221 [01:02<00:01,  3.24it/s][A
 99%|█████████▊| 218/221 [01:03<00:00,  3.04it/s][A
 99%|█████████▉| 219/221 [01:03<00:00,  3.05it/s][A
100%|█████████▉| 220/221 [01:03<00:00,  3.47it/s][A
100%|██████████| 221/221 [01:03<00:00,  3.66it/s][A100%|██████████| 221/221 [01:03<00:00,  3.46it/s]
08/30/2024 21:01:34 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 3563--===========

08/30/2024 21:01:34 - INFO - __main__ -   {'area_r1': 5.7, 'area_recall': '5.7/14.9/21.3', 'area_ravg': 14.0}
08/30/2024 21:01:34 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 3563--===========

08/30/2024 21:01:34 - INFO - __main__ -   {'area_r1': 42.1, 'area_recall': '42.1/73.5/83.0', 'area_ravg': 66.2}
08/30/2024 21:01:34 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 3563--===========

08/30/2024 21:01:34 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/30/2024 21:01:34 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/30/2024 21:01:34 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/30/2024 21:01:34 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 3563--===========

08/30/2024 21:01:34 - INFO - __main__ -   {'video_r1': 56.8, 'video_recall': '56.8/79.4/86.0', 'video_ravg': 74.1}
08/30/2024 21:01:34 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 3563=======

08/30/2024 21:01:34 - INFO - __main__ -   {'video_r1': 56.8, 'video_recall': '56.8/79.4/86.0', 'video_ravg': 74.1}
 20%|█▉        | 3564/17834 [1:47:49<217:24:14, 54.85s/it] 20%|█▉        | 3565/17834 [1:47:51<154:13:34, 38.91s/it] 20%|█▉        | 3566/17834 [1:47:52<110:01:13, 27.76s/it] 20%|██        | 3567/17834 [1:47:54<79:02:40, 19.95s/it]  20%|██        | 3568/17834 [1:47:56<57:29:31, 14.51s/it] 20%|██        | 3569/17834 [1:47:58<42:17:06, 10.67s/it] 20%|██        | 3570/17834 [1:47:59<31:38:28,  7.99s/it] 20%|██        | 3571/17834 [1:48:01<24:14:37,  6.12s/it] 20%|██        | 3572/17834 [1:48:03<19:02:42,  4.81s/it] 20%|██        | 3573/17834 [1:48:04<15:22:09,  3.88s/it] 20%|██        | 3574/17834 [1:48:06<12:52:33,  3.25s/it] 20%|██        | 3575/17834 [1:48:08<11:04:26,  2.80s/it] 20%|██        | 3576/17834 [1:48:10<9:53:20,  2.50s/it]  20%|██        | 3577/17834 [1:48:12<8:58:21,  2.27s/it] 20%|██        | 3578/17834 [1:48:13<8:19:07,  2.10s/it] 20%|██        | 3579/17834 [1:48:15<7:56:03,  2.00s/it] 20%|██        | 3580/17834 [1:48:17<7:37:58,  1.93s/it] 20%|██        | 3581/17834 [1:48:19<7:24:24,  1.87s/it] 20%|██        | 3582/17834 [1:48:20<7:13:45,  1.83s/it] 20%|██        | 3583/17834 [1:48:22<7:06:17,  1.79s/it] 20%|██        | 3584/17834 [1:48:24<7:05:30,  1.79s/it] 20%|██        | 3585/17834 [1:48:26<7:05:45,  1.79s/it] 20%|██        | 3586/17834 [1:48:27<7:03:13,  1.78s/it] 20%|██        | 3587/17834 [1:48:29<6:59:17,  1.77s/it] 20%|██        | 3588/17834 [1:48:31<6:59:52,  1.77s/it] 20%|██        | 3589/17834 [1:48:33<7:04:41,  1.79s/it] 20%|██        | 3590/17834 [1:48:34<7:05:21,  1.79s/it] 20%|██        | 3591/17834 [1:48:36<7:03:23,  1.78s/it] 20%|██        | 3592/17834 [1:48:38<7:05:59,  1.79s/it] 20%|██        | 3593/17834 [1:48:40<7:00:27,  1.77s/it] 20%|██        | 3594/17834 [1:48:42<7:07:19,  1.80s/it] 20%|██        | 3595/17834 [1:48:43<7:01:15,  1.78s/it] 20%|██        | 3596/17834 [1:48:45<6:58:34,  1.76s/it] 20%|██        | 3597/17834 [1:48:47<7:00:22,  1.77s/it] 20%|██        | 3598/17834 [1:48:48<6:51:04,  1.73s/it] 20%|██        | 3599/17834 [1:48:50<6:54:20,  1.75s/it]08/30/2024 21:03:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5675244331359863, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.037660498172044754, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3046512603759766, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9098362922668457}
 20%|██        | 3600/17834 [1:48:52<6:49:41,  1.73s/it] 20%|██        | 3601/17834 [1:48:54<6:52:47,  1.74s/it] 20%|██        | 3602/17834 [1:48:55<6:52:36,  1.74s/it] 20%|██        | 3603/17834 [1:48:57<6:52:00,  1.74s/it] 20%|██        | 3604/17834 [1:48:59<6:49:41,  1.73s/it] 20%|██        | 3605/17834 [1:49:01<6:47:58,  1.72s/it] 20%|██        | 3606/17834 [1:49:02<6:42:54,  1.70s/it] 20%|██        | 3607/17834 [1:49:04<6:52:02,  1.74s/it] 20%|██        | 3608/17834 [1:49:06<6:51:16,  1.73s/it] 20%|██        | 3609/17834 [1:49:07<6:47:27,  1.72s/it] 20%|██        | 3610/17834 [1:49:09<6:46:11,  1.71s/it] 20%|██        | 3611/17834 [1:49:11<6:48:56,  1.73s/it] 20%|██        | 3612/17834 [1:49:13<6:49:53,  1.73s/it] 20%|██        | 3613/17834 [1:49:14<6:51:44,  1.74s/it] 20%|██        | 3614/17834 [1:49:16<6:52:10,  1.74s/it] 20%|██        | 3615/17834 [1:49:18<6:52:27,  1.74s/it] 20%|██        | 3616/17834 [1:49:20<6:54:09,  1.75s/it] 20%|██        | 3617/17834 [1:49:21<6:56:24,  1.76s/it] 20%|██        | 3618/17834 [1:49:23<6:58:29,  1.77s/it] 20%|██        | 3619/17834 [1:49:25<6:55:49,  1.76s/it] 20%|██        | 3620/17834 [1:49:27<6:53:49,  1.75s/it] 20%|██        | 3621/17834 [1:49:28<6:57:03,  1.76s/it] 20%|██        | 3622/17834 [1:49:30<6:52:13,  1.74s/it] 20%|██        | 3623/17834 [1:49:32<6:52:04,  1.74s/it] 20%|██        | 3624/17834 [1:49:34<7:00:24,  1.78s/it] 20%|██        | 3625/17834 [1:49:36<6:59:24,  1.77s/it] 20%|██        | 3626/17834 [1:49:37<6:58:33,  1.77s/it] 20%|██        | 3627/17834 [1:49:39<6:54:39,  1.75s/it] 20%|██        | 3628/17834 [1:49:41<6:56:45,  1.76s/it] 20%|██        | 3629/17834 [1:49:43<6:54:51,  1.75s/it] 20%|██        | 3630/17834 [1:49:44<6:53:10,  1.75s/it] 20%|██        | 3631/17834 [1:49:46<6:56:40,  1.76s/it] 20%|██        | 3632/17834 [1:49:48<7:01:57,  1.78s/it] 20%|██        | 3633/17834 [1:49:50<7:00:34,  1.78s/it] 20%|██        | 3634/17834 [1:49:51<6:56:00,  1.76s/it] 20%|██        | 3635/17834 [1:49:53<6:54:06,  1.75s/it] 20%|██        | 3636/17834 [1:49:55<6:58:42,  1.77s/it] 20%|██        | 3637/17834 [1:49:57<6:55:43,  1.76s/it] 20%|██        | 3638/17834 [1:49:58<6:58:29,  1.77s/it] 20%|██        | 3639/17834 [1:50:00<6:55:11,  1.75s/it] 20%|██        | 3640/17834 [1:50:02<6:59:09,  1.77s/it] 20%|██        | 3641/17834 [1:50:04<6:55:31,  1.76s/it] 20%|██        | 3642/17834 [1:50:05<6:55:54,  1.76s/it] 20%|██        | 3643/17834 [1:50:07<6:54:17,  1.75s/it] 20%|██        | 3644/17834 [1:50:09<6:56:33,  1.76s/it] 20%|██        | 3645/17834 [1:50:11<7:00:17,  1.78s/it] 20%|██        | 3646/17834 [1:50:12<6:55:11,  1.76s/it] 20%|██        | 3647/17834 [1:50:14<6:58:39,  1.77s/it] 20%|██        | 3648/17834 [1:50:16<6:55:51,  1.76s/it] 20%|██        | 3649/17834 [1:50:18<6:48:25,  1.73s/it]08/30/2024 21:04:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6029269695281982, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04219881445169449, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.250235080718994, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8953609466552734}
 20%|██        | 3650/17834 [1:50:20<6:54:50,  1.75s/it] 20%|██        | 3651/17834 [1:50:21<6:56:54,  1.76s/it] 20%|██        | 3652/17834 [1:50:23<7:02:24,  1.79s/it] 20%|██        | 3653/17834 [1:50:25<7:02:43,  1.79s/it] 20%|██        | 3654/17834 [1:50:27<6:56:48,  1.76s/it] 20%|██        | 3655/17834 [1:50:28<6:53:09,  1.75s/it] 21%|██        | 3656/17834 [1:50:30<7:00:26,  1.78s/it] 21%|██        | 3657/17834 [1:50:32<6:53:24,  1.75s/it] 21%|██        | 3658/17834 [1:50:34<6:50:58,  1.74s/it] 21%|██        | 3659/17834 [1:50:35<6:51:42,  1.74s/it] 21%|██        | 3660/17834 [1:50:37<6:47:13,  1.72s/it] 21%|██        | 3661/17834 [1:50:39<6:49:55,  1.74s/it] 21%|██        | 3662/17834 [1:50:41<6:54:57,  1.76s/it] 21%|██        | 3663/17834 [1:50:42<6:54:24,  1.75s/it] 21%|██        | 3664/17834 [1:50:44<6:55:25,  1.76s/it] 21%|██        | 3665/17834 [1:50:46<6:55:48,  1.76s/it] 21%|██        | 3666/17834 [1:50:48<6:55:32,  1.76s/it] 21%|██        | 3667/17834 [1:50:49<6:57:56,  1.77s/it] 21%|██        | 3668/17834 [1:50:51<6:58:44,  1.77s/it] 21%|██        | 3669/17834 [1:50:53<6:58:45,  1.77s/it] 21%|██        | 3670/17834 [1:50:55<6:57:29,  1.77s/it] 21%|██        | 3671/17834 [1:50:56<6:56:41,  1.77s/it] 21%|██        | 3672/17834 [1:50:58<6:52:59,  1.75s/it] 21%|██        | 3673/17834 [1:51:00<6:56:23,  1.76s/it] 21%|██        | 3674/17834 [1:51:02<6:55:18,  1.76s/it] 21%|██        | 3675/17834 [1:51:03<6:51:14,  1.74s/it] 21%|██        | 3676/17834 [1:51:05<6:52:24,  1.75s/it] 21%|██        | 3677/17834 [1:51:07<6:54:24,  1.76s/it] 21%|██        | 3678/17834 [1:51:09<6:57:00,  1.77s/it] 21%|██        | 3679/17834 [1:51:11<6:55:50,  1.76s/it] 21%|██        | 3680/17834 [1:51:12<6:55:07,  1.76s/it] 21%|██        | 3681/17834 [1:51:14<7:00:11,  1.78s/it] 21%|██        | 3682/17834 [1:51:16<6:59:26,  1.78s/it] 21%|██        | 3683/17834 [1:51:18<6:52:54,  1.75s/it] 21%|██        | 3684/17834 [1:51:19<6:48:57,  1.73s/it] 21%|██        | 3685/17834 [1:51:21<6:57:03,  1.77s/it] 21%|██        | 3686/17834 [1:51:23<6:53:59,  1.76s/it] 21%|██        | 3687/17834 [1:51:25<6:53:08,  1.75s/it] 21%|██        | 3688/17834 [1:51:26<6:53:25,  1.75s/it] 21%|██        | 3689/17834 [1:51:28<6:58:36,  1.78s/it] 21%|██        | 3690/17834 [1:51:30<6:56:05,  1.77s/it] 21%|██        | 3691/17834 [1:51:32<6:52:40,  1.75s/it] 21%|██        | 3692/17834 [1:51:33<6:52:05,  1.75s/it] 21%|██        | 3693/17834 [1:51:35<6:56:48,  1.77s/it] 21%|██        | 3694/17834 [1:51:37<6:57:51,  1.77s/it] 21%|██        | 3695/17834 [1:51:39<6:55:30,  1.76s/it] 21%|██        | 3696/17834 [1:51:40<6:50:28,  1.74s/it] 21%|██        | 3697/17834 [1:51:42<6:50:56,  1.74s/it] 21%|██        | 3698/17834 [1:51:44<6:48:25,  1.73s/it] 21%|██        | 3699/17834 [1:51:46<6:45:30,  1.72s/it]08/30/2024 21:06:05 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4527451992034912, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04025175794959068, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.253075361251831, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.746072292327881}
 21%|██        | 3700/17834 [1:51:47<6:50:12,  1.74s/it] 21%|██        | 3701/17834 [1:51:49<6:49:19,  1.74s/it] 21%|██        | 3702/17834 [1:51:51<6:50:12,  1.74s/it] 21%|██        | 3703/17834 [1:51:53<6:52:11,  1.75s/it] 21%|██        | 3704/17834 [1:51:54<6:53:36,  1.76s/it] 21%|██        | 3705/17834 [1:51:56<6:53:05,  1.75s/it] 21%|██        | 3706/17834 [1:51:58<6:49:43,  1.74s/it] 21%|██        | 3707/17834 [1:52:00<6:51:46,  1.75s/it] 21%|██        | 3708/17834 [1:52:01<6:54:40,  1.76s/it] 21%|██        | 3709/17834 [1:52:03<6:56:35,  1.77s/it] 21%|██        | 3710/17834 [1:52:05<6:54:02,  1.76s/it] 21%|██        | 3711/17834 [1:52:07<6:55:59,  1.77s/it] 21%|██        | 3712/17834 [1:52:08<6:50:27,  1.74s/it] 21%|██        | 3713/17834 [1:52:10<6:52:48,  1.75s/it] 21%|██        | 3714/17834 [1:52:12<6:49:41,  1.74s/it] 21%|██        | 3715/17834 [1:52:14<6:51:57,  1.75s/it] 21%|██        | 3716/17834 [1:52:15<6:47:12,  1.73s/it] 21%|██        | 3717/17834 [1:52:17<6:48:54,  1.74s/it] 21%|██        | 3718/17834 [1:52:19<6:43:16,  1.71s/it] 21%|██        | 3719/17834 [1:52:20<6:45:57,  1.73s/it] 21%|██        | 3720/17834 [1:52:22<6:53:41,  1.76s/it] 21%|██        | 3721/17834 [1:52:24<7:00:07,  1.79s/it] 21%|██        | 3722/17834 [1:52:26<6:53:36,  1.76s/it] 21%|██        | 3723/17834 [1:52:28<6:46:23,  1.73s/it] 21%|██        | 3724/17834 [1:52:29<6:45:19,  1.72s/it] 21%|██        | 3725/17834 [1:52:31<6:47:57,  1.73s/it] 21%|██        | 3726/17834 [1:52:33<6:45:55,  1.73s/it] 21%|██        | 3727/17834 [1:52:34<6:47:53,  1.73s/it] 21%|██        | 3728/17834 [1:52:36<6:56:46,  1.77s/it] 21%|██        | 3729/17834 [1:52:38<6:51:59,  1.75s/it] 21%|██        | 3730/17834 [1:52:40<6:51:14,  1.75s/it] 21%|██        | 3731/17834 [1:52:42<6:51:10,  1.75s/it] 21%|██        | 3732/17834 [1:52:43<6:51:19,  1.75s/it] 21%|██        | 3733/17834 [1:52:45<6:52:04,  1.75s/it] 21%|██        | 3734/17834 [1:52:47<6:47:08,  1.73s/it] 21%|██        | 3735/17834 [1:52:48<6:48:26,  1.74s/it] 21%|██        | 3736/17834 [1:52:50<6:47:50,  1.74s/it] 21%|██        | 3737/17834 [1:52:52<6:43:05,  1.72s/it] 21%|██        | 3738/17834 [1:52:54<6:48:02,  1.74s/it] 21%|██        | 3739/17834 [1:52:55<6:46:48,  1.73s/it] 21%|██        | 3740/17834 [1:52:57<6:47:55,  1.74s/it] 21%|██        | 3741/17834 [1:52:59<6:43:39,  1.72s/it] 21%|██        | 3742/17834 [1:53:01<6:48:57,  1.74s/it] 21%|██        | 3743/17834 [1:53:02<6:43:40,  1.72s/it] 21%|██        | 3744/17834 [1:53:04<6:47:31,  1.74s/it] 21%|██        | 3745/17834 [1:53:06<6:48:08,  1.74s/it] 21%|██        | 3746/17834 [1:53:08<6:49:44,  1.75s/it] 21%|██        | 3747/17834 [1:53:09<6:48:29,  1.74s/it] 21%|██        | 3748/17834 [1:53:11<6:51:08,  1.75s/it] 21%|██        | 3749/17834 [1:53:13<6:56:19,  1.77s/it]08/30/2024 21:07:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 2.0736327171325684, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.049195546656847, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4678144454956055, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.590642929077148}
 21%|██        | 3750/17834 [1:53:15<6:54:11,  1.76s/it] 21%|██        | 3751/17834 [1:53:16<6:51:19,  1.75s/it] 21%|██        | 3752/17834 [1:53:18<6:51:49,  1.75s/it] 21%|██        | 3753/17834 [1:53:20<6:53:13,  1.76s/it] 21%|██        | 3754/17834 [1:53:22<6:49:43,  1.75s/it] 21%|██        | 3755/17834 [1:53:23<6:48:42,  1.74s/it] 21%|██        | 3756/17834 [1:53:25<6:50:09,  1.75s/it] 21%|██        | 3757/17834 [1:53:27<6:50:06,  1.75s/it] 21%|██        | 3758/17834 [1:53:29<6:52:36,  1.76s/it] 21%|██        | 3759/17834 [1:53:30<6:49:33,  1.75s/it] 21%|██        | 3760/17834 [1:53:32<6:52:36,  1.76s/it] 21%|██        | 3761/17834 [1:53:34<6:51:00,  1.75s/it] 21%|██        | 3762/17834 [1:53:36<6:51:07,  1.75s/it] 21%|██        | 3763/17834 [1:53:37<6:54:40,  1.77s/it] 21%|██        | 3764/17834 [1:53:39<6:53:21,  1.76s/it] 21%|██        | 3765/17834 [1:53:41<6:53:02,  1.76s/it] 21%|██        | 3766/17834 [1:53:43<6:53:00,  1.76s/it] 21%|██        | 3767/17834 [1:53:44<6:46:04,  1.73s/it] 21%|██        | 3768/17834 [1:53:46<6:49:03,  1.74s/it] 21%|██        | 3769/17834 [1:53:48<6:49:34,  1.75s/it] 21%|██        | 3770/17834 [1:53:50<6:46:25,  1.73s/it] 21%|██        | 3771/17834 [1:53:51<6:46:13,  1.73s/it] 21%|██        | 3772/17834 [1:53:53<6:48:31,  1.74s/it] 21%|██        | 3773/17834 [1:53:55<6:54:42,  1.77s/it] 21%|██        | 3774/17834 [1:53:57<6:54:11,  1.77s/it] 21%|██        | 3775/17834 [1:53:58<6:53:28,  1.76s/it] 21%|██        | 3776/17834 [1:54:00<6:56:42,  1.78s/it] 21%|██        | 3777/17834 [1:54:02<6:51:20,  1.76s/it] 21%|██        | 3778/17834 [1:54:04<6:47:15,  1.74s/it] 21%|██        | 3779/17834 [1:54:06<7:03:49,  1.81s/it] 21%|██        | 3780/17834 [1:54:07<6:55:06,  1.77s/it] 21%|██        | 3781/17834 [1:54:09<6:50:58,  1.75s/it] 21%|██        | 3782/17834 [1:54:11<6:51:01,  1.76s/it] 21%|██        | 3783/17834 [1:54:12<6:47:21,  1.74s/it] 21%|██        | 3784/17834 [1:54:14<6:48:59,  1.75s/it] 21%|██        | 3785/17834 [1:54:16<6:48:55,  1.75s/it] 21%|██        | 3786/17834 [1:54:18<6:50:34,  1.75s/it] 21%|██        | 3787/17834 [1:54:19<6:48:53,  1.75s/it] 21%|██        | 3788/17834 [1:54:21<6:48:07,  1.74s/it] 21%|██        | 3789/17834 [1:54:23<6:48:51,  1.75s/it] 21%|██▏       | 3790/17834 [1:54:25<6:44:08,  1.73s/it] 21%|██▏       | 3791/17834 [1:54:26<6:44:40,  1.73s/it] 21%|██▏       | 3792/17834 [1:54:28<6:47:13,  1.74s/it] 21%|██▏       | 3793/17834 [1:54:30<6:47:58,  1.74s/it] 21%|██▏       | 3794/17834 [1:54:32<6:51:24,  1.76s/it] 21%|██▏       | 3795/17834 [1:54:33<6:52:25,  1.76s/it] 21%|██▏       | 3796/17834 [1:54:35<6:49:20,  1.75s/it] 21%|██▏       | 3797/17834 [1:54:37<6:52:23,  1.76s/it] 21%|██▏       | 3798/17834 [1:54:39<6:52:40,  1.76s/it] 21%|██▏       | 3799/17834 [1:54:41<6:52:31,  1.76s/it]08/30/2024 21:09:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4704484939575195, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.051428839564323425, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.305609703063965, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.827486991882324}
 21%|██▏       | 3800/17834 [1:54:42<6:50:05,  1.75s/it] 21%|██▏       | 3801/17834 [1:54:44<6:51:11,  1.76s/it] 21%|██▏       | 3802/17834 [1:54:46<6:50:56,  1.76s/it] 21%|██▏       | 3803/17834 [1:54:47<6:48:56,  1.75s/it] 21%|██▏       | 3804/17834 [1:54:49<6:43:58,  1.73s/it] 21%|██▏       | 3805/17834 [1:54:51<6:44:42,  1.73s/it] 21%|██▏       | 3806/17834 [1:54:53<6:44:18,  1.73s/it] 21%|██▏       | 3807/17834 [1:54:54<6:49:07,  1.75s/it] 21%|██▏       | 3808/17834 [1:54:56<6:55:20,  1.78s/it] 21%|██▏       | 3809/17834 [1:54:58<6:54:10,  1.77s/it] 21%|██▏       | 3810/17834 [1:55:00<6:53:26,  1.77s/it] 21%|██▏       | 3811/17834 [1:55:02<6:52:52,  1.77s/it] 21%|██▏       | 3812/17834 [1:55:03<6:46:14,  1.74s/it] 21%|██▏       | 3813/17834 [1:55:05<6:47:31,  1.74s/it] 21%|██▏       | 3814/17834 [1:55:07<6:49:11,  1.75s/it] 21%|██▏       | 3815/17834 [1:55:08<6:44:56,  1.73s/it] 21%|██▏       | 3816/17834 [1:55:10<6:54:09,  1.77s/it] 21%|██▏       | 3817/17834 [1:55:12<6:50:10,  1.76s/it] 21%|██▏       | 3818/17834 [1:55:14<6:52:37,  1.77s/it] 21%|██▏       | 3819/17834 [1:55:16<6:51:45,  1.76s/it] 21%|██▏       | 3820/17834 [1:55:17<6:47:39,  1.75s/it] 21%|██▏       | 3821/17834 [1:55:19<6:45:36,  1.74s/it] 21%|██▏       | 3822/17834 [1:55:21<6:45:24,  1.74s/it] 21%|██▏       | 3823/17834 [1:55:22<6:45:48,  1.74s/it] 21%|██▏       | 3824/17834 [1:55:24<6:42:49,  1.73s/it] 21%|██▏       | 3825/17834 [1:55:26<6:51:24,  1.76s/it] 21%|██▏       | 3826/17834 [1:55:28<6:50:06,  1.76s/it] 21%|██▏       | 3827/17834 [1:55:30<6:52:32,  1.77s/it] 21%|██▏       | 3828/17834 [1:55:31<6:49:02,  1.75s/it] 21%|██▏       | 3829/17834 [1:55:33<6:51:31,  1.76s/it] 21%|██▏       | 3830/17834 [1:55:35<6:54:42,  1.78s/it] 21%|██▏       | 3831/17834 [1:55:37<6:49:43,  1.76s/it] 21%|██▏       | 3832/17834 [1:55:38<6:45:41,  1.74s/it] 21%|██▏       | 3833/17834 [1:55:40<6:43:47,  1.73s/it] 21%|██▏       | 3834/17834 [1:55:42<6:46:37,  1.74s/it] 22%|██▏       | 3835/17834 [1:55:43<6:43:15,  1.73s/it] 22%|██▏       | 3836/17834 [1:55:45<6:45:46,  1.74s/it] 22%|██▏       | 3837/17834 [1:55:47<6:43:12,  1.73s/it] 22%|██▏       | 3838/17834 [1:55:49<6:45:33,  1.74s/it] 22%|██▏       | 3839/17834 [1:55:50<6:45:21,  1.74s/it] 22%|██▏       | 3840/17834 [1:55:52<6:42:56,  1.73s/it] 22%|██▏       | 3841/17834 [1:55:54<6:42:02,  1.72s/it] 22%|██▏       | 3842/17834 [1:55:56<6:50:23,  1.76s/it] 22%|██▏       | 3843/17834 [1:55:57<6:43:49,  1.73s/it] 22%|██▏       | 3844/17834 [1:55:59<6:46:06,  1.74s/it] 22%|██▏       | 3845/17834 [1:56:01<6:43:37,  1.73s/it] 22%|██▏       | 3846/17834 [1:56:03<6:43:17,  1.73s/it] 22%|██▏       | 3847/17834 [1:56:04<6:42:26,  1.73s/it] 22%|██▏       | 3848/17834 [1:56:06<6:49:05,  1.75s/it] 22%|██▏       | 3849/17834 [1:56:08<6:47:24,  1.75s/it]08/30/2024 21:10:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4716002941131592, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.043963998556137085, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1922338008880615, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7077980041503906}
 22%|██▏       | 3850/17834 [1:56:10<6:46:03,  1.74s/it] 22%|██▏       | 3851/17834 [1:56:11<6:46:40,  1.74s/it] 22%|██▏       | 3852/17834 [1:56:13<6:50:57,  1.76s/it] 22%|██▏       | 3853/17834 [1:56:15<6:50:26,  1.76s/it] 22%|██▏       | 3854/17834 [1:56:17<6:52:18,  1.77s/it] 22%|██▏       | 3855/17834 [1:56:18<6:49:40,  1.76s/it] 22%|██▏       | 3856/17834 [1:56:20<6:45:44,  1.74s/it] 22%|██▏       | 3857/17834 [1:56:22<6:45:39,  1.74s/it] 22%|██▏       | 3858/17834 [1:56:24<6:41:45,  1.72s/it] 22%|██▏       | 3859/17834 [1:56:25<6:46:20,  1.74s/it] 22%|██▏       | 3860/17834 [1:56:27<6:50:11,  1.76s/it] 22%|██▏       | 3861/17834 [1:56:29<6:44:45,  1.74s/it] 22%|██▏       | 3862/17834 [1:56:31<6:43:50,  1.73s/it] 22%|██▏       | 3863/17834 [1:56:32<6:42:42,  1.73s/it] 22%|██▏       | 3864/17834 [1:56:34<6:44:31,  1.74s/it] 22%|██▏       | 3865/17834 [1:56:36<6:47:20,  1.75s/it] 22%|██▏       | 3866/17834 [1:56:38<6:50:14,  1.76s/it] 22%|██▏       | 3867/17834 [1:56:39<6:51:46,  1.77s/it] 22%|██▏       | 3868/17834 [1:56:41<6:47:12,  1.75s/it] 22%|██▏       | 3869/17834 [1:56:43<6:46:16,  1.75s/it] 22%|██▏       | 3870/17834 [1:56:45<6:46:22,  1.75s/it] 22%|██▏       | 3871/17834 [1:56:46<6:43:22,  1.73s/it] 22%|██▏       | 3872/17834 [1:56:48<6:42:20,  1.73s/it] 22%|██▏       | 3873/17834 [1:56:50<6:43:56,  1.74s/it] 22%|██▏       | 3874/17834 [1:56:51<6:45:50,  1.74s/it] 22%|██▏       | 3875/17834 [1:56:53<6:46:36,  1.75s/it] 22%|██▏       | 3876/17834 [1:56:55<6:43:15,  1.73s/it] 22%|██▏       | 3877/17834 [1:56:57<6:42:36,  1.73s/it] 22%|██▏       | 3878/17834 [1:56:58<6:43:27,  1.73s/it] 22%|██▏       | 3879/17834 [1:57:00<6:48:05,  1.75s/it] 22%|██▏       | 3880/17834 [1:57:02<6:49:21,  1.76s/it] 22%|██▏       | 3881/17834 [1:57:04<6:45:51,  1.75s/it] 22%|██▏       | 3882/17834 [1:57:05<6:46:52,  1.75s/it] 22%|██▏       | 3883/17834 [1:57:07<6:48:41,  1.76s/it] 22%|██▏       | 3884/17834 [1:57:09<6:46:44,  1.75s/it] 22%|██▏       | 3885/17834 [1:57:11<6:54:53,  1.78s/it] 22%|██▏       | 3886/17834 [1:57:12<6:45:44,  1.75s/it] 22%|██▏       | 3887/17834 [1:57:14<6:43:09,  1.73s/it] 22%|██▏       | 3888/17834 [1:57:16<6:40:21,  1.72s/it] 22%|██▏       | 3889/17834 [1:57:18<6:48:35,  1.76s/it] 22%|██▏       | 3890/17834 [1:57:19<6:50:15,  1.77s/it] 22%|██▏       | 3891/17834 [1:57:21<6:48:34,  1.76s/it] 22%|██▏       | 3892/17834 [1:57:23<6:43:47,  1.74s/it] 22%|██▏       | 3893/17834 [1:57:25<6:48:14,  1.76s/it] 22%|██▏       | 3894/17834 [1:57:26<6:47:17,  1.75s/it] 22%|██▏       | 3895/17834 [1:57:28<6:50:07,  1.77s/it] 22%|██▏       | 3896/17834 [1:57:30<6:48:14,  1.76s/it] 22%|██▏       | 3897/17834 [1:57:32<6:44:06,  1.74s/it] 22%|██▏       | 3898/17834 [1:57:33<6:42:06,  1.73s/it] 22%|██▏       | 3899/17834 [1:57:35<6:43:04,  1.74s/it]08/30/2024 21:11:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.219301462173462, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03988240659236908, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2646942138671875, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5238780975341797}
 22%|██▏       | 3900/17834 [1:57:37<6:44:29,  1.74s/it] 22%|██▏       | 3901/17834 [1:57:39<6:40:38,  1.73s/it] 22%|██▏       | 3902/17834 [1:57:40<6:44:37,  1.74s/it] 22%|██▏       | 3903/17834 [1:57:42<6:49:01,  1.76s/it] 22%|██▏       | 3904/17834 [1:57:44<6:44:00,  1.74s/it] 22%|██▏       | 3905/17834 [1:57:46<6:41:47,  1.73s/it] 22%|██▏       | 3906/17834 [1:57:47<6:44:19,  1.74s/it] 22%|██▏       | 3907/17834 [1:57:49<6:41:29,  1.73s/it] 22%|██▏       | 3908/17834 [1:57:51<6:43:05,  1.74s/it] 22%|██▏       | 3909/17834 [1:57:53<6:43:35,  1.74s/it] 22%|██▏       | 3910/17834 [1:57:54<6:56:46,  1.80s/it] 22%|██▏       | 3911/17834 [1:57:56<6:49:27,  1.76s/it] 22%|██▏       | 3912/17834 [1:57:58<6:52:32,  1.78s/it] 22%|██▏       | 3913/17834 [1:58:00<6:51:12,  1.77s/it] 22%|██▏       | 3914/17834 [1:58:01<6:47:41,  1.76s/it] 22%|██▏       | 3915/17834 [1:58:03<6:52:56,  1.78s/it] 22%|██▏       | 3916/17834 [1:58:05<6:55:22,  1.79s/it] 22%|██▏       | 3917/17834 [1:58:07<6:51:53,  1.78s/it] 22%|██▏       | 3918/17834 [1:58:09<6:50:05,  1.77s/it] 22%|██▏       | 3919/17834 [1:58:10<6:49:08,  1.76s/it] 22%|██▏       | 3920/17834 [1:58:12<6:45:13,  1.75s/it] 22%|██▏       | 3921/17834 [1:58:14<6:43:14,  1.74s/it] 22%|██▏       | 3922/17834 [1:58:15<6:41:35,  1.73s/it] 22%|██▏       | 3923/17834 [1:58:17<6:47:48,  1.76s/it] 22%|██▏       | 3924/17834 [1:58:19<6:50:15,  1.77s/it] 22%|██▏       | 3925/17834 [1:58:21<6:45:54,  1.75s/it] 22%|██▏       | 3926/17834 [1:58:23<6:41:25,  1.73s/it] 22%|██▏       | 3927/17834 [1:58:24<6:49:08,  1.77s/it] 22%|██▏       | 3928/17834 [1:58:26<6:45:18,  1.75s/it] 22%|██▏       | 3929/17834 [1:58:28<6:49:07,  1.77s/it] 22%|██▏       | 3930/17834 [1:58:30<6:46:13,  1.75s/it] 22%|██▏       | 3931/17834 [1:58:31<6:45:16,  1.75s/it] 22%|██▏       | 3932/17834 [1:58:33<6:53:26,  1.78s/it] 22%|██▏       | 3933/17834 [1:58:35<6:49:15,  1.77s/it] 22%|██▏       | 3934/17834 [1:58:37<6:45:07,  1.75s/it] 22%|██▏       | 3935/17834 [1:58:38<6:44:30,  1.75s/it] 22%|██▏       | 3936/17834 [1:58:40<6:41:23,  1.73s/it] 22%|██▏       | 3937/17834 [1:58:42<6:43:09,  1.74s/it] 22%|██▏       | 3938/17834 [1:58:44<6:44:36,  1.75s/it] 22%|██▏       | 3939/17834 [1:58:45<6:40:44,  1.73s/it] 22%|██▏       | 3940/17834 [1:58:47<6:42:02,  1.74s/it] 22%|██▏       | 3941/17834 [1:58:49<6:43:55,  1.74s/it] 22%|██▏       | 3942/17834 [1:58:51<6:42:21,  1.74s/it] 22%|██▏       | 3943/17834 [1:58:52<6:45:35,  1.75s/it] 22%|██▏       | 3944/17834 [1:58:54<6:45:08,  1.75s/it] 22%|██▏       | 3945/17834 [1:58:56<6:47:19,  1.76s/it] 22%|██▏       | 3946/17834 [1:58:58<6:47:15,  1.76s/it] 22%|██▏       | 3947/17834 [1:58:59<6:44:44,  1.75s/it] 22%|██▏       | 3948/17834 [1:59:01<6:44:46,  1.75s/it] 22%|██▏       | 3949/17834 [1:59:03<6:43:38,  1.74s/it]08/30/2024 21:13:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6540874242782593, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0529634952545166, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3983824253082275, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.105433464050293}
 22%|██▏       | 3950/17834 [1:59:05<6:41:24,  1.73s/it] 22%|██▏       | 3951/17834 [1:59:06<6:40:54,  1.73s/it] 22%|██▏       | 3952/17834 [1:59:08<6:41:58,  1.74s/it] 22%|██▏       | 3953/17834 [1:59:10<6:47:04,  1.76s/it] 22%|██▏       | 3954/17834 [1:59:11<6:42:56,  1.74s/it] 22%|██▏       | 3955/17834 [1:59:13<6:45:23,  1.75s/it] 22%|██▏       | 3956/17834 [1:59:15<6:45:39,  1.75s/it] 22%|██▏       | 3957/17834 [1:59:17<6:42:06,  1.74s/it] 22%|██▏       | 3958/17834 [1:59:18<6:42:45,  1.74s/it] 22%|██▏       | 3959/17834 [1:59:20<6:43:40,  1.75s/it] 22%|██▏       | 3960/17834 [1:59:22<6:40:42,  1.73s/it] 22%|██▏       | 3961/17834 [1:59:24<6:41:08,  1.73s/it] 22%|██▏       | 3962/17834 [1:59:25<6:41:53,  1.74s/it] 22%|██▏       | 3963/17834 [1:59:27<6:41:38,  1.74s/it] 22%|██▏       | 3964/17834 [1:59:29<6:48:16,  1.77s/it] 22%|██▏       | 3965/17834 [1:59:31<6:44:35,  1.75s/it] 22%|██▏       | 3966/17834 [1:59:32<6:46:20,  1.76s/it] 22%|██▏       | 3967/17834 [1:59:34<6:47:07,  1.76s/it] 22%|██▏       | 3968/17834 [1:59:36<6:43:55,  1.75s/it] 22%|██▏       | 3969/17834 [1:59:38<6:42:46,  1.74s/it] 22%|██▏       | 3970/17834 [1:59:39<6:43:49,  1.75s/it] 22%|██▏       | 3971/17834 [1:59:41<6:40:09,  1.73s/it] 22%|██▏       | 3972/17834 [1:59:43<6:49:09,  1.77s/it] 22%|██▏       | 3973/17834 [1:59:45<6:46:01,  1.76s/it] 22%|██▏       | 3974/17834 [1:59:47<6:47:37,  1.76s/it] 22%|██▏       | 3975/17834 [1:59:48<6:40:33,  1.73s/it] 22%|██▏       | 3976/17834 [1:59:50<6:39:43,  1.73s/it] 22%|██▏       | 3977/17834 [1:59:52<6:44:06,  1.75s/it] 22%|██▏       | 3978/17834 [1:59:53<6:42:43,  1.74s/it] 22%|██▏       | 3979/17834 [1:59:55<6:40:17,  1.73s/it] 22%|██▏       | 3980/17834 [1:59:57<6:42:37,  1.74s/it] 22%|██▏       | 3981/17834 [1:59:59<6:44:27,  1.75s/it] 22%|██▏       | 3982/17834 [2:00:00<6:41:56,  1.74s/it] 22%|██▏       | 3983/17834 [2:00:02<6:42:43,  1.74s/it] 22%|██▏       | 3984/17834 [2:00:04<6:40:36,  1.74s/it] 22%|██▏       | 3985/17834 [2:00:06<6:46:36,  1.76s/it] 22%|██▏       | 3986/17834 [2:00:07<6:45:37,  1.76s/it] 22%|██▏       | 3987/17834 [2:00:09<6:43:28,  1.75s/it] 22%|██▏       | 3988/17834 [2:00:11<6:45:33,  1.76s/it] 22%|██▏       | 3989/17834 [2:00:13<6:43:16,  1.75s/it] 22%|██▏       | 3990/17834 [2:00:14<6:42:07,  1.74s/it] 22%|██▏       | 3991/17834 [2:00:16<6:43:46,  1.75s/it] 22%|██▏       | 3992/17834 [2:00:18<6:40:02,  1.73s/it] 22%|██▏       | 3993/17834 [2:00:20<6:40:28,  1.74s/it] 22%|██▏       | 3994/17834 [2:00:21<6:42:18,  1.74s/it] 22%|██▏       | 3995/17834 [2:00:23<6:37:11,  1.72s/it] 22%|██▏       | 3996/17834 [2:00:25<6:41:23,  1.74s/it] 22%|██▏       | 3997/17834 [2:00:27<6:44:32,  1.75s/it] 22%|██▏       | 3998/17834 [2:00:28<6:44:03,  1.75s/it] 22%|██▏       | 3999/17834 [2:00:30<6:40:05,  1.74s/it]08/30/2024 21:14:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2108207941055298, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03705883026123047, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1547141075134277, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4025936126708984}
 22%|██▏       | 4000/17834 [2:00:32<6:47:17,  1.77s/it] 22%|██▏       | 4001/17834 [2:00:34<6:43:26,  1.75s/it] 22%|██▏       | 4002/17834 [2:00:35<6:42:07,  1.74s/it] 22%|██▏       | 4003/17834 [2:00:37<6:40:20,  1.74s/it] 22%|██▏       | 4004/17834 [2:00:39<6:39:20,  1.73s/it] 22%|██▏       | 4005/17834 [2:00:41<6:42:30,  1.75s/it] 22%|██▏       | 4006/17834 [2:00:42<6:42:54,  1.75s/it] 22%|██▏       | 4007/17834 [2:00:44<6:42:50,  1.75s/it] 22%|██▏       | 4008/17834 [2:00:46<6:44:59,  1.76s/it] 22%|██▏       | 4009/17834 [2:00:48<6:46:11,  1.76s/it] 22%|██▏       | 4010/17834 [2:00:49<6:47:58,  1.77s/it] 22%|██▏       | 4011/17834 [2:00:51<6:50:02,  1.78s/it] 22%|██▏       | 4012/17834 [2:00:53<6:49:30,  1.78s/it] 23%|██▎       | 4013/17834 [2:00:55<6:48:05,  1.77s/it] 23%|██▎       | 4014/17834 [2:00:56<6:47:19,  1.77s/it] 23%|██▎       | 4015/17834 [2:00:58<6:42:33,  1.75s/it] 23%|██▎       | 4016/17834 [2:01:00<6:41:28,  1.74s/it] 23%|██▎       | 4017/17834 [2:01:02<6:38:39,  1.73s/it] 23%|██▎       | 4018/17834 [2:01:03<6:36:09,  1.72s/it] 23%|██▎       | 4019/17834 [2:01:05<6:38:02,  1.73s/it] 23%|██▎       | 4020/17834 [2:01:07<6:42:35,  1.75s/it] 23%|██▎       | 4021/17834 [2:01:09<6:43:36,  1.75s/it] 23%|██▎       | 4022/17834 [2:01:10<6:40:11,  1.74s/it] 23%|██▎       | 4023/17834 [2:01:12<6:43:42,  1.75s/it] 23%|██▎       | 4024/17834 [2:01:14<6:42:55,  1.75s/it] 23%|██▎       | 4025/17834 [2:01:16<6:46:54,  1.77s/it] 23%|██▎       | 4026/17834 [2:01:17<6:41:59,  1.75s/it] 23%|██▎       | 4027/17834 [2:01:19<6:40:36,  1.74s/it] 23%|██▎       | 4028/17834 [2:01:21<6:39:50,  1.74s/it] 23%|██▎       | 4029/17834 [2:01:23<6:43:58,  1.76s/it] 23%|██▎       | 4030/17834 [2:01:24<6:41:05,  1.74s/it] 23%|██▎       | 4031/17834 [2:01:26<6:38:48,  1.73s/it] 23%|██▎       | 4032/17834 [2:01:28<6:39:48,  1.74s/it] 23%|██▎       | 4033/17834 [2:01:30<6:43:47,  1.76s/it] 23%|██▎       | 4034/17834 [2:01:31<6:43:17,  1.75s/it] 23%|██▎       | 4035/17834 [2:01:33<6:40:12,  1.74s/it] 23%|██▎       | 4036/17834 [2:01:35<6:41:33,  1.75s/it] 23%|██▎       | 4037/17834 [2:01:36<6:35:35,  1.72s/it] 23%|██▎       | 4038/17834 [2:01:38<6:46:01,  1.77s/it] 23%|██▎       | 4039/17834 [2:01:40<6:45:47,  1.76s/it] 23%|██▎       | 4040/17834 [2:01:42<6:44:55,  1.76s/it] 23%|██▎       | 4041/17834 [2:01:44<6:42:55,  1.75s/it] 23%|██▎       | 4042/17834 [2:01:45<6:42:04,  1.75s/it] 23%|██▎       | 4043/17834 [2:01:47<6:37:21,  1.73s/it] 23%|██▎       | 4044/17834 [2:01:49<6:37:13,  1.73s/it] 23%|██▎       | 4045/17834 [2:01:51<6:47:28,  1.77s/it] 23%|██▎       | 4046/17834 [2:01:52<6:42:45,  1.75s/it] 23%|██▎       | 4047/17834 [2:01:54<6:42:39,  1.75s/it] 23%|██▎       | 4048/17834 [2:01:56<6:41:22,  1.75s/it] 23%|██▎       | 4049/17834 [2:01:58<6:38:02,  1.73s/it]08/30/2024 21:16:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6783455610275269, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045073002576828, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3496313095092773, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.073050022125244}
 23%|██▎       | 4050/17834 [2:01:59<6:34:43,  1.72s/it] 23%|██▎       | 4051/17834 [2:02:01<6:31:25,  1.70s/it] 23%|██▎       | 4052/17834 [2:02:03<6:33:33,  1.71s/it] 23%|██▎       | 4053/17834 [2:02:04<6:32:52,  1.71s/it] 23%|██▎       | 4054/17834 [2:02:06<6:38:45,  1.74s/it] 23%|██▎       | 4055/17834 [2:02:08<6:39:41,  1.74s/it] 23%|██▎       | 4056/17834 [2:02:10<6:46:08,  1.77s/it] 23%|██▎       | 4057/17834 [2:02:11<6:47:22,  1.77s/it] 23%|██▎       | 4058/17834 [2:02:13<6:48:22,  1.78s/it] 23%|██▎       | 4059/17834 [2:02:15<6:43:40,  1.76s/it] 23%|██▎       | 4060/17834 [2:02:17<6:46:17,  1.77s/it] 23%|██▎       | 4061/17834 [2:02:19<6:47:18,  1.77s/it] 23%|██▎       | 4062/17834 [2:02:20<6:50:11,  1.79s/it] 23%|██▎       | 4063/17834 [2:02:22<6:45:42,  1.77s/it] 23%|██▎       | 4064/17834 [2:02:24<6:44:34,  1.76s/it] 23%|██▎       | 4065/17834 [2:02:26<6:42:10,  1.75s/it] 23%|██▎       | 4066/17834 [2:02:27<6:41:56,  1.75s/it] 23%|██▎       | 4067/17834 [2:02:29<6:39:56,  1.74s/it] 23%|██▎       | 4068/17834 [2:02:31<6:39:18,  1.74s/it] 23%|██▎       | 4069/17834 [2:02:33<6:40:57,  1.75s/it] 23%|██▎       | 4070/17834 [2:02:34<6:44:20,  1.76s/it] 23%|██▎       | 4071/17834 [2:02:36<6:45:48,  1.77s/it] 23%|██▎       | 4072/17834 [2:02:38<6:44:17,  1.76s/it] 23%|██▎       | 4073/17834 [2:02:40<6:40:36,  1.75s/it] 23%|██▎       | 4074/17834 [2:02:41<6:40:07,  1.74s/it] 23%|██▎       | 4075/17834 [2:02:43<6:38:25,  1.74s/it] 23%|██▎       | 4076/17834 [2:02:45<6:33:55,  1.72s/it] 23%|██▎       | 4077/17834 [2:02:46<6:33:24,  1.72s/it] 23%|██▎       | 4078/17834 [2:02:48<6:36:19,  1.73s/it] 23%|██▎       | 4079/17834 [2:02:50<6:35:18,  1.72s/it] 23%|██▎       | 4080/17834 [2:02:52<6:35:21,  1.72s/it] 23%|██▎       | 4081/17834 [2:02:53<6:39:52,  1.74s/it] 23%|██▎       | 4082/17834 [2:02:55<6:41:13,  1.75s/it] 23%|██▎       | 4083/17834 [2:02:57<6:43:56,  1.76s/it] 23%|██▎       | 4084/17834 [2:02:59<6:37:46,  1.74s/it] 23%|██▎       | 4085/17834 [2:03:00<6:38:07,  1.74s/it] 23%|██▎       | 4086/17834 [2:03:02<6:41:52,  1.75s/it] 23%|██▎       | 4087/17834 [2:03:04<6:39:58,  1.75s/it] 23%|██▎       | 4088/17834 [2:03:06<6:37:13,  1.73s/it] 23%|██▎       | 4089/17834 [2:03:07<6:38:56,  1.74s/it] 23%|██▎       | 4090/17834 [2:03:09<6:36:18,  1.73s/it] 23%|██▎       | 4091/17834 [2:03:11<6:35:44,  1.73s/it] 23%|██▎       | 4092/17834 [2:03:13<6:35:50,  1.73s/it] 23%|██▎       | 4093/17834 [2:03:14<6:35:44,  1.73s/it] 23%|██▎       | 4094/17834 [2:03:16<6:41:09,  1.75s/it] 23%|██▎       | 4095/17834 [2:03:18<6:37:39,  1.74s/it] 23%|██▎       | 4096/17834 [2:03:19<6:33:32,  1.72s/it] 23%|██▎       | 4097/17834 [2:03:21<6:37:50,  1.74s/it] 23%|██▎       | 4098/17834 [2:03:23<6:45:03,  1.77s/it] 23%|██▎       | 4099/17834 [2:03:25<6:42:32,  1.76s/it]08/30/2024 21:17:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3041836023330688, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03941256180405617, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.34222412109375, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6858203411102295}
 23%|██▎       | 4100/17834 [2:03:27<6:42:11,  1.76s/it] 23%|██▎       | 4101/17834 [2:03:28<6:38:58,  1.74s/it] 23%|██▎       | 4102/17834 [2:03:30<6:42:02,  1.76s/it] 23%|██▎       | 4103/17834 [2:03:32<6:39:31,  1.75s/it] 23%|██▎       | 4104/17834 [2:03:33<6:37:56,  1.74s/it] 23%|██▎       | 4105/17834 [2:03:35<6:37:44,  1.74s/it] 23%|██▎       | 4106/17834 [2:03:37<6:37:32,  1.74s/it] 23%|██▎       | 4107/17834 [2:03:39<6:35:25,  1.73s/it] 23%|██▎       | 4108/17834 [2:03:41<6:45:42,  1.77s/it] 23%|██▎       | 4109/17834 [2:03:42<6:44:09,  1.77s/it] 23%|██▎       | 4110/17834 [2:03:44<6:39:10,  1.75s/it] 23%|██▎       | 4111/17834 [2:03:46<6:40:00,  1.75s/it] 23%|██▎       | 4112/17834 [2:03:47<6:38:07,  1.74s/it] 23%|██▎       | 4113/17834 [2:03:49<6:36:52,  1.74s/it] 23%|██▎       | 4114/17834 [2:03:51<6:34:14,  1.72s/it] 23%|██▎       | 4115/17834 [2:03:53<6:34:42,  1.73s/it] 23%|██▎       | 4116/17834 [2:03:54<6:37:06,  1.74s/it] 23%|██▎       | 4117/17834 [2:03:56<6:34:10,  1.72s/it] 23%|██▎       | 4118/17834 [2:03:58<6:33:31,  1.72s/it] 23%|██▎       | 4119/17834 [2:04:00<6:37:14,  1.74s/it] 23%|██▎       | 4120/17834 [2:04:01<6:43:32,  1.77s/it] 23%|██▎       | 4121/17834 [2:04:03<6:40:26,  1.75s/it] 23%|██▎       | 4122/17834 [2:04:05<6:38:01,  1.74s/it] 23%|██▎       | 4123/17834 [2:04:07<6:38:11,  1.74s/it] 23%|██▎       | 4124/17834 [2:04:08<6:37:39,  1.74s/it] 23%|██▎       | 4125/17834 [2:04:10<6:32:21,  1.72s/it] 23%|██▎       | 4126/17834 [2:04:12<6:36:09,  1.73s/it] 23%|██▎       | 4127/17834 [2:04:14<6:39:04,  1.75s/it] 23%|██▎       | 4128/17834 [2:04:15<6:40:18,  1.75s/it] 23%|██▎       | 4129/17834 [2:04:17<6:42:06,  1.76s/it] 23%|██▎       | 4130/17834 [2:04:19<6:41:35,  1.76s/it] 23%|██▎       | 4131/17834 [2:04:21<6:43:00,  1.76s/it] 23%|██▎       | 4132/17834 [2:04:22<6:40:08,  1.75s/it] 23%|██▎       | 4133/17834 [2:04:24<6:38:54,  1.75s/it] 23%|██▎       | 4134/17834 [2:04:26<6:38:00,  1.74s/it] 23%|██▎       | 4135/17834 [2:04:28<6:35:56,  1.73s/it] 23%|██▎       | 4136/17834 [2:04:29<6:36:03,  1.73s/it] 23%|██▎       | 4137/17834 [2:04:31<6:36:33,  1.74s/it] 23%|██▎       | 4138/17834 [2:04:33<6:40:49,  1.76s/it] 23%|██▎       | 4139/17834 [2:04:35<6:40:04,  1.75s/it] 23%|██▎       | 4140/17834 [2:04:36<6:38:35,  1.75s/it] 23%|██▎       | 4141/17834 [2:04:38<6:35:48,  1.73s/it] 23%|██▎       | 4142/17834 [2:04:40<6:35:36,  1.73s/it] 23%|██▎       | 4143/17834 [2:04:41<6:37:29,  1.74s/it] 23%|██▎       | 4144/17834 [2:04:43<6:36:57,  1.74s/it] 23%|██▎       | 4145/17834 [2:04:45<6:39:21,  1.75s/it] 23%|██▎       | 4146/17834 [2:04:47<6:38:22,  1.75s/it] 23%|██▎       | 4147/17834 [2:04:48<6:39:42,  1.75s/it] 23%|██▎       | 4148/17834 [2:04:50<6:37:42,  1.74s/it] 23%|██▎       | 4149/17834 [2:04:52<6:36:24,  1.74s/it]08/30/2024 21:19:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3938534259796143, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.042724378407001495, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2510311603546143, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6876089572906494}
 23%|██▎       | 4150/17834 [2:04:54<6:36:02,  1.74s/it] 23%|██▎       | 4151/17834 [2:04:55<6:36:05,  1.74s/it] 23%|██▎       | 4152/17834 [2:04:57<6:42:19,  1.76s/it] 23%|██▎       | 4153/17834 [2:04:59<6:41:44,  1.76s/it] 23%|██▎       | 4154/17834 [2:05:01<6:40:37,  1.76s/it] 23%|██▎       | 4155/17834 [2:05:02<6:33:41,  1.73s/it] 23%|██▎       | 4156/17834 [2:05:04<6:34:13,  1.73s/it] 23%|██▎       | 4157/17834 [2:05:06<6:38:16,  1.75s/it] 23%|██▎       | 4158/17834 [2:05:08<6:43:05,  1.77s/it] 23%|██▎       | 4159/17834 [2:05:09<6:39:37,  1.75s/it] 23%|██▎       | 4160/17834 [2:05:11<6:34:25,  1.73s/it] 23%|██▎       | 4161/17834 [2:05:13<6:32:29,  1.72s/it] 23%|██▎       | 4162/17834 [2:05:15<6:32:34,  1.72s/it] 23%|██▎       | 4163/17834 [2:05:16<6:33:13,  1.73s/it] 23%|██▎       | 4164/17834 [2:05:18<6:33:08,  1.73s/it] 23%|██▎       | 4165/17834 [2:05:20<6:33:33,  1.73s/it] 23%|██▎       | 4166/17834 [2:05:22<6:40:54,  1.76s/it] 23%|██▎       | 4167/17834 [2:05:23<6:40:53,  1.76s/it] 23%|██▎       | 4168/17834 [2:05:25<6:39:30,  1.75s/it] 23%|██▎       | 4169/17834 [2:05:27<6:40:29,  1.76s/it] 23%|██▎       | 4170/17834 [2:05:29<6:37:21,  1.74s/it] 23%|██▎       | 4171/17834 [2:05:30<6:46:51,  1.79s/it] 23%|██▎       | 4172/17834 [2:05:32<6:40:58,  1.76s/it] 23%|██▎       | 4173/17834 [2:05:34<6:40:42,  1.76s/it] 23%|██▎       | 4174/17834 [2:05:36<6:36:47,  1.74s/it] 23%|██▎       | 4175/17834 [2:05:37<6:38:21,  1.75s/it] 23%|██▎       | 4176/17834 [2:05:39<6:37:07,  1.74s/it] 23%|██▎       | 4177/17834 [2:05:41<6:42:01,  1.77s/it] 23%|██▎       | 4178/17834 [2:05:43<6:39:34,  1.76s/it] 23%|██▎       | 4179/17834 [2:05:44<6:38:45,  1.75s/it] 23%|██▎       | 4180/17834 [2:05:46<6:45:02,  1.78s/it] 23%|██▎       | 4181/17834 [2:05:48<6:38:37,  1.75s/it] 23%|██▎       | 4182/17834 [2:05:50<6:39:28,  1.76s/it] 23%|██▎       | 4183/17834 [2:05:51<6:41:37,  1.77s/it] 23%|██▎       | 4184/17834 [2:05:53<6:37:51,  1.75s/it] 23%|██▎       | 4185/17834 [2:05:55<6:33:01,  1.73s/it] 23%|██▎       | 4186/17834 [2:05:57<6:36:21,  1.74s/it] 23%|██▎       | 4187/17834 [2:05:58<6:40:05,  1.76s/it] 23%|██▎       | 4188/17834 [2:06:00<6:39:44,  1.76s/it] 23%|██▎       | 4189/17834 [2:06:02<6:44:55,  1.78s/it] 23%|██▎       | 4190/17834 [2:06:04<6:42:40,  1.77s/it] 24%|██▎       | 4191/17834 [2:06:06<6:43:31,  1.77s/it] 24%|██▎       | 4192/17834 [2:06:07<6:41:39,  1.77s/it] 24%|██▎       | 4193/17834 [2:06:09<6:40:45,  1.76s/it] 24%|██▎       | 4194/17834 [2:06:11<6:40:25,  1.76s/it] 24%|██▎       | 4195/17834 [2:06:13<6:37:52,  1.75s/it] 24%|██▎       | 4196/17834 [2:06:14<6:40:14,  1.76s/it] 24%|██▎       | 4197/17834 [2:06:16<6:36:47,  1.75s/it] 24%|██▎       | 4198/17834 [2:06:18<6:35:01,  1.74s/it] 24%|██▎       | 4199/17834 [2:06:20<6:37:03,  1.75s/it]08/30/2024 21:20:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0395011901855469, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.031156469136476517, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1602020263671875, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2308597564697266}
 24%|██▎       | 4200/17834 [2:06:21<6:39:48,  1.76s/it] 24%|██▎       | 4201/17834 [2:06:23<6:36:34,  1.75s/it] 24%|██▎       | 4202/17834 [2:06:25<6:34:04,  1.73s/it] 24%|██▎       | 4203/17834 [2:06:26<6:32:27,  1.73s/it] 24%|██▎       | 4204/17834 [2:06:28<6:34:27,  1.74s/it] 24%|██▎       | 4205/17834 [2:06:30<6:33:57,  1.73s/it] 24%|██▎       | 4206/17834 [2:06:32<6:32:37,  1.73s/it] 24%|██▎       | 4207/17834 [2:06:33<6:29:18,  1.71s/it] 24%|██▎       | 4208/17834 [2:06:35<6:41:53,  1.77s/it] 24%|██▎       | 4209/17834 [2:06:37<6:50:19,  1.81s/it] 24%|██▎       | 4210/17834 [2:06:39<6:44:49,  1.78s/it] 24%|██▎       | 4211/17834 [2:06:41<6:42:11,  1.77s/it] 24%|██▎       | 4212/17834 [2:06:42<6:40:02,  1.76s/it] 24%|██▎       | 4213/17834 [2:06:44<6:37:14,  1.75s/it] 24%|██▎       | 4214/17834 [2:06:46<6:38:03,  1.75s/it] 24%|██▎       | 4215/17834 [2:06:48<6:38:34,  1.76s/it] 24%|██▎       | 4216/17834 [2:06:49<6:38:03,  1.75s/it] 24%|██▎       | 4217/17834 [2:06:51<6:36:39,  1.75s/it] 24%|██▎       | 4218/17834 [2:06:53<6:39:25,  1.76s/it] 24%|██▎       | 4219/17834 [2:06:55<6:38:31,  1.76s/it] 24%|██▎       | 4220/17834 [2:06:56<6:34:00,  1.74s/it] 24%|██▎       | 4221/17834 [2:06:58<6:39:12,  1.76s/it] 24%|██▎       | 4222/17834 [2:07:00<6:42:30,  1.77s/it] 24%|██▎       | 4223/17834 [2:07:02<6:37:12,  1.75s/it] 24%|██▎       | 4224/17834 [2:07:03<6:35:42,  1.74s/it] 24%|██▎       | 4225/17834 [2:07:05<6:32:00,  1.73s/it] 24%|██▎       | 4226/17834 [2:07:07<6:35:26,  1.74s/it] 24%|██▎       | 4227/17834 [2:07:09<6:34:30,  1.74s/it] 24%|██▎       | 4228/17834 [2:07:10<6:37:21,  1.75s/it] 24%|██▎       | 4229/17834 [2:07:12<6:33:31,  1.74s/it] 24%|██▎       | 4230/17834 [2:07:14<6:36:30,  1.75s/it] 24%|██▎       | 4231/17834 [2:07:15<6:32:37,  1.73s/it] 24%|██▎       | 4232/17834 [2:07:17<6:34:20,  1.74s/it] 24%|██▎       | 4233/17834 [2:07:19<6:39:33,  1.76s/it] 24%|██▎       | 4234/17834 [2:07:21<6:43:14,  1.78s/it] 24%|██▎       | 4235/17834 [2:07:23<6:51:07,  1.81s/it] 24%|██▍       | 4236/17834 [2:07:24<6:42:01,  1.77s/it] 24%|██▍       | 4237/17834 [2:07:26<6:39:01,  1.76s/it] 24%|██▍       | 4238/17834 [2:07:28<6:46:43,  1.79s/it] 24%|██▍       | 4239/17834 [2:07:30<6:39:05,  1.76s/it] 24%|██▍       | 4240/17834 [2:07:32<6:39:33,  1.76s/it] 24%|██▍       | 4241/17834 [2:07:33<6:41:16,  1.77s/it] 24%|██▍       | 4242/17834 [2:07:35<6:37:30,  1.75s/it] 24%|██▍       | 4243/17834 [2:07:37<6:32:52,  1.73s/it] 24%|██▍       | 4244/17834 [2:07:38<6:29:26,  1.72s/it] 24%|██▍       | 4245/17834 [2:07:40<6:33:02,  1.74s/it] 24%|██▍       | 4246/17834 [2:07:42<6:28:36,  1.72s/it] 24%|██▍       | 4247/17834 [2:07:44<6:28:00,  1.71s/it] 24%|██▍       | 4248/17834 [2:07:45<6:26:38,  1.71s/it] 24%|██▍       | 4249/17834 [2:07:47<6:28:47,  1.72s/it]08/30/2024 21:22:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2266786098480225, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04093114659190178, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1999597549438477, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.467569351196289}
 24%|██▍       | 4250/17834 [2:07:49<6:31:43,  1.73s/it] 24%|██▍       | 4251/17834 [2:07:50<6:32:33,  1.73s/it] 24%|██▍       | 4252/17834 [2:07:52<6:33:56,  1.74s/it] 24%|██▍       | 4253/17834 [2:07:54<6:32:40,  1.73s/it] 24%|██▍       | 4254/17834 [2:07:56<6:28:32,  1.72s/it] 24%|██▍       | 4255/17834 [2:07:57<6:31:55,  1.73s/it] 24%|██▍       | 4256/17834 [2:07:59<6:30:46,  1.73s/it] 24%|██▍       | 4257/17834 [2:08:01<6:37:12,  1.76s/it] 24%|██▍       | 4258/17834 [2:08:03<6:35:59,  1.75s/it] 24%|██▍       | 4259/17834 [2:08:04<6:34:36,  1.74s/it] 24%|██▍       | 4260/17834 [2:08:06<6:30:15,  1.73s/it] 24%|██▍       | 4261/17834 [2:08:08<6:33:30,  1.74s/it] 24%|██▍       | 4262/17834 [2:08:10<6:33:48,  1.74s/it] 24%|██▍       | 4263/17834 [2:08:11<6:33:37,  1.74s/it] 24%|██▍       | 4264/17834 [2:08:13<6:36:01,  1.75s/it] 24%|██▍       | 4265/17834 [2:08:15<6:33:39,  1.74s/it] 24%|██▍       | 4266/17834 [2:08:17<6:34:04,  1.74s/it] 24%|██▍       | 4267/17834 [2:08:18<6:31:25,  1.73s/it] 24%|██▍       | 4268/17834 [2:08:20<6:32:43,  1.74s/it] 24%|██▍       | 4269/17834 [2:08:22<6:32:49,  1.74s/it] 24%|██▍       | 4270/17834 [2:08:24<6:40:04,  1.77s/it] 24%|██▍       | 4271/17834 [2:08:25<6:36:24,  1.75s/it] 24%|██▍       | 4272/17834 [2:08:27<6:32:10,  1.74s/it] 24%|██▍       | 4273/17834 [2:08:29<6:29:43,  1.72s/it] 24%|██▍       | 4274/17834 [2:08:31<6:33:47,  1.74s/it] 24%|██▍       | 4275/17834 [2:08:32<6:33:14,  1.74s/it] 24%|██▍       | 4276/17834 [2:08:34<6:38:23,  1.76s/it] 24%|██▍       | 4277/17834 [2:08:36<6:32:27,  1.74s/it] 24%|██▍       | 4278/17834 [2:08:38<6:37:00,  1.76s/it] 24%|██▍       | 4279/17834 [2:08:39<6:33:59,  1.74s/it] 24%|██▍       | 4280/17834 [2:08:41<6:37:09,  1.76s/it] 24%|██▍       | 4281/17834 [2:08:43<6:36:09,  1.75s/it] 24%|██▍       | 4282/17834 [2:08:45<6:36:53,  1.76s/it] 24%|██▍       | 4283/17834 [2:08:46<6:32:58,  1.74s/it] 24%|██▍       | 4284/17834 [2:08:48<6:33:07,  1.74s/it] 24%|██▍       | 4285/17834 [2:08:50<6:27:57,  1.72s/it] 24%|██▍       | 4286/17834 [2:08:51<6:30:52,  1.73s/it] 24%|██▍       | 4287/17834 [2:08:53<6:32:10,  1.74s/it] 24%|██▍       | 4288/17834 [2:08:55<6:32:01,  1.74s/it] 24%|██▍       | 4289/17834 [2:08:57<6:32:45,  1.74s/it] 24%|██▍       | 4290/17834 [2:08:59<6:39:49,  1.77s/it] 24%|██▍       | 4291/17834 [2:09:00<6:38:51,  1.77s/it] 24%|██▍       | 4292/17834 [2:09:02<6:38:34,  1.77s/it] 24%|██▍       | 4293/17834 [2:09:04<6:32:56,  1.74s/it] 24%|██▍       | 4294/17834 [2:09:05<6:32:34,  1.74s/it] 24%|██▍       | 4295/17834 [2:09:07<6:35:22,  1.75s/it] 24%|██▍       | 4296/17834 [2:09:09<6:34:02,  1.75s/it] 24%|██▍       | 4297/17834 [2:09:11<6:38:10,  1.76s/it] 24%|██▍       | 4298/17834 [2:09:13<6:37:43,  1.76s/it] 24%|██▍       | 4299/17834 [2:09:14<6:40:22,  1.77s/it]08/30/2024 21:23:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.9050883054733276, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04733051359653473, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5087928771972656, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.461211681365967}
 24%|██▍       | 4300/17834 [2:09:16<6:37:12,  1.76s/it] 24%|██▍       | 4301/17834 [2:09:18<6:31:29,  1.74s/it] 24%|██▍       | 4302/17834 [2:09:20<6:33:44,  1.75s/it] 24%|██▍       | 4303/17834 [2:09:21<6:32:06,  1.74s/it] 24%|██▍       | 4304/17834 [2:09:23<6:32:55,  1.74s/it] 24%|██▍       | 4305/17834 [2:09:25<6:35:39,  1.75s/it] 24%|██▍       | 4306/17834 [2:09:26<6:34:15,  1.75s/it] 24%|██▍       | 4307/17834 [2:09:28<6:30:16,  1.73s/it] 24%|██▍       | 4308/17834 [2:09:30<6:31:01,  1.73s/it] 24%|██▍       | 4309/17834 [2:09:32<6:35:36,  1.76s/it] 24%|██▍       | 4310/17834 [2:09:34<6:39:01,  1.77s/it] 24%|██▍       | 4311/17834 [2:09:35<6:36:30,  1.76s/it] 24%|██▍       | 4312/17834 [2:09:37<6:31:30,  1.74s/it] 24%|██▍       | 4313/17834 [2:09:39<6:32:07,  1.74s/it] 24%|██▍       | 4314/17834 [2:09:40<6:31:15,  1.74s/it] 24%|██▍       | 4315/17834 [2:09:42<6:34:50,  1.75s/it] 24%|██▍       | 4316/17834 [2:09:44<6:34:39,  1.75s/it] 24%|██▍       | 4317/17834 [2:09:46<6:29:53,  1.73s/it] 24%|██▍       | 4318/17834 [2:09:47<6:32:05,  1.74s/it] 24%|██▍       | 4319/17834 [2:09:49<6:31:34,  1.74s/it] 24%|██▍       | 4320/17834 [2:09:51<6:36:02,  1.76s/it] 24%|██▍       | 4321/17834 [2:09:53<6:37:39,  1.77s/it] 24%|██▍       | 4322/17834 [2:09:54<6:35:40,  1.76s/it] 24%|██▍       | 4323/17834 [2:09:56<6:35:59,  1.76s/it] 24%|██▍       | 4324/17834 [2:09:58<6:30:47,  1.74s/it] 24%|██▍       | 4325/17834 [2:10:00<6:34:38,  1.75s/it] 24%|██▍       | 4326/17834 [2:10:01<6:30:51,  1.74s/it] 24%|██▍       | 4327/17834 [2:10:03<6:29:41,  1.73s/it] 24%|██▍       | 4328/17834 [2:10:05<6:30:32,  1.74s/it] 24%|██▍       | 4329/17834 [2:10:07<6:28:40,  1.73s/it] 24%|██▍       | 4330/17834 [2:10:08<6:28:57,  1.73s/it] 24%|██▍       | 4331/17834 [2:10:10<6:26:44,  1.72s/it] 24%|██▍       | 4332/17834 [2:10:12<6:28:07,  1.72s/it] 24%|██▍       | 4333/17834 [2:10:14<6:30:02,  1.73s/it] 24%|██▍       | 4334/17834 [2:10:15<6:31:25,  1.74s/it] 24%|██▍       | 4335/17834 [2:10:17<6:31:21,  1.74s/it] 24%|██▍       | 4336/17834 [2:10:19<6:32:38,  1.75s/it] 24%|██▍       | 4337/17834 [2:10:20<6:27:08,  1.72s/it] 24%|██▍       | 4338/17834 [2:10:22<6:37:41,  1.77s/it] 24%|██▍       | 4339/17834 [2:10:24<6:36:44,  1.76s/it] 24%|██▍       | 4340/17834 [2:10:26<6:37:22,  1.77s/it] 24%|██▍       | 4341/17834 [2:10:28<6:33:57,  1.75s/it] 24%|██▍       | 4342/17834 [2:10:29<6:35:19,  1.76s/it] 24%|██▍       | 4343/17834 [2:10:31<6:33:42,  1.75s/it] 24%|██▍       | 4344/17834 [2:10:33<6:28:28,  1.73s/it] 24%|██▍       | 4345/17834 [2:10:34<6:26:31,  1.72s/it] 24%|██▍       | 4346/17834 [2:10:36<6:34:48,  1.76s/it] 24%|██▍       | 4347/17834 [2:10:38<6:39:22,  1.78s/it] 24%|██▍       | 4348/17834 [2:10:40<6:34:41,  1.76s/it] 24%|██▍       | 4349/17834 [2:10:42<6:35:04,  1.76s/it]08/30/2024 21:25:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3890875577926636, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045695893466472626, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.209918737411499, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.644702196121216}
 24%|██▍       | 4350/17834 [2:10:43<6:32:44,  1.75s/it] 24%|██▍       | 4351/17834 [2:10:45<6:33:38,  1.75s/it] 24%|██▍       | 4352/17834 [2:10:47<6:38:57,  1.78s/it] 24%|██▍       | 4353/17834 [2:10:49<6:34:21,  1.76s/it] 24%|██▍       | 4354/17834 [2:10:50<6:36:53,  1.77s/it] 24%|██▍       | 4355/17834 [2:10:52<6:32:39,  1.75s/it] 24%|██▍       | 4356/17834 [2:10:54<6:33:45,  1.75s/it] 24%|██▍       | 4357/17834 [2:10:56<6:42:01,  1.79s/it] 24%|██▍       | 4358/17834 [2:10:57<6:35:30,  1.76s/it] 24%|██▍       | 4359/17834 [2:10:59<6:37:04,  1.77s/it] 24%|██▍       | 4360/17834 [2:11:01<6:39:52,  1.78s/it] 24%|██▍       | 4361/17834 [2:11:03<6:36:32,  1.77s/it] 24%|██▍       | 4362/17834 [2:11:05<6:37:52,  1.77s/it] 24%|██▍       | 4363/17834 [2:11:06<6:33:58,  1.75s/it] 24%|██▍       | 4364/17834 [2:11:08<6:38:44,  1.78s/it] 24%|██▍       | 4365/17834 [2:11:10<6:34:20,  1.76s/it] 24%|██▍       | 4366/17834 [2:11:12<6:34:33,  1.76s/it] 24%|██▍       | 4367/17834 [2:11:13<6:30:00,  1.74s/it] 24%|██▍       | 4368/17834 [2:11:15<6:30:18,  1.74s/it] 24%|██▍       | 4369/17834 [2:11:17<6:33:11,  1.75s/it] 25%|██▍       | 4370/17834 [2:11:19<6:36:10,  1.77s/it] 25%|██▍       | 4371/17834 [2:11:20<6:35:43,  1.76s/it] 25%|██▍       | 4372/17834 [2:11:22<6:47:08,  1.81s/it] 25%|██▍       | 4373/17834 [2:11:24<6:43:53,  1.80s/it] 25%|██▍       | 4374/17834 [2:11:26<6:40:09,  1.78s/it] 25%|██▍       | 4375/17834 [2:11:27<6:37:05,  1.77s/it] 25%|██▍       | 4376/17834 [2:11:29<6:33:10,  1.75s/it] 25%|██▍       | 4377/17834 [2:11:31<6:31:10,  1.74s/it] 25%|██▍       | 4378/17834 [2:11:33<6:28:57,  1.73s/it] 25%|██▍       | 4379/17834 [2:11:34<6:35:46,  1.76s/it] 25%|██▍       | 4380/17834 [2:11:36<6:29:39,  1.74s/it] 25%|██▍       | 4381/17834 [2:11:38<6:29:04,  1.74s/it] 25%|██▍       | 4382/17834 [2:11:40<6:27:07,  1.73s/it] 25%|██▍       | 4383/17834 [2:11:41<6:30:24,  1.74s/it] 25%|██▍       | 4384/17834 [2:11:43<6:28:32,  1.73s/it] 25%|██▍       | 4385/17834 [2:11:45<6:26:34,  1.72s/it] 25%|██▍       | 4386/17834 [2:11:47<6:36:22,  1.77s/it] 25%|██▍       | 4387/17834 [2:11:48<6:32:43,  1.75s/it] 25%|██▍       | 4388/17834 [2:11:50<6:32:38,  1.75s/it] 25%|██▍       | 4389/17834 [2:11:52<6:32:59,  1.75s/it] 25%|██▍       | 4390/17834 [2:11:54<6:32:47,  1.75s/it] 25%|██▍       | 4391/17834 [2:11:55<6:34:54,  1.76s/it] 25%|██▍       | 4392/17834 [2:11:57<6:35:53,  1.77s/it] 25%|██▍       | 4393/17834 [2:11:59<6:30:21,  1.74s/it] 25%|██▍       | 4394/17834 [2:12:01<6:31:49,  1.75s/it] 25%|██▍       | 4395/17834 [2:12:02<6:29:27,  1.74s/it] 25%|██▍       | 4396/17834 [2:12:04<6:30:28,  1.74s/it] 25%|██▍       | 4397/17834 [2:12:06<6:32:04,  1.75s/it] 25%|██▍       | 4398/17834 [2:12:08<6:31:45,  1.75s/it] 25%|██▍       | 4399/17834 [2:12:09<6:31:22,  1.75s/it]08/30/2024 21:26:29 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6063764095306396, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.039677128195762634, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.324291467666626, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9703450202941895}
 25%|██▍       | 4400/17834 [2:12:11<6:30:17,  1.74s/it] 25%|██▍       | 4401/17834 [2:12:13<6:29:13,  1.74s/it] 25%|██▍       | 4402/17834 [2:12:15<6:32:50,  1.75s/it] 25%|██▍       | 4403/17834 [2:12:16<6:34:49,  1.76s/it] 25%|██▍       | 4404/17834 [2:12:18<6:33:31,  1.76s/it] 25%|██▍       | 4405/17834 [2:12:20<6:35:11,  1.77s/it] 25%|██▍       | 4406/17834 [2:12:22<6:33:55,  1.76s/it] 25%|██▍       | 4407/17834 [2:12:23<6:28:11,  1.73s/it] 25%|██▍       | 4408/17834 [2:12:25<6:34:57,  1.77s/it] 25%|██▍       | 4409/17834 [2:12:27<6:28:52,  1.74s/it] 25%|██▍       | 4410/17834 [2:12:29<6:29:19,  1.74s/it] 25%|██▍       | 4411/17834 [2:12:30<6:29:54,  1.74s/it] 25%|██▍       | 4412/17834 [2:12:32<6:30:15,  1.74s/it] 25%|██▍       | 4413/17834 [2:12:34<6:27:07,  1.73s/it] 25%|██▍       | 4414/17834 [2:12:36<6:29:40,  1.74s/it] 25%|██▍       | 4415/17834 [2:12:37<6:33:27,  1.76s/it] 25%|██▍       | 4416/17834 [2:12:39<6:31:17,  1.75s/it] 25%|██▍       | 4417/17834 [2:12:41<6:38:04,  1.78s/it] 25%|██▍       | 4418/17834 [2:12:43<6:34:38,  1.76s/it] 25%|██▍       | 4419/17834 [2:12:44<6:33:44,  1.76s/it] 25%|██▍       | 4420/17834 [2:12:46<6:31:59,  1.75s/it] 25%|██▍       | 4421/17834 [2:12:48<6:29:23,  1.74s/it] 25%|██▍       | 4422/17834 [2:12:50<6:31:08,  1.75s/it] 25%|██▍       | 4423/17834 [2:12:51<6:29:43,  1.74s/it] 25%|██▍       | 4424/17834 [2:12:53<6:26:21,  1.73s/it] 25%|██▍       | 4425/17834 [2:12:55<6:29:41,  1.74s/it] 25%|██▍       | 4426/17834 [2:12:57<6:30:31,  1.75s/it] 25%|██▍       | 4427/17834 [2:12:58<6:37:52,  1.78s/it] 25%|██▍       | 4428/17834 [2:13:00<6:35:13,  1.77s/it] 25%|██▍       | 4429/17834 [2:13:02<6:40:07,  1.79s/it] 25%|██▍       | 4430/17834 [2:13:04<6:40:18,  1.79s/it] 25%|██▍       | 4431/17834 [2:13:06<6:38:56,  1.79s/it] 25%|██▍       | 4432/17834 [2:13:07<6:34:26,  1.77s/it] 25%|██▍       | 4433/17834 [2:13:09<6:32:39,  1.76s/it] 25%|██▍       | 4434/17834 [2:13:11<6:32:16,  1.76s/it] 25%|██▍       | 4435/17834 [2:13:13<6:42:35,  1.80s/it] 25%|██▍       | 4436/17834 [2:13:15<6:42:58,  1.80s/it] 25%|██▍       | 4437/17834 [2:13:16<6:36:26,  1.78s/it] 25%|██▍       | 4438/17834 [2:13:18<6:30:41,  1.75s/it] 25%|██▍       | 4439/17834 [2:13:20<6:31:29,  1.75s/it] 25%|██▍       | 4440/17834 [2:13:21<6:31:46,  1.76s/it] 25%|██▍       | 4441/17834 [2:13:23<6:33:19,  1.76s/it] 25%|██▍       | 4442/17834 [2:13:25<6:31:29,  1.75s/it] 25%|██▍       | 4443/17834 [2:13:27<6:27:05,  1.73s/it] 25%|██▍       | 4444/17834 [2:13:28<6:29:23,  1.74s/it] 25%|██▍       | 4445/17834 [2:13:30<6:26:26,  1.73s/it] 25%|██▍       | 4446/17834 [2:13:32<6:26:13,  1.73s/it] 25%|██▍       | 4447/17834 [2:13:34<6:29:08,  1.74s/it] 25%|██▍       | 4448/17834 [2:13:35<6:28:40,  1.74s/it] 25%|██▍       | 4449/17834 [2:13:37<6:30:37,  1.75s/it]08/30/2024 21:27:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6615022420883179, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04460769519209862, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4600815773010254, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.166191577911377}
 25%|██▍       | 4450/17834 [2:13:39<6:36:54,  1.78s/it] 25%|██▍       | 4451/17834 [2:13:41<6:34:28,  1.77s/it] 25%|██▍       | 4452/17834 [2:13:42<6:29:24,  1.75s/it] 25%|██▍       | 4453/17834 [2:13:44<6:30:25,  1.75s/it] 25%|██▍       | 4454/17834 [2:13:46<6:30:44,  1.75s/it] 25%|██▍       | 4455/17834 [2:13:48<6:30:52,  1.75s/it] 25%|██▍       | 4456/17834 [2:13:50<6:34:27,  1.77s/it] 25%|██▍       | 4457/17834 [2:13:51<6:31:51,  1.76s/it] 25%|██▍       | 4458/17834 [2:13:53<6:26:37,  1.73s/it] 25%|██▌       | 4459/17834 [2:13:55<6:27:28,  1.74s/it] 25%|██▌       | 4460/17834 [2:13:56<6:28:52,  1.74s/it] 25%|██▌       | 4461/17834 [2:13:58<6:29:39,  1.75s/it] 25%|██▌       | 4462/17834 [2:14:00<6:30:49,  1.75s/it] 25%|██▌       | 4463/17834 [2:14:02<6:34:34,  1.77s/it] 25%|██▌       | 4464/17834 [2:14:03<6:31:25,  1.76s/it] 25%|██▌       | 4465/17834 [2:14:05<6:33:12,  1.76s/it] 25%|██▌       | 4466/17834 [2:14:07<6:26:25,  1.73s/it] 25%|██▌       | 4467/17834 [2:14:09<6:29:27,  1.75s/it] 25%|██▌       | 4468/17834 [2:14:11<6:32:59,  1.76s/it] 25%|██▌       | 4469/17834 [2:14:12<6:32:22,  1.76s/it] 25%|██▌       | 4470/17834 [2:14:14<6:30:25,  1.75s/it] 25%|██▌       | 4471/17834 [2:14:16<6:30:44,  1.75s/it] 25%|██▌       | 4472/17834 [2:14:18<6:31:15,  1.76s/it] 25%|██▌       | 4473/17834 [2:14:19<6:26:40,  1.74s/it] 25%|██▌       | 4474/17834 [2:14:21<6:29:39,  1.75s/it] 25%|██▌       | 4475/17834 [2:14:23<6:27:10,  1.74s/it] 25%|██▌       | 4476/17834 [2:14:24<6:27:25,  1.74s/it] 25%|██▌       | 4477/17834 [2:14:26<6:29:09,  1.75s/it] 25%|██▌       | 4478/17834 [2:14:28<6:26:48,  1.74s/it] 25%|██▌       | 4479/17834 [2:14:30<6:28:03,  1.74s/it] 25%|██▌       | 4480/17834 [2:14:31<6:26:56,  1.74s/it] 25%|██▌       | 4481/17834 [2:14:33<6:24:56,  1.73s/it] 25%|██▌       | 4482/17834 [2:14:35<6:26:03,  1.73s/it] 25%|██▌       | 4483/17834 [2:14:37<6:24:27,  1.73s/it] 25%|██▌       | 4484/17834 [2:14:38<6:25:19,  1.73s/it] 25%|██▌       | 4485/17834 [2:14:40<6:26:24,  1.74s/it] 25%|██▌       | 4486/17834 [2:14:42<6:28:21,  1.75s/it] 25%|██▌       | 4487/17834 [2:14:44<6:32:46,  1.77s/it] 25%|██▌       | 4488/17834 [2:14:45<6:30:16,  1.75s/it] 25%|██▌       | 4489/17834 [2:14:47<6:30:12,  1.75s/it] 25%|██▌       | 4490/17834 [2:14:49<6:31:31,  1.76s/it] 25%|██▌       | 4491/17834 [2:14:51<6:32:23,  1.76s/it] 25%|██▌       | 4492/17834 [2:14:52<6:30:54,  1.76s/it] 25%|██▌       | 4493/17834 [2:14:54<6:29:51,  1.75s/it] 25%|██▌       | 4494/17834 [2:14:56<6:26:34,  1.74s/it] 25%|██▌       | 4495/17834 [2:14:58<6:27:38,  1.74s/it] 25%|██▌       | 4496/17834 [2:14:59<6:28:08,  1.75s/it] 25%|██▌       | 4497/17834 [2:15:01<6:27:13,  1.74s/it] 25%|██▌       | 4498/17834 [2:15:03<6:28:29,  1.75s/it] 25%|██▌       | 4499/17834 [2:15:05<6:31:43,  1.76s/it]08/30/2024 21:29:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2298319339752197, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03660354018211365, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.152780771255493, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4192161560058594}
 25%|██▌       | 4500/17834 [2:15:06<6:33:24,  1.77s/it] 25%|██▌       | 4501/17834 [2:15:08<6:30:22,  1.76s/it] 25%|██▌       | 4502/17834 [2:15:10<6:32:12,  1.77s/it] 25%|██▌       | 4503/17834 [2:15:12<6:24:49,  1.73s/it] 25%|██▌       | 4504/17834 [2:15:13<6:29:51,  1.75s/it] 25%|██▌       | 4505/17834 [2:15:15<6:33:22,  1.77s/it] 25%|██▌       | 4506/17834 [2:15:17<6:28:41,  1.75s/it] 25%|██▌       | 4507/17834 [2:15:19<6:31:14,  1.76s/it] 25%|██▌       | 4508/17834 [2:15:21<6:36:50,  1.79s/it] 25%|██▌       | 4509/17834 [2:15:22<6:35:26,  1.78s/it] 25%|██▌       | 4510/17834 [2:15:24<6:35:13,  1.78s/it] 25%|██▌       | 4511/17834 [2:15:26<6:29:30,  1.75s/it] 25%|██▌       | 4512/17834 [2:15:28<6:30:29,  1.76s/it] 25%|██▌       | 4513/17834 [2:15:29<6:28:16,  1.75s/it] 25%|██▌       | 4514/17834 [2:15:31<6:33:42,  1.77s/it] 25%|██▌       | 4515/17834 [2:15:33<6:31:35,  1.76s/it] 25%|██▌       | 4516/17834 [2:15:35<6:28:29,  1.75s/it] 25%|██▌       | 4517/17834 [2:15:36<6:33:25,  1.77s/it] 25%|██▌       | 4518/17834 [2:15:38<6:31:46,  1.77s/it] 25%|██▌       | 4519/17834 [2:15:40<6:31:44,  1.77s/it] 25%|██▌       | 4520/17834 [2:15:42<6:30:23,  1.76s/it] 25%|██▌       | 4521/17834 [2:15:43<6:25:23,  1.74s/it] 25%|██▌       | 4522/17834 [2:15:45<6:29:28,  1.76s/it] 25%|██▌       | 4523/17834 [2:15:47<6:25:33,  1.74s/it] 25%|██▌       | 4524/17834 [2:15:49<6:32:18,  1.77s/it] 25%|██▌       | 4525/17834 [2:15:50<6:31:38,  1.77s/it] 25%|██▌       | 4526/17834 [2:15:52<6:27:52,  1.75s/it] 25%|██▌       | 4527/17834 [2:15:54<6:27:10,  1.75s/it] 25%|██▌       | 4528/17834 [2:15:56<6:28:41,  1.75s/it] 25%|██▌       | 4529/17834 [2:15:57<6:25:32,  1.74s/it] 25%|██▌       | 4530/17834 [2:15:59<6:27:09,  1.75s/it] 25%|██▌       | 4531/17834 [2:16:01<6:25:55,  1.74s/it] 25%|██▌       | 4532/17834 [2:16:03<6:30:44,  1.76s/it] 25%|██▌       | 4533/17834 [2:16:05<6:34:17,  1.78s/it] 25%|██▌       | 4534/17834 [2:16:06<6:29:53,  1.76s/it] 25%|██▌       | 4535/17834 [2:16:08<6:28:08,  1.75s/it] 25%|██▌       | 4536/17834 [2:16:10<6:25:09,  1.74s/it] 25%|██▌       | 4537/17834 [2:16:11<6:22:55,  1.73s/it] 25%|██▌       | 4538/17834 [2:16:13<6:26:43,  1.75s/it] 25%|██▌       | 4539/17834 [2:16:15<6:30:09,  1.76s/it] 25%|██▌       | 4540/17834 [2:16:17<6:28:56,  1.76s/it] 25%|██▌       | 4541/17834 [2:16:18<6:28:49,  1.76s/it] 25%|██▌       | 4542/17834 [2:16:20<6:27:27,  1.75s/it] 25%|██▌       | 4543/17834 [2:16:22<6:30:04,  1.76s/it] 25%|██▌       | 4544/17834 [2:16:24<6:24:55,  1.74s/it] 25%|██▌       | 4545/17834 [2:16:25<6:25:30,  1.74s/it] 25%|██▌       | 4546/17834 [2:16:27<6:24:17,  1.74s/it] 25%|██▌       | 4547/17834 [2:16:29<6:22:12,  1.73s/it] 26%|██▌       | 4548/17834 [2:16:31<6:28:20,  1.75s/it] 26%|██▌       | 4549/17834 [2:16:32<6:23:07,  1.73s/it]08/30/2024 21:30:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2091050148010254, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03552880138158798, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1661109924316406, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4107446670532227}
 26%|██▌       | 4550/17834 [2:16:34<6:32:35,  1.77s/it] 26%|██▌       | 4551/17834 [2:16:36<6:28:20,  1.75s/it] 26%|██▌       | 4552/17834 [2:16:38<6:32:55,  1.78s/it] 26%|██▌       | 4553/17834 [2:16:39<6:32:22,  1.77s/it] 26%|██▌       | 4554/17834 [2:16:41<6:25:47,  1.74s/it] 26%|██▌       | 4555/17834 [2:16:43<6:22:44,  1.73s/it] 26%|██▌       | 4556/17834 [2:16:45<6:26:10,  1.75s/it] 26%|██▌       | 4557/17834 [2:16:46<6:24:22,  1.74s/it] 26%|██▌       | 4558/17834 [2:16:48<6:19:05,  1.71s/it] 26%|██▌       | 4559/17834 [2:16:50<6:22:27,  1.73s/it] 26%|██▌       | 4560/17834 [2:16:52<6:22:02,  1.73s/it] 26%|██▌       | 4561/17834 [2:16:53<6:19:50,  1.72s/it] 26%|██▌       | 4562/17834 [2:16:55<6:20:02,  1.72s/it] 26%|██▌       | 4563/17834 [2:16:57<6:21:36,  1.73s/it] 26%|██▌       | 4564/17834 [2:16:58<6:21:33,  1.73s/it] 26%|██▌       | 4565/17834 [2:17:00<6:25:44,  1.74s/it] 26%|██▌       | 4566/17834 [2:17:02<6:25:36,  1.74s/it] 26%|██▌       | 4567/17834 [2:17:04<6:22:22,  1.73s/it] 26%|██▌       | 4568/17834 [2:17:05<6:27:29,  1.75s/it] 26%|██▌       | 4569/17834 [2:17:07<6:24:55,  1.74s/it] 26%|██▌       | 4570/17834 [2:17:09<6:25:52,  1.75s/it] 26%|██▌       | 4571/17834 [2:17:11<6:24:36,  1.74s/it] 26%|██▌       | 4572/17834 [2:17:12<6:28:07,  1.76s/it] 26%|██▌       | 4573/17834 [2:17:14<6:23:27,  1.73s/it] 26%|██▌       | 4574/17834 [2:17:16<6:21:28,  1.73s/it] 26%|██▌       | 4575/17834 [2:17:18<6:23:54,  1.74s/it] 26%|██▌       | 4576/17834 [2:17:19<6:28:55,  1.76s/it] 26%|██▌       | 4577/17834 [2:17:21<6:30:45,  1.77s/it] 26%|██▌       | 4578/17834 [2:17:23<6:26:47,  1.75s/it] 26%|██▌       | 4579/17834 [2:17:25<6:23:30,  1.74s/it] 26%|██▌       | 4580/17834 [2:17:26<6:27:10,  1.75s/it] 26%|██▌       | 4581/17834 [2:17:28<6:26:53,  1.75s/it] 26%|██▌       | 4582/17834 [2:17:30<6:27:02,  1.75s/it] 26%|██▌       | 4583/17834 [2:17:32<6:26:19,  1.75s/it] 26%|██▌       | 4584/17834 [2:17:33<6:23:07,  1.73s/it] 26%|██▌       | 4585/17834 [2:17:35<6:21:48,  1.73s/it] 26%|██▌       | 4586/17834 [2:17:37<6:26:59,  1.75s/it] 26%|██▌       | 4587/17834 [2:17:39<6:27:57,  1.76s/it] 26%|██▌       | 4588/17834 [2:17:40<6:30:20,  1.77s/it] 26%|██▌       | 4589/17834 [2:17:42<6:27:32,  1.76s/it] 26%|██▌       | 4590/17834 [2:17:44<6:30:44,  1.77s/it] 26%|██▌       | 4591/17834 [2:17:46<6:28:08,  1.76s/it] 26%|██▌       | 4592/17834 [2:17:47<6:23:18,  1.74s/it] 26%|██▌       | 4593/17834 [2:17:49<6:26:42,  1.75s/it] 26%|██▌       | 4594/17834 [2:17:51<6:25:06,  1.75s/it] 26%|██▌       | 4595/17834 [2:17:53<6:22:48,  1.73s/it] 26%|██▌       | 4596/17834 [2:17:54<6:25:45,  1.75s/it] 26%|██▌       | 4597/17834 [2:17:56<6:28:00,  1.76s/it] 26%|██▌       | 4598/17834 [2:17:58<6:26:41,  1.75s/it] 26%|██▌       | 4599/17834 [2:18:00<6:25:38,  1.75s/it]08/30/2024 21:32:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6079933643341064, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0489024817943573, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2719082832336426, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9288041591644287}
 26%|██▌       | 4600/17834 [2:18:01<6:26:29,  1.75s/it] 26%|██▌       | 4601/17834 [2:18:03<6:32:55,  1.78s/it] 26%|██▌       | 4602/17834 [2:18:05<6:30:38,  1.77s/it] 26%|██▌       | 4603/17834 [2:18:07<6:28:36,  1.76s/it] 26%|██▌       | 4604/17834 [2:18:09<6:31:22,  1.77s/it] 26%|██▌       | 4605/17834 [2:18:10<6:26:48,  1.75s/it] 26%|██▌       | 4606/17834 [2:18:12<6:26:46,  1.75s/it] 26%|██▌       | 4607/17834 [2:18:14<6:26:31,  1.75s/it] 26%|██▌       | 4608/17834 [2:18:15<6:26:35,  1.75s/it] 26%|██▌       | 4609/17834 [2:18:17<6:27:06,  1.76s/it] 26%|██▌       | 4610/17834 [2:18:19<6:24:48,  1.75s/it] 26%|██▌       | 4611/17834 [2:18:21<6:26:33,  1.75s/it] 26%|██▌       | 4612/17834 [2:18:22<6:24:06,  1.74s/it] 26%|██▌       | 4613/17834 [2:18:24<6:22:23,  1.74s/it] 26%|██▌       | 4614/17834 [2:18:26<6:23:48,  1.74s/it] 26%|██▌       | 4615/17834 [2:18:28<6:25:36,  1.75s/it] 26%|██▌       | 4616/17834 [2:18:30<6:29:13,  1.77s/it] 26%|██▌       | 4617/17834 [2:18:31<6:28:53,  1.77s/it] 26%|██▌       | 4618/17834 [2:18:33<6:25:25,  1.75s/it] 26%|██▌       | 4619/17834 [2:18:35<6:21:19,  1.73s/it] 26%|██▌       | 4620/17834 [2:18:36<6:23:01,  1.74s/it] 26%|██▌       | 4621/17834 [2:18:38<6:20:26,  1.73s/it] 26%|██▌       | 4622/17834 [2:18:40<6:21:47,  1.73s/it] 26%|██▌       | 4623/17834 [2:18:42<6:26:33,  1.76s/it] 26%|██▌       | 4624/17834 [2:18:44<6:30:34,  1.77s/it] 26%|██▌       | 4625/17834 [2:18:45<6:29:08,  1.77s/it] 26%|██▌       | 4626/17834 [2:18:47<6:28:15,  1.76s/it] 26%|██▌       | 4627/17834 [2:18:49<6:27:57,  1.76s/it] 26%|██▌       | 4628/17834 [2:18:51<6:28:54,  1.77s/it] 26%|██▌       | 4629/17834 [2:18:52<6:26:59,  1.76s/it] 26%|██▌       | 4630/17834 [2:18:54<6:26:11,  1.75s/it] 26%|██▌       | 4631/17834 [2:18:56<6:25:51,  1.75s/it] 26%|██▌       | 4632/17834 [2:18:58<6:28:57,  1.77s/it] 26%|██▌       | 4633/17834 [2:18:59<6:26:37,  1.76s/it] 26%|██▌       | 4634/17834 [2:19:01<6:22:22,  1.74s/it] 26%|██▌       | 4635/17834 [2:19:03<6:24:14,  1.75s/it] 26%|██▌       | 4636/17834 [2:19:04<6:21:02,  1.73s/it] 26%|██▌       | 4637/17834 [2:19:06<6:20:54,  1.73s/it] 26%|██▌       | 4638/17834 [2:19:08<6:25:37,  1.75s/it] 26%|██▌       | 4639/17834 [2:19:10<6:29:59,  1.77s/it] 26%|██▌       | 4640/17834 [2:19:12<6:24:40,  1.75s/it] 26%|██▌       | 4641/17834 [2:19:13<6:23:16,  1.74s/it] 26%|██▌       | 4642/17834 [2:19:15<6:20:51,  1.73s/it] 26%|██▌       | 4643/17834 [2:19:17<6:21:57,  1.74s/it] 26%|██▌       | 4644/17834 [2:19:18<6:23:29,  1.74s/it] 26%|██▌       | 4645/17834 [2:19:20<6:20:33,  1.73s/it] 26%|██▌       | 4646/17834 [2:19:22<6:24:02,  1.75s/it] 26%|██▌       | 4647/17834 [2:19:24<6:25:51,  1.76s/it] 26%|██▌       | 4648/17834 [2:19:25<6:23:33,  1.75s/it] 26%|██▌       | 4649/17834 [2:19:27<6:24:16,  1.75s/it]08/30/2024 21:33:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.421783685684204, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.036466337740421295, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.300577163696289, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7588272094726562}
 26%|██▌       | 4650/17834 [2:19:29<6:24:18,  1.75s/it] 26%|██▌       | 4651/17834 [2:19:31<6:24:21,  1.75s/it] 26%|██▌       | 4652/17834 [2:19:32<6:23:16,  1.74s/it] 26%|██▌       | 4653/17834 [2:19:34<6:27:07,  1.76s/it] 26%|██▌       | 4654/17834 [2:19:36<6:31:22,  1.78s/it] 26%|██▌       | 4655/17834 [2:19:38<6:29:45,  1.77s/it] 26%|██▌       | 4656/17834 [2:19:40<6:28:32,  1.77s/it] 26%|██▌       | 4657/17834 [2:19:41<6:30:04,  1.78s/it] 26%|██▌       | 4658/17834 [2:19:43<6:27:27,  1.76s/it] 26%|██▌       | 4659/17834 [2:19:45<6:26:04,  1.76s/it] 26%|██▌       | 4660/17834 [2:19:47<6:22:52,  1.74s/it] 26%|██▌       | 4661/17834 [2:19:48<6:25:21,  1.76s/it] 26%|██▌       | 4662/17834 [2:19:50<6:26:56,  1.76s/it] 26%|██▌       | 4663/17834 [2:19:52<6:22:43,  1.74s/it] 26%|██▌       | 4664/17834 [2:19:54<6:27:48,  1.77s/it] 26%|██▌       | 4665/17834 [2:19:55<6:27:15,  1.76s/it] 26%|██▌       | 4666/17834 [2:19:57<6:25:20,  1.76s/it] 26%|██▌       | 4667/17834 [2:19:59<6:28:04,  1.77s/it] 26%|██▌       | 4668/17834 [2:20:01<6:26:05,  1.76s/it] 26%|██▌       | 4669/17834 [2:20:02<6:22:12,  1.74s/it] 26%|██▌       | 4670/17834 [2:20:04<6:29:25,  1.77s/it] 26%|██▌       | 4671/17834 [2:20:06<6:22:50,  1.75s/it] 26%|██▌       | 4672/17834 [2:20:08<6:23:58,  1.75s/it] 26%|██▌       | 4673/17834 [2:20:09<6:25:55,  1.76s/it] 26%|██▌       | 4674/17834 [2:20:11<6:23:30,  1.75s/it] 26%|██▌       | 4675/17834 [2:20:13<6:28:56,  1.77s/it] 26%|██▌       | 4676/17834 [2:20:15<6:29:23,  1.78s/it] 26%|██▌       | 4677/17834 [2:20:17<6:27:48,  1.77s/it] 26%|██▌       | 4678/17834 [2:20:18<6:23:45,  1.75s/it] 26%|██▌       | 4679/17834 [2:20:20<6:25:07,  1.76s/it] 26%|██▌       | 4680/17834 [2:20:22<6:24:49,  1.76s/it] 26%|██▌       | 4681/17834 [2:20:24<6:26:36,  1.76s/it] 26%|██▋       | 4682/17834 [2:20:25<6:32:00,  1.79s/it] 26%|██▋       | 4683/17834 [2:20:27<6:29:31,  1.78s/it] 26%|██▋       | 4684/17834 [2:20:29<6:25:01,  1.76s/it] 26%|██▋       | 4685/17834 [2:20:31<6:25:56,  1.76s/it] 26%|██▋       | 4686/17834 [2:20:32<6:29:03,  1.78s/it] 26%|██▋       | 4687/17834 [2:20:34<6:23:32,  1.75s/it] 26%|██▋       | 4688/17834 [2:20:36<6:21:59,  1.74s/it] 26%|██▋       | 4689/17834 [2:20:38<6:36:04,  1.81s/it] 26%|██▋       | 4690/17834 [2:20:40<6:29:34,  1.78s/it] 26%|██▋       | 4691/17834 [2:20:41<6:25:38,  1.76s/it] 26%|██▋       | 4692/17834 [2:20:43<6:26:52,  1.77s/it] 26%|██▋       | 4693/17834 [2:20:45<6:27:04,  1.77s/it] 26%|██▋       | 4694/17834 [2:20:46<6:20:40,  1.74s/it] 26%|██▋       | 4695/17834 [2:20:48<6:18:59,  1.73s/it] 26%|██▋       | 4696/17834 [2:20:50<6:19:40,  1.73s/it] 26%|██▋       | 4697/17834 [2:20:52<6:24:25,  1.76s/it] 26%|██▋       | 4698/17834 [2:20:53<6:21:28,  1.74s/it] 26%|██▋       | 4699/17834 [2:20:55<6:24:15,  1.76s/it]08/30/2024 21:35:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.424041986465454, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04399851709604263, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.321556568145752, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7895970344543457}
 26%|██▋       | 4700/17834 [2:20:57<6:20:35,  1.74s/it] 26%|██▋       | 4701/17834 [2:20:59<6:24:30,  1.76s/it] 26%|██▋       | 4702/17834 [2:21:00<6:23:39,  1.75s/it] 26%|██▋       | 4703/17834 [2:21:02<6:23:40,  1.75s/it] 26%|██▋       | 4704/17834 [2:21:04<6:18:01,  1.73s/it] 26%|██▋       | 4705/17834 [2:21:06<6:24:04,  1.76s/it] 26%|██▋       | 4706/17834 [2:21:07<6:20:48,  1.74s/it] 26%|██▋       | 4707/17834 [2:21:09<6:18:36,  1.73s/it] 26%|██▋       | 4708/17834 [2:21:11<6:21:31,  1.74s/it] 26%|██▋       | 4709/17834 [2:21:13<6:20:59,  1.74s/it] 26%|██▋       | 4710/17834 [2:21:14<6:27:28,  1.77s/it] 26%|██▋       | 4711/17834 [2:21:16<6:25:20,  1.76s/it] 26%|██▋       | 4712/17834 [2:21:18<6:23:29,  1.75s/it] 26%|██▋       | 4713/17834 [2:21:20<6:23:56,  1.76s/it] 26%|██▋       | 4714/17834 [2:21:21<6:21:17,  1.74s/it] 26%|██▋       | 4715/17834 [2:21:23<6:23:53,  1.76s/it] 26%|██▋       | 4716/17834 [2:21:25<6:22:48,  1.75s/it] 26%|██▋       | 4717/17834 [2:21:27<6:23:01,  1.75s/it] 26%|██▋       | 4718/17834 [2:21:28<6:18:47,  1.73s/it] 26%|██▋       | 4719/17834 [2:21:30<6:16:12,  1.72s/it] 26%|██▋       | 4720/17834 [2:21:32<6:13:24,  1.71s/it] 26%|██▋       | 4721/17834 [2:21:34<6:17:18,  1.73s/it] 26%|██▋       | 4722/17834 [2:21:35<6:20:34,  1.74s/it] 26%|██▋       | 4723/17834 [2:21:37<6:24:44,  1.76s/it] 26%|██▋       | 4724/17834 [2:21:39<6:22:19,  1.75s/it] 26%|██▋       | 4725/17834 [2:21:41<6:21:39,  1.75s/it] 26%|██▋       | 4726/17834 [2:21:42<6:23:24,  1.75s/it] 27%|██▋       | 4727/17834 [2:21:44<6:30:57,  1.79s/it] 27%|██▋       | 4728/17834 [2:21:46<6:28:17,  1.78s/it] 27%|██▋       | 4729/17834 [2:21:48<6:25:32,  1.77s/it] 27%|██▋       | 4730/17834 [2:21:49<6:23:56,  1.76s/it] 27%|██▋       | 4731/17834 [2:21:51<6:25:38,  1.77s/it] 27%|██▋       | 4732/17834 [2:21:53<6:27:10,  1.77s/it] 27%|██▋       | 4733/17834 [2:21:55<6:22:25,  1.75s/it] 27%|██▋       | 4734/17834 [2:21:56<6:22:39,  1.75s/it] 27%|██▋       | 4735/17834 [2:21:58<6:23:42,  1.76s/it] 27%|██▋       | 4736/17834 [2:22:00<6:20:47,  1.74s/it] 27%|██▋       | 4737/17834 [2:22:02<6:18:30,  1.73s/it] 27%|██▋       | 4738/17834 [2:22:03<6:19:04,  1.74s/it] 27%|██▋       | 4739/17834 [2:22:05<6:20:05,  1.74s/it] 27%|██▋       | 4740/17834 [2:22:07<6:16:11,  1.72s/it] 27%|██▋       | 4741/17834 [2:22:09<6:17:59,  1.73s/it] 27%|██▋       | 4742/17834 [2:22:10<6:14:14,  1.72s/it] 27%|██▋       | 4743/17834 [2:22:12<6:15:04,  1.72s/it] 27%|██▋       | 4744/17834 [2:22:14<6:16:58,  1.73s/it] 27%|██▋       | 4745/17834 [2:22:16<6:19:43,  1.74s/it] 27%|██▋       | 4746/17834 [2:22:17<6:16:19,  1.73s/it] 27%|██▋       | 4747/17834 [2:22:19<6:15:01,  1.72s/it] 27%|██▋       | 4748/17834 [2:22:21<6:21:07,  1.75s/it] 27%|██▋       | 4749/17834 [2:22:23<6:22:08,  1.75s/it]08/30/2024 21:36:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3779242038726807, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.049022652208805084, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3259525299072266, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.752899408340454}
 27%|██▋       | 4750/17834 [2:22:24<6:21:05,  1.75s/it] 27%|██▋       | 4751/17834 [2:22:26<6:20:39,  1.75s/it] 27%|██▋       | 4752/17834 [2:22:28<6:24:20,  1.76s/it] 27%|██▋       | 4753/17834 [2:22:29<6:18:35,  1.74s/it] 27%|██▋       | 4754/17834 [2:22:31<6:16:00,  1.72s/it] 27%|██▋       | 4755/17834 [2:22:33<6:19:24,  1.74s/it] 27%|██▋       | 4756/17834 [2:22:35<6:31:25,  1.80s/it] 27%|██▋       | 4757/17834 [2:22:37<6:21:49,  1.75s/it] 27%|██▋       | 4758/17834 [2:22:38<6:20:24,  1.75s/it] 27%|██▋       | 4759/17834 [2:22:40<6:18:40,  1.74s/it] 27%|██▋       | 4760/17834 [2:22:42<6:19:24,  1.74s/it] 27%|██▋       | 4761/17834 [2:22:43<6:20:27,  1.75s/it] 27%|██▋       | 4762/17834 [2:22:45<6:21:09,  1.75s/it] 27%|██▋       | 4763/17834 [2:22:47<6:22:18,  1.75s/it] 27%|██▋       | 4764/17834 [2:22:49<6:26:51,  1.78s/it] 27%|██▋       | 4765/17834 [2:22:51<6:26:31,  1.77s/it] 27%|██▋       | 4766/17834 [2:22:52<6:22:04,  1.75s/it] 27%|██▋       | 4767/17834 [2:22:54<6:20:30,  1.75s/it] 27%|██▋       | 4768/17834 [2:22:56<6:26:49,  1.78s/it] 27%|██▋       | 4769/17834 [2:22:58<6:19:46,  1.74s/it] 27%|██▋       | 4770/17834 [2:22:59<6:22:40,  1.76s/it] 27%|██▋       | 4771/17834 [2:23:01<6:17:00,  1.73s/it] 27%|██▋       | 4772/17834 [2:23:03<6:27:26,  1.78s/it] 27%|██▋       | 4773/17834 [2:23:05<6:18:13,  1.74s/it] 27%|██▋       | 4774/17834 [2:23:06<6:25:02,  1.77s/it] 27%|██▋       | 4775/17834 [2:23:08<6:24:47,  1.77s/it] 27%|██▋       | 4776/17834 [2:23:10<6:23:36,  1.76s/it] 27%|██▋       | 4777/17834 [2:23:12<6:31:55,  1.80s/it] 27%|██▋       | 4778/17834 [2:23:14<6:30:07,  1.79s/it] 27%|██▋       | 4779/17834 [2:23:15<6:30:49,  1.80s/it] 27%|██▋       | 4780/17834 [2:23:17<6:25:06,  1.77s/it] 27%|██▋       | 4781/17834 [2:23:19<6:19:27,  1.74s/it] 27%|██▋       | 4782/17834 [2:23:21<6:20:14,  1.75s/it] 27%|██▋       | 4783/17834 [2:23:22<6:17:40,  1.74s/it] 27%|██▋       | 4784/17834 [2:23:24<6:17:01,  1.73s/it] 27%|██▋       | 4785/17834 [2:23:26<6:17:28,  1.74s/it] 27%|██▋       | 4786/17834 [2:23:27<6:17:55,  1.74s/it] 27%|██▋       | 4787/17834 [2:23:29<6:16:34,  1.73s/it] 27%|██▋       | 4788/17834 [2:23:31<6:20:01,  1.75s/it] 27%|██▋       | 4789/17834 [2:23:33<6:19:51,  1.75s/it] 27%|██▋       | 4790/17834 [2:23:34<6:19:35,  1.75s/it] 27%|██▋       | 4791/17834 [2:23:36<6:18:02,  1.74s/it] 27%|██▋       | 4792/17834 [2:23:38<6:16:57,  1.73s/it] 27%|██▋       | 4793/17834 [2:23:40<6:13:14,  1.72s/it] 27%|██▋       | 4794/17834 [2:23:41<6:21:42,  1.76s/it] 27%|██▋       | 4795/17834 [2:23:43<6:19:22,  1.75s/it] 27%|██▋       | 4796/17834 [2:23:45<6:21:04,  1.75s/it] 27%|██▋       | 4797/17834 [2:23:47<6:17:58,  1.74s/it] 27%|██▋       | 4798/17834 [2:23:48<6:19:37,  1.75s/it] 27%|██▋       | 4799/17834 [2:23:50<6:14:39,  1.72s/it]08/30/2024 21:38:09 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1897071599960327, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04620777815580368, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2435083389282227, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4794232845306396}
 27%|██▋       | 4800/17834 [2:23:52<6:15:35,  1.73s/it] 27%|██▋       | 4801/17834 [2:23:54<6:16:59,  1.74s/it] 27%|██▋       | 4802/17834 [2:23:55<6:16:22,  1.73s/it] 27%|██▋       | 4803/17834 [2:23:57<6:17:11,  1.74s/it] 27%|██▋       | 4804/17834 [2:23:59<6:22:10,  1.76s/it] 27%|██▋       | 4805/17834 [2:24:01<6:23:45,  1.77s/it] 27%|██▋       | 4806/17834 [2:24:02<6:24:20,  1.77s/it] 27%|██▋       | 4807/17834 [2:24:04<6:21:41,  1.76s/it] 27%|██▋       | 4808/17834 [2:24:06<6:19:49,  1.75s/it] 27%|██▋       | 4809/17834 [2:24:08<6:21:40,  1.76s/it] 27%|██▋       | 4810/17834 [2:24:09<6:20:34,  1.75s/it] 27%|██▋       | 4811/17834 [2:24:11<6:19:08,  1.75s/it] 27%|██▋       | 4812/17834 [2:24:13<6:19:12,  1.75s/it] 27%|██▋       | 4813/17834 [2:24:15<6:18:23,  1.74s/it] 27%|██▋       | 4814/17834 [2:24:16<6:23:56,  1.77s/it] 27%|██▋       | 4815/17834 [2:24:18<6:22:44,  1.76s/it] 27%|██▋       | 4816/17834 [2:24:20<6:19:24,  1.75s/it] 27%|██▋       | 4817/17834 [2:24:22<6:27:10,  1.78s/it] 27%|██▋       | 4818/17834 [2:24:23<6:25:06,  1.78s/it] 27%|██▋       | 4819/17834 [2:24:25<6:18:41,  1.75s/it] 27%|██▋       | 4820/17834 [2:24:27<6:18:08,  1.74s/it] 27%|██▋       | 4821/17834 [2:24:29<6:17:42,  1.74s/it] 27%|██▋       | 4822/17834 [2:24:30<6:19:32,  1.75s/it] 27%|██▋       | 4823/17834 [2:24:32<6:22:29,  1.76s/it] 27%|██▋       | 4824/17834 [2:24:34<6:22:02,  1.76s/it] 27%|██▋       | 4825/17834 [2:24:36<6:21:28,  1.76s/it] 27%|██▋       | 4826/17834 [2:24:37<6:17:47,  1.74s/it] 27%|██▋       | 4827/17834 [2:24:39<6:21:48,  1.76s/it] 27%|██▋       | 4828/17834 [2:24:41<6:14:43,  1.73s/it] 27%|██▋       | 4829/17834 [2:24:43<6:16:52,  1.74s/it] 27%|██▋       | 4830/17834 [2:24:44<6:17:24,  1.74s/it] 27%|██▋       | 4831/17834 [2:24:46<6:13:59,  1.73s/it] 27%|██▋       | 4832/17834 [2:24:48<6:12:58,  1.72s/it] 27%|██▋       | 4833/17834 [2:24:50<6:19:18,  1.75s/it] 27%|██▋       | 4834/17834 [2:24:51<6:16:08,  1.74s/it] 27%|██▋       | 4835/17834 [2:24:53<6:19:54,  1.75s/it] 27%|██▋       | 4836/17834 [2:24:55<6:25:35,  1.78s/it] 27%|██▋       | 4837/17834 [2:24:57<6:17:37,  1.74s/it] 27%|██▋       | 4838/17834 [2:24:58<6:19:31,  1.75s/it] 27%|██▋       | 4839/17834 [2:25:00<6:22:46,  1.77s/it] 27%|██▋       | 4840/17834 [2:25:02<6:19:50,  1.75s/it] 27%|██▋       | 4841/17834 [2:25:04<6:20:35,  1.76s/it] 27%|██▋       | 4842/17834 [2:25:05<6:20:12,  1.76s/it] 27%|██▋       | 4843/17834 [2:25:07<6:19:54,  1.75s/it] 27%|██▋       | 4844/17834 [2:25:09<6:21:28,  1.76s/it] 27%|██▋       | 4845/17834 [2:25:11<6:21:51,  1.76s/it] 27%|██▋       | 4846/17834 [2:25:12<6:20:24,  1.76s/it] 27%|██▋       | 4847/17834 [2:25:14<6:21:38,  1.76s/it] 27%|██▋       | 4848/17834 [2:25:16<6:25:08,  1.78s/it] 27%|██▋       | 4849/17834 [2:25:18<6:23:45,  1.77s/it]08/30/2024 21:39:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1858019828796387, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.036267250776290894, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1806602478027344, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4027295112609863}
 27%|██▋       | 4850/17834 [2:25:20<6:22:54,  1.77s/it] 27%|██▋       | 4851/17834 [2:25:21<6:21:57,  1.77s/it] 27%|██▋       | 4852/17834 [2:25:23<6:20:31,  1.76s/it] 27%|██▋       | 4853/17834 [2:25:25<6:23:21,  1.77s/it] 27%|██▋       | 4854/17834 [2:25:27<6:19:21,  1.75s/it] 27%|██▋       | 4855/17834 [2:25:28<6:18:10,  1.75s/it] 27%|██▋       | 4856/17834 [2:25:30<6:20:39,  1.76s/it] 27%|██▋       | 4857/17834 [2:25:32<6:19:55,  1.76s/it] 27%|██▋       | 4858/17834 [2:25:34<6:20:22,  1.76s/it] 27%|██▋       | 4859/17834 [2:25:35<6:17:26,  1.75s/it] 27%|██▋       | 4860/17834 [2:25:37<6:17:53,  1.75s/it] 27%|██▋       | 4861/17834 [2:25:39<6:20:02,  1.76s/it] 27%|██▋       | 4862/17834 [2:25:41<6:17:15,  1.74s/it] 27%|██▋       | 4863/17834 [2:25:42<6:18:09,  1.75s/it] 27%|██▋       | 4864/17834 [2:25:44<6:21:01,  1.76s/it] 27%|██▋       | 4865/17834 [2:25:46<6:21:22,  1.76s/it] 27%|██▋       | 4866/17834 [2:25:48<6:24:12,  1.78s/it] 27%|██▋       | 4867/17834 [2:25:49<6:21:25,  1.76s/it] 27%|██▋       | 4868/17834 [2:25:51<6:17:33,  1.75s/it] 27%|██▋       | 4869/17834 [2:25:53<6:16:05,  1.74s/it] 27%|██▋       | 4870/17834 [2:25:55<6:16:04,  1.74s/it] 27%|██▋       | 4871/17834 [2:25:56<6:18:58,  1.75s/it] 27%|██▋       | 4872/17834 [2:25:58<6:23:47,  1.78s/it] 27%|██▋       | 4873/17834 [2:26:00<6:27:28,  1.79s/it] 27%|██▋       | 4874/17834 [2:26:02<6:23:09,  1.77s/it] 27%|██▋       | 4875/17834 [2:26:03<6:17:23,  1.75s/it] 27%|██▋       | 4876/17834 [2:26:05<6:12:13,  1.72s/it] 27%|██▋       | 4877/17834 [2:26:07<6:12:59,  1.73s/it] 27%|██▋       | 4878/17834 [2:26:09<6:19:07,  1.76s/it] 27%|██▋       | 4879/17834 [2:26:10<6:20:11,  1.76s/it] 27%|██▋       | 4880/17834 [2:26:12<6:18:45,  1.75s/it] 27%|██▋       | 4881/17834 [2:26:14<6:22:27,  1.77s/it] 27%|██▋       | 4882/17834 [2:26:16<6:17:11,  1.75s/it] 27%|██▋       | 4883/17834 [2:26:17<6:18:47,  1.75s/it] 27%|██▋       | 4884/17834 [2:26:19<6:23:55,  1.78s/it] 27%|██▋       | 4885/17834 [2:26:21<6:21:55,  1.77s/it] 27%|██▋       | 4886/17834 [2:26:23<6:18:51,  1.76s/it] 27%|██▋       | 4887/17834 [2:26:25<6:17:56,  1.75s/it] 27%|██▋       | 4888/17834 [2:26:26<6:18:06,  1.75s/it] 27%|██▋       | 4889/17834 [2:26:28<6:14:31,  1.74s/it] 27%|██▋       | 4890/17834 [2:26:30<6:13:27,  1.73s/it] 27%|██▋       | 4891/17834 [2:26:32<6:18:37,  1.76s/it] 27%|██▋       | 4892/17834 [2:26:33<6:13:18,  1.73s/it] 27%|██▋       | 4893/17834 [2:26:35<6:11:50,  1.72s/it] 27%|██▋       | 4894/17834 [2:26:37<6:13:33,  1.73s/it] 27%|██▋       | 4895/17834 [2:26:38<6:16:02,  1.74s/it] 27%|██▋       | 4896/17834 [2:26:40<6:16:18,  1.75s/it] 27%|██▋       | 4897/17834 [2:26:42<6:13:52,  1.73s/it] 27%|██▋       | 4898/17834 [2:26:44<6:16:51,  1.75s/it] 27%|██▋       | 4899/17834 [2:26:45<6:14:59,  1.74s/it]08/30/2024 21:41:05 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4137318134307861, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04594755917787552, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3435583114624023, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8032376766204834}
 27%|██▋       | 4900/17834 [2:26:47<6:15:35,  1.74s/it] 27%|██▋       | 4901/17834 [2:26:49<6:11:38,  1.72s/it] 27%|██▋       | 4902/17834 [2:26:51<6:13:37,  1.73s/it] 27%|██▋       | 4903/17834 [2:26:52<6:16:48,  1.75s/it] 27%|██▋       | 4904/17834 [2:26:54<6:13:51,  1.73s/it] 28%|██▊       | 4905/17834 [2:26:56<6:14:14,  1.74s/it] 28%|██▊       | 4906/17834 [2:26:58<6:24:47,  1.79s/it] 28%|██▊       | 4907/17834 [2:26:59<6:16:21,  1.75s/it] 28%|██▊       | 4908/17834 [2:27:01<6:16:22,  1.75s/it] 28%|██▊       | 4909/17834 [2:27:03<6:11:16,  1.72s/it] 28%|██▊       | 4910/17834 [2:27:05<6:13:04,  1.73s/it] 28%|██▊       | 4911/17834 [2:27:06<6:11:59,  1.73s/it] 28%|██▊       | 4912/17834 [2:27:08<6:13:18,  1.73s/it] 28%|██▊       | 4913/17834 [2:27:10<6:11:23,  1.72s/it] 28%|██▊       | 4914/17834 [2:27:11<6:13:47,  1.74s/it] 28%|██▊       | 4915/17834 [2:27:13<6:09:56,  1.72s/it] 28%|██▊       | 4916/17834 [2:27:15<6:23:18,  1.78s/it] 28%|██▊       | 4917/17834 [2:27:17<6:19:23,  1.76s/it] 28%|██▊       | 4918/17834 [2:27:19<6:18:14,  1.76s/it] 28%|██▊       | 4919/17834 [2:27:20<6:12:39,  1.73s/it] 28%|██▊       | 4920/17834 [2:27:22<6:16:18,  1.75s/it] 28%|██▊       | 4921/17834 [2:27:24<6:19:29,  1.76s/it] 28%|██▊       | 4922/17834 [2:27:26<6:24:02,  1.78s/it] 28%|██▊       | 4923/17834 [2:27:27<6:19:57,  1.77s/it] 28%|██▊       | 4924/17834 [2:27:29<6:20:39,  1.77s/it] 28%|██▊       | 4925/17834 [2:27:31<6:17:56,  1.76s/it] 28%|██▊       | 4926/17834 [2:27:33<6:15:55,  1.75s/it] 28%|██▊       | 4927/17834 [2:27:34<6:20:09,  1.77s/it] 28%|██▊       | 4928/17834 [2:27:36<6:16:26,  1.75s/it] 28%|██▊       | 4929/17834 [2:27:38<6:16:45,  1.75s/it] 28%|██▊       | 4930/17834 [2:27:40<6:19:29,  1.76s/it] 28%|██▊       | 4931/17834 [2:27:41<6:14:19,  1.74s/it] 28%|██▊       | 4932/17834 [2:27:43<6:12:13,  1.73s/it] 28%|██▊       | 4933/17834 [2:27:45<6:13:24,  1.74s/it] 28%|██▊       | 4934/17834 [2:27:47<6:13:21,  1.74s/it] 28%|██▊       | 4935/17834 [2:27:48<6:16:22,  1.75s/it] 28%|██▊       | 4936/17834 [2:27:50<6:08:42,  1.72s/it] 28%|██▊       | 4937/17834 [2:27:52<6:11:05,  1.73s/it] 28%|██▊       | 4938/17834 [2:27:53<6:03:31,  1.69s/it] 28%|██▊       | 4939/17834 [2:27:55<5:57:24,  1.66s/it] 28%|██▊       | 4940/17834 [2:27:56<5:53:08,  1.64s/it] 28%|██▊       | 4941/17834 [2:27:58<5:50:05,  1.63s/it] 28%|██▊       | 4942/17834 [2:28:00<5:47:57,  1.62s/it] 28%|██▊       | 4943/17834 [2:28:01<5:46:46,  1.61s/it] 28%|██▊       | 4944/17834 [2:28:03<5:46:00,  1.61s/it] 28%|██▊       | 4945/17834 [2:28:04<5:45:24,  1.61s/it] 28%|██▊       | 4946/17834 [2:28:06<5:45:00,  1.61s/it] 28%|██▊       | 4947/17834 [2:28:08<5:44:46,  1.61s/it] 28%|██▊       | 4948/17834 [2:28:09<5:44:29,  1.60s/it] 28%|██▊       | 4949/17834 [2:28:11<5:44:17,  1.60s/it]08/30/2024 21:42:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3737159967422485, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04607691615819931, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1548240184783936, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5746169090270996}
 28%|██▊       | 4950/17834 [2:28:13<5:49:58,  1.63s/it] 28%|██▊       | 4951/17834 [2:28:14<5:46:48,  1.62s/it] 28%|██▊       | 4952/17834 [2:28:16<5:45:55,  1.61s/it] 28%|██▊       | 4953/17834 [2:28:17<5:45:18,  1.61s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
 28%|██▊       | 4954/17834 [2:28:42<30:55:59,  8.65s/it] 28%|██▊       | 4955/17834 [2:28:45<24:12:55,  6.77s/it] 28%|██▊       | 4956/17834 [2:28:47<19:04:47,  5.33s/it] 28%|██▊       | 4957/17834 [2:28:49<15:17:53,  4.28s/it] 28%|██▊       | 4958/17834 [2:28:50<12:40:05,  3.54s/it] 28%|██▊       | 4959/17834 [2:28:52<10:44:11,  3.00s/it] 28%|██▊       | 4960/17834 [2:28:54<9:23:00,  2.62s/it]  28%|██▊       | 4961/17834 [2:28:56<8:26:21,  2.36s/it] 28%|██▊       | 4962/17834 [2:28:57<7:45:08,  2.17s/it] 28%|██▊       | 4963/17834 [2:28:59<7:21:49,  2.06s/it] 28%|██▊       | 4964/17834 [2:29:01<7:04:07,  1.98s/it] 28%|██▊       | 4965/17834 [2:29:03<6:44:17,  1.88s/it] 28%|██▊       | 4966/17834 [2:29:04<6:36:51,  1.85s/it] 28%|██▊       | 4967/17834 [2:29:06<6:39:12,  1.86s/it] 28%|██▊       | 4968/17834 [2:29:08<6:31:44,  1.83s/it] 28%|██▊       | 4969/17834 [2:29:10<6:26:26,  1.80s/it] 28%|██▊       | 4970/17834 [2:29:12<6:23:59,  1.79s/it] 28%|██▊       | 4971/17834 [2:29:13<6:26:01,  1.80s/it] 28%|██▊       | 4972/17834 [2:29:15<6:19:14,  1.77s/it] 28%|██▊       | 4973/17834 [2:29:17<6:21:00,  1.78s/it] 28%|██▊       | 4974/17834 [2:29:19<6:18:42,  1.77s/it] 28%|██▊       | 4975/17834 [2:29:20<6:18:32,  1.77s/it] 28%|██▊       | 4976/17834 [2:29:22<6:12:49,  1.74s/it] 28%|██▊       | 4977/17834 [2:29:24<6:21:31,  1.78s/it] 28%|██▊       | 4978/17834 [2:29:26<6:19:09,  1.77s/it] 28%|██▊       | 4979/17834 [2:29:27<6:21:33,  1.78s/it] 28%|██▊       | 4980/17834 [2:29:29<6:22:45,  1.79s/it] 28%|██▊       | 4981/17834 [2:29:31<6:18:32,  1.77s/it] 28%|██▊       | 4982/17834 [2:29:33<6:11:25,  1.73s/it] 28%|██▊       | 4983/17834 [2:29:34<6:14:05,  1.75s/it] 28%|██▊       | 4984/17834 [2:29:36<6:12:34,  1.74s/it] 28%|██▊       | 4985/17834 [2:29:38<6:18:15,  1.77s/it] 28%|██▊       | 4986/17834 [2:29:40<6:21:22,  1.78s/it] 28%|██▊       | 4987/17834 [2:29:42<6:20:28,  1.78s/it] 28%|██▊       | 4988/17834 [2:29:43<6:19:19,  1.77s/it] 28%|██▊       | 4989/17834 [2:29:45<6:14:01,  1.75s/it] 28%|██▊       | 4990/17834 [2:29:47<6:14:33,  1.75s/it] 28%|██▊       | 4991/17834 [2:29:48<6:10:50,  1.73s/it] 28%|██▊       | 4992/17834 [2:29:50<6:18:12,  1.77s/it] 28%|██▊       | 4993/17834 [2:29:52<6:13:16,  1.74s/it] 28%|██▊       | 4994/17834 [2:29:54<6:16:48,  1.76s/it] 28%|██▊       | 4995/17834 [2:29:56<6:12:13,  1.74s/it] 28%|██▊       | 4996/17834 [2:29:57<6:16:56,  1.76s/it] 28%|██▊       | 4997/17834 [2:29:59<6:14:48,  1.75s/it] 28%|██▊       | 4998/17834 [2:30:01<6:21:59,  1.79s/it] 28%|██▊       | 4999/17834 [2:30:03<6:19:16,  1.77s/it]08/30/2024 21:44:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.015574336051941, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03815096244215965, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.186182975769043, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.239908218383789}
 28%|██▊       | 5000/17834 [2:30:04<6:14:59,  1.75s/it] 28%|██▊       | 5001/17834 [2:30:06<6:19:17,  1.77s/it] 28%|██▊       | 5002/17834 [2:30:08<6:18:50,  1.77s/it] 28%|██▊       | 5003/17834 [2:30:10<6:17:26,  1.77s/it] 28%|██▊       | 5004/17834 [2:30:11<6:15:39,  1.76s/it] 28%|██▊       | 5005/17834 [2:30:13<6:13:23,  1.75s/it] 28%|██▊       | 5006/17834 [2:30:15<6:23:29,  1.79s/it] 28%|██▊       | 5007/17834 [2:30:17<6:19:06,  1.77s/it] 28%|██▊       | 5008/17834 [2:30:18<6:14:43,  1.75s/it] 28%|██▊       | 5009/17834 [2:30:20<6:10:07,  1.73s/it] 28%|██▊       | 5010/17834 [2:30:22<6:19:28,  1.78s/it] 28%|██▊       | 5011/17834 [2:30:24<6:18:29,  1.77s/it] 28%|██▊       | 5012/17834 [2:30:26<6:23:51,  1.80s/it] 28%|██▊       | 5013/17834 [2:30:27<6:16:32,  1.76s/it] 28%|██▊       | 5014/17834 [2:30:29<6:18:39,  1.77s/it] 28%|██▊       | 5015/17834 [2:30:31<6:16:52,  1.76s/it] 28%|██▊       | 5016/17834 [2:30:33<6:13:12,  1.75s/it] 28%|██▊       | 5017/17834 [2:30:34<6:14:35,  1.75s/it] 28%|██▊       | 5018/17834 [2:30:36<6:15:05,  1.76s/it] 28%|██▊       | 5019/17834 [2:30:38<6:11:19,  1.74s/it] 28%|██▊       | 5020/17834 [2:30:40<6:12:31,  1.74s/it] 28%|██▊       | 5021/17834 [2:30:41<6:10:14,  1.73s/it] 28%|██▊       | 5022/17834 [2:30:43<6:12:08,  1.74s/it] 28%|██▊       | 5023/17834 [2:30:45<6:10:36,  1.74s/it] 28%|██▊       | 5024/17834 [2:30:46<6:09:13,  1.73s/it] 28%|██▊       | 5025/17834 [2:30:48<6:12:05,  1.74s/it] 28%|██▊       | 5026/17834 [2:30:50<6:09:24,  1.73s/it] 28%|██▊       | 5027/17834 [2:30:52<6:14:15,  1.75s/it] 28%|██▊       | 5028/17834 [2:30:54<6:15:42,  1.76s/it] 28%|██▊       | 5029/17834 [2:30:55<6:15:39,  1.76s/it] 28%|██▊       | 5030/17834 [2:30:57<6:18:26,  1.77s/it] 28%|██▊       | 5031/17834 [2:30:59<6:12:40,  1.75s/it] 28%|██▊       | 5032/17834 [2:31:01<6:19:21,  1.78s/it] 28%|██▊       | 5033/17834 [2:31:02<6:15:06,  1.76s/it] 28%|██▊       | 5034/17834 [2:31:04<6:11:46,  1.74s/it] 28%|██▊       | 5035/17834 [2:31:06<6:10:34,  1.74s/it] 28%|██▊       | 5036/17834 [2:31:08<6:11:00,  1.74s/it] 28%|██▊       | 5037/17834 [2:31:09<6:10:50,  1.74s/it] 28%|██▊       | 5038/17834 [2:31:11<6:19:19,  1.78s/it] 28%|██▊       | 5039/17834 [2:31:13<6:14:21,  1.76s/it] 28%|██▊       | 5040/17834 [2:31:15<6:09:27,  1.73s/it] 28%|██▊       | 5041/17834 [2:31:16<6:16:44,  1.77s/it] 28%|██▊       | 5042/17834 [2:31:18<6:15:09,  1.76s/it] 28%|██▊       | 5043/17834 [2:31:20<6:16:59,  1.77s/it] 28%|██▊       | 5044/17834 [2:31:22<6:20:06,  1.78s/it] 28%|██▊       | 5045/17834 [2:31:23<6:17:56,  1.77s/it] 28%|██▊       | 5046/17834 [2:31:25<6:17:37,  1.77s/it] 28%|██▊       | 5047/17834 [2:31:27<6:17:01,  1.77s/it] 28%|██▊       | 5048/17834 [2:31:29<6:12:20,  1.75s/it] 28%|██▊       | 5049/17834 [2:31:30<6:12:53,  1.75s/it]08/30/2024 21:45:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.135844349861145, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03293086215853691, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.133683443069458, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3024587631225586}
 28%|██▊       | 5050/17834 [2:31:32<6:07:59,  1.73s/it] 28%|██▊       | 5051/17834 [2:31:34<6:07:00,  1.72s/it] 28%|██▊       | 5052/17834 [2:31:36<6:10:28,  1.74s/it] 28%|██▊       | 5053/17834 [2:31:37<6:09:31,  1.73s/it] 28%|██▊       | 5054/17834 [2:31:39<6:08:08,  1.73s/it] 28%|██▊       | 5055/17834 [2:31:41<6:08:15,  1.73s/it] 28%|██▊       | 5056/17834 [2:31:43<6:07:15,  1.72s/it] 28%|██▊       | 5057/17834 [2:31:44<6:13:03,  1.75s/it] 28%|██▊       | 5058/17834 [2:31:46<6:15:00,  1.76s/it] 28%|██▊       | 5059/17834 [2:31:48<6:15:08,  1.76s/it] 28%|██▊       | 5060/17834 [2:31:50<6:13:47,  1.76s/it] 28%|██▊       | 5061/17834 [2:31:51<6:11:12,  1.74s/it] 28%|██▊       | 5062/17834 [2:31:53<6:12:15,  1.75s/it] 28%|██▊       | 5063/17834 [2:31:55<6:07:26,  1.73s/it] 28%|██▊       | 5064/17834 [2:31:57<6:17:26,  1.77s/it] 28%|██▊       | 5065/17834 [2:31:58<6:14:15,  1.76s/it] 28%|██▊       | 5066/17834 [2:32:00<6:15:02,  1.76s/it] 28%|██▊       | 5067/17834 [2:32:02<6:11:51,  1.75s/it] 28%|██▊       | 5068/17834 [2:32:04<6:10:50,  1.74s/it] 28%|██▊       | 5069/17834 [2:32:05<6:10:22,  1.74s/it] 28%|██▊       | 5070/17834 [2:32:07<6:09:44,  1.74s/it] 28%|██▊       | 5071/17834 [2:32:09<6:09:48,  1.74s/it] 28%|██▊       | 5072/17834 [2:32:11<6:13:08,  1.75s/it] 28%|██▊       | 5073/17834 [2:32:12<6:11:09,  1.75s/it] 28%|██▊       | 5074/17834 [2:32:14<6:10:00,  1.74s/it] 28%|██▊       | 5075/17834 [2:32:16<6:09:50,  1.74s/it] 28%|██▊       | 5076/17834 [2:32:17<6:07:58,  1.73s/it] 28%|██▊       | 5077/17834 [2:32:19<6:10:18,  1.74s/it] 28%|██▊       | 5078/17834 [2:32:21<6:09:49,  1.74s/it] 28%|██▊       | 5079/17834 [2:32:23<6:10:19,  1.74s/it] 28%|██▊       | 5080/17834 [2:32:24<6:09:35,  1.74s/it] 28%|██▊       | 5081/17834 [2:32:26<6:14:48,  1.76s/it] 28%|██▊       | 5082/17834 [2:32:28<6:15:23,  1.77s/it] 29%|██▊       | 5083/17834 [2:32:30<6:11:55,  1.75s/it] 29%|██▊       | 5084/17834 [2:32:32<6:11:12,  1.75s/it] 29%|██▊       | 5085/17834 [2:32:33<6:10:36,  1.74s/it] 29%|██▊       | 5086/17834 [2:32:35<6:15:15,  1.77s/it] 29%|██▊       | 5087/17834 [2:32:37<6:15:01,  1.77s/it] 29%|██▊       | 5088/17834 [2:32:39<6:12:42,  1.75s/it] 29%|██▊       | 5089/17834 [2:32:40<6:15:04,  1.77s/it] 29%|██▊       | 5090/17834 [2:32:42<6:16:44,  1.77s/it] 29%|██▊       | 5091/17834 [2:32:44<6:15:23,  1.77s/it] 29%|██▊       | 5092/17834 [2:32:46<6:13:21,  1.76s/it] 29%|██▊       | 5093/17834 [2:32:47<6:11:01,  1.75s/it] 29%|██▊       | 5094/17834 [2:32:49<6:09:00,  1.74s/it] 29%|██▊       | 5095/17834 [2:32:51<6:09:32,  1.74s/it] 29%|██▊       | 5096/17834 [2:32:53<6:09:35,  1.74s/it] 29%|██▊       | 5097/17834 [2:32:54<6:05:12,  1.72s/it] 29%|██▊       | 5098/17834 [2:32:56<6:01:13,  1.70s/it] 29%|██▊       | 5099/17834 [2:32:58<6:04:10,  1.72s/it]08/30/2024 21:47:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4753696918487549, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03722231462597847, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.259422779083252, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.772014617919922}
 29%|██▊       | 5100/17834 [2:32:59<6:06:42,  1.73s/it] 29%|██▊       | 5101/17834 [2:33:01<6:04:24,  1.72s/it] 29%|██▊       | 5102/17834 [2:33:03<6:05:55,  1.72s/it] 29%|██▊       | 5103/17834 [2:33:05<6:12:46,  1.76s/it] 29%|██▊       | 5104/17834 [2:33:06<6:07:23,  1.73s/it] 29%|██▊       | 5105/17834 [2:33:08<6:07:13,  1.73s/it] 29%|██▊       | 5106/17834 [2:33:10<6:09:09,  1.74s/it] 29%|██▊       | 5107/17834 [2:33:12<6:11:34,  1.75s/it] 29%|██▊       | 5108/17834 [2:33:13<6:13:06,  1.76s/it] 29%|██▊       | 5109/17834 [2:33:15<6:10:05,  1.75s/it] 29%|██▊       | 5110/17834 [2:33:17<6:15:55,  1.77s/it] 29%|██▊       | 5111/17834 [2:33:19<6:13:57,  1.76s/it] 29%|██▊       | 5112/17834 [2:33:20<6:09:24,  1.74s/it] 29%|██▊       | 5113/17834 [2:33:22<6:12:00,  1.75s/it] 29%|██▊       | 5114/17834 [2:33:24<6:07:08,  1.73s/it] 29%|██▊       | 5115/17834 [2:33:26<6:08:25,  1.74s/it] 29%|██▊       | 5116/17834 [2:33:27<6:13:27,  1.76s/it] 29%|██▊       | 5117/17834 [2:33:29<6:16:17,  1.78s/it] 29%|██▊       | 5118/17834 [2:33:31<6:18:11,  1.78s/it] 29%|██▊       | 5119/17834 [2:33:33<6:13:49,  1.76s/it] 29%|██▊       | 5120/17834 [2:33:34<6:13:20,  1.76s/it] 29%|██▊       | 5121/17834 [2:33:36<6:10:38,  1.75s/it] 29%|██▊       | 5122/17834 [2:33:38<6:15:08,  1.77s/it] 29%|██▊       | 5123/17834 [2:33:40<6:10:53,  1.75s/it] 29%|██▊       | 5124/17834 [2:33:41<6:11:37,  1.75s/it] 29%|██▊       | 5125/17834 [2:33:43<6:11:42,  1.75s/it] 29%|██▊       | 5126/17834 [2:33:45<6:09:29,  1.74s/it] 29%|██▊       | 5127/17834 [2:33:47<6:09:47,  1.75s/it] 29%|██▉       | 5128/17834 [2:33:48<6:06:30,  1.73s/it] 29%|██▉       | 5129/17834 [2:33:50<6:10:02,  1.75s/it] 29%|██▉       | 5130/17834 [2:33:52<6:14:52,  1.77s/it] 29%|██▉       | 5131/17834 [2:33:54<6:18:18,  1.79s/it] 29%|██▉       | 5132/17834 [2:33:55<6:08:56,  1.74s/it] 29%|██▉       | 5133/17834 [2:33:57<6:06:48,  1.73s/it] 29%|██▉       | 5134/17834 [2:33:59<6:07:01,  1.73s/it] 29%|██▉       | 5135/17834 [2:34:01<6:10:08,  1.75s/it] 29%|██▉       | 5136/17834 [2:34:02<6:07:18,  1.74s/it] 29%|██▉       | 5137/17834 [2:34:04<6:11:55,  1.76s/it] 29%|██▉       | 5138/17834 [2:34:06<6:11:06,  1.75s/it] 29%|██▉       | 5139/17834 [2:34:08<6:07:10,  1.74s/it] 29%|██▉       | 5140/17834 [2:34:09<6:08:04,  1.74s/it] 29%|██▉       | 5141/17834 [2:34:11<6:15:40,  1.78s/it] 29%|██▉       | 5142/17834 [2:34:13<6:13:15,  1.76s/it] 29%|██▉       | 5143/17834 [2:34:15<6:12:30,  1.76s/it] 29%|██▉       | 5144/17834 [2:34:16<6:10:38,  1.75s/it] 29%|██▉       | 5145/17834 [2:34:18<6:12:10,  1.76s/it] 29%|██▉       | 5146/17834 [2:34:20<6:09:05,  1.75s/it] 29%|██▉       | 5147/17834 [2:34:22<6:08:30,  1.74s/it] 29%|██▉       | 5148/17834 [2:34:23<6:09:11,  1.75s/it] 29%|██▉       | 5149/17834 [2:34:25<6:07:12,  1.74s/it]08/30/2024 21:48:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.267369270324707, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03753219172358513, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.207455635070801, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.512357234954834}
 29%|██▉       | 5150/17834 [2:34:27<6:05:55,  1.73s/it] 29%|██▉       | 5151/17834 [2:34:29<6:08:56,  1.75s/it] 29%|██▉       | 5152/17834 [2:34:30<6:08:38,  1.74s/it] 29%|██▉       | 5153/17834 [2:34:32<6:07:48,  1.74s/it] 29%|██▉       | 5154/17834 [2:34:34<6:08:01,  1.74s/it] 29%|██▉       | 5155/17834 [2:34:36<6:12:02,  1.76s/it] 29%|██▉       | 5156/17834 [2:34:37<6:13:49,  1.77s/it] 29%|██▉       | 5157/17834 [2:34:39<6:11:53,  1.76s/it] 29%|██▉       | 5158/17834 [2:34:41<6:12:12,  1.76s/it] 29%|██▉       | 5159/17834 [2:34:43<6:08:03,  1.74s/it] 29%|██▉       | 5160/17834 [2:34:44<6:09:30,  1.75s/it] 29%|██▉       | 5161/17834 [2:34:46<6:10:20,  1.75s/it] 29%|██▉       | 5162/17834 [2:34:48<6:10:49,  1.76s/it] 29%|██▉       | 5163/17834 [2:34:50<6:08:29,  1.74s/it] 29%|██▉       | 5164/17834 [2:34:51<6:10:06,  1.75s/it] 29%|██▉       | 5165/17834 [2:34:53<6:06:07,  1.73s/it] 29%|██▉       | 5166/17834 [2:34:55<6:05:12,  1.73s/it] 29%|██▉       | 5167/17834 [2:34:57<6:09:31,  1.75s/it] 29%|██▉       | 5168/17834 [2:34:58<6:07:57,  1.74s/it] 29%|██▉       | 5169/17834 [2:35:00<6:15:21,  1.78s/it] 29%|██▉       | 5170/17834 [2:35:02<6:12:19,  1.76s/it] 29%|██▉       | 5171/17834 [2:35:04<6:14:00,  1.77s/it] 29%|██▉       | 5172/17834 [2:35:06<6:12:04,  1.76s/it] 29%|██▉       | 5173/17834 [2:35:07<6:17:47,  1.79s/it] 29%|██▉       | 5174/17834 [2:35:09<6:13:19,  1.77s/it] 29%|██▉       | 5175/17834 [2:35:11<6:13:15,  1.77s/it] 29%|██▉       | 5176/17834 [2:35:13<6:11:01,  1.76s/it] 29%|██▉       | 5177/17834 [2:35:14<6:12:02,  1.76s/it] 29%|██▉       | 5178/17834 [2:35:16<6:11:41,  1.76s/it] 29%|██▉       | 5179/17834 [2:35:18<6:10:42,  1.76s/it] 29%|██▉       | 5180/17834 [2:35:20<6:10:40,  1.76s/it] 29%|██▉       | 5181/17834 [2:35:21<6:06:42,  1.74s/it] 29%|██▉       | 5182/17834 [2:35:23<6:10:42,  1.76s/it] 29%|██▉       | 5183/17834 [2:35:25<6:09:44,  1.75s/it] 29%|██▉       | 5184/17834 [2:35:27<6:09:31,  1.75s/it] 29%|██▉       | 5185/17834 [2:35:28<6:10:02,  1.76s/it] 29%|██▉       | 5186/17834 [2:35:30<6:12:12,  1.77s/it] 29%|██▉       | 5187/17834 [2:35:32<6:14:10,  1.78s/it] 29%|██▉       | 5188/17834 [2:35:34<6:14:17,  1.78s/it] 29%|██▉       | 5189/17834 [2:35:36<6:16:14,  1.79s/it] 29%|██▉       | 5190/17834 [2:35:37<6:14:54,  1.78s/it] 29%|██▉       | 5191/17834 [2:35:39<6:13:34,  1.77s/it] 29%|██▉       | 5192/17834 [2:35:41<6:13:16,  1.77s/it] 29%|██▉       | 5193/17834 [2:35:43<6:11:26,  1.76s/it] 29%|██▉       | 5194/17834 [2:35:44<6:11:02,  1.76s/it] 29%|██▉       | 5195/17834 [2:35:46<6:09:33,  1.75s/it] 29%|██▉       | 5196/17834 [2:35:48<6:08:41,  1.75s/it] 29%|██▉       | 5197/17834 [2:35:50<6:07:39,  1.75s/it] 29%|██▉       | 5198/17834 [2:35:51<6:08:24,  1.75s/it] 29%|██▉       | 5199/17834 [2:35:53<6:07:25,  1.74s/it]08/30/2024 21:50:12 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.943464994430542, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0347248874604702, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.13093900680542, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.109128952026367}
 29%|██▉       | 5200/17834 [2:35:55<6:08:32,  1.75s/it] 29%|██▉       | 5201/17834 [2:35:57<6:06:30,  1.74s/it] 29%|██▉       | 5202/17834 [2:35:58<6:08:38,  1.75s/it] 29%|██▉       | 5203/17834 [2:36:00<6:05:16,  1.74s/it] 29%|██▉       | 5204/17834 [2:36:02<6:06:19,  1.74s/it] 29%|██▉       | 5205/17834 [2:36:03<6:03:46,  1.73s/it] 29%|██▉       | 5206/17834 [2:36:05<6:00:37,  1.71s/it] 29%|██▉       | 5207/17834 [2:36:07<6:07:07,  1.74s/it] 29%|██▉       | 5208/17834 [2:36:09<6:08:09,  1.75s/it] 29%|██▉       | 5209/17834 [2:36:11<6:09:46,  1.76s/it] 29%|██▉       | 5210/17834 [2:36:12<6:07:44,  1.75s/it] 29%|██▉       | 5211/17834 [2:36:14<6:11:39,  1.77s/it] 29%|██▉       | 5212/17834 [2:36:16<6:11:14,  1.76s/it] 29%|██▉       | 5213/17834 [2:36:18<6:08:38,  1.75s/it] 29%|██▉       | 5214/17834 [2:36:19<6:09:11,  1.76s/it] 29%|██▉       | 5215/17834 [2:36:21<6:15:13,  1.78s/it] 29%|██▉       | 5216/17834 [2:36:23<6:10:10,  1.76s/it] 29%|██▉       | 5217/17834 [2:36:25<6:07:21,  1.75s/it] 29%|██▉       | 5218/17834 [2:36:26<6:09:14,  1.76s/it] 29%|██▉       | 5219/17834 [2:36:28<6:07:21,  1.75s/it] 29%|██▉       | 5220/17834 [2:36:30<6:07:54,  1.75s/it] 29%|██▉       | 5221/17834 [2:36:32<6:07:13,  1.75s/it] 29%|██▉       | 5222/17834 [2:36:33<6:05:29,  1.74s/it] 29%|██▉       | 5223/17834 [2:36:35<6:02:20,  1.72s/it] 29%|██▉       | 5224/17834 [2:36:37<6:02:23,  1.72s/it] 29%|██▉       | 5225/17834 [2:36:38<6:03:32,  1.73s/it] 29%|██▉       | 5226/17834 [2:36:40<5:58:25,  1.71s/it] 29%|██▉       | 5227/17834 [2:36:42<6:02:00,  1.72s/it] 29%|██▉       | 5228/17834 [2:36:44<6:11:02,  1.77s/it] 29%|██▉       | 5229/17834 [2:36:45<6:07:30,  1.75s/it] 29%|██▉       | 5230/17834 [2:36:47<6:06:59,  1.75s/it] 29%|██▉       | 5231/17834 [2:36:49<6:02:53,  1.73s/it] 29%|██▉       | 5232/17834 [2:36:51<6:03:37,  1.73s/it] 29%|██▉       | 5233/17834 [2:36:52<6:04:49,  1.74s/it] 29%|██▉       | 5234/17834 [2:36:54<6:04:36,  1.74s/it] 29%|██▉       | 5235/17834 [2:36:56<6:06:49,  1.75s/it] 29%|██▉       | 5236/17834 [2:36:58<6:06:47,  1.75s/it] 29%|██▉       | 5237/17834 [2:36:59<6:03:33,  1.73s/it] 29%|██▉       | 5238/17834 [2:37:01<6:00:29,  1.72s/it] 29%|██▉       | 5239/17834 [2:37:03<6:07:39,  1.75s/it] 29%|██▉       | 5240/17834 [2:37:05<6:07:10,  1.75s/it] 29%|██▉       | 5241/17834 [2:37:06<6:08:36,  1.76s/it] 29%|██▉       | 5242/17834 [2:37:08<6:08:37,  1.76s/it] 29%|██▉       | 5243/17834 [2:37:10<6:09:42,  1.76s/it] 29%|██▉       | 5244/17834 [2:37:12<6:06:58,  1.75s/it] 29%|██▉       | 5245/17834 [2:37:13<6:01:20,  1.72s/it] 29%|██▉       | 5246/17834 [2:37:15<6:04:15,  1.74s/it] 29%|██▉       | 5247/17834 [2:37:17<6:11:28,  1.77s/it] 29%|██▉       | 5248/17834 [2:37:19<6:08:50,  1.76s/it] 29%|██▉       | 5249/17834 [2:37:20<6:08:32,  1.76s/it]08/30/2024 21:51:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.460789680480957, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.042097195982933044, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2850611209869385, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7879481315612793}
 29%|██▉       | 5250/17834 [2:37:22<6:06:58,  1.75s/it] 29%|██▉       | 5251/17834 [2:37:24<6:11:15,  1.77s/it] 29%|██▉       | 5252/17834 [2:37:26<6:07:35,  1.75s/it] 29%|██▉       | 5253/17834 [2:37:27<6:06:19,  1.75s/it] 29%|██▉       | 5254/17834 [2:37:29<6:01:06,  1.72s/it] 29%|██▉       | 5255/17834 [2:37:31<5:59:54,  1.72s/it] 29%|██▉       | 5256/17834 [2:37:32<6:01:07,  1.72s/it] 29%|██▉       | 5257/17834 [2:37:34<6:00:31,  1.72s/it] 29%|██▉       | 5258/17834 [2:37:36<6:03:05,  1.73s/it] 29%|██▉       | 5259/17834 [2:37:38<5:59:16,  1.71s/it] 29%|██▉       | 5260/17834 [2:37:39<6:04:25,  1.74s/it] 29%|██▉       | 5261/17834 [2:37:41<6:01:51,  1.73s/it] 30%|██▉       | 5262/17834 [2:37:43<6:01:25,  1.72s/it] 30%|██▉       | 5263/17834 [2:37:45<6:00:28,  1.72s/it] 30%|██▉       | 5264/17834 [2:37:46<6:06:29,  1.75s/it] 30%|██▉       | 5265/17834 [2:37:48<6:04:33,  1.74s/it] 30%|██▉       | 5266/17834 [2:37:50<6:11:43,  1.77s/it] 30%|██▉       | 5267/17834 [2:37:52<6:11:44,  1.77s/it] 30%|██▉       | 5268/17834 [2:37:53<6:11:42,  1.77s/it] 30%|██▉       | 5269/17834 [2:37:55<6:14:30,  1.79s/it] 30%|██▉       | 5270/17834 [2:37:57<6:13:17,  1.78s/it] 30%|██▉       | 5271/17834 [2:37:59<6:16:44,  1.80s/it] 30%|██▉       | 5272/17834 [2:38:01<6:08:42,  1.76s/it] 30%|██▉       | 5273/17834 [2:38:02<6:13:41,  1.79s/it] 30%|██▉       | 5274/17834 [2:38:04<6:11:06,  1.77s/it] 30%|██▉       | 5275/17834 [2:38:06<6:07:00,  1.75s/it] 30%|██▉       | 5276/17834 [2:38:08<6:08:06,  1.76s/it] 30%|██▉       | 5277/17834 [2:38:09<6:08:30,  1.76s/it] 30%|██▉       | 5278/17834 [2:38:11<6:05:01,  1.74s/it] 30%|██▉       | 5279/17834 [2:38:13<6:07:22,  1.76s/it] 30%|██▉       | 5280/17834 [2:38:15<6:04:59,  1.74s/it] 30%|██▉       | 5281/17834 [2:38:16<6:07:26,  1.76s/it] 30%|██▉       | 5282/17834 [2:38:18<6:02:11,  1.73s/it] 30%|██▉       | 5283/17834 [2:38:20<5:59:22,  1.72s/it] 30%|██▉       | 5284/17834 [2:38:21<6:02:00,  1.73s/it] 30%|██▉       | 5285/17834 [2:38:23<6:03:18,  1.74s/it] 30%|██▉       | 5286/17834 [2:38:25<6:02:12,  1.73s/it] 30%|██▉       | 5287/17834 [2:38:27<6:04:08,  1.74s/it] 30%|██▉       | 5288/17834 [2:38:29<6:06:45,  1.75s/it] 30%|██▉       | 5289/17834 [2:38:30<6:10:25,  1.77s/it] 30%|██▉       | 5290/17834 [2:38:32<6:10:49,  1.77s/it] 30%|██▉       | 5291/17834 [2:38:34<6:04:57,  1.75s/it] 30%|██▉       | 5292/17834 [2:38:36<6:12:07,  1.78s/it] 30%|██▉       | 5293/17834 [2:38:37<6:08:16,  1.76s/it] 30%|██▉       | 5294/17834 [2:38:39<6:07:06,  1.76s/it] 30%|██▉       | 5295/17834 [2:38:41<6:05:07,  1.75s/it] 30%|██▉       | 5296/17834 [2:38:43<6:04:18,  1.74s/it] 30%|██▉       | 5297/17834 [2:38:44<6:04:43,  1.75s/it] 30%|██▉       | 5298/17834 [2:38:46<6:09:03,  1.77s/it] 30%|██▉       | 5299/17834 [2:38:48<6:07:29,  1.76s/it]08/30/2024 21:53:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.7040252685546875, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05085502564907074, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.429851770401001, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.184731960296631}
 30%|██▉       | 5300/17834 [2:38:50<6:09:15,  1.77s/it] 30%|██▉       | 5301/17834 [2:38:51<6:04:36,  1.75s/it] 30%|██▉       | 5302/17834 [2:38:53<6:09:38,  1.77s/it] 30%|██▉       | 5303/17834 [2:38:55<6:05:50,  1.75s/it] 30%|██▉       | 5304/17834 [2:38:57<6:11:08,  1.78s/it] 30%|██▉       | 5305/17834 [2:38:58<6:05:46,  1.75s/it] 30%|██▉       | 5306/17834 [2:39:00<6:04:32,  1.75s/it] 30%|██▉       | 5307/17834 [2:39:02<6:02:01,  1.73s/it] 30%|██▉       | 5308/17834 [2:39:04<6:02:19,  1.74s/it] 30%|██▉       | 5309/17834 [2:39:05<6:09:29,  1.77s/it] 30%|██▉       | 5310/17834 [2:39:07<6:11:30,  1.78s/it] 30%|██▉       | 5311/17834 [2:39:09<6:06:59,  1.76s/it] 30%|██▉       | 5312/17834 [2:39:11<6:04:22,  1.75s/it] 30%|██▉       | 5313/17834 [2:39:12<6:05:24,  1.75s/it] 30%|██▉       | 5314/17834 [2:39:14<6:10:14,  1.77s/it] 30%|██▉       | 5315/17834 [2:39:16<6:12:35,  1.79s/it] 30%|██▉       | 5316/17834 [2:39:18<6:07:20,  1.76s/it] 30%|██▉       | 5317/17834 [2:39:20<6:06:57,  1.76s/it] 30%|██▉       | 5318/17834 [2:39:21<6:12:21,  1.79s/it] 30%|██▉       | 5319/17834 [2:39:23<6:14:42,  1.80s/it] 30%|██▉       | 5320/17834 [2:39:25<6:06:33,  1.76s/it] 30%|██▉       | 5321/17834 [2:39:27<6:09:24,  1.77s/it] 30%|██▉       | 5322/17834 [2:39:28<6:07:10,  1.76s/it] 30%|██▉       | 5323/17834 [2:39:30<6:05:51,  1.75s/it] 30%|██▉       | 5324/17834 [2:39:32<6:08:36,  1.77s/it] 30%|██▉       | 5325/17834 [2:39:34<6:07:48,  1.76s/it] 30%|██▉       | 5326/17834 [2:39:35<6:03:50,  1.75s/it] 30%|██▉       | 5327/17834 [2:39:37<6:01:26,  1.73s/it] 30%|██▉       | 5328/17834 [2:39:39<6:03:55,  1.75s/it] 30%|██▉       | 5329/17834 [2:39:41<6:01:14,  1.73s/it] 30%|██▉       | 5330/17834 [2:39:42<6:04:34,  1.75s/it] 30%|██▉       | 5331/17834 [2:39:44<6:08:25,  1.77s/it] 30%|██▉       | 5332/17834 [2:39:46<6:07:05,  1.76s/it] 30%|██▉       | 5333/17834 [2:39:48<6:11:30,  1.78s/it] 30%|██▉       | 5334/17834 [2:39:50<6:09:52,  1.78s/it] 30%|██▉       | 5335/17834 [2:39:51<6:12:08,  1.79s/it] 30%|██▉       | 5336/17834 [2:39:53<6:08:55,  1.77s/it] 30%|██▉       | 5337/17834 [2:39:55<6:08:00,  1.77s/it] 30%|██▉       | 5338/17834 [2:39:57<6:08:20,  1.77s/it] 30%|██▉       | 5339/17834 [2:39:58<6:11:15,  1.78s/it] 30%|██▉       | 5340/17834 [2:40:00<6:07:42,  1.77s/it] 30%|██▉       | 5341/17834 [2:40:02<6:04:26,  1.75s/it] 30%|██▉       | 5342/17834 [2:40:04<6:09:45,  1.78s/it] 30%|██▉       | 5343/17834 [2:40:05<6:04:54,  1.75s/it] 30%|██▉       | 5344/17834 [2:40:07<6:03:25,  1.75s/it] 30%|██▉       | 5345/17834 [2:40:09<5:59:08,  1.73s/it]08/30/2024 21:54:28 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/30/2024 21:54:28 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:35,  6.23it/s][A
  1%|▏         | 3/221 [00:00<00:44,  4.93it/s][A
  2%|▏         | 4/221 [00:00<00:36,  5.87it/s][A
  2%|▏         | 5/221 [00:01<00:52,  4.15it/s][A
  3%|▎         | 6/221 [00:01<00:48,  4.47it/s][A
  3%|▎         | 7/221 [00:01<00:55,  3.89it/s][A
  4%|▎         | 8/221 [00:01<00:58,  3.62it/s][A
  4%|▍         | 9/221 [00:02<01:10,  2.99it/s][A
  5%|▍         | 10/221 [00:02<01:19,  2.66it/s][A
  5%|▍         | 11/221 [00:02<01:01,  3.40it/s][A
  5%|▌         | 12/221 [00:03<00:54,  3.86it/s][A
  6%|▌         | 13/221 [00:03<00:50,  4.13it/s][A
  6%|▋         | 14/221 [00:03<00:52,  3.98it/s][A
  7%|▋         | 15/221 [00:03<00:52,  3.94it/s][A
  7%|▋         | 16/221 [00:04<00:53,  3.86it/s][A
  8%|▊         | 17/221 [00:04<00:50,  4.02it/s][A
  8%|▊         | 18/221 [00:04<00:43,  4.63it/s][A
  9%|▊         | 19/221 [00:04<00:43,  4.68it/s][A
  9%|▉         | 20/221 [00:04<00:36,  5.47it/s][A
 10%|▉         | 21/221 [00:05<00:39,  5.08it/s][A
 10%|▉         | 22/221 [00:05<00:49,  4.03it/s][A
 10%|█         | 23/221 [00:05<00:49,  4.03it/s][A
 11%|█         | 24/221 [00:05<00:44,  4.40it/s][A
 11%|█▏        | 25/221 [00:06<00:46,  4.22it/s][A
 12%|█▏        | 26/221 [00:06<00:42,  4.59it/s][A
 12%|█▏        | 27/221 [00:06<01:00,  3.22it/s][A
 13%|█▎        | 28/221 [00:07<01:00,  3.21it/s][A
 13%|█▎        | 29/221 [00:07<01:08,  2.82it/s][A
 14%|█▎        | 30/221 [00:08<01:17,  2.48it/s][A
 14%|█▍        | 31/221 [00:08<01:11,  2.64it/s][A
 14%|█▍        | 32/221 [00:08<01:08,  2.76it/s][A
 15%|█▍        | 33/221 [00:08<01:02,  3.01it/s][A
 15%|█▌        | 34/221 [00:09<00:59,  3.14it/s][A
 16%|█▌        | 35/221 [00:09<01:04,  2.87it/s][A
 16%|█▋        | 36/221 [00:09<00:57,  3.19it/s][A
 17%|█▋        | 37/221 [00:10<00:49,  3.70it/s][A
 17%|█▋        | 38/221 [00:10<00:50,  3.61it/s][A
 18%|█▊        | 39/221 [00:10<00:50,  3.58it/s][A
 18%|█▊        | 40/221 [00:10<00:51,  3.53it/s][A
 19%|█▊        | 41/221 [00:11<00:56,  3.16it/s][A
 19%|█▉        | 43/221 [00:11<00:39,  4.51it/s][A
 20%|█▉        | 44/221 [00:11<00:45,  3.90it/s][A
 20%|██        | 45/221 [00:12<00:52,  3.34it/s][A
 21%|██        | 46/221 [00:12<00:48,  3.60it/s][A
 21%|██▏       | 47/221 [00:12<00:42,  4.07it/s][A
 22%|██▏       | 48/221 [00:12<00:38,  4.50it/s][A
 22%|██▏       | 49/221 [00:13<00:38,  4.49it/s][A
 23%|██▎       | 50/221 [00:13<00:43,  3.93it/s][A
 23%|██▎       | 51/221 [00:13<00:45,  3.76it/s][A
 24%|██▎       | 52/221 [00:14<00:45,  3.69it/s][A
 24%|██▍       | 53/221 [00:14<00:40,  4.19it/s][A
 24%|██▍       | 54/221 [00:14<00:40,  4.09it/s][A
 25%|██▍       | 55/221 [00:14<00:40,  4.07it/s][A
 25%|██▌       | 56/221 [00:15<00:46,  3.52it/s][A
 26%|██▌       | 57/221 [00:15<00:47,  3.44it/s][A
 26%|██▌       | 58/221 [00:15<00:41,  3.93it/s][A
 27%|██▋       | 59/221 [00:15<00:35,  4.60it/s][A
 27%|██▋       | 60/221 [00:15<00:33,  4.86it/s][A
 28%|██▊       | 61/221 [00:16<00:42,  3.77it/s][A
 28%|██▊       | 62/221 [00:16<00:40,  3.88it/s][A
 29%|██▊       | 63/221 [00:16<00:39,  4.04it/s][A
 29%|██▉       | 64/221 [00:17<00:47,  3.31it/s][A
 29%|██▉       | 65/221 [00:17<00:52,  2.95it/s][A
 30%|██▉       | 66/221 [00:18<00:58,  2.65it/s][A
 30%|███       | 67/221 [00:18<01:01,  2.51it/s][A
 31%|███       | 68/221 [00:18<00:54,  2.80it/s][A
 31%|███       | 69/221 [00:19<01:06,  2.29it/s][A
 32%|███▏      | 70/221 [00:19<01:02,  2.43it/s][A
 32%|███▏      | 71/221 [00:19<00:53,  2.79it/s][A
 33%|███▎      | 72/221 [00:20<00:53,  2.78it/s][A
 33%|███▎      | 73/221 [00:20<00:48,  3.07it/s][A
 33%|███▎      | 74/221 [00:20<00:38,  3.82it/s][A
 34%|███▍      | 75/221 [00:21<00:40,  3.58it/s][A
 34%|███▍      | 76/221 [00:21<00:51,  2.81it/s][A
 35%|███▍      | 77/221 [00:21<00:53,  2.67it/s][A
 35%|███▌      | 78/221 [00:22<00:52,  2.72it/s][A
 36%|███▌      | 79/221 [00:22<00:52,  2.70it/s][A
 36%|███▌      | 80/221 [00:22<00:44,  3.20it/s][A
 37%|███▋      | 82/221 [00:23<00:34,  3.98it/s][A
 38%|███▊      | 83/221 [00:23<00:42,  3.26it/s][A
 38%|███▊      | 84/221 [00:23<00:38,  3.59it/s][A
 38%|███▊      | 85/221 [00:24<00:35,  3.79it/s][A
 39%|███▉      | 86/221 [00:24<00:44,  3.00it/s][A
 39%|███▉      | 87/221 [00:25<00:50,  2.68it/s][A
 40%|███▉      | 88/221 [00:25<00:46,  2.86it/s][A
 40%|████      | 89/221 [00:25<00:45,  2.88it/s][A
 41%|████      | 90/221 [00:26<00:42,  3.10it/s][A
 41%|████      | 91/221 [00:26<00:36,  3.53it/s][A
 42%|████▏     | 92/221 [00:26<00:38,  3.33it/s][A
 42%|████▏     | 93/221 [00:27<00:45,  2.79it/s][A
 43%|████▎     | 94/221 [00:27<00:41,  3.09it/s][A
 43%|████▎     | 95/221 [00:27<00:38,  3.29it/s][A
 43%|████▎     | 96/221 [00:27<00:34,  3.59it/s][A
 44%|████▍     | 97/221 [00:28<00:32,  3.77it/s][A
 44%|████▍     | 98/221 [00:28<00:36,  3.35it/s][A
 45%|████▍     | 99/221 [00:28<00:34,  3.58it/s][A
 45%|████▌     | 100/221 [00:29<00:38,  3.17it/s][A
 46%|████▌     | 101/221 [00:29<00:36,  3.30it/s][A
 46%|████▌     | 102/221 [00:29<00:37,  3.16it/s][A
 47%|████▋     | 103/221 [00:29<00:34,  3.42it/s][A
 47%|████▋     | 104/221 [00:30<00:41,  2.79it/s][A
 48%|████▊     | 105/221 [00:30<00:41,  2.81it/s][A
 48%|████▊     | 106/221 [00:30<00:36,  3.17it/s][A
 48%|████▊     | 107/221 [00:31<00:36,  3.09it/s][A
 49%|████▉     | 108/221 [00:31<00:34,  3.23it/s][A
 49%|████▉     | 109/221 [00:31<00:30,  3.66it/s][A
 50%|████▉     | 110/221 [00:32<00:29,  3.79it/s][A
 50%|█████     | 111/221 [00:32<00:30,  3.62it/s][A
 51%|█████     | 112/221 [00:32<00:31,  3.43it/s][A
 51%|█████     | 113/221 [00:32<00:28,  3.75it/s][A
 52%|█████▏    | 114/221 [00:33<00:25,  4.20it/s][A
 52%|█████▏    | 115/221 [00:33<00:31,  3.40it/s][A
 52%|█████▏    | 116/221 [00:33<00:28,  3.72it/s][A
 53%|█████▎    | 117/221 [00:33<00:26,  3.94it/s][A
 53%|█████▎    | 118/221 [00:34<00:27,  3.68it/s][A
 54%|█████▍    | 119/221 [00:34<00:30,  3.40it/s][A
 54%|█████▍    | 120/221 [00:34<00:34,  2.92it/s][A
 55%|█████▍    | 121/221 [00:35<00:29,  3.43it/s][A
 55%|█████▌    | 122/221 [00:35<00:29,  3.36it/s][A
 56%|█████▌    | 123/221 [00:35<00:26,  3.77it/s][A
 56%|█████▌    | 124/221 [00:36<00:27,  3.47it/s][A
 57%|█████▋    | 125/221 [00:36<00:30,  3.12it/s][A
 57%|█████▋    | 126/221 [00:36<00:24,  3.90it/s][A
 57%|█████▋    | 127/221 [00:36<00:24,  3.78it/s][A
 58%|█████▊    | 128/221 [00:37<00:25,  3.69it/s][A
 58%|█████▊    | 129/221 [00:37<00:22,  4.11it/s][A
 59%|█████▉    | 130/221 [00:37<00:25,  3.55it/s][A
 59%|█████▉    | 131/221 [00:37<00:26,  3.35it/s][A
 60%|█████▉    | 132/221 [00:38<00:36,  2.46it/s][A
 60%|██████    | 133/221 [00:39<00:35,  2.47it/s][A
 61%|██████    | 134/221 [00:39<00:36,  2.36it/s][A
 61%|██████    | 135/221 [00:39<00:32,  2.67it/s][A
 62%|██████▏   | 136/221 [00:39<00:27,  3.08it/s][A
 62%|██████▏   | 137/221 [00:40<00:24,  3.41it/s][A
 62%|██████▏   | 138/221 [00:40<00:23,  3.53it/s][A
 63%|██████▎   | 139/221 [00:40<00:21,  3.73it/s][A
 63%|██████▎   | 140/221 [00:40<00:20,  3.92it/s][A
 64%|██████▍   | 141/221 [00:41<00:24,  3.30it/s][A
 64%|██████▍   | 142/221 [00:41<00:20,  3.92it/s][A
 65%|██████▍   | 143/221 [00:41<00:23,  3.26it/s][A
 65%|██████▌   | 144/221 [00:42<00:24,  3.12it/s][A
 66%|██████▌   | 145/221 [00:42<00:23,  3.27it/s][A
 66%|██████▌   | 146/221 [00:42<00:21,  3.55it/s][A
 67%|██████▋   | 147/221 [00:43<00:20,  3.57it/s][A
 67%|██████▋   | 148/221 [00:43<00:18,  3.95it/s][A
 67%|██████▋   | 149/221 [00:43<00:20,  3.57it/s][A
 68%|██████▊   | 150/221 [00:43<00:16,  4.36it/s][A
 68%|██████▊   | 151/221 [00:44<00:22,  3.17it/s][A
 69%|██████▉   | 152/221 [00:44<00:23,  2.94it/s][A
 69%|██████▉   | 153/221 [00:44<00:19,  3.58it/s][A
 70%|██████▉   | 154/221 [00:45<00:19,  3.35it/s][A
 70%|███████   | 155/221 [00:45<00:20,  3.15it/s][A
 71%|███████   | 156/221 [00:45<00:20,  3.16it/s][A
 71%|███████   | 157/221 [00:46<00:20,  3.16it/s][A
 71%|███████▏  | 158/221 [00:46<00:23,  2.66it/s][A
 72%|███████▏  | 159/221 [00:46<00:20,  3.04it/s][A
 72%|███████▏  | 160/221 [00:47<00:19,  3.07it/s][A
 73%|███████▎  | 161/221 [00:47<00:24,  2.47it/s][A
 73%|███████▎  | 162/221 [00:48<00:23,  2.55it/s][A
 74%|███████▍  | 163/221 [00:48<00:22,  2.53it/s][A
 74%|███████▍  | 164/221 [00:48<00:17,  3.17it/s][A
 75%|███████▍  | 165/221 [00:48<00:16,  3.38it/s][A
 75%|███████▌  | 166/221 [00:49<00:15,  3.53it/s][A
 76%|███████▌  | 167/221 [00:49<00:14,  3.74it/s][A
 76%|███████▌  | 168/221 [00:49<00:15,  3.49it/s][A
 76%|███████▋  | 169/221 [00:49<00:14,  3.71it/s][A
 77%|███████▋  | 170/221 [00:50<00:16,  3.16it/s][A
 77%|███████▋  | 171/221 [00:50<00:15,  3.23it/s][A
 78%|███████▊  | 172/221 [00:50<00:13,  3.53it/s][A
 78%|███████▊  | 173/221 [00:51<00:14,  3.22it/s][A
 79%|███████▊  | 174/221 [00:51<00:14,  3.23it/s][A
 79%|███████▉  | 175/221 [00:51<00:16,  2.81it/s][A
 80%|███████▉  | 176/221 [00:52<00:13,  3.25it/s][A
 80%|████████  | 177/221 [00:52<00:12,  3.62it/s][A
 81%|████████  | 178/221 [00:52<00:11,  3.69it/s][A
 81%|████████  | 179/221 [00:52<00:11,  3.72it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.77it/s][A
 82%|████████▏ | 181/221 [00:53<00:10,  3.72it/s][A
 82%|████████▏ | 182/221 [00:53<00:10,  3.70it/s][A
 83%|████████▎ | 183/221 [00:54<00:12,  3.00it/s][A
 83%|████████▎ | 184/221 [00:54<00:12,  2.85it/s][A
 84%|████████▎ | 185/221 [00:54<00:11,  3.15it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.25it/s][A
 85%|████████▍ | 187/221 [00:55<00:11,  3.03it/s][A
 85%|████████▌ | 188/221 [00:55<00:10,  3.09it/s][A
 86%|████████▌ | 189/221 [00:55<00:08,  3.85it/s][A
 86%|████████▌ | 190/221 [00:56<00:08,  3.63it/s][A
 86%|████████▋ | 191/221 [00:56<00:08,  3.70it/s][A
 87%|████████▋ | 192/221 [00:56<00:07,  3.65it/s][A
 87%|████████▋ | 193/221 [00:56<00:07,  3.79it/s][A
 88%|████████▊ | 194/221 [00:57<00:06,  3.92it/s][A
 88%|████████▊ | 195/221 [00:57<00:07,  3.66it/s][A
 89%|████████▊ | 196/221 [00:57<00:08,  3.06it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.09it/s][A
 90%|████████▉ | 198/221 [00:58<00:07,  3.08it/s][A
 90%|█████████ | 199/221 [00:58<00:05,  3.70it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.75it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.44it/s][A
 91%|█████████▏| 202/221 [00:59<00:04,  3.82it/s][A
 92%|█████████▏| 203/221 [00:59<00:04,  4.30it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.81it/s][A
 93%|█████████▎| 205/221 [01:00<00:04,  3.83it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.17it/s][A
 94%|█████████▎| 207/221 [01:00<00:03,  3.81it/s][A
 94%|█████████▍| 208/221 [01:00<00:03,  4.25it/s][A
 95%|█████████▍| 209/221 [01:01<00:02,  4.52it/s][A
 95%|█████████▌| 210/221 [01:01<00:02,  4.83it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  4.52it/s][A
 96%|█████████▌| 212/221 [01:01<00:02,  3.93it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.85it/s][A
 97%|█████████▋| 214/221 [01:02<00:02,  3.25it/s][A
 98%|█████████▊| 216/221 [01:02<00:01,  4.07it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.71it/s][A
 99%|█████████▊| 218/221 [01:03<00:00,  3.56it/s][A
 99%|█████████▉| 219/221 [01:03<00:00,  3.44it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.85it/s][A
100%|██████████| 221/221 [01:04<00:00,  4.23it/s][A100%|██████████| 221/221 [01:04<00:00,  3.44it/s]
08/30/2024 21:56:52 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 5345--===========

08/30/2024 21:56:52 - INFO - __main__ -   {'area_r1': 5.3, 'area_recall': '5.3/15.3/20.2', 'area_ravg': 13.6}
08/30/2024 21:56:52 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 5345--===========

08/30/2024 21:56:52 - INFO - __main__ -   {'area_r1': 41.7, 'area_recall': '41.7/73.2/83.0', 'area_ravg': 66.0}
08/30/2024 21:56:52 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 5345--===========

08/30/2024 21:56:52 - INFO - __main__ -   {'video_r1': 36.2, 'video_recall': '36.2/68.3/79.0', 'video_ravg': 61.2}
08/30/2024 21:56:52 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/30/2024 21:56:52 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/30/2024 21:56:52 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 5345--===========

08/30/2024 21:56:52 - INFO - __main__ -   {'video_r1': 56.2, 'video_recall': '56.2/79.1/86.0', 'video_ravg': 73.8}
08/30/2024 21:56:52 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 3563=======

08/30/2024 21:56:52 - INFO - __main__ -   {'video_r1': 56.8, 'video_recall': '56.8/79.4/86.0', 'video_ravg': 74.1}
 30%|██▉       | 5346/17834 [2:42:54<176:27:08, 50.87s/it] 30%|██▉       | 5347/17834 [2:42:56<125:15:45, 36.11s/it] 30%|██▉       | 5348/17834 [2:42:58<89:29:31, 25.80s/it]  30%|██▉       | 5349/17834 [2:43:00<64:29:33, 18.60s/it]08/30/2024 21:57:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3523247241973877, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.041473738849163055, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.412369966506958, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.806168556213379}
 30%|██▉       | 5350/17834 [2:43:01<46:57:27, 13.54s/it] 30%|███       | 5351/17834 [2:43:03<34:43:40, 10.02s/it] 30%|███       | 5352/17834 [2:43:05<26:07:20,  7.53s/it] 30%|███       | 5353/17834 [2:43:07<20:05:47,  5.80s/it] 30%|███       | 5354/17834 [2:43:08<15:48:42,  4.56s/it] 30%|███       | 5355/17834 [2:43:10<12:51:46,  3.71s/it] 30%|███       | 5356/17834 [2:43:12<10:48:52,  3.12s/it] 30%|███       | 5357/17834 [2:43:13<9:23:17,  2.71s/it]  30%|███       | 5358/17834 [2:43:15<8:24:02,  2.42s/it] 30%|███       | 5359/17834 [2:43:17<7:37:10,  2.20s/it] 30%|███       | 5360/17834 [2:43:19<7:11:52,  2.08s/it] 30%|███       | 5361/17834 [2:43:20<6:48:49,  1.97s/it] 30%|███       | 5362/17834 [2:43:22<6:37:06,  1.91s/it] 30%|███       | 5363/17834 [2:43:24<6:29:53,  1.88s/it] 30%|███       | 5364/17834 [2:43:26<6:19:57,  1.83s/it] 30%|███       | 5365/17834 [2:43:27<6:16:07,  1.81s/it] 30%|███       | 5366/17834 [2:43:29<6:12:53,  1.79s/it] 30%|███       | 5367/17834 [2:43:31<6:11:38,  1.79s/it] 30%|███       | 5368/17834 [2:43:33<6:09:04,  1.78s/it] 30%|███       | 5369/17834 [2:43:34<6:06:37,  1.76s/it] 30%|███       | 5370/17834 [2:43:36<6:05:22,  1.76s/it] 30%|███       | 5371/17834 [2:43:38<6:05:41,  1.76s/it] 30%|███       | 5372/17834 [2:43:40<6:02:35,  1.75s/it] 30%|███       | 5373/17834 [2:43:41<6:02:36,  1.75s/it] 30%|███       | 5374/17834 [2:43:43<6:00:58,  1.74s/it] 30%|███       | 5375/17834 [2:43:45<6:02:00,  1.74s/it] 30%|███       | 5376/17834 [2:43:47<6:03:26,  1.75s/it] 30%|███       | 5377/17834 [2:43:48<6:01:56,  1.74s/it] 30%|███       | 5378/17834 [2:43:50<5:58:46,  1.73s/it] 30%|███       | 5379/17834 [2:43:52<5:58:41,  1.73s/it] 30%|███       | 5380/17834 [2:43:54<6:03:26,  1.75s/it] 30%|███       | 5381/17834 [2:43:55<6:05:00,  1.76s/it] 30%|███       | 5382/17834 [2:43:57<6:02:32,  1.75s/it] 30%|███       | 5383/17834 [2:43:59<6:01:24,  1.74s/it] 30%|███       | 5384/17834 [2:44:01<6:05:04,  1.76s/it] 30%|███       | 5385/17834 [2:44:02<6:04:48,  1.76s/it] 30%|███       | 5386/17834 [2:44:04<6:07:07,  1.77s/it] 30%|███       | 5387/17834 [2:44:06<6:02:48,  1.75s/it] 30%|███       | 5388/17834 [2:44:08<6:02:05,  1.75s/it] 30%|███       | 5389/17834 [2:44:09<5:58:48,  1.73s/it] 30%|███       | 5390/17834 [2:44:11<5:59:59,  1.74s/it] 30%|███       | 5391/17834 [2:44:13<6:01:42,  1.74s/it] 30%|███       | 5392/17834 [2:44:15<6:02:59,  1.75s/it] 30%|███       | 5393/17834 [2:44:16<6:03:57,  1.76s/it] 30%|███       | 5394/17834 [2:44:18<6:02:13,  1.75s/it] 30%|███       | 5395/17834 [2:44:20<6:02:01,  1.75s/it] 30%|███       | 5396/17834 [2:44:22<6:06:56,  1.77s/it] 30%|███       | 5397/17834 [2:44:23<6:06:19,  1.77s/it] 30%|███       | 5398/17834 [2:44:25<6:06:13,  1.77s/it] 30%|███       | 5399/17834 [2:44:27<6:04:58,  1.76s/it]08/30/2024 21:58:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4537431001663208, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.059362828731536865, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4017539024353027, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9148597717285156}
 30%|███       | 5400/17834 [2:44:29<6:03:54,  1.76s/it] 30%|███       | 5401/17834 [2:44:30<6:02:39,  1.75s/it] 30%|███       | 5402/17834 [2:44:32<6:02:35,  1.75s/it] 30%|███       | 5403/17834 [2:44:34<6:02:31,  1.75s/it] 30%|███       | 5404/17834 [2:44:36<6:05:01,  1.76s/it] 30%|███       | 5405/17834 [2:44:37<6:03:17,  1.75s/it] 30%|███       | 5406/17834 [2:44:39<6:04:18,  1.76s/it] 30%|███       | 5407/17834 [2:44:41<6:04:56,  1.76s/it] 30%|███       | 5408/17834 [2:44:43<6:01:39,  1.75s/it] 30%|███       | 5409/17834 [2:44:44<6:00:10,  1.74s/it] 30%|███       | 5410/17834 [2:44:46<6:05:17,  1.76s/it] 30%|███       | 5411/17834 [2:44:48<6:02:10,  1.75s/it] 30%|███       | 5412/17834 [2:44:50<6:00:07,  1.74s/it] 30%|███       | 5413/17834 [2:44:51<5:59:21,  1.74s/it] 30%|███       | 5414/17834 [2:44:53<5:58:32,  1.73s/it] 30%|███       | 5415/17834 [2:44:55<6:00:30,  1.74s/it] 30%|███       | 5416/17834 [2:44:57<6:02:25,  1.75s/it] 30%|███       | 5417/17834 [2:44:58<5:58:47,  1.73s/it] 30%|███       | 5418/17834 [2:45:00<6:01:42,  1.75s/it] 30%|███       | 5419/17834 [2:45:02<6:02:14,  1.75s/it] 30%|███       | 5420/17834 [2:45:04<5:56:34,  1.72s/it] 30%|███       | 5421/17834 [2:45:05<5:58:20,  1.73s/it] 30%|███       | 5422/17834 [2:45:07<5:59:34,  1.74s/it] 30%|███       | 5423/17834 [2:45:09<6:01:56,  1.75s/it] 30%|███       | 5424/17834 [2:45:11<6:02:15,  1.75s/it] 30%|███       | 5425/17834 [2:45:12<5:59:28,  1.74s/it] 30%|███       | 5426/17834 [2:45:14<5:57:13,  1.73s/it] 30%|███       | 5427/17834 [2:45:16<5:57:29,  1.73s/it] 30%|███       | 5428/17834 [2:45:18<6:06:49,  1.77s/it] 30%|███       | 5429/17834 [2:45:19<6:02:20,  1.75s/it] 30%|███       | 5430/17834 [2:45:21<6:00:49,  1.75s/it] 30%|███       | 5431/17834 [2:45:23<5:59:51,  1.74s/it] 30%|███       | 5432/17834 [2:45:25<6:01:46,  1.75s/it] 30%|███       | 5433/17834 [2:45:26<6:03:42,  1.76s/it] 30%|███       | 5434/17834 [2:45:28<6:02:37,  1.75s/it] 30%|███       | 5435/17834 [2:45:30<6:01:26,  1.75s/it] 30%|███       | 5436/17834 [2:45:32<5:58:48,  1.74s/it] 30%|███       | 5437/17834 [2:45:33<5:56:30,  1.73s/it] 30%|███       | 5438/17834 [2:45:35<5:54:34,  1.72s/it] 30%|███       | 5439/17834 [2:45:37<5:54:48,  1.72s/it] 31%|███       | 5440/17834 [2:45:38<5:59:49,  1.74s/it] 31%|███       | 5441/17834 [2:45:40<6:03:46,  1.76s/it] 31%|███       | 5442/17834 [2:45:42<6:00:54,  1.75s/it] 31%|███       | 5443/17834 [2:45:44<5:57:08,  1.73s/it] 31%|███       | 5444/17834 [2:45:45<5:56:40,  1.73s/it] 31%|███       | 5445/17834 [2:45:47<6:00:58,  1.75s/it] 31%|███       | 5446/17834 [2:45:49<6:03:38,  1.76s/it] 31%|███       | 5447/17834 [2:45:51<6:00:12,  1.74s/it] 31%|███       | 5448/17834 [2:45:52<5:57:57,  1.73s/it] 31%|███       | 5449/17834 [2:45:54<5:57:19,  1.73s/it]08/30/2024 22:00:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5795618295669556, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04971444234251976, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.355813980102539, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9850902557373047}
 31%|███       | 5450/17834 [2:45:56<5:58:24,  1.74s/it] 31%|███       | 5451/17834 [2:45:58<5:59:25,  1.74s/it] 31%|███       | 5452/17834 [2:45:59<6:00:41,  1.75s/it] 31%|███       | 5453/17834 [2:46:01<6:03:41,  1.76s/it] 31%|███       | 5454/17834 [2:46:03<6:00:44,  1.75s/it] 31%|███       | 5455/17834 [2:46:05<6:00:42,  1.75s/it] 31%|███       | 5456/17834 [2:46:06<5:58:38,  1.74s/it] 31%|███       | 5457/17834 [2:46:08<6:02:55,  1.76s/it] 31%|███       | 5458/17834 [2:46:10<6:01:18,  1.75s/it] 31%|███       | 5459/17834 [2:46:12<6:02:12,  1.76s/it] 31%|███       | 5460/17834 [2:46:13<5:59:51,  1.74s/it] 31%|███       | 5461/17834 [2:46:15<5:58:46,  1.74s/it] 31%|███       | 5462/17834 [2:46:17<5:56:21,  1.73s/it] 31%|███       | 5463/17834 [2:46:19<6:04:39,  1.77s/it] 31%|███       | 5464/17834 [2:46:20<5:57:40,  1.73s/it] 31%|███       | 5465/17834 [2:46:22<6:00:38,  1.75s/it] 31%|███       | 5466/17834 [2:46:24<6:05:33,  1.77s/it] 31%|███       | 5467/17834 [2:46:26<6:00:08,  1.75s/it] 31%|███       | 5468/17834 [2:46:27<5:58:16,  1.74s/it] 31%|███       | 5469/17834 [2:46:29<6:01:28,  1.75s/it] 31%|███       | 5470/17834 [2:46:31<5:58:20,  1.74s/it] 31%|███       | 5471/17834 [2:46:33<6:03:23,  1.76s/it] 31%|███       | 5472/17834 [2:46:34<6:04:58,  1.77s/it] 31%|███       | 5473/17834 [2:46:36<6:02:55,  1.76s/it] 31%|███       | 5474/17834 [2:46:38<6:01:55,  1.76s/it] 31%|███       | 5475/17834 [2:46:40<5:57:43,  1.74s/it] 31%|███       | 5476/17834 [2:46:41<6:02:24,  1.76s/it] 31%|███       | 5477/17834 [2:46:43<6:00:45,  1.75s/it] 31%|███       | 5478/17834 [2:46:45<5:58:08,  1.74s/it] 31%|███       | 5479/17834 [2:46:47<5:58:54,  1.74s/it] 31%|███       | 5480/17834 [2:46:48<5:57:07,  1.73s/it] 31%|███       | 5481/17834 [2:46:50<6:02:23,  1.76s/it] 31%|███       | 5482/17834 [2:46:52<6:01:54,  1.76s/it] 31%|███       | 5483/17834 [2:46:54<6:00:30,  1.75s/it] 31%|███       | 5484/17834 [2:46:55<6:01:22,  1.76s/it] 31%|███       | 5485/17834 [2:46:57<6:09:37,  1.80s/it] 31%|███       | 5486/17834 [2:46:59<6:06:56,  1.78s/it] 31%|███       | 5487/17834 [2:47:01<6:12:11,  1.81s/it] 31%|███       | 5488/17834 [2:47:03<6:06:32,  1.78s/it] 31%|███       | 5489/17834 [2:47:04<6:05:14,  1.78s/it] 31%|███       | 5490/17834 [2:47:06<6:03:07,  1.77s/it] 31%|███       | 5491/17834 [2:47:08<6:04:50,  1.77s/it] 31%|███       | 5492/17834 [2:47:10<6:03:04,  1.77s/it] 31%|███       | 5493/17834 [2:47:11<5:59:43,  1.75s/it] 31%|███       | 5494/17834 [2:47:13<6:00:48,  1.75s/it] 31%|███       | 5495/17834 [2:47:15<6:01:34,  1.76s/it] 31%|███       | 5496/17834 [2:47:17<5:58:11,  1.74s/it] 31%|███       | 5497/17834 [2:47:18<6:01:40,  1.76s/it] 31%|███       | 5498/17834 [2:47:20<5:57:07,  1.74s/it] 31%|███       | 5499/17834 [2:47:22<5:54:12,  1.72s/it]08/30/2024 22:01:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5171699523925781, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03811294212937355, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.243365526199341, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7986483573913574}
 31%|███       | 5500/17834 [2:47:24<5:54:19,  1.72s/it] 31%|███       | 5501/17834 [2:47:25<5:56:07,  1.73s/it] 31%|███       | 5502/17834 [2:47:27<5:58:18,  1.74s/it] 31%|███       | 5503/17834 [2:47:29<5:57:52,  1.74s/it] 31%|███       | 5504/17834 [2:47:31<5:58:34,  1.74s/it] 31%|███       | 5505/17834 [2:47:32<6:03:21,  1.77s/it] 31%|███       | 5506/17834 [2:47:34<6:04:42,  1.77s/it] 31%|███       | 5507/17834 [2:47:36<6:02:37,  1.77s/it] 31%|███       | 5508/17834 [2:47:38<6:07:00,  1.79s/it] 31%|███       | 5509/17834 [2:47:39<6:03:54,  1.77s/it] 31%|███       | 5510/17834 [2:47:41<6:02:45,  1.77s/it] 31%|███       | 5511/17834 [2:47:43<6:00:36,  1.76s/it] 31%|███       | 5512/17834 [2:47:45<6:00:36,  1.76s/it] 31%|███       | 5513/17834 [2:47:46<6:00:50,  1.76s/it] 31%|███       | 5514/17834 [2:47:48<5:57:03,  1.74s/it] 31%|███       | 5515/17834 [2:47:50<6:00:15,  1.75s/it] 31%|███       | 5516/17834 [2:47:52<5:58:21,  1.75s/it] 31%|███       | 5517/17834 [2:47:53<5:54:35,  1.73s/it] 31%|███       | 5518/17834 [2:47:55<5:57:46,  1.74s/it] 31%|███       | 5519/17834 [2:47:57<5:57:00,  1.74s/it] 31%|███       | 5520/17834 [2:47:59<6:00:36,  1.76s/it] 31%|███       | 5521/17834 [2:48:00<6:01:01,  1.76s/it] 31%|███       | 5522/17834 [2:48:02<5:58:35,  1.75s/it] 31%|███       | 5523/17834 [2:48:04<5:59:41,  1.75s/it] 31%|███       | 5524/17834 [2:48:06<6:07:50,  1.79s/it] 31%|███       | 5525/17834 [2:48:08<6:01:40,  1.76s/it] 31%|███       | 5526/17834 [2:48:09<6:02:34,  1.77s/it] 31%|███       | 5527/17834 [2:48:11<6:03:12,  1.77s/it] 31%|███       | 5528/17834 [2:48:13<6:03:15,  1.77s/it] 31%|███       | 5529/17834 [2:48:15<5:58:11,  1.75s/it] 31%|███       | 5530/17834 [2:48:16<5:56:34,  1.74s/it] 31%|███       | 5531/17834 [2:48:18<5:54:37,  1.73s/it] 31%|███       | 5532/17834 [2:48:20<5:56:41,  1.74s/it] 31%|███       | 5533/17834 [2:48:21<5:55:50,  1.74s/it] 31%|███       | 5534/17834 [2:48:23<5:55:36,  1.73s/it] 31%|███       | 5535/17834 [2:48:25<5:55:46,  1.74s/it] 31%|███       | 5536/17834 [2:48:27<5:58:36,  1.75s/it] 31%|███       | 5537/17834 [2:48:28<5:58:11,  1.75s/it] 31%|███       | 5538/17834 [2:48:30<5:56:22,  1.74s/it] 31%|███       | 5539/17834 [2:48:32<5:59:14,  1.75s/it] 31%|███       | 5540/17834 [2:48:34<6:02:30,  1.77s/it] 31%|███       | 5541/17834 [2:48:36<6:00:38,  1.76s/it] 31%|███       | 5542/17834 [2:48:37<6:01:38,  1.77s/it] 31%|███       | 5543/17834 [2:48:39<6:03:14,  1.77s/it] 31%|███       | 5544/17834 [2:48:41<5:57:29,  1.75s/it] 31%|███       | 5545/17834 [2:48:42<5:57:17,  1.74s/it] 31%|███       | 5546/17834 [2:48:44<5:59:13,  1.75s/it] 31%|███       | 5547/17834 [2:48:46<5:56:53,  1.74s/it] 31%|███       | 5548/17834 [2:48:48<5:55:22,  1.74s/it] 31%|███       | 5549/17834 [2:48:49<5:57:00,  1.74s/it]08/30/2024 22:03:09 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2383201122283936, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04159846156835556, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.255366086959839, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5352845191955566}
 31%|███       | 5550/17834 [2:48:51<5:54:30,  1.73s/it] 31%|███       | 5551/17834 [2:48:53<6:00:03,  1.76s/it] 31%|███       | 5552/17834 [2:48:55<5:59:15,  1.76s/it] 31%|███       | 5553/17834 [2:48:56<5:55:42,  1.74s/it] 31%|███       | 5554/17834 [2:48:58<5:56:43,  1.74s/it] 31%|███       | 5555/17834 [2:49:00<6:03:49,  1.78s/it] 31%|███       | 5556/17834 [2:49:02<6:00:17,  1.76s/it] 31%|███       | 5557/17834 [2:49:03<5:57:05,  1.75s/it] 31%|███       | 5558/17834 [2:49:05<5:57:37,  1.75s/it] 31%|███       | 5559/17834 [2:49:07<5:56:14,  1.74s/it] 31%|███       | 5560/17834 [2:49:09<6:02:53,  1.77s/it] 31%|███       | 5561/17834 [2:49:11<6:00:29,  1.76s/it] 31%|███       | 5562/17834 [2:49:12<6:00:56,  1.76s/it] 31%|███       | 5563/17834 [2:49:14<6:03:21,  1.78s/it] 31%|███       | 5564/17834 [2:49:16<5:59:29,  1.76s/it] 31%|███       | 5565/17834 [2:49:18<6:04:25,  1.78s/it] 31%|███       | 5566/17834 [2:49:19<5:59:33,  1.76s/it] 31%|███       | 5567/17834 [2:49:21<5:58:59,  1.76s/it] 31%|███       | 5568/17834 [2:49:23<5:59:11,  1.76s/it] 31%|███       | 5569/17834 [2:49:25<5:56:28,  1.74s/it] 31%|███       | 5570/17834 [2:49:26<5:55:52,  1.74s/it] 31%|███       | 5571/17834 [2:49:28<5:58:33,  1.75s/it] 31%|███       | 5572/17834 [2:49:30<5:58:59,  1.76s/it] 31%|███       | 5573/17834 [2:49:32<6:08:26,  1.80s/it] 31%|███▏      | 5574/17834 [2:49:34<6:03:43,  1.78s/it] 31%|███▏      | 5575/17834 [2:49:35<6:01:13,  1.77s/it] 31%|███▏      | 5576/17834 [2:49:37<6:03:24,  1.78s/it] 31%|███▏      | 5577/17834 [2:49:39<6:01:18,  1.77s/it] 31%|███▏      | 5578/17834 [2:49:41<5:58:12,  1.75s/it] 31%|███▏      | 5579/17834 [2:49:42<5:56:47,  1.75s/it] 31%|███▏      | 5580/17834 [2:49:44<5:55:44,  1.74s/it] 31%|███▏      | 5581/17834 [2:49:46<5:57:11,  1.75s/it] 31%|███▏      | 5582/17834 [2:49:47<5:55:00,  1.74s/it] 31%|███▏      | 5583/17834 [2:49:49<5:58:25,  1.76s/it] 31%|███▏      | 5584/17834 [2:49:51<5:57:13,  1.75s/it] 31%|███▏      | 5585/17834 [2:49:53<6:03:55,  1.78s/it] 31%|███▏      | 5586/17834 [2:49:55<6:02:24,  1.78s/it] 31%|███▏      | 5587/17834 [2:49:56<5:57:20,  1.75s/it] 31%|███▏      | 5588/17834 [2:49:58<5:57:36,  1.75s/it] 31%|███▏      | 5589/17834 [2:50:00<5:58:23,  1.76s/it] 31%|███▏      | 5590/17834 [2:50:02<5:57:32,  1.75s/it] 31%|███▏      | 5591/17834 [2:50:03<5:57:57,  1.75s/it] 31%|███▏      | 5592/17834 [2:50:05<5:56:05,  1.75s/it] 31%|███▏      | 5593/17834 [2:50:07<5:56:36,  1.75s/it] 31%|███▏      | 5594/17834 [2:50:09<5:56:19,  1.75s/it] 31%|███▏      | 5595/17834 [2:50:10<5:54:42,  1.74s/it] 31%|███▏      | 5596/17834 [2:50:12<5:54:43,  1.74s/it] 31%|███▏      | 5597/17834 [2:50:14<5:56:58,  1.75s/it] 31%|███▏      | 5598/17834 [2:50:16<5:56:00,  1.75s/it] 31%|███▏      | 5599/17834 [2:50:17<6:00:11,  1.77s/it]08/30/2024 22:04:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3487308025360107, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03997066989541054, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1270339488983154, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.515735387802124}
 31%|███▏      | 5600/17834 [2:50:19<5:57:03,  1.75s/it] 31%|███▏      | 5601/17834 [2:50:21<5:59:53,  1.77s/it] 31%|███▏      | 5602/17834 [2:50:23<5:55:26,  1.74s/it] 31%|███▏      | 5603/17834 [2:50:24<5:56:29,  1.75s/it] 31%|███▏      | 5604/17834 [2:50:26<5:59:57,  1.77s/it] 31%|███▏      | 5605/17834 [2:50:28<5:56:23,  1.75s/it] 31%|███▏      | 5606/17834 [2:50:30<5:53:45,  1.74s/it] 31%|███▏      | 5607/17834 [2:50:31<5:51:29,  1.72s/it] 31%|███▏      | 5608/17834 [2:50:33<5:54:21,  1.74s/it] 31%|███▏      | 5609/17834 [2:50:35<5:51:56,  1.73s/it] 31%|███▏      | 5610/17834 [2:50:36<5:53:59,  1.74s/it] 31%|███▏      | 5611/17834 [2:50:38<5:50:00,  1.72s/it] 31%|███▏      | 5612/17834 [2:50:40<5:58:32,  1.76s/it] 31%|███▏      | 5613/17834 [2:50:42<5:57:17,  1.75s/it] 31%|███▏      | 5614/17834 [2:50:44<5:58:39,  1.76s/it] 31%|███▏      | 5615/17834 [2:50:45<5:57:46,  1.76s/it] 31%|███▏      | 5616/17834 [2:50:47<5:55:03,  1.74s/it] 31%|███▏      | 5617/17834 [2:50:49<6:00:33,  1.77s/it] 32%|███▏      | 5618/17834 [2:50:51<6:00:22,  1.77s/it] 32%|███▏      | 5619/17834 [2:50:52<5:58:52,  1.76s/it] 32%|███▏      | 5620/17834 [2:50:54<5:59:54,  1.77s/it] 32%|███▏      | 5621/17834 [2:50:56<5:57:41,  1.76s/it] 32%|███▏      | 5622/17834 [2:50:58<6:05:06,  1.79s/it] 32%|███▏      | 5623/17834 [2:50:59<5:58:24,  1.76s/it] 32%|███▏      | 5624/17834 [2:51:01<5:54:29,  1.74s/it] 32%|███▏      | 5625/17834 [2:51:03<5:58:38,  1.76s/it] 32%|███▏      | 5626/17834 [2:51:05<5:57:44,  1.76s/it] 32%|███▏      | 5627/17834 [2:51:06<5:58:05,  1.76s/it] 32%|███▏      | 5628/17834 [2:51:08<5:57:32,  1.76s/it] 32%|███▏      | 5629/17834 [2:51:10<6:00:08,  1.77s/it] 32%|███▏      | 5630/17834 [2:51:12<6:00:23,  1.77s/it] 32%|███▏      | 5631/17834 [2:51:13<5:55:04,  1.75s/it] 32%|███▏      | 5632/17834 [2:51:15<5:56:24,  1.75s/it] 32%|███▏      | 5633/17834 [2:51:17<5:57:00,  1.76s/it] 32%|███▏      | 5634/17834 [2:51:19<5:53:07,  1.74s/it] 32%|███▏      | 5635/17834 [2:51:20<5:53:08,  1.74s/it] 32%|███▏      | 5636/17834 [2:51:22<5:51:41,  1.73s/it] 32%|███▏      | 5637/17834 [2:51:24<5:49:49,  1.72s/it] 32%|███▏      | 5638/17834 [2:51:26<5:57:42,  1.76s/it] 32%|███▏      | 5639/17834 [2:51:27<5:56:05,  1.75s/it] 32%|███▏      | 5640/17834 [2:51:29<5:57:50,  1.76s/it] 32%|███▏      | 5641/17834 [2:51:31<6:01:08,  1.78s/it] 32%|███▏      | 5642/17834 [2:51:33<6:00:01,  1.77s/it] 32%|███▏      | 5643/17834 [2:51:34<5:58:32,  1.76s/it] 32%|███▏      | 5644/17834 [2:51:36<5:56:30,  1.75s/it] 32%|███▏      | 5645/17834 [2:51:38<5:52:52,  1.74s/it] 32%|███▏      | 5646/17834 [2:51:40<5:56:51,  1.76s/it] 32%|███▏      | 5647/17834 [2:51:41<5:54:30,  1.75s/it] 32%|███▏      | 5648/17834 [2:51:43<6:00:45,  1.78s/it] 32%|███▏      | 5649/17834 [2:51:45<5:59:56,  1.77s/it]08/30/2024 22:06:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1738049983978271, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030550412833690643, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.223233461380005, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.427588939666748}
 32%|███▏      | 5650/17834 [2:51:47<5:54:09,  1.74s/it] 32%|███▏      | 5651/17834 [2:51:48<5:54:42,  1.75s/it] 32%|███▏      | 5652/17834 [2:51:50<5:53:20,  1.74s/it] 32%|███▏      | 5653/17834 [2:51:52<5:51:35,  1.73s/it] 32%|███▏      | 5654/17834 [2:51:54<5:57:15,  1.76s/it] 32%|███▏      | 5655/17834 [2:51:55<5:50:41,  1.73s/it] 32%|███▏      | 5656/17834 [2:51:57<5:59:54,  1.77s/it] 32%|███▏      | 5657/17834 [2:51:59<5:55:04,  1.75s/it] 32%|███▏      | 5658/17834 [2:52:01<5:53:13,  1.74s/it] 32%|███▏      | 5659/17834 [2:52:02<5:52:51,  1.74s/it] 32%|███▏      | 5660/17834 [2:52:04<5:54:01,  1.74s/it] 32%|███▏      | 5661/17834 [2:52:06<5:59:47,  1.77s/it] 32%|███▏      | 5662/17834 [2:52:08<6:06:26,  1.81s/it] 32%|███▏      | 5663/17834 [2:52:10<5:59:26,  1.77s/it] 32%|███▏      | 5664/17834 [2:52:11<5:56:13,  1.76s/it] 32%|███▏      | 5665/17834 [2:52:13<5:53:47,  1.74s/it] 32%|███▏      | 5666/17834 [2:52:15<5:55:02,  1.75s/it] 32%|███▏      | 5667/17834 [2:52:17<5:55:36,  1.75s/it] 32%|███▏      | 5668/17834 [2:52:18<5:54:22,  1.75s/it] 32%|███▏      | 5669/17834 [2:52:20<5:58:04,  1.77s/it] 32%|███▏      | 5670/17834 [2:52:22<5:51:53,  1.74s/it] 32%|███▏      | 5671/17834 [2:52:23<5:50:31,  1.73s/it] 32%|███▏      | 5672/17834 [2:52:25<5:50:55,  1.73s/it] 32%|███▏      | 5673/17834 [2:52:27<5:51:34,  1.73s/it] 32%|███▏      | 5674/17834 [2:52:29<5:53:22,  1.74s/it] 32%|███▏      | 5675/17834 [2:52:30<5:51:48,  1.74s/it] 32%|███▏      | 5676/17834 [2:52:32<5:55:55,  1.76s/it] 32%|███▏      | 5677/17834 [2:52:34<5:58:22,  1.77s/it] 32%|███▏      | 5678/17834 [2:52:36<5:58:39,  1.77s/it] 32%|███▏      | 5679/17834 [2:52:38<6:04:24,  1.80s/it] 32%|███▏      | 5680/17834 [2:52:39<5:58:51,  1.77s/it] 32%|███▏      | 5681/17834 [2:52:41<5:57:04,  1.76s/it] 32%|███▏      | 5682/17834 [2:52:43<5:55:26,  1.75s/it] 32%|███▏      | 5683/17834 [2:52:45<5:54:24,  1.75s/it] 32%|███▏      | 5684/17834 [2:52:46<5:50:41,  1.73s/it] 32%|███▏      | 5685/17834 [2:52:48<5:49:43,  1.73s/it] 32%|███▏      | 5686/17834 [2:52:50<5:51:59,  1.74s/it] 32%|███▏      | 5687/17834 [2:52:52<5:50:58,  1.73s/it] 32%|███▏      | 5688/17834 [2:52:53<5:52:46,  1.74s/it] 32%|███▏      | 5689/17834 [2:52:55<5:54:48,  1.75s/it] 32%|███▏      | 5690/17834 [2:52:57<5:51:05,  1.73s/it] 32%|███▏      | 5691/17834 [2:52:59<5:53:30,  1.75s/it] 32%|███▏      | 5692/17834 [2:53:00<5:53:03,  1.74s/it] 32%|███▏      | 5693/17834 [2:53:02<5:54:10,  1.75s/it] 32%|███▏      | 5694/17834 [2:53:04<5:55:34,  1.76s/it] 32%|███▏      | 5695/17834 [2:53:06<6:02:04,  1.79s/it] 32%|███▏      | 5696/17834 [2:53:07<6:00:41,  1.78s/it] 32%|███▏      | 5697/17834 [2:53:09<5:58:21,  1.77s/it] 32%|███▏      | 5698/17834 [2:53:11<5:57:42,  1.77s/it] 32%|███▏      | 5699/17834 [2:53:13<5:54:48,  1.75s/it]08/30/2024 22:07:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.9396954774856567, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045080795884132385, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.539229393005371, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.524005889892578}
 32%|███▏      | 5700/17834 [2:53:14<5:53:29,  1.75s/it] 32%|███▏      | 5701/17834 [2:53:16<5:55:33,  1.76s/it] 32%|███▏      | 5702/17834 [2:53:18<5:56:59,  1.77s/it] 32%|███▏      | 5703/17834 [2:53:20<5:54:04,  1.75s/it] 32%|███▏      | 5704/17834 [2:53:21<5:58:57,  1.78s/it] 32%|███▏      | 5705/17834 [2:53:23<5:57:48,  1.77s/it] 32%|███▏      | 5706/17834 [2:53:25<5:52:50,  1.75s/it] 32%|███▏      | 5707/17834 [2:53:27<5:54:54,  1.76s/it] 32%|███▏      | 5708/17834 [2:53:29<5:56:20,  1.76s/it] 32%|███▏      | 5709/17834 [2:53:30<5:53:42,  1.75s/it] 32%|███▏      | 5710/17834 [2:53:32<5:53:59,  1.75s/it] 32%|███▏      | 5711/17834 [2:53:34<5:54:15,  1.75s/it] 32%|███▏      | 5712/17834 [2:53:36<5:57:42,  1.77s/it] 32%|███▏      | 5713/17834 [2:53:37<5:59:29,  1.78s/it] 32%|███▏      | 5714/17834 [2:53:39<5:54:09,  1.75s/it] 32%|███▏      | 5715/17834 [2:53:41<5:53:50,  1.75s/it] 32%|███▏      | 5716/17834 [2:53:42<5:51:08,  1.74s/it] 32%|███▏      | 5717/17834 [2:53:44<5:56:37,  1.77s/it] 32%|███▏      | 5718/17834 [2:53:46<5:54:22,  1.75s/it] 32%|███▏      | 5719/17834 [2:53:48<5:54:55,  1.76s/it] 32%|███▏      | 5720/17834 [2:53:50<5:58:44,  1.78s/it] 32%|███▏      | 5721/17834 [2:53:51<5:54:19,  1.76s/it] 32%|███▏      | 5722/17834 [2:53:53<5:53:44,  1.75s/it] 32%|███▏      | 5723/17834 [2:53:55<5:52:00,  1.74s/it] 32%|███▏      | 5724/17834 [2:53:57<5:53:03,  1.75s/it] 32%|███▏      | 5725/17834 [2:53:58<5:57:43,  1.77s/it] 32%|███▏      | 5726/17834 [2:54:00<6:00:14,  1.79s/it] 32%|███▏      | 5727/17834 [2:54:02<5:55:25,  1.76s/it] 32%|███▏      | 5728/17834 [2:54:04<5:52:46,  1.75s/it] 32%|███▏      | 5729/17834 [2:54:05<5:56:07,  1.77s/it] 32%|███▏      | 5730/17834 [2:54:07<5:54:18,  1.76s/it] 32%|███▏      | 5731/17834 [2:54:09<5:50:56,  1.74s/it] 32%|███▏      | 5732/17834 [2:54:11<5:49:12,  1.73s/it] 32%|███▏      | 5733/17834 [2:54:12<5:53:12,  1.75s/it] 32%|███▏      | 5734/17834 [2:54:14<5:57:10,  1.77s/it] 32%|███▏      | 5735/17834 [2:54:16<5:54:11,  1.76s/it] 32%|███▏      | 5736/17834 [2:54:18<5:50:58,  1.74s/it] 32%|███▏      | 5737/17834 [2:54:19<5:51:24,  1.74s/it] 32%|███▏      | 5738/17834 [2:54:21<5:50:42,  1.74s/it] 32%|███▏      | 5739/17834 [2:54:23<5:58:46,  1.78s/it] 32%|███▏      | 5740/17834 [2:54:25<5:57:53,  1.78s/it] 32%|███▏      | 5741/17834 [2:54:26<5:55:09,  1.76s/it] 32%|███▏      | 5742/17834 [2:54:28<5:55:48,  1.77s/it] 32%|███▏      | 5743/17834 [2:54:30<5:52:09,  1.75s/it] 32%|███▏      | 5744/17834 [2:54:32<5:56:46,  1.77s/it] 32%|███▏      | 5745/17834 [2:54:34<5:53:22,  1.75s/it] 32%|███▏      | 5746/17834 [2:54:35<5:53:57,  1.76s/it] 32%|███▏      | 5747/17834 [2:54:37<5:52:05,  1.75s/it] 32%|███▏      | 5748/17834 [2:54:39<5:51:11,  1.74s/it] 32%|███▏      | 5749/17834 [2:54:41<5:55:00,  1.76s/it]08/30/2024 22:09:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0401065349578857, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03031134605407715, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2154102325439453, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.285828113555908}
 32%|███▏      | 5750/17834 [2:54:42<5:59:39,  1.79s/it] 32%|███▏      | 5751/17834 [2:54:44<5:55:42,  1.77s/it] 32%|███▏      | 5752/17834 [2:54:46<5:55:00,  1.76s/it] 32%|███▏      | 5753/17834 [2:54:48<5:52:35,  1.75s/it] 32%|███▏      | 5754/17834 [2:54:49<5:55:05,  1.76s/it] 32%|███▏      | 5755/17834 [2:54:51<5:49:35,  1.74s/it] 32%|███▏      | 5756/17834 [2:54:53<5:50:25,  1.74s/it] 32%|███▏      | 5757/17834 [2:54:55<5:49:45,  1.74s/it] 32%|███▏      | 5758/17834 [2:54:56<5:48:17,  1.73s/it] 32%|███▏      | 5759/17834 [2:54:58<5:51:00,  1.74s/it] 32%|███▏      | 5760/17834 [2:55:00<5:47:27,  1.73s/it] 32%|███▏      | 5761/17834 [2:55:01<5:45:19,  1.72s/it] 32%|███▏      | 5762/17834 [2:55:03<5:44:05,  1.71s/it] 32%|███▏      | 5763/17834 [2:55:05<5:46:01,  1.72s/it] 32%|███▏      | 5764/17834 [2:55:07<5:55:21,  1.77s/it] 32%|███▏      | 5765/17834 [2:55:08<5:52:13,  1.75s/it] 32%|███▏      | 5766/17834 [2:55:10<5:54:26,  1.76s/it] 32%|███▏      | 5767/17834 [2:55:12<5:53:01,  1.76s/it] 32%|███▏      | 5768/17834 [2:55:14<5:51:03,  1.75s/it] 32%|███▏      | 5769/17834 [2:55:15<5:51:34,  1.75s/it] 32%|███▏      | 5770/17834 [2:55:17<5:50:09,  1.74s/it] 32%|███▏      | 5771/17834 [2:55:19<5:49:26,  1.74s/it] 32%|███▏      | 5772/17834 [2:55:21<5:46:03,  1.72s/it] 32%|███▏      | 5773/17834 [2:55:22<5:46:32,  1.72s/it] 32%|███▏      | 5774/17834 [2:55:24<5:49:27,  1.74s/it] 32%|███▏      | 5775/17834 [2:55:26<5:49:24,  1.74s/it] 32%|███▏      | 5776/17834 [2:55:28<5:58:55,  1.79s/it] 32%|███▏      | 5777/17834 [2:55:29<5:53:37,  1.76s/it] 32%|███▏      | 5778/17834 [2:55:31<5:49:59,  1.74s/it] 32%|███▏      | 5779/17834 [2:55:33<5:51:43,  1.75s/it] 32%|███▏      | 5780/17834 [2:55:35<5:49:37,  1.74s/it] 32%|███▏      | 5781/17834 [2:55:36<5:51:59,  1.75s/it] 32%|███▏      | 5782/17834 [2:55:38<5:52:57,  1.76s/it] 32%|███▏      | 5783/17834 [2:55:40<5:53:37,  1.76s/it] 32%|███▏      | 5784/17834 [2:55:42<5:53:37,  1.76s/it] 32%|███▏      | 5785/17834 [2:55:43<5:54:14,  1.76s/it] 32%|███▏      | 5786/17834 [2:55:45<5:49:25,  1.74s/it] 32%|███▏      | 5787/17834 [2:55:47<5:46:08,  1.72s/it] 32%|███▏      | 5788/17834 [2:55:49<5:48:02,  1.73s/it] 32%|███▏      | 5789/17834 [2:55:50<5:53:13,  1.76s/it] 32%|███▏      | 5790/17834 [2:55:52<5:50:22,  1.75s/it] 32%|███▏      | 5791/17834 [2:55:54<5:47:50,  1.73s/it] 32%|███▏      | 5792/17834 [2:55:56<5:50:06,  1.74s/it] 32%|███▏      | 5793/17834 [2:55:57<5:51:59,  1.75s/it] 32%|███▏      | 5794/17834 [2:55:59<5:51:37,  1.75s/it] 32%|███▏      | 5795/17834 [2:56:01<5:50:41,  1.75s/it] 32%|███▏      | 5796/17834 [2:56:03<5:51:24,  1.75s/it] 33%|███▎      | 5797/17834 [2:56:04<5:55:06,  1.77s/it] 33%|███▎      | 5798/17834 [2:56:06<5:51:28,  1.75s/it] 33%|███▎      | 5799/17834 [2:56:08<5:51:24,  1.75s/it]08/30/2024 22:10:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1098875999450684, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03718898445367813, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.244060516357422, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.39113712310791}
 33%|███▎      | 5800/17834 [2:56:10<5:53:10,  1.76s/it] 33%|███▎      | 5801/17834 [2:56:11<5:53:49,  1.76s/it] 33%|███▎      | 5802/17834 [2:56:13<5:51:00,  1.75s/it] 33%|███▎      | 5803/17834 [2:56:15<5:47:55,  1.74s/it] 33%|███▎      | 5804/17834 [2:56:17<5:48:34,  1.74s/it] 33%|███▎      | 5805/17834 [2:56:18<5:46:54,  1.73s/it] 33%|███▎      | 5806/17834 [2:56:20<5:52:13,  1.76s/it] 33%|███▎      | 5807/17834 [2:56:22<5:53:17,  1.76s/it] 33%|███▎      | 5808/17834 [2:56:24<6:02:06,  1.81s/it] 33%|███▎      | 5809/17834 [2:56:26<5:56:38,  1.78s/it] 33%|███▎      | 5810/17834 [2:56:27<6:02:15,  1.81s/it] 33%|███▎      | 5811/17834 [2:56:29<5:58:19,  1.79s/it] 33%|███▎      | 5812/17834 [2:56:31<6:02:05,  1.81s/it] 33%|███▎      | 5813/17834 [2:56:33<5:56:19,  1.78s/it] 33%|███▎      | 5814/17834 [2:56:34<5:53:25,  1.76s/it] 33%|███▎      | 5815/17834 [2:56:36<5:57:05,  1.78s/it] 33%|███▎      | 5816/17834 [2:56:38<5:54:03,  1.77s/it] 33%|███▎      | 5817/17834 [2:56:40<5:52:49,  1.76s/it] 33%|███▎      | 5818/17834 [2:56:42<5:55:18,  1.77s/it] 33%|███▎      | 5819/17834 [2:56:43<5:54:23,  1.77s/it] 33%|███▎      | 5820/17834 [2:56:45<5:50:59,  1.75s/it] 33%|███▎      | 5821/17834 [2:56:47<5:48:39,  1.74s/it] 33%|███▎      | 5822/17834 [2:56:48<5:46:25,  1.73s/it] 33%|███▎      | 5823/17834 [2:56:50<5:41:56,  1.71s/it] 33%|███▎      | 5824/17834 [2:56:52<5:43:54,  1.72s/it] 33%|███▎      | 5825/17834 [2:56:54<5:48:14,  1.74s/it] 33%|███▎      | 5826/17834 [2:56:55<5:48:13,  1.74s/it] 33%|███▎      | 5827/17834 [2:56:57<5:54:46,  1.77s/it] 33%|███▎      | 5828/17834 [2:56:59<5:51:26,  1.76s/it] 33%|███▎      | 5829/17834 [2:57:01<5:49:10,  1.75s/it] 33%|███▎      | 5830/17834 [2:57:02<5:51:34,  1.76s/it] 33%|███▎      | 5831/17834 [2:57:04<5:54:02,  1.77s/it] 33%|███▎      | 5832/17834 [2:57:06<5:49:45,  1.75s/it] 33%|███▎      | 5833/17834 [2:57:08<5:50:03,  1.75s/it] 33%|███▎      | 5834/17834 [2:57:09<5:47:53,  1.74s/it] 33%|███▎      | 5835/17834 [2:57:11<5:52:16,  1.76s/it] 33%|███▎      | 5836/17834 [2:57:13<5:52:49,  1.76s/it] 33%|███▎      | 5837/17834 [2:57:15<5:52:56,  1.77s/it] 33%|███▎      | 5838/17834 [2:57:16<5:48:37,  1.74s/it] 33%|███▎      | 5839/17834 [2:57:18<5:52:18,  1.76s/it] 33%|███▎      | 5840/17834 [2:57:20<5:50:10,  1.75s/it] 33%|███▎      | 5841/17834 [2:57:22<5:48:12,  1.74s/it] 33%|███▎      | 5842/17834 [2:57:24<5:52:13,  1.76s/it] 33%|███▎      | 5843/17834 [2:57:25<5:50:51,  1.76s/it] 33%|███▎      | 5844/17834 [2:57:27<5:49:18,  1.75s/it] 33%|███▎      | 5845/17834 [2:57:29<5:48:29,  1.74s/it] 33%|███▎      | 5846/17834 [2:57:30<5:48:45,  1.75s/it] 33%|███▎      | 5847/17834 [2:57:32<5:44:38,  1.73s/it] 33%|███▎      | 5848/17834 [2:57:34<5:39:18,  1.70s/it] 33%|███▎      | 5849/17834 [2:57:36<5:42:18,  1.71s/it]08/30/2024 22:11:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1685887575149536, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03880862146615982, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2386856079101562, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4460830688476562}
 33%|███▎      | 5850/17834 [2:57:37<5:40:16,  1.70s/it] 33%|███▎      | 5851/17834 [2:57:39<5:45:38,  1.73s/it] 33%|███▎      | 5852/17834 [2:57:41<5:44:11,  1.72s/it] 33%|███▎      | 5853/17834 [2:57:43<5:51:14,  1.76s/it] 33%|███▎      | 5854/17834 [2:57:44<5:51:21,  1.76s/it] 33%|███▎      | 5855/17834 [2:57:46<5:50:38,  1.76s/it] 33%|███▎      | 5856/17834 [2:57:48<5:45:33,  1.73s/it] 33%|███▎      | 5857/17834 [2:57:49<5:44:08,  1.72s/it] 33%|███▎      | 5858/17834 [2:57:51<5:44:17,  1.72s/it] 33%|███▎      | 5859/17834 [2:57:53<5:41:39,  1.71s/it] 33%|███▎      | 5860/17834 [2:57:55<5:46:03,  1.73s/it] 33%|███▎      | 5861/17834 [2:57:56<5:43:16,  1.72s/it] 33%|███▎      | 5862/17834 [2:57:58<5:41:08,  1.71s/it] 33%|███▎      | 5863/17834 [2:58:00<5:41:59,  1.71s/it] 33%|███▎      | 5864/17834 [2:58:01<5:41:34,  1.71s/it] 33%|███▎      | 5865/17834 [2:58:03<5:42:54,  1.72s/it] 33%|███▎      | 5866/17834 [2:58:05<5:48:19,  1.75s/it] 33%|███▎      | 5867/17834 [2:58:07<5:44:48,  1.73s/it] 33%|███▎      | 5868/17834 [2:58:08<5:50:11,  1.76s/it] 33%|███▎      | 5869/17834 [2:58:10<5:47:28,  1.74s/it] 33%|███▎      | 5870/17834 [2:58:12<5:50:40,  1.76s/it] 33%|███▎      | 5871/17834 [2:58:14<5:44:44,  1.73s/it] 33%|███▎      | 5872/17834 [2:58:15<5:43:59,  1.73s/it] 33%|███▎      | 5873/17834 [2:58:17<5:46:36,  1.74s/it] 33%|███▎      | 5874/17834 [2:58:19<5:48:27,  1.75s/it] 33%|███▎      | 5875/17834 [2:58:21<5:46:04,  1.74s/it] 33%|███▎      | 5876/17834 [2:58:22<5:48:31,  1.75s/it] 33%|███▎      | 5877/17834 [2:58:24<5:47:05,  1.74s/it] 33%|███▎      | 5878/17834 [2:58:26<5:54:36,  1.78s/it] 33%|███▎      | 5879/17834 [2:58:28<5:51:30,  1.76s/it] 33%|███▎      | 5880/17834 [2:58:29<5:49:32,  1.75s/it] 33%|███▎      | 5881/17834 [2:58:31<5:52:09,  1.77s/it] 33%|███▎      | 5882/17834 [2:58:33<5:50:07,  1.76s/it] 33%|███▎      | 5883/17834 [2:58:35<5:48:50,  1.75s/it] 33%|███▎      | 5884/17834 [2:58:37<5:53:48,  1.78s/it] 33%|███▎      | 5885/17834 [2:58:38<5:50:56,  1.76s/it] 33%|███▎      | 5886/17834 [2:58:40<5:45:39,  1.74s/it] 33%|███▎      | 5887/17834 [2:58:42<5:45:10,  1.73s/it] 33%|███▎      | 5888/17834 [2:58:43<5:45:49,  1.74s/it] 33%|███▎      | 5889/17834 [2:58:45<5:43:26,  1.73s/it] 33%|███▎      | 5890/17834 [2:58:47<5:45:15,  1.73s/it] 33%|███▎      | 5891/17834 [2:58:49<5:46:17,  1.74s/it] 33%|███▎      | 5892/17834 [2:58:50<5:44:06,  1.73s/it] 33%|███▎      | 5893/17834 [2:58:52<5:42:05,  1.72s/it] 33%|███▎      | 5894/17834 [2:58:54<5:41:09,  1.71s/it] 33%|███▎      | 5895/17834 [2:58:56<5:47:51,  1.75s/it] 33%|███▎      | 5896/17834 [2:58:57<5:46:06,  1.74s/it] 33%|███▎      | 5897/17834 [2:58:59<5:44:40,  1.73s/it] 33%|███▎      | 5898/17834 [2:59:01<5:51:23,  1.77s/it] 33%|███▎      | 5899/17834 [2:59:03<5:51:19,  1.77s/it]08/30/2024 22:13:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4859459400177002, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03292827680706978, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3153066635131836, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8341808319091797}
 33%|███▎      | 5900/17834 [2:59:04<5:46:03,  1.74s/it] 33%|███▎      | 5901/17834 [2:59:06<5:46:21,  1.74s/it] 33%|███▎      | 5902/17834 [2:59:08<5:45:10,  1.74s/it] 33%|███▎      | 5903/17834 [2:59:10<5:46:50,  1.74s/it] 33%|███▎      | 5904/17834 [2:59:11<5:46:17,  1.74s/it] 33%|███▎      | 5905/17834 [2:59:13<5:47:24,  1.75s/it] 33%|███▎      | 5906/17834 [2:59:15<5:44:59,  1.74s/it] 33%|███▎      | 5907/17834 [2:59:16<5:40:53,  1.71s/it] 33%|███▎      | 5908/17834 [2:59:18<5:43:23,  1.73s/it] 33%|███▎      | 5909/17834 [2:59:20<5:41:23,  1.72s/it] 33%|███▎      | 5910/17834 [2:59:22<5:45:12,  1.74s/it] 33%|███▎      | 5911/17834 [2:59:23<5:45:30,  1.74s/it] 33%|███▎      | 5912/17834 [2:59:25<5:48:04,  1.75s/it] 33%|███▎      | 5913/17834 [2:59:27<5:47:20,  1.75s/it] 33%|███▎      | 5914/17834 [2:59:29<5:51:51,  1.77s/it] 33%|███▎      | 5915/17834 [2:59:30<5:47:17,  1.75s/it] 33%|███▎      | 5916/17834 [2:59:32<5:45:19,  1.74s/it] 33%|███▎      | 5917/17834 [2:59:34<5:44:38,  1.74s/it] 33%|███▎      | 5918/17834 [2:59:36<5:44:25,  1.73s/it] 33%|███▎      | 5919/17834 [2:59:37<5:45:01,  1.74s/it] 33%|███▎      | 5920/17834 [2:59:39<5:53:50,  1.78s/it] 33%|███▎      | 5921/17834 [2:59:41<5:50:59,  1.77s/it] 33%|███▎      | 5922/17834 [2:59:43<5:49:37,  1.76s/it] 33%|███▎      | 5923/17834 [2:59:44<5:48:14,  1.75s/it] 33%|███▎      | 5924/17834 [2:59:46<5:51:47,  1.77s/it] 33%|███▎      | 5925/17834 [2:59:48<5:50:00,  1.76s/it] 33%|███▎      | 5926/17834 [2:59:50<5:49:14,  1.76s/it] 33%|███▎      | 5927/17834 [2:59:52<5:49:13,  1.76s/it] 33%|███▎      | 5928/17834 [2:59:53<5:42:46,  1.73s/it] 33%|███▎      | 5929/17834 [2:59:55<5:46:16,  1.75s/it] 33%|███▎      | 5930/17834 [2:59:57<5:49:30,  1.76s/it] 33%|███▎      | 5931/17834 [2:59:59<5:49:44,  1.76s/it] 33%|███▎      | 5932/17834 [3:00:00<5:49:24,  1.76s/it] 33%|███▎      | 5933/17834 [3:00:02<5:48:32,  1.76s/it] 33%|███▎      | 5934/17834 [3:00:04<5:47:17,  1.75s/it] 33%|███▎      | 5935/17834 [3:00:06<5:51:52,  1.77s/it] 33%|███▎      | 5936/17834 [3:00:07<5:52:01,  1.78s/it] 33%|███▎      | 5937/17834 [3:00:09<5:53:41,  1.78s/it] 33%|███▎      | 5938/17834 [3:00:11<5:51:09,  1.77s/it] 33%|███▎      | 5939/17834 [3:00:13<5:48:17,  1.76s/it] 33%|███▎      | 5940/17834 [3:00:14<5:44:02,  1.74s/it] 33%|███▎      | 5941/17834 [3:00:16<5:47:24,  1.75s/it] 33%|███▎      | 5942/17834 [3:00:18<5:47:49,  1.75s/it] 33%|███▎      | 5943/17834 [3:00:20<5:52:48,  1.78s/it] 33%|███▎      | 5944/17834 [3:00:21<5:47:13,  1.75s/it] 33%|███▎      | 5945/17834 [3:00:23<5:47:00,  1.75s/it] 33%|███▎      | 5946/17834 [3:00:25<5:45:27,  1.74s/it] 33%|███▎      | 5947/17834 [3:00:27<5:41:46,  1.73s/it] 33%|███▎      | 5948/17834 [3:00:28<5:39:28,  1.71s/it] 33%|███▎      | 5949/17834 [3:00:30<5:46:35,  1.75s/it]08/30/2024 22:14:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6981439590454102, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05292578414082527, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4161202907562256, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.167190074920654}
 33%|███▎      | 5950/17834 [3:00:32<5:50:10,  1.77s/it] 33%|███▎      | 5951/17834 [3:00:34<5:48:36,  1.76s/it] 33%|███▎      | 5952/17834 [3:00:35<5:44:08,  1.74s/it] 33%|███▎      | 5953/17834 [3:00:37<5:44:07,  1.74s/it] 33%|███▎      | 5954/17834 [3:00:39<5:47:57,  1.76s/it] 33%|███▎      | 5955/17834 [3:00:41<5:50:14,  1.77s/it] 33%|███▎      | 5956/17834 [3:00:42<5:49:23,  1.76s/it] 33%|███▎      | 5957/17834 [3:00:44<5:47:37,  1.76s/it] 33%|███▎      | 5958/17834 [3:00:46<5:43:50,  1.74s/it] 33%|███▎      | 5959/17834 [3:00:48<5:45:22,  1.75s/it] 33%|███▎      | 5960/17834 [3:00:49<5:44:53,  1.74s/it] 33%|███▎      | 5961/17834 [3:00:51<5:49:36,  1.77s/it] 33%|███▎      | 5962/17834 [3:00:53<5:45:49,  1.75s/it] 33%|███▎      | 5963/17834 [3:00:55<5:44:28,  1.74s/it] 33%|███▎      | 5964/17834 [3:00:56<5:42:00,  1.73s/it] 33%|███▎      | 5965/17834 [3:00:58<5:44:04,  1.74s/it] 33%|███▎      | 5966/17834 [3:01:00<5:40:52,  1.72s/it] 33%|███▎      | 5967/17834 [3:01:01<5:43:12,  1.74s/it] 33%|███▎      | 5968/17834 [3:01:03<5:44:15,  1.74s/it] 33%|███▎      | 5969/17834 [3:01:05<5:48:24,  1.76s/it] 33%|███▎      | 5970/17834 [3:01:07<5:42:53,  1.73s/it] 33%|███▎      | 5971/17834 [3:01:08<5:44:00,  1.74s/it] 33%|███▎      | 5972/17834 [3:01:10<5:45:04,  1.75s/it] 33%|███▎      | 5973/17834 [3:01:12<5:46:38,  1.75s/it] 33%|███▎      | 5974/17834 [3:01:14<5:46:45,  1.75s/it] 34%|███▎      | 5975/17834 [3:01:15<5:43:22,  1.74s/it] 34%|███▎      | 5976/17834 [3:01:17<5:45:54,  1.75s/it] 34%|███▎      | 5977/17834 [3:01:19<5:42:17,  1.73s/it] 34%|███▎      | 5978/17834 [3:01:21<5:41:25,  1.73s/it] 34%|███▎      | 5979/17834 [3:01:22<5:39:57,  1.72s/it] 34%|███▎      | 5980/17834 [3:01:24<5:42:47,  1.74s/it] 34%|███▎      | 5981/17834 [3:01:26<5:47:54,  1.76s/it] 34%|███▎      | 5982/17834 [3:01:28<5:46:17,  1.75s/it] 34%|███▎      | 5983/17834 [3:01:29<5:43:29,  1.74s/it] 34%|███▎      | 5984/17834 [3:01:31<5:44:21,  1.74s/it] 34%|███▎      | 5985/17834 [3:01:33<5:44:05,  1.74s/it] 34%|███▎      | 5986/17834 [3:01:35<5:50:41,  1.78s/it] 34%|███▎      | 5987/17834 [3:01:36<5:47:44,  1.76s/it] 34%|███▎      | 5988/17834 [3:01:38<5:46:47,  1.76s/it] 34%|███▎      | 5989/17834 [3:01:40<5:48:00,  1.76s/it] 34%|███▎      | 5990/17834 [3:01:42<5:46:39,  1.76s/it] 34%|███▎      | 5991/17834 [3:01:43<5:43:27,  1.74s/it] 34%|███▎      | 5992/17834 [3:01:45<5:40:41,  1.73s/it] 34%|███▎      | 5993/17834 [3:01:47<5:39:43,  1.72s/it] 34%|███▎      | 5994/17834 [3:01:49<5:40:53,  1.73s/it] 34%|███▎      | 5995/17834 [3:01:50<5:37:25,  1.71s/it] 34%|███▎      | 5996/17834 [3:01:52<5:38:45,  1.72s/it] 34%|███▎      | 5997/17834 [3:01:54<5:40:07,  1.72s/it] 34%|███▎      | 5998/17834 [3:01:55<5:40:08,  1.72s/it] 34%|███▎      | 5999/17834 [3:01:57<5:42:13,  1.73s/it]08/30/2024 22:16:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9716819524765015, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025960169732570648, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1507441997528076, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.148386240005493}
 34%|███▎      | 6000/17834 [3:01:59<5:44:48,  1.75s/it] 34%|███▎      | 6001/17834 [3:02:01<5:47:39,  1.76s/it] 34%|███▎      | 6002/17834 [3:02:03<5:48:58,  1.77s/it] 34%|███▎      | 6003/17834 [3:02:04<5:44:25,  1.75s/it] 34%|███▎      | 6004/17834 [3:02:06<5:42:03,  1.73s/it] 34%|███▎      | 6005/17834 [3:02:08<5:43:25,  1.74s/it] 34%|███▎      | 6006/17834 [3:02:09<5:41:56,  1.73s/it] 34%|███▎      | 6007/17834 [3:02:11<5:43:06,  1.74s/it] 34%|███▎      | 6008/17834 [3:02:13<5:44:23,  1.75s/it] 34%|███▎      | 6009/17834 [3:02:15<5:45:28,  1.75s/it] 34%|███▎      | 6010/17834 [3:02:16<5:45:38,  1.75s/it] 34%|███▎      | 6011/17834 [3:02:18<5:41:33,  1.73s/it] 34%|███▎      | 6012/17834 [3:02:20<5:42:26,  1.74s/it] 34%|███▎      | 6013/17834 [3:02:22<5:43:47,  1.74s/it] 34%|███▎      | 6014/17834 [3:02:23<5:40:08,  1.73s/it] 34%|███▎      | 6015/17834 [3:02:25<5:41:15,  1.73s/it] 34%|███▎      | 6016/17834 [3:02:27<5:40:36,  1.73s/it] 34%|███▎      | 6017/17834 [3:02:29<5:41:02,  1.73s/it] 34%|███▎      | 6018/17834 [3:02:30<5:37:11,  1.71s/it] 34%|███▍      | 6019/17834 [3:02:32<5:42:59,  1.74s/it] 34%|███▍      | 6020/17834 [3:02:34<5:40:29,  1.73s/it] 34%|███▍      | 6021/17834 [3:02:36<5:49:19,  1.77s/it] 34%|███▍      | 6022/17834 [3:02:37<5:42:47,  1.74s/it] 34%|███▍      | 6023/17834 [3:02:39<5:38:42,  1.72s/it] 34%|███▍      | 6024/17834 [3:02:41<5:43:48,  1.75s/it] 34%|███▍      | 6025/17834 [3:02:42<5:41:34,  1.74s/it] 34%|███▍      | 6026/17834 [3:02:44<5:48:03,  1.77s/it] 34%|███▍      | 6027/17834 [3:02:46<5:45:24,  1.76s/it] 34%|███▍      | 6028/17834 [3:02:48<5:43:29,  1.75s/it] 34%|███▍      | 6029/17834 [3:02:50<5:46:47,  1.76s/it] 34%|███▍      | 6030/17834 [3:02:51<5:44:02,  1.75s/it] 34%|███▍      | 6031/17834 [3:02:53<5:45:51,  1.76s/it] 34%|███▍      | 6032/17834 [3:02:55<5:44:19,  1.75s/it] 34%|███▍      | 6033/17834 [3:02:57<5:42:37,  1.74s/it] 34%|███▍      | 6034/17834 [3:02:58<5:40:27,  1.73s/it] 34%|███▍      | 6035/17834 [3:03:00<5:47:57,  1.77s/it] 34%|███▍      | 6036/17834 [3:03:02<5:43:01,  1.74s/it] 34%|███▍      | 6037/17834 [3:03:04<5:43:50,  1.75s/it] 34%|███▍      | 6038/17834 [3:03:05<5:42:11,  1.74s/it] 34%|███▍      | 6039/17834 [3:03:07<5:43:27,  1.75s/it] 34%|███▍      | 6040/17834 [3:03:09<5:42:47,  1.74s/it] 34%|███▍      | 6041/17834 [3:03:11<5:47:10,  1.77s/it] 34%|███▍      | 6042/17834 [3:03:12<5:45:49,  1.76s/it] 34%|███▍      | 6043/17834 [3:03:14<5:47:29,  1.77s/it] 34%|███▍      | 6044/17834 [3:03:16<5:46:50,  1.77s/it] 34%|███▍      | 6045/17834 [3:03:18<5:46:09,  1.76s/it] 34%|███▍      | 6046/17834 [3:03:19<5:46:07,  1.76s/it] 34%|███▍      | 6047/17834 [3:03:21<5:47:16,  1.77s/it] 34%|███▍      | 6048/17834 [3:03:23<5:48:14,  1.77s/it] 34%|███▍      | 6049/17834 [3:03:25<5:55:12,  1.81s/it]08/30/2024 22:17:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2344636917114258, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038969479501247406, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.219949960708618, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4933831691741943}
 34%|███▍      | 6050/17834 [3:03:27<5:49:02,  1.78s/it] 34%|███▍      | 6051/17834 [3:03:28<5:49:11,  1.78s/it] 34%|███▍      | 6052/17834 [3:03:30<5:48:25,  1.77s/it] 34%|███▍      | 6053/17834 [3:03:32<5:53:23,  1.80s/it] 34%|███▍      | 6054/17834 [3:03:34<5:46:57,  1.77s/it] 34%|███▍      | 6055/17834 [3:03:35<5:43:07,  1.75s/it] 34%|███▍      | 6056/17834 [3:03:37<5:42:24,  1.74s/it] 34%|███▍      | 6057/17834 [3:03:39<5:45:04,  1.76s/it] 34%|███▍      | 6058/17834 [3:03:41<5:42:36,  1.75s/it] 34%|███▍      | 6059/17834 [3:03:42<5:40:58,  1.74s/it] 34%|███▍      | 6060/17834 [3:03:44<5:39:52,  1.73s/it] 34%|███▍      | 6061/17834 [3:03:46<5:36:37,  1.72s/it] 34%|███▍      | 6062/17834 [3:03:47<5:39:10,  1.73s/it] 34%|███▍      | 6063/17834 [3:03:49<5:43:12,  1.75s/it] 34%|███▍      | 6064/17834 [3:03:51<5:38:39,  1.73s/it] 34%|███▍      | 6065/17834 [3:03:53<5:37:39,  1.72s/it] 34%|███▍      | 6066/17834 [3:03:54<5:39:46,  1.73s/it] 34%|███▍      | 6067/17834 [3:03:56<5:38:52,  1.73s/it] 34%|███▍      | 6068/17834 [3:03:58<5:42:35,  1.75s/it] 34%|███▍      | 6069/17834 [3:04:00<5:42:18,  1.75s/it] 34%|███▍      | 6070/17834 [3:04:01<5:39:48,  1.73s/it] 34%|███▍      | 6071/17834 [3:04:03<5:41:59,  1.74s/it] 34%|███▍      | 6072/17834 [3:04:05<5:45:32,  1.76s/it] 34%|███▍      | 6073/17834 [3:04:07<5:43:24,  1.75s/it] 34%|███▍      | 6074/17834 [3:04:08<5:45:12,  1.76s/it] 34%|███▍      | 6075/17834 [3:04:10<5:41:46,  1.74s/it] 34%|███▍      | 6076/17834 [3:04:12<5:42:58,  1.75s/it] 34%|███▍      | 6077/17834 [3:04:14<5:46:17,  1.77s/it] 34%|███▍      | 6078/17834 [3:04:15<5:46:00,  1.77s/it] 34%|███▍      | 6079/17834 [3:04:17<5:42:35,  1.75s/it] 34%|███▍      | 6080/17834 [3:04:19<5:37:15,  1.72s/it] 34%|███▍      | 6081/17834 [3:04:21<5:37:37,  1.72s/it] 34%|███▍      | 6082/17834 [3:04:22<5:38:36,  1.73s/it] 34%|███▍      | 6083/17834 [3:04:24<5:42:39,  1.75s/it] 34%|███▍      | 6084/17834 [3:04:26<5:42:33,  1.75s/it] 34%|███▍      | 6085/17834 [3:04:28<5:42:00,  1.75s/it] 34%|███▍      | 6086/17834 [3:04:29<5:40:58,  1.74s/it] 34%|███▍      | 6087/17834 [3:04:31<5:45:56,  1.77s/it] 34%|███▍      | 6088/17834 [3:04:33<5:48:21,  1.78s/it] 34%|███▍      | 6089/17834 [3:04:35<5:44:35,  1.76s/it] 34%|███▍      | 6090/17834 [3:04:36<5:41:59,  1.75s/it] 34%|███▍      | 6091/17834 [3:04:38<5:41:31,  1.75s/it] 34%|███▍      | 6092/17834 [3:04:40<5:39:05,  1.73s/it] 34%|███▍      | 6093/17834 [3:04:42<5:40:57,  1.74s/it] 34%|███▍      | 6094/17834 [3:04:43<5:40:05,  1.74s/it] 34%|███▍      | 6095/17834 [3:04:45<5:40:45,  1.74s/it] 34%|███▍      | 6096/17834 [3:04:47<5:38:57,  1.73s/it] 34%|███▍      | 6097/17834 [3:04:49<5:39:28,  1.74s/it] 34%|███▍      | 6098/17834 [3:04:50<5:39:33,  1.74s/it] 34%|███▍      | 6099/17834 [3:04:52<5:42:56,  1.75s/it]08/30/2024 22:19:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0386772155761719, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032258860766887665, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2631218433380127, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3340578079223633}
 34%|███▍      | 6100/17834 [3:04:54<5:41:57,  1.75s/it] 34%|███▍      | 6101/17834 [3:04:55<5:36:46,  1.72s/it] 34%|███▍      | 6102/17834 [3:04:57<5:40:32,  1.74s/it] 34%|███▍      | 6103/17834 [3:04:59<5:39:59,  1.74s/it] 34%|███▍      | 6104/17834 [3:05:01<5:36:42,  1.72s/it] 34%|███▍      | 6105/17834 [3:05:02<5:37:10,  1.72s/it] 34%|███▍      | 6106/17834 [3:05:04<5:37:57,  1.73s/it] 34%|███▍      | 6107/17834 [3:05:06<5:40:35,  1.74s/it] 34%|███▍      | 6108/17834 [3:05:08<5:39:34,  1.74s/it] 34%|███▍      | 6109/17834 [3:05:09<5:41:21,  1.75s/it] 34%|███▍      | 6110/17834 [3:05:11<5:38:27,  1.73s/it] 34%|███▍      | 6111/17834 [3:05:13<5:40:00,  1.74s/it] 34%|███▍      | 6112/17834 [3:05:15<5:37:02,  1.73s/it] 34%|███▍      | 6113/17834 [3:05:16<5:44:00,  1.76s/it] 34%|███▍      | 6114/17834 [3:05:18<5:42:57,  1.76s/it] 34%|███▍      | 6115/17834 [3:05:20<5:39:06,  1.74s/it] 34%|███▍      | 6116/17834 [3:05:22<5:39:31,  1.74s/it] 34%|███▍      | 6117/17834 [3:05:23<5:38:39,  1.73s/it] 34%|███▍      | 6118/17834 [3:05:25<5:38:36,  1.73s/it] 34%|███▍      | 6119/17834 [3:05:27<5:41:00,  1.75s/it] 34%|███▍      | 6120/17834 [3:05:29<5:38:38,  1.73s/it] 34%|███▍      | 6121/17834 [3:05:30<5:39:35,  1.74s/it] 34%|███▍      | 6122/17834 [3:05:32<5:38:57,  1.74s/it] 34%|███▍      | 6123/17834 [3:05:34<5:35:50,  1.72s/it] 34%|███▍      | 6124/17834 [3:05:35<5:38:29,  1.73s/it] 34%|███▍      | 6125/17834 [3:05:37<5:41:16,  1.75s/it] 34%|███▍      | 6126/17834 [3:05:39<5:42:22,  1.75s/it] 34%|███▍      | 6127/17834 [3:05:41<5:39:12,  1.74s/it] 34%|███▍      | 6128/17834 [3:05:42<5:40:22,  1.74s/it] 34%|███▍      | 6129/17834 [3:05:44<5:38:08,  1.73s/it] 34%|███▍      | 6130/17834 [3:05:46<5:37:59,  1.73s/it] 34%|███▍      | 6131/17834 [3:05:48<5:35:44,  1.72s/it] 34%|███▍      | 6132/17834 [3:05:49<5:39:47,  1.74s/it] 34%|███▍      | 6133/17834 [3:05:51<5:39:48,  1.74s/it] 34%|███▍      | 6134/17834 [3:05:53<5:42:22,  1.76s/it] 34%|███▍      | 6135/17834 [3:05:55<5:39:53,  1.74s/it] 34%|███▍      | 6136/17834 [3:05:56<5:39:07,  1.74s/it] 34%|███▍      | 6137/17834 [3:05:58<5:42:29,  1.76s/it] 34%|███▍      | 6138/17834 [3:06:00<5:47:30,  1.78s/it] 34%|███▍      | 6139/17834 [3:06:02<5:47:15,  1.78s/it] 34%|███▍      | 6140/17834 [3:06:04<5:46:38,  1.78s/it] 34%|███▍      | 6141/17834 [3:06:05<5:48:48,  1.79s/it] 34%|███▍      | 6142/17834 [3:06:07<5:41:08,  1.75s/it] 34%|███▍      | 6143/17834 [3:06:09<5:47:08,  1.78s/it] 34%|███▍      | 6144/17834 [3:06:11<5:44:17,  1.77s/it] 34%|███▍      | 6145/17834 [3:06:12<5:44:26,  1.77s/it] 34%|███▍      | 6146/17834 [3:06:14<5:42:40,  1.76s/it] 34%|███▍      | 6147/17834 [3:06:16<5:40:45,  1.75s/it] 34%|███▍      | 6148/17834 [3:06:18<5:40:37,  1.75s/it] 34%|███▍      | 6149/17834 [3:06:19<5:39:59,  1.75s/it]08/30/2024 22:20:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9653569459915161, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028123807162046432, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1617398262023926, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1552205085754395}
 34%|███▍      | 6150/17834 [3:06:21<5:38:06,  1.74s/it] 34%|███▍      | 6151/17834 [3:06:23<5:38:49,  1.74s/it] 34%|███▍      | 6152/17834 [3:06:25<5:37:12,  1.73s/it] 35%|███▍      | 6153/17834 [3:06:26<5:33:08,  1.71s/it] 35%|███▍      | 6154/17834 [3:06:28<5:35:42,  1.72s/it] 35%|███▍      | 6155/17834 [3:06:30<5:36:56,  1.73s/it] 35%|███▍      | 6156/17834 [3:06:32<5:44:16,  1.77s/it] 35%|███▍      | 6157/17834 [3:06:33<5:41:12,  1.75s/it] 35%|███▍      | 6158/17834 [3:06:35<5:37:51,  1.74s/it] 35%|███▍      | 6159/17834 [3:06:37<5:35:49,  1.73s/it] 35%|███▍      | 6160/17834 [3:06:38<5:38:46,  1.74s/it] 35%|███▍      | 6161/17834 [3:06:40<5:36:23,  1.73s/it] 35%|███▍      | 6162/17834 [3:06:42<5:37:42,  1.74s/it] 35%|███▍      | 6163/17834 [3:06:44<5:33:45,  1.72s/it] 35%|███▍      | 6164/17834 [3:06:45<5:34:25,  1.72s/it] 35%|███▍      | 6165/17834 [3:06:47<5:44:06,  1.77s/it] 35%|███▍      | 6166/17834 [3:06:49<5:41:00,  1.75s/it] 35%|███▍      | 6167/17834 [3:06:51<5:49:06,  1.80s/it] 35%|███▍      | 6168/17834 [3:06:52<5:42:39,  1.76s/it] 35%|███▍      | 6169/17834 [3:06:54<5:41:00,  1.75s/it] 35%|███▍      | 6170/17834 [3:06:56<5:43:01,  1.76s/it] 35%|███▍      | 6171/17834 [3:06:58<5:41:40,  1.76s/it] 35%|███▍      | 6172/17834 [3:07:00<5:44:51,  1.77s/it] 35%|███▍      | 6173/17834 [3:07:01<5:49:02,  1.80s/it] 35%|███▍      | 6174/17834 [3:07:03<5:50:33,  1.80s/it] 35%|███▍      | 6175/17834 [3:07:05<5:47:27,  1.79s/it] 35%|███▍      | 6176/17834 [3:07:07<5:43:43,  1.77s/it] 35%|███▍      | 6177/17834 [3:07:08<5:45:32,  1.78s/it] 35%|███▍      | 6178/17834 [3:07:10<5:46:35,  1.78s/it] 35%|███▍      | 6179/17834 [3:07:12<5:41:41,  1.76s/it] 35%|███▍      | 6180/17834 [3:07:14<5:40:00,  1.75s/it] 35%|███▍      | 6181/17834 [3:07:15<5:41:20,  1.76s/it] 35%|███▍      | 6182/17834 [3:07:17<5:41:59,  1.76s/it] 35%|███▍      | 6183/17834 [3:07:19<5:37:03,  1.74s/it] 35%|███▍      | 6184/17834 [3:07:21<5:38:27,  1.74s/it] 35%|███▍      | 6185/17834 [3:07:22<5:36:36,  1.73s/it] 35%|███▍      | 6186/17834 [3:07:24<5:38:03,  1.74s/it] 35%|███▍      | 6187/17834 [3:07:26<5:40:32,  1.75s/it] 35%|███▍      | 6188/17834 [3:07:28<5:37:12,  1.74s/it] 35%|███▍      | 6189/17834 [3:07:29<5:41:48,  1.76s/it] 35%|███▍      | 6190/17834 [3:07:31<5:44:10,  1.77s/it] 35%|███▍      | 6191/17834 [3:07:33<5:39:54,  1.75s/it] 35%|███▍      | 6192/17834 [3:07:35<5:44:10,  1.77s/it] 35%|███▍      | 6193/17834 [3:07:36<5:40:04,  1.75s/it] 35%|███▍      | 6194/17834 [3:07:38<5:42:52,  1.77s/it] 35%|███▍      | 6195/17834 [3:07:40<5:39:47,  1.75s/it] 35%|███▍      | 6196/17834 [3:07:42<5:43:42,  1.77s/it] 35%|███▍      | 6197/17834 [3:07:44<5:39:27,  1.75s/it] 35%|███▍      | 6198/17834 [3:07:45<5:38:25,  1.75s/it] 35%|███▍      | 6199/17834 [3:07:47<5:37:38,  1.74s/it]08/30/2024 22:22:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3227810859680176, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05451197922229767, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2533044815063477, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6305975914001465}
 35%|███▍      | 6200/17834 [3:07:49<5:42:24,  1.77s/it] 35%|███▍      | 6201/17834 [3:07:51<5:38:56,  1.75s/it] 35%|███▍      | 6202/17834 [3:07:52<5:36:17,  1.73s/it] 35%|███▍      | 6203/17834 [3:07:54<5:34:10,  1.72s/it] 35%|███▍      | 6204/17834 [3:07:56<5:33:17,  1.72s/it] 35%|███▍      | 6205/17834 [3:07:57<5:35:04,  1.73s/it] 35%|███▍      | 6206/17834 [3:07:59<5:37:31,  1.74s/it] 35%|███▍      | 6207/17834 [3:08:01<5:36:13,  1.74s/it] 35%|███▍      | 6208/17834 [3:08:03<5:31:19,  1.71s/it] 35%|███▍      | 6209/17834 [3:08:04<5:33:42,  1.72s/it] 35%|███▍      | 6210/17834 [3:08:06<5:34:22,  1.73s/it] 35%|███▍      | 6211/17834 [3:08:08<5:48:08,  1.80s/it] 35%|███▍      | 6212/17834 [3:08:10<5:42:56,  1.77s/it] 35%|███▍      | 6213/17834 [3:08:11<5:40:28,  1.76s/it] 35%|███▍      | 6214/17834 [3:08:13<5:40:54,  1.76s/it] 35%|███▍      | 6215/17834 [3:08:15<5:38:43,  1.75s/it] 35%|███▍      | 6216/17834 [3:08:17<5:41:29,  1.76s/it] 35%|███▍      | 6217/17834 [3:08:18<5:40:17,  1.76s/it] 35%|███▍      | 6218/17834 [3:08:20<5:38:14,  1.75s/it] 35%|███▍      | 6219/17834 [3:08:22<5:38:18,  1.75s/it] 35%|███▍      | 6220/17834 [3:08:24<5:34:26,  1.73s/it] 35%|███▍      | 6221/17834 [3:08:25<5:31:50,  1.71s/it] 35%|███▍      | 6222/17834 [3:08:27<5:35:27,  1.73s/it] 35%|███▍      | 6223/17834 [3:08:29<5:37:17,  1.74s/it] 35%|███▍      | 6224/17834 [3:08:31<5:38:42,  1.75s/it] 35%|███▍      | 6225/17834 [3:08:32<5:39:15,  1.75s/it] 35%|███▍      | 6226/17834 [3:08:34<5:36:42,  1.74s/it] 35%|███▍      | 6227/17834 [3:08:36<5:36:17,  1.74s/it] 35%|███▍      | 6228/17834 [3:08:38<5:40:18,  1.76s/it] 35%|███▍      | 6229/17834 [3:08:39<5:41:02,  1.76s/it] 35%|███▍      | 6230/17834 [3:08:41<5:40:16,  1.76s/it] 35%|███▍      | 6231/17834 [3:08:43<5:39:53,  1.76s/it] 35%|███▍      | 6232/17834 [3:08:45<5:40:39,  1.76s/it] 35%|███▍      | 6233/17834 [3:08:46<5:37:26,  1.75s/it] 35%|███▍      | 6234/17834 [3:08:48<5:42:06,  1.77s/it] 35%|███▍      | 6235/17834 [3:08:50<5:38:39,  1.75s/it] 35%|███▍      | 6236/17834 [3:08:52<5:46:21,  1.79s/it] 35%|███▍      | 6237/17834 [3:08:54<5:43:30,  1.78s/it] 35%|███▍      | 6238/17834 [3:08:55<5:42:54,  1.77s/it] 35%|███▍      | 6239/17834 [3:08:57<5:42:40,  1.77s/it] 35%|███▍      | 6240/17834 [3:08:59<5:38:04,  1.75s/it] 35%|███▍      | 6241/17834 [3:09:00<5:37:49,  1.75s/it] 35%|███▌      | 6242/17834 [3:09:02<5:37:47,  1.75s/it] 35%|███▌      | 6243/17834 [3:09:04<5:39:36,  1.76s/it] 35%|███▌      | 6244/17834 [3:09:06<5:35:35,  1.74s/it] 35%|███▌      | 6245/17834 [3:09:07<5:38:32,  1.75s/it] 35%|███▌      | 6246/17834 [3:09:09<5:41:25,  1.77s/it] 35%|███▌      | 6247/17834 [3:09:11<5:39:39,  1.76s/it] 35%|███▌      | 6248/17834 [3:09:13<5:41:37,  1.77s/it] 35%|███▌      | 6249/17834 [3:09:15<5:39:30,  1.76s/it]08/30/2024 22:23:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1153678894042969, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03520742058753967, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1727497577667236, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3233251571655273}
 35%|███▌      | 6250/17834 [3:09:16<5:39:51,  1.76s/it] 35%|███▌      | 6251/17834 [3:09:18<5:35:58,  1.74s/it] 35%|███▌      | 6252/17834 [3:09:20<5:41:22,  1.77s/it] 35%|███▌      | 6253/17834 [3:09:22<5:39:45,  1.76s/it] 35%|███▌      | 6254/17834 [3:09:23<5:36:32,  1.74s/it] 35%|███▌      | 6255/17834 [3:09:25<5:35:14,  1.74s/it] 35%|███▌      | 6256/17834 [3:09:27<5:33:47,  1.73s/it] 35%|███▌      | 6257/17834 [3:09:28<5:35:33,  1.74s/it] 35%|███▌      | 6258/17834 [3:09:30<5:36:42,  1.75s/it] 35%|███▌      | 6259/17834 [3:09:32<5:41:47,  1.77s/it] 35%|███▌      | 6260/17834 [3:09:34<5:39:38,  1.76s/it] 35%|███▌      | 6261/17834 [3:09:36<5:38:46,  1.76s/it] 35%|███▌      | 6262/17834 [3:09:37<5:41:43,  1.77s/it] 35%|███▌      | 6263/17834 [3:09:39<5:38:52,  1.76s/it] 35%|███▌      | 6264/17834 [3:09:41<5:36:54,  1.75s/it] 35%|███▌      | 6265/17834 [3:09:43<5:37:17,  1.75s/it] 35%|███▌      | 6266/17834 [3:09:44<5:36:00,  1.74s/it] 35%|███▌      | 6267/17834 [3:09:46<5:36:06,  1.74s/it] 35%|███▌      | 6268/17834 [3:09:48<5:36:35,  1.75s/it] 35%|███▌      | 6269/17834 [3:09:49<5:33:03,  1.73s/it] 35%|███▌      | 6270/17834 [3:09:51<5:36:35,  1.75s/it] 35%|███▌      | 6271/17834 [3:09:53<5:33:06,  1.73s/it] 35%|███▌      | 6272/17834 [3:09:55<5:32:36,  1.73s/it] 35%|███▌      | 6273/17834 [3:09:56<5:31:54,  1.72s/it] 35%|███▌      | 6274/17834 [3:09:58<5:37:35,  1.75s/it] 35%|███▌      | 6275/17834 [3:10:00<5:34:47,  1.74s/it] 35%|███▌      | 6276/17834 [3:10:02<5:33:59,  1.73s/it] 35%|███▌      | 6277/17834 [3:10:03<5:35:45,  1.74s/it] 35%|███▌      | 6278/17834 [3:10:05<5:35:34,  1.74s/it] 35%|███▌      | 6279/17834 [3:10:07<5:37:54,  1.75s/it] 35%|███▌      | 6280/17834 [3:10:09<5:36:40,  1.75s/it] 35%|███▌      | 6281/17834 [3:10:10<5:35:34,  1.74s/it] 35%|███▌      | 6282/17834 [3:10:12<5:35:06,  1.74s/it] 35%|███▌      | 6283/17834 [3:10:14<5:36:02,  1.75s/it] 35%|███▌      | 6284/17834 [3:10:16<5:33:12,  1.73s/it] 35%|███▌      | 6285/17834 [3:10:17<5:35:01,  1.74s/it] 35%|███▌      | 6286/17834 [3:10:19<5:32:26,  1.73s/it] 35%|███▌      | 6287/17834 [3:10:21<5:31:37,  1.72s/it] 35%|███▌      | 6288/17834 [3:10:23<5:32:30,  1.73s/it] 35%|███▌      | 6289/17834 [3:10:24<5:29:12,  1.71s/it] 35%|███▌      | 6290/17834 [3:10:26<5:33:36,  1.73s/it] 35%|███▌      | 6291/17834 [3:10:28<5:31:58,  1.73s/it] 35%|███▌      | 6292/17834 [3:10:29<5:35:16,  1.74s/it] 35%|███▌      | 6293/17834 [3:10:31<5:34:38,  1.74s/it] 35%|███▌      | 6294/17834 [3:10:33<5:35:57,  1.75s/it] 35%|███▌      | 6295/17834 [3:10:35<5:40:15,  1.77s/it] 35%|███▌      | 6296/17834 [3:10:37<5:39:01,  1.76s/it] 35%|███▌      | 6297/17834 [3:10:38<5:34:47,  1.74s/it] 35%|███▌      | 6298/17834 [3:10:40<5:39:11,  1.76s/it] 35%|███▌      | 6299/17834 [3:10:42<5:40:00,  1.77s/it]08/30/2024 22:25:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2143547534942627, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03593697398900986, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.289126396179199, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5394182205200195}
 35%|███▌      | 6300/17834 [3:10:44<5:44:32,  1.79s/it] 35%|███▌      | 6301/17834 [3:10:45<5:39:55,  1.77s/it] 35%|███▌      | 6302/17834 [3:10:47<5:41:14,  1.78s/it] 35%|███▌      | 6303/17834 [3:10:49<5:40:40,  1.77s/it] 35%|███▌      | 6304/17834 [3:10:51<5:37:35,  1.76s/it] 35%|███▌      | 6305/17834 [3:10:52<5:37:20,  1.76s/it] 35%|███▌      | 6306/17834 [3:10:54<5:34:41,  1.74s/it] 35%|███▌      | 6307/17834 [3:10:56<5:34:04,  1.74s/it] 35%|███▌      | 6308/17834 [3:10:58<5:34:46,  1.74s/it] 35%|███▌      | 6309/17834 [3:10:59<5:33:16,  1.74s/it] 35%|███▌      | 6310/17834 [3:11:01<5:32:13,  1.73s/it] 35%|███▌      | 6311/17834 [3:11:03<5:33:36,  1.74s/it] 35%|███▌      | 6312/17834 [3:11:04<5:32:00,  1.73s/it] 35%|███▌      | 6313/17834 [3:11:06<5:43:40,  1.79s/it] 35%|███▌      | 6314/17834 [3:11:08<5:41:31,  1.78s/it] 35%|███▌      | 6315/17834 [3:11:10<5:36:39,  1.75s/it] 35%|███▌      | 6316/17834 [3:11:12<5:36:41,  1.75s/it] 35%|███▌      | 6317/17834 [3:11:13<5:33:12,  1.74s/it] 35%|███▌      | 6318/17834 [3:11:15<5:31:40,  1.73s/it] 35%|███▌      | 6319/17834 [3:11:17<5:38:58,  1.77s/it] 35%|███▌      | 6320/17834 [3:11:19<5:35:48,  1.75s/it] 35%|███▌      | 6321/17834 [3:11:20<5:36:09,  1.75s/it] 35%|███▌      | 6322/17834 [3:11:22<5:35:11,  1.75s/it] 35%|███▌      | 6323/17834 [3:11:24<5:37:34,  1.76s/it] 35%|███▌      | 6324/17834 [3:11:26<5:40:11,  1.77s/it] 35%|███▌      | 6325/17834 [3:11:27<5:33:03,  1.74s/it] 35%|███▌      | 6326/17834 [3:11:29<5:37:01,  1.76s/it] 35%|███▌      | 6327/17834 [3:11:31<5:36:33,  1.75s/it] 35%|███▌      | 6328/17834 [3:11:33<5:36:17,  1.75s/it] 35%|███▌      | 6329/17834 [3:11:34<5:32:58,  1.74s/it] 35%|███▌      | 6330/17834 [3:11:36<5:33:51,  1.74s/it] 35%|███▌      | 6331/17834 [3:11:38<5:36:30,  1.76s/it] 36%|███▌      | 6332/17834 [3:11:40<5:31:00,  1.73s/it] 36%|███▌      | 6333/17834 [3:11:41<5:29:56,  1.72s/it] 36%|███▌      | 6334/17834 [3:11:43<5:28:35,  1.71s/it] 36%|███▌      | 6335/17834 [3:11:45<5:28:42,  1.72s/it] 36%|███▌      | 6336/17834 [3:11:46<5:32:33,  1.74s/it] 36%|███▌      | 6337/17834 [3:11:48<5:38:26,  1.77s/it] 36%|███▌      | 6338/17834 [3:11:50<5:34:37,  1.75s/it] 36%|███▌      | 6339/17834 [3:11:52<5:34:02,  1.74s/it] 36%|███▌      | 6340/17834 [3:11:54<5:39:36,  1.77s/it] 36%|███▌      | 6341/17834 [3:11:55<5:37:02,  1.76s/it] 36%|███▌      | 6342/17834 [3:11:57<5:37:02,  1.76s/it] 36%|███▌      | 6343/17834 [3:11:59<5:32:15,  1.73s/it] 36%|███▌      | 6344/17834 [3:12:00<5:32:22,  1.74s/it] 36%|███▌      | 6345/17834 [3:12:02<5:32:03,  1.73s/it] 36%|███▌      | 6346/17834 [3:12:04<5:34:02,  1.74s/it] 36%|███▌      | 6347/17834 [3:12:06<5:31:33,  1.73s/it] 36%|███▌      | 6348/17834 [3:12:07<5:34:10,  1.75s/it] 36%|███▌      | 6349/17834 [3:12:09<5:36:10,  1.76s/it]08/30/2024 22:26:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3234784603118896, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04461172595620155, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2799532413482666, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.648043394088745}
 36%|███▌      | 6350/17834 [3:12:11<5:39:05,  1.77s/it] 36%|███▌      | 6351/17834 [3:12:13<5:37:51,  1.77s/it] 36%|███▌      | 6352/17834 [3:12:15<5:42:51,  1.79s/it] 36%|███▌      | 6353/17834 [3:12:16<5:39:47,  1.78s/it] 36%|███▌      | 6354/17834 [3:12:18<5:38:39,  1.77s/it] 36%|███▌      | 6355/17834 [3:12:20<5:37:12,  1.76s/it] 36%|███▌      | 6356/17834 [3:12:22<5:32:44,  1.74s/it] 36%|███▌      | 6357/17834 [3:12:23<5:33:02,  1.74s/it] 36%|███▌      | 6358/17834 [3:12:25<5:33:20,  1.74s/it] 36%|███▌      | 6359/17834 [3:12:27<5:31:42,  1.73s/it] 36%|███▌      | 6360/17834 [3:12:29<5:35:03,  1.75s/it] 36%|███▌      | 6361/17834 [3:12:30<5:35:54,  1.76s/it] 36%|███▌      | 6362/17834 [3:12:32<5:30:20,  1.73s/it] 36%|███▌      | 6363/17834 [3:12:34<5:34:12,  1.75s/it] 36%|███▌      | 6364/17834 [3:12:35<5:32:16,  1.74s/it] 36%|███▌      | 6365/17834 [3:12:37<5:29:37,  1.72s/it] 36%|███▌      | 6366/17834 [3:12:39<5:29:24,  1.72s/it] 36%|███▌      | 6367/17834 [3:12:41<5:34:02,  1.75s/it] 36%|███▌      | 6368/17834 [3:12:42<5:34:04,  1.75s/it] 36%|███▌      | 6369/17834 [3:12:44<5:35:33,  1.76s/it] 36%|███▌      | 6370/17834 [3:12:46<5:44:20,  1.80s/it] 36%|███▌      | 6371/17834 [3:12:48<5:39:23,  1.78s/it] 36%|███▌      | 6372/17834 [3:12:50<5:33:45,  1.75s/it] 36%|███▌      | 6373/17834 [3:12:51<5:32:28,  1.74s/it] 36%|███▌      | 6374/17834 [3:12:53<5:31:56,  1.74s/it] 36%|███▌      | 6375/17834 [3:12:55<5:31:18,  1.73s/it] 36%|███▌      | 6376/17834 [3:12:57<5:39:12,  1.78s/it] 36%|███▌      | 6377/17834 [3:12:58<5:36:10,  1.76s/it] 36%|███▌      | 6378/17834 [3:13:00<5:32:13,  1.74s/it] 36%|███▌      | 6379/17834 [3:13:02<5:29:39,  1.73s/it] 36%|███▌      | 6380/17834 [3:13:03<5:27:38,  1.72s/it] 36%|███▌      | 6381/17834 [3:13:05<5:32:58,  1.74s/it] 36%|███▌      | 6382/17834 [3:13:07<5:33:15,  1.75s/it] 36%|███▌      | 6383/17834 [3:13:09<5:31:19,  1.74s/it] 36%|███▌      | 6384/17834 [3:13:10<5:32:40,  1.74s/it] 36%|███▌      | 6385/17834 [3:13:12<5:31:35,  1.74s/it] 36%|███▌      | 6386/17834 [3:13:14<5:32:36,  1.74s/it] 36%|███▌      | 6387/17834 [3:13:16<5:29:46,  1.73s/it] 36%|███▌      | 6388/17834 [3:13:17<5:35:30,  1.76s/it] 36%|███▌      | 6389/17834 [3:13:19<5:35:27,  1.76s/it] 36%|███▌      | 6390/17834 [3:13:21<5:37:54,  1.77s/it] 36%|███▌      | 6391/17834 [3:13:23<5:36:26,  1.76s/it] 36%|███▌      | 6392/17834 [3:13:24<5:33:29,  1.75s/it] 36%|███▌      | 6393/17834 [3:13:26<5:35:40,  1.76s/it] 36%|███▌      | 6394/17834 [3:13:28<5:35:03,  1.76s/it] 36%|███▌      | 6395/17834 [3:13:30<5:39:09,  1.78s/it] 36%|███▌      | 6396/17834 [3:13:32<5:41:19,  1.79s/it] 36%|███▌      | 6397/17834 [3:13:33<5:34:19,  1.75s/it] 36%|███▌      | 6398/17834 [3:13:35<5:34:25,  1.75s/it] 36%|███▌      | 6399/17834 [3:13:37<5:31:00,  1.74s/it]08/30/2024 22:27:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.103135347366333, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04465768486261368, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2032060623168945, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.350999116897583}
 36%|███▌      | 6400/17834 [3:13:39<5:34:00,  1.75s/it] 36%|███▌      | 6401/17834 [3:13:40<5:36:17,  1.76s/it] 36%|███▌      | 6402/17834 [3:13:42<5:35:06,  1.76s/it] 36%|███▌      | 6403/17834 [3:13:44<5:30:40,  1.74s/it] 36%|███▌      | 6404/17834 [3:13:46<5:31:20,  1.74s/it] 36%|███▌      | 6405/17834 [3:13:47<5:28:56,  1.73s/it] 36%|███▌      | 6406/17834 [3:13:49<5:32:28,  1.75s/it] 36%|███▌      | 6407/17834 [3:13:51<5:31:20,  1.74s/it] 36%|███▌      | 6408/17834 [3:13:52<5:30:05,  1.73s/it] 36%|███▌      | 6409/17834 [3:13:54<5:31:04,  1.74s/it] 36%|███▌      | 6410/17834 [3:13:56<5:28:15,  1.72s/it] 36%|███▌      | 6411/17834 [3:13:58<5:28:22,  1.72s/it] 36%|███▌      | 6412/17834 [3:13:59<5:28:52,  1.73s/it] 36%|███▌      | 6413/17834 [3:14:01<5:27:05,  1.72s/it] 36%|███▌      | 6414/17834 [3:14:03<5:25:26,  1.71s/it] 36%|███▌      | 6415/17834 [3:14:04<5:25:58,  1.71s/it] 36%|███▌      | 6416/17834 [3:14:06<5:28:30,  1.73s/it] 36%|███▌      | 6417/17834 [3:14:08<5:29:06,  1.73s/it] 36%|███▌      | 6418/17834 [3:14:10<5:30:03,  1.73s/it] 36%|███▌      | 6419/17834 [3:14:11<5:31:01,  1.74s/it] 36%|███▌      | 6420/17834 [3:14:13<5:32:12,  1.75s/it] 36%|███▌      | 6421/17834 [3:14:15<5:34:44,  1.76s/it] 36%|███▌      | 6422/17834 [3:14:17<5:35:05,  1.76s/it] 36%|███▌      | 6423/17834 [3:14:19<5:33:27,  1.75s/it] 36%|███▌      | 6424/17834 [3:14:20<5:34:21,  1.76s/it] 36%|███▌      | 6425/17834 [3:14:22<5:32:29,  1.75s/it] 36%|███▌      | 6426/17834 [3:14:24<5:31:23,  1.74s/it] 36%|███▌      | 6427/17834 [3:14:25<5:28:21,  1.73s/it] 36%|███▌      | 6428/17834 [3:14:27<5:29:22,  1.73s/it] 36%|███▌      | 6429/17834 [3:14:29<5:29:55,  1.74s/it] 36%|███▌      | 6430/17834 [3:14:31<5:27:20,  1.72s/it] 36%|███▌      | 6431/17834 [3:14:32<5:33:48,  1.76s/it] 36%|███▌      | 6432/17834 [3:14:34<5:29:23,  1.73s/it] 36%|███▌      | 6433/17834 [3:14:36<5:26:22,  1.72s/it] 36%|███▌      | 6434/17834 [3:14:38<5:25:39,  1.71s/it] 36%|███▌      | 6435/17834 [3:14:39<5:25:57,  1.72s/it] 36%|███▌      | 6436/17834 [3:14:41<5:28:44,  1.73s/it] 36%|███▌      | 6437/17834 [3:14:43<5:29:33,  1.74s/it] 36%|███▌      | 6438/17834 [3:14:44<5:26:54,  1.72s/it] 36%|███▌      | 6439/17834 [3:14:46<5:31:02,  1.74s/it] 36%|███▌      | 6440/17834 [3:14:48<5:30:31,  1.74s/it] 36%|███▌      | 6441/17834 [3:14:50<5:31:16,  1.74s/it] 36%|███▌      | 6442/17834 [3:14:51<5:32:34,  1.75s/it] 36%|███▌      | 6443/17834 [3:14:53<5:33:12,  1.76s/it] 36%|███▌      | 6444/17834 [3:14:55<5:36:14,  1.77s/it] 36%|███▌      | 6445/17834 [3:14:57<5:37:44,  1.78s/it] 36%|███▌      | 6446/17834 [3:14:59<5:37:58,  1.78s/it] 36%|███▌      | 6447/17834 [3:15:00<5:34:07,  1.76s/it] 36%|███▌      | 6448/17834 [3:15:02<5:35:33,  1.77s/it] 36%|███▌      | 6449/17834 [3:15:04<5:32:21,  1.75s/it]08/30/2024 22:29:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.7722599506378174, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0510011725127697, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3442931175231934, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.1675543785095215}
 36%|███▌      | 6450/17834 [3:15:06<5:32:28,  1.75s/it] 36%|███▌      | 6451/17834 [3:15:07<5:39:25,  1.79s/it] 36%|███▌      | 6452/17834 [3:15:09<5:33:11,  1.76s/it] 36%|███▌      | 6453/17834 [3:15:11<5:31:43,  1.75s/it] 36%|███▌      | 6454/17834 [3:15:13<5:32:13,  1.75s/it] 36%|███▌      | 6455/17834 [3:15:14<5:34:49,  1.77s/it] 36%|███▌      | 6456/17834 [3:15:16<5:31:40,  1.75s/it] 36%|███▌      | 6457/17834 [3:15:18<5:29:53,  1.74s/it] 36%|███▌      | 6458/17834 [3:15:20<5:34:34,  1.76s/it] 36%|███▌      | 6459/17834 [3:15:21<5:30:04,  1.74s/it] 36%|███▌      | 6460/17834 [3:15:23<5:29:04,  1.74s/it] 36%|███▌      | 6461/17834 [3:15:25<5:36:18,  1.77s/it] 36%|███▌      | 6462/17834 [3:15:27<5:39:29,  1.79s/it] 36%|███▌      | 6463/17834 [3:15:29<5:37:09,  1.78s/it] 36%|███▌      | 6464/17834 [3:15:30<5:35:01,  1.77s/it] 36%|███▋      | 6465/17834 [3:15:32<5:30:43,  1.75s/it] 36%|███▋      | 6466/17834 [3:15:34<5:29:37,  1.74s/it] 36%|███▋      | 6467/17834 [3:15:35<5:30:58,  1.75s/it] 36%|███▋      | 6468/17834 [3:15:37<5:26:53,  1.73s/it] 36%|███▋      | 6469/17834 [3:15:39<5:36:04,  1.77s/it] 36%|███▋      | 6470/17834 [3:15:41<5:33:50,  1.76s/it] 36%|███▋      | 6471/17834 [3:15:43<5:32:33,  1.76s/it] 36%|███▋      | 6472/17834 [3:15:44<5:35:38,  1.77s/it] 36%|███▋      | 6473/17834 [3:15:46<5:34:52,  1.77s/it] 36%|███▋      | 6474/17834 [3:15:48<5:30:08,  1.74s/it] 36%|███▋      | 6475/17834 [3:15:50<5:33:47,  1.76s/it] 36%|███▋      | 6476/17834 [3:15:51<5:35:45,  1.77s/it] 36%|███▋      | 6477/17834 [3:15:53<5:35:17,  1.77s/it] 36%|███▋      | 6478/17834 [3:15:55<5:40:27,  1.80s/it] 36%|███▋      | 6479/17834 [3:15:57<5:36:02,  1.78s/it] 36%|███▋      | 6480/17834 [3:15:58<5:31:42,  1.75s/it] 36%|███▋      | 6481/17834 [3:16:00<5:33:08,  1.76s/it] 36%|███▋      | 6482/17834 [3:16:02<5:34:52,  1.77s/it] 36%|███▋      | 6483/17834 [3:16:04<5:33:51,  1.76s/it] 36%|███▋      | 6484/17834 [3:16:05<5:29:53,  1.74s/it] 36%|███▋      | 6485/17834 [3:16:07<5:30:15,  1.75s/it] 36%|███▋      | 6486/17834 [3:16:09<5:30:21,  1.75s/it] 36%|███▋      | 6487/17834 [3:16:11<5:29:15,  1.74s/it] 36%|███▋      | 6488/17834 [3:16:12<5:28:47,  1.74s/it] 36%|███▋      | 6489/17834 [3:16:14<5:32:47,  1.76s/it] 36%|███▋      | 6490/17834 [3:16:16<5:30:57,  1.75s/it] 36%|███▋      | 6491/17834 [3:16:18<5:32:35,  1.76s/it] 36%|███▋      | 6492/17834 [3:16:20<5:34:20,  1.77s/it] 36%|███▋      | 6493/17834 [3:16:21<5:35:26,  1.77s/it] 36%|███▋      | 6494/17834 [3:16:23<5:35:26,  1.77s/it] 36%|███▋      | 6495/17834 [3:16:25<5:31:56,  1.76s/it] 36%|███▋      | 6496/17834 [3:16:27<5:32:36,  1.76s/it] 36%|███▋      | 6497/17834 [3:16:28<5:33:25,  1.76s/it] 36%|███▋      | 6498/17834 [3:16:30<5:30:27,  1.75s/it] 36%|███▋      | 6499/17834 [3:16:32<5:27:14,  1.73s/it]08/30/2024 22:30:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3944923877716064, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05111221969127655, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.281308174133301, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7269127368927}
 36%|███▋      | 6500/17834 [3:16:33<5:24:46,  1.72s/it] 36%|███▋      | 6501/17834 [3:16:35<5:26:18,  1.73s/it] 36%|███▋      | 6502/17834 [3:16:37<5:26:24,  1.73s/it] 36%|███▋      | 6503/17834 [3:16:39<5:30:42,  1.75s/it] 36%|███▋      | 6504/17834 [3:16:41<5:33:49,  1.77s/it] 36%|███▋      | 6505/17834 [3:16:42<5:34:09,  1.77s/it] 36%|███▋      | 6506/17834 [3:16:44<5:31:40,  1.76s/it] 36%|███▋      | 6507/17834 [3:16:46<5:32:14,  1.76s/it] 36%|███▋      | 6508/17834 [3:16:48<5:31:02,  1.75s/it] 36%|███▋      | 6509/17834 [3:16:49<5:32:57,  1.76s/it] 37%|███▋      | 6510/17834 [3:16:51<5:32:19,  1.76s/it] 37%|███▋      | 6511/17834 [3:16:53<5:38:50,  1.80s/it] 37%|███▋      | 6512/17834 [3:16:55<5:35:44,  1.78s/it] 37%|███▋      | 6513/17834 [3:16:56<5:33:45,  1.77s/it] 37%|███▋      | 6514/17834 [3:16:58<5:31:49,  1.76s/it] 37%|███▋      | 6515/17834 [3:17:00<5:32:26,  1.76s/it] 37%|███▋      | 6516/17834 [3:17:02<5:37:54,  1.79s/it] 37%|███▋      | 6517/17834 [3:17:03<5:32:02,  1.76s/it] 37%|███▋      | 6518/17834 [3:17:05<5:33:15,  1.77s/it] 37%|███▋      | 6519/17834 [3:17:07<5:31:30,  1.76s/it] 37%|███▋      | 6520/17834 [3:17:09<5:31:59,  1.76s/it] 37%|███▋      | 6521/17834 [3:17:11<5:33:45,  1.77s/it] 37%|███▋      | 6522/17834 [3:17:12<5:31:19,  1.76s/it] 37%|███▋      | 6523/17834 [3:17:14<5:32:05,  1.76s/it] 37%|███▋      | 6524/17834 [3:17:16<5:37:33,  1.79s/it] 37%|███▋      | 6525/17834 [3:17:18<5:35:29,  1.78s/it] 37%|███▋      | 6526/17834 [3:17:19<5:37:18,  1.79s/it] 37%|███▋      | 6527/17834 [3:17:21<5:33:25,  1.77s/it] 37%|███▋      | 6528/17834 [3:17:23<5:27:28,  1.74s/it] 37%|███▋      | 6529/17834 [3:17:25<5:30:01,  1.75s/it] 37%|███▋      | 6530/17834 [3:17:27<5:38:35,  1.80s/it] 37%|███▋      | 6531/17834 [3:17:28<5:32:22,  1.76s/it] 37%|███▋      | 6532/17834 [3:17:30<5:30:22,  1.75s/it] 37%|███▋      | 6533/17834 [3:17:32<5:28:43,  1.75s/it] 37%|███▋      | 6534/17834 [3:17:33<5:29:26,  1.75s/it] 37%|███▋      | 6535/17834 [3:17:35<5:31:15,  1.76s/it] 37%|███▋      | 6536/17834 [3:17:37<5:31:16,  1.76s/it] 37%|███▋      | 6537/17834 [3:17:39<5:25:26,  1.73s/it] 37%|███▋      | 6538/17834 [3:17:41<5:33:54,  1.77s/it] 37%|███▋      | 6539/17834 [3:17:42<5:30:55,  1.76s/it] 37%|███▋      | 6540/17834 [3:17:44<5:35:56,  1.78s/it] 37%|███▋      | 6541/17834 [3:17:46<5:32:04,  1.76s/it] 37%|███▋      | 6542/17834 [3:17:48<5:29:26,  1.75s/it] 37%|███▋      | 6543/17834 [3:17:49<5:33:54,  1.77s/it] 37%|███▋      | 6544/17834 [3:17:51<5:31:25,  1.76s/it] 37%|███▋      | 6545/17834 [3:17:53<5:30:54,  1.76s/it] 37%|███▋      | 6546/17834 [3:17:55<5:32:06,  1.77s/it] 37%|███▋      | 6547/17834 [3:17:56<5:28:14,  1.74s/it] 37%|███▋      | 6548/17834 [3:17:58<5:30:10,  1.76s/it] 37%|███▋      | 6549/17834 [3:18:00<5:30:19,  1.76s/it]08/30/2024 22:32:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3012301921844482, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03933548927307129, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2445173263549805, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5850830078125}
 37%|███▋      | 6550/17834 [3:18:02<5:26:21,  1.74s/it] 37%|███▋      | 6551/17834 [3:18:03<5:31:16,  1.76s/it] 37%|███▋      | 6552/17834 [3:18:05<5:33:30,  1.77s/it] 37%|███▋      | 6553/17834 [3:18:07<5:29:46,  1.75s/it] 37%|███▋      | 6554/17834 [3:18:09<5:27:57,  1.74s/it] 37%|███▋      | 6555/17834 [3:18:10<5:27:01,  1.74s/it] 37%|███▋      | 6556/17834 [3:18:12<5:26:00,  1.73s/it] 37%|███▋      | 6557/17834 [3:18:14<5:24:14,  1.73s/it] 37%|███▋      | 6558/17834 [3:18:16<5:29:34,  1.75s/it] 37%|███▋      | 6559/17834 [3:18:17<5:29:25,  1.75s/it] 37%|███▋      | 6560/17834 [3:18:19<5:31:10,  1.76s/it] 37%|███▋      | 6561/17834 [3:18:21<5:30:48,  1.76s/it] 37%|███▋      | 6562/17834 [3:18:23<5:26:58,  1.74s/it] 37%|███▋      | 6563/17834 [3:18:24<5:26:08,  1.74s/it] 37%|███▋      | 6564/17834 [3:18:26<5:29:18,  1.75s/it] 37%|███▋      | 6565/17834 [3:18:28<5:33:10,  1.77s/it] 37%|███▋      | 6566/17834 [3:18:30<5:27:51,  1.75s/it] 37%|███▋      | 6567/17834 [3:18:31<5:30:47,  1.76s/it] 37%|███▋      | 6568/17834 [3:18:33<5:29:10,  1.75s/it] 37%|███▋      | 6569/17834 [3:18:35<5:29:14,  1.75s/it] 37%|███▋      | 6570/17834 [3:18:37<5:27:16,  1.74s/it] 37%|███▋      | 6571/17834 [3:18:38<5:25:59,  1.74s/it] 37%|███▋      | 6572/17834 [3:18:40<5:22:55,  1.72s/it] 37%|███▋      | 6573/17834 [3:18:42<5:26:32,  1.74s/it] 37%|███▋      | 6574/17834 [3:18:43<5:24:49,  1.73s/it] 37%|███▋      | 6575/17834 [3:18:45<5:26:29,  1.74s/it] 37%|███▋      | 6576/17834 [3:18:47<5:27:09,  1.74s/it] 37%|███▋      | 6577/17834 [3:18:49<5:23:53,  1.73s/it] 37%|███▋      | 6578/17834 [3:18:51<5:30:05,  1.76s/it] 37%|███▋      | 6579/17834 [3:18:52<5:31:47,  1.77s/it] 37%|███▋      | 6580/17834 [3:18:54<5:36:35,  1.79s/it] 37%|███▋      | 6581/17834 [3:18:56<5:34:18,  1.78s/it] 37%|███▋      | 6582/17834 [3:18:58<5:27:05,  1.74s/it] 37%|███▋      | 6583/17834 [3:18:59<5:25:40,  1.74s/it] 37%|███▋      | 6584/17834 [3:19:01<5:29:57,  1.76s/it] 37%|███▋      | 6585/17834 [3:19:03<5:33:11,  1.78s/it] 37%|███▋      | 6586/17834 [3:19:05<5:29:33,  1.76s/it] 37%|███▋      | 6587/17834 [3:19:06<5:33:57,  1.78s/it] 37%|███▋      | 6588/17834 [3:19:08<5:31:33,  1.77s/it] 37%|███▋      | 6589/17834 [3:19:10<5:31:48,  1.77s/it] 37%|███▋      | 6590/17834 [3:19:12<5:29:42,  1.76s/it] 37%|███▋      | 6591/17834 [3:19:13<5:27:01,  1.75s/it] 37%|███▋      | 6592/17834 [3:19:15<5:26:13,  1.74s/it] 37%|███▋      | 6593/17834 [3:19:17<5:24:33,  1.73s/it] 37%|███▋      | 6594/17834 [3:19:19<5:32:57,  1.78s/it] 37%|███▋      | 6595/17834 [3:19:21<5:32:08,  1.77s/it] 37%|███▋      | 6596/17834 [3:19:22<5:29:17,  1.76s/it] 37%|███▋      | 6597/17834 [3:19:24<5:27:03,  1.75s/it] 37%|███▋      | 6598/17834 [3:19:26<5:25:59,  1.74s/it] 37%|███▋      | 6599/17834 [3:19:27<5:25:27,  1.74s/it]08/30/2024 22:33:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.625076413154602, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.057132795453071594, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4437761306762695, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.125985145568848}
 37%|███▋      | 6600/17834 [3:19:29<5:25:09,  1.74s/it] 37%|███▋      | 6601/17834 [3:19:31<5:21:31,  1.72s/it] 37%|███▋      | 6602/17834 [3:19:33<5:31:30,  1.77s/it] 37%|███▋      | 6603/17834 [3:19:34<5:27:02,  1.75s/it] 37%|███▋      | 6604/17834 [3:19:36<5:27:07,  1.75s/it] 37%|███▋      | 6605/17834 [3:19:38<5:33:54,  1.78s/it] 37%|███▋      | 6606/17834 [3:19:40<5:34:27,  1.79s/it] 37%|███▋      | 6607/17834 [3:19:42<5:31:30,  1.77s/it] 37%|███▋      | 6608/17834 [3:19:43<5:32:36,  1.78s/it] 37%|███▋      | 6609/17834 [3:19:45<5:30:31,  1.77s/it] 37%|███▋      | 6610/17834 [3:19:47<5:27:16,  1.75s/it] 37%|███▋      | 6611/17834 [3:19:49<5:31:38,  1.77s/it] 37%|███▋      | 6612/17834 [3:19:50<5:35:03,  1.79s/it] 37%|███▋      | 6613/17834 [3:19:52<5:34:38,  1.79s/it] 37%|███▋      | 6614/17834 [3:19:54<5:31:45,  1.77s/it] 37%|███▋      | 6615/17834 [3:19:56<5:26:24,  1.75s/it] 37%|███▋      | 6616/17834 [3:19:57<5:24:46,  1.74s/it] 37%|███▋      | 6617/17834 [3:19:59<5:24:47,  1.74s/it] 37%|███▋      | 6618/17834 [3:20:01<5:24:47,  1.74s/it] 37%|███▋      | 6619/17834 [3:20:03<5:23:47,  1.73s/it] 37%|███▋      | 6620/17834 [3:20:04<5:21:37,  1.72s/it] 37%|███▋      | 6621/17834 [3:20:06<5:23:41,  1.73s/it] 37%|███▋      | 6622/17834 [3:20:08<5:24:38,  1.74s/it] 37%|███▋      | 6623/17834 [3:20:10<5:26:01,  1.74s/it] 37%|███▋      | 6624/17834 [3:20:11<5:25:53,  1.74s/it] 37%|███▋      | 6625/17834 [3:20:13<5:25:09,  1.74s/it] 37%|███▋      | 6626/17834 [3:20:15<5:29:44,  1.77s/it] 37%|███▋      | 6627/17834 [3:20:17<5:24:56,  1.74s/it] 37%|███▋      | 6628/17834 [3:20:18<5:30:11,  1.77s/it] 37%|███▋      | 6629/17834 [3:20:20<5:32:37,  1.78s/it] 37%|███▋      | 6630/17834 [3:20:22<5:31:36,  1.78s/it] 37%|███▋      | 6631/17834 [3:20:24<5:37:00,  1.80s/it] 37%|███▋      | 6632/17834 [3:20:26<5:32:13,  1.78s/it] 37%|███▋      | 6633/17834 [3:20:27<5:31:02,  1.77s/it] 37%|███▋      | 6634/17834 [3:20:29<5:28:29,  1.76s/it] 37%|███▋      | 6635/17834 [3:20:31<5:28:27,  1.76s/it] 37%|███▋      | 6636/17834 [3:20:32<5:25:01,  1.74s/it] 37%|███▋      | 6637/17834 [3:20:34<5:24:53,  1.74s/it] 37%|███▋      | 6638/17834 [3:20:36<5:25:42,  1.75s/it] 37%|███▋      | 6639/17834 [3:20:38<5:23:08,  1.73s/it] 37%|███▋      | 6640/17834 [3:20:39<5:24:26,  1.74s/it] 37%|███▋      | 6641/17834 [3:20:41<5:25:14,  1.74s/it] 37%|███▋      | 6642/17834 [3:20:43<5:25:14,  1.74s/it] 37%|███▋      | 6643/17834 [3:20:45<5:28:15,  1.76s/it] 37%|███▋      | 6644/17834 [3:20:47<5:29:48,  1.77s/it] 37%|███▋      | 6645/17834 [3:20:48<5:27:29,  1.76s/it] 37%|███▋      | 6646/17834 [3:20:50<5:24:53,  1.74s/it] 37%|███▋      | 6647/17834 [3:20:52<5:29:45,  1.77s/it] 37%|███▋      | 6648/17834 [3:20:54<5:30:13,  1.77s/it] 37%|███▋      | 6649/17834 [3:20:55<5:28:35,  1.76s/it]08/30/2024 22:35:15 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5728209018707275, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04602256789803505, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2593483924865723, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8781919479370117}
 37%|███▋      | 6650/17834 [3:20:57<5:26:53,  1.75s/it] 37%|███▋      | 6651/17834 [3:20:59<5:27:09,  1.76s/it] 37%|███▋      | 6652/17834 [3:21:01<5:31:20,  1.78s/it] 37%|███▋      | 6653/17834 [3:21:02<5:26:53,  1.75s/it] 37%|███▋      | 6654/17834 [3:21:04<5:28:14,  1.76s/it] 37%|███▋      | 6655/17834 [3:21:06<5:24:38,  1.74s/it] 37%|███▋      | 6656/17834 [3:21:08<5:25:50,  1.75s/it] 37%|███▋      | 6657/17834 [3:21:09<5:28:34,  1.76s/it] 37%|███▋      | 6658/17834 [3:21:11<5:24:56,  1.74s/it] 37%|███▋      | 6659/17834 [3:21:13<5:24:17,  1.74s/it] 37%|███▋      | 6660/17834 [3:21:15<5:23:00,  1.73s/it] 37%|███▋      | 6661/17834 [3:21:16<5:27:42,  1.76s/it] 37%|███▋      | 6662/17834 [3:21:18<5:27:16,  1.76s/it] 37%|███▋      | 6663/17834 [3:21:20<5:27:36,  1.76s/it] 37%|███▋      | 6664/17834 [3:21:22<5:23:34,  1.74s/it] 37%|███▋      | 6665/17834 [3:21:23<5:21:49,  1.73s/it] 37%|███▋      | 6666/17834 [3:21:25<5:23:23,  1.74s/it] 37%|███▋      | 6667/17834 [3:21:27<5:22:21,  1.73s/it] 37%|███▋      | 6668/17834 [3:21:28<5:21:30,  1.73s/it] 37%|███▋      | 6669/17834 [3:21:30<5:25:00,  1.75s/it] 37%|███▋      | 6670/17834 [3:21:32<5:20:55,  1.72s/it] 37%|███▋      | 6671/17834 [3:21:34<5:22:13,  1.73s/it] 37%|███▋      | 6672/17834 [3:21:35<5:27:03,  1.76s/it] 37%|███▋      | 6673/17834 [3:21:37<5:24:49,  1.75s/it] 37%|███▋      | 6674/17834 [3:21:39<5:25:09,  1.75s/it] 37%|███▋      | 6675/17834 [3:21:41<5:26:58,  1.76s/it] 37%|███▋      | 6676/17834 [3:21:42<5:24:07,  1.74s/it] 37%|███▋      | 6677/17834 [3:21:44<5:22:07,  1.73s/it] 37%|███▋      | 6678/17834 [3:21:46<5:24:13,  1.74s/it] 37%|███▋      | 6679/17834 [3:21:48<5:25:40,  1.75s/it] 37%|███▋      | 6680/17834 [3:21:49<5:28:28,  1.77s/it] 37%|███▋      | 6681/17834 [3:21:51<5:25:38,  1.75s/it] 37%|███▋      | 6682/17834 [3:21:53<5:22:34,  1.74s/it] 37%|███▋      | 6683/17834 [3:21:55<5:29:31,  1.77s/it] 37%|███▋      | 6684/17834 [3:21:56<5:26:56,  1.76s/it] 37%|███▋      | 6685/17834 [3:21:58<5:24:21,  1.75s/it] 37%|███▋      | 6686/17834 [3:22:00<5:25:02,  1.75s/it] 37%|███▋      | 6687/17834 [3:22:02<5:23:38,  1.74s/it] 38%|███▊      | 6688/17834 [3:22:03<5:22:10,  1.73s/it] 38%|███▊      | 6689/17834 [3:22:05<5:23:25,  1.74s/it] 38%|███▊      | 6690/17834 [3:22:07<5:26:12,  1.76s/it] 38%|███▊      | 6691/17834 [3:22:09<5:28:14,  1.77s/it] 38%|███▊      | 6692/17834 [3:22:11<5:33:03,  1.79s/it] 38%|███▊      | 6693/17834 [3:22:12<5:27:02,  1.76s/it] 38%|███▊      | 6694/17834 [3:22:14<5:25:15,  1.75s/it] 38%|███▊      | 6695/17834 [3:22:16<5:23:32,  1.74s/it] 38%|███▊      | 6696/17834 [3:22:17<5:22:31,  1.74s/it] 38%|███▊      | 6697/17834 [3:22:19<5:24:39,  1.75s/it] 38%|███▊      | 6698/17834 [3:22:21<5:21:49,  1.73s/it] 38%|███▊      | 6699/17834 [3:22:23<5:18:35,  1.72s/it]08/30/2024 22:36:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2868815660476685, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03241240233182907, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2841873168945312, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6034812927246094}
 38%|███▊      | 6700/17834 [3:22:24<5:20:46,  1.73s/it] 38%|███▊      | 6701/17834 [3:22:26<5:25:48,  1.76s/it] 38%|███▊      | 6702/17834 [3:22:28<5:21:56,  1.74s/it] 38%|███▊      | 6703/17834 [3:22:30<5:21:41,  1.73s/it] 38%|███▊      | 6704/17834 [3:22:31<5:20:41,  1.73s/it] 38%|███▊      | 6705/17834 [3:22:33<5:25:52,  1.76s/it] 38%|███▊      | 6706/17834 [3:22:35<5:22:21,  1.74s/it] 38%|███▊      | 6707/17834 [3:22:37<5:22:07,  1.74s/it] 38%|███▊      | 6708/17834 [3:22:38<5:25:48,  1.76s/it] 38%|███▊      | 6709/17834 [3:22:40<5:22:56,  1.74s/it] 38%|███▊      | 6710/17834 [3:22:42<5:23:51,  1.75s/it] 38%|███▊      | 6711/17834 [3:22:44<5:25:39,  1.76s/it] 38%|███▊      | 6712/17834 [3:22:45<5:23:55,  1.75s/it] 38%|███▊      | 6713/17834 [3:22:47<5:21:30,  1.73s/it] 38%|███▊      | 6714/17834 [3:22:49<5:20:24,  1.73s/it] 38%|███▊      | 6715/17834 [3:22:50<5:18:23,  1.72s/it] 38%|███▊      | 6716/17834 [3:22:52<5:23:11,  1.74s/it] 38%|███▊      | 6717/17834 [3:22:54<5:21:08,  1.73s/it] 38%|███▊      | 6718/17834 [3:22:56<5:20:59,  1.73s/it] 38%|███▊      | 6719/17834 [3:22:58<5:25:04,  1.75s/it] 38%|███▊      | 6720/17834 [3:22:59<5:25:01,  1.75s/it] 38%|███▊      | 6721/17834 [3:23:01<5:24:31,  1.75s/it] 38%|███▊      | 6722/17834 [3:23:03<5:31:45,  1.79s/it] 38%|███▊      | 6723/17834 [3:23:05<5:26:57,  1.77s/it] 38%|███▊      | 6724/17834 [3:23:06<5:23:33,  1.75s/it] 38%|███▊      | 6725/17834 [3:23:08<5:24:32,  1.75s/it] 38%|███▊      | 6726/17834 [3:23:10<5:27:53,  1.77s/it] 38%|███▊      | 6727/17834 [3:23:12<5:29:46,  1.78s/it] 38%|███▊      | 6728/17834 [3:23:13<5:30:41,  1.79s/it] 38%|███▊      | 6729/17834 [3:23:15<5:27:19,  1.77s/it] 38%|███▊      | 6730/17834 [3:23:17<5:30:45,  1.79s/it] 38%|███▊      | 6731/17834 [3:23:19<5:28:29,  1.78s/it] 38%|███▊      | 6732/17834 [3:23:21<5:32:05,  1.79s/it] 38%|███▊      | 6733/17834 [3:23:22<5:26:45,  1.77s/it] 38%|███▊      | 6734/17834 [3:23:24<5:23:24,  1.75s/it] 38%|███▊      | 6735/17834 [3:23:26<5:22:25,  1.74s/it] 38%|███▊      | 6736/17834 [3:23:28<5:24:09,  1.75s/it] 38%|███▊      | 6737/17834 [3:23:29<5:22:26,  1.74s/it] 38%|███▊      | 6738/17834 [3:23:31<5:20:03,  1.73s/it] 38%|███▊      | 6739/17834 [3:23:33<5:28:07,  1.77s/it] 38%|███▊      | 6740/17834 [3:23:35<5:31:47,  1.79s/it] 38%|███▊      | 6741/17834 [3:23:36<5:27:10,  1.77s/it] 38%|███▊      | 6742/17834 [3:23:38<5:20:20,  1.73s/it] 38%|███▊      | 6743/17834 [3:23:40<5:23:51,  1.75s/it] 38%|███▊      | 6744/17834 [3:23:42<5:23:13,  1.75s/it] 38%|███▊      | 6745/17834 [3:23:43<5:27:21,  1.77s/it] 38%|███▊      | 6746/17834 [3:23:45<5:25:21,  1.76s/it] 38%|███▊      | 6747/17834 [3:23:47<5:23:38,  1.75s/it] 38%|███▊      | 6748/17834 [3:23:49<5:22:53,  1.75s/it] 38%|███▊      | 6749/17834 [3:23:50<5:23:56,  1.75s/it]08/30/2024 22:38:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.11649751663208, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03524716943502426, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.111637592315674, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2633824348449707}
 38%|███▊      | 6750/17834 [3:23:52<5:25:47,  1.76s/it] 38%|███▊      | 6751/17834 [3:23:54<5:22:30,  1.75s/it] 38%|███▊      | 6752/17834 [3:23:56<5:22:21,  1.75s/it] 38%|███▊      | 6753/17834 [3:23:57<5:21:35,  1.74s/it] 38%|███▊      | 6754/17834 [3:23:59<5:26:10,  1.77s/it] 38%|███▊      | 6755/17834 [3:24:01<5:24:32,  1.76s/it] 38%|███▊      | 6756/17834 [3:24:03<5:22:59,  1.75s/it] 38%|███▊      | 6757/17834 [3:24:04<5:26:17,  1.77s/it] 38%|███▊      | 6758/17834 [3:24:06<5:22:44,  1.75s/it] 38%|███▊      | 6759/17834 [3:24:08<5:23:03,  1.75s/it] 38%|███▊      | 6760/17834 [3:24:10<5:24:18,  1.76s/it] 38%|███▊      | 6761/17834 [3:24:11<5:26:11,  1.77s/it] 38%|███▊      | 6762/17834 [3:24:13<5:20:47,  1.74s/it] 38%|███▊      | 6763/17834 [3:24:15<5:21:49,  1.74s/it] 38%|███▊      | 6764/17834 [3:24:17<5:21:40,  1.74s/it] 38%|███▊      | 6765/17834 [3:24:18<5:23:10,  1.75s/it] 38%|███▊      | 6766/17834 [3:24:20<5:19:39,  1.73s/it] 38%|███▊      | 6767/17834 [3:24:22<5:21:43,  1.74s/it] 38%|███▊      | 6768/17834 [3:24:24<5:24:09,  1.76s/it] 38%|███▊      | 6769/17834 [3:24:25<5:25:58,  1.77s/it] 38%|███▊      | 6770/17834 [3:24:27<5:28:28,  1.78s/it] 38%|███▊      | 6771/17834 [3:24:29<5:27:39,  1.78s/it] 38%|███▊      | 6772/17834 [3:24:31<5:24:47,  1.76s/it] 38%|███▊      | 6773/17834 [3:24:32<5:23:02,  1.75s/it] 38%|███▊      | 6774/17834 [3:24:34<5:26:12,  1.77s/it] 38%|███▊      | 6775/17834 [3:24:36<5:23:24,  1.75s/it] 38%|███▊      | 6776/17834 [3:24:38<5:19:12,  1.73s/it] 38%|███▊      | 6777/17834 [3:24:39<5:22:54,  1.75s/it] 38%|███▊      | 6778/17834 [3:24:41<5:22:18,  1.75s/it] 38%|███▊      | 6779/17834 [3:24:43<5:18:26,  1.73s/it] 38%|███▊      | 6780/17834 [3:24:45<5:18:22,  1.73s/it] 38%|███▊      | 6781/17834 [3:24:46<5:18:52,  1.73s/it] 38%|███▊      | 6782/17834 [3:24:48<5:19:42,  1.74s/it] 38%|███▊      | 6783/17834 [3:24:50<5:19:28,  1.73s/it] 38%|███▊      | 6784/17834 [3:24:52<5:17:15,  1.72s/it] 38%|███▊      | 6785/17834 [3:24:53<5:14:49,  1.71s/it] 38%|███▊      | 6786/17834 [3:24:55<5:13:00,  1.70s/it] 38%|███▊      | 6787/17834 [3:24:57<5:17:14,  1.72s/it] 38%|███▊      | 6788/17834 [3:24:58<5:21:18,  1.75s/it] 38%|███▊      | 6789/17834 [3:25:00<5:20:07,  1.74s/it] 38%|███▊      | 6790/17834 [3:25:02<5:19:29,  1.74s/it] 38%|███▊      | 6791/17834 [3:25:04<5:16:43,  1.72s/it] 38%|███▊      | 6792/17834 [3:25:05<5:14:58,  1.71s/it] 38%|███▊      | 6793/17834 [3:25:07<5:15:13,  1.71s/it] 38%|███▊      | 6794/17834 [3:25:09<5:19:39,  1.74s/it] 38%|███▊      | 6795/17834 [3:25:11<5:21:14,  1.75s/it] 38%|███▊      | 6796/17834 [3:25:12<5:18:32,  1.73s/it] 38%|███▊      | 6797/17834 [3:25:14<5:20:23,  1.74s/it] 38%|███▊      | 6798/17834 [3:25:16<5:23:50,  1.76s/it] 38%|███▊      | 6799/17834 [3:25:18<5:21:54,  1.75s/it]08/30/2024 22:39:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3666355609893799, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.044887837022542953, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3709583282470703, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7824816703796387}
 38%|███▊      | 6800/17834 [3:25:19<5:25:16,  1.77s/it] 38%|███▊      | 6801/17834 [3:25:21<5:25:54,  1.77s/it] 38%|███▊      | 6802/17834 [3:25:23<5:20:31,  1.74s/it] 38%|███▊      | 6803/17834 [3:25:25<5:20:04,  1.74s/it] 38%|███▊      | 6804/17834 [3:25:26<5:16:56,  1.72s/it] 38%|███▊      | 6805/17834 [3:25:28<5:19:33,  1.74s/it] 38%|███▊      | 6806/17834 [3:25:30<5:21:27,  1.75s/it] 38%|███▊      | 6807/17834 [3:25:32<5:24:11,  1.76s/it] 38%|███▊      | 6808/17834 [3:25:33<5:23:34,  1.76s/it] 38%|███▊      | 6809/17834 [3:25:35<5:23:56,  1.76s/it] 38%|███▊      | 6810/17834 [3:25:37<5:21:50,  1.75s/it] 38%|███▊      | 6811/17834 [3:25:39<5:25:34,  1.77s/it] 38%|███▊      | 6812/17834 [3:25:40<5:19:50,  1.74s/it] 38%|███▊      | 6813/17834 [3:25:42<5:21:37,  1.75s/it] 38%|███▊      | 6814/17834 [3:25:44<5:23:21,  1.76s/it] 38%|███▊      | 6815/17834 [3:25:46<5:20:51,  1.75s/it] 38%|███▊      | 6816/17834 [3:25:47<5:17:25,  1.73s/it] 38%|███▊      | 6817/17834 [3:25:49<5:16:36,  1.72s/it] 38%|███▊      | 6818/17834 [3:25:51<5:20:46,  1.75s/it] 38%|███▊      | 6819/17834 [3:25:53<5:21:45,  1.75s/it] 38%|███▊      | 6820/17834 [3:25:54<5:21:38,  1.75s/it] 38%|███▊      | 6821/17834 [3:25:56<5:20:43,  1.75s/it] 38%|███▊      | 6822/17834 [3:25:58<5:19:02,  1.74s/it] 38%|███▊      | 6823/17834 [3:26:00<5:23:37,  1.76s/it] 38%|███▊      | 6824/17834 [3:26:01<5:21:34,  1.75s/it] 38%|███▊      | 6825/17834 [3:26:03<5:23:23,  1.76s/it] 38%|███▊      | 6826/17834 [3:26:05<5:23:18,  1.76s/it] 38%|███▊      | 6827/17834 [3:26:07<5:19:24,  1.74s/it] 38%|███▊      | 6828/17834 [3:26:08<5:17:22,  1.73s/it] 38%|███▊      | 6829/17834 [3:26:10<5:16:21,  1.72s/it] 38%|███▊      | 6830/17834 [3:26:12<5:20:55,  1.75s/it] 38%|███▊      | 6831/17834 [3:26:14<5:19:22,  1.74s/it] 38%|███▊      | 6832/17834 [3:26:15<5:22:59,  1.76s/it] 38%|███▊      | 6833/17834 [3:26:17<5:19:34,  1.74s/it] 38%|███▊      | 6834/17834 [3:26:19<5:19:29,  1.74s/it] 38%|███▊      | 6835/17834 [3:26:20<5:14:51,  1.72s/it] 38%|███▊      | 6836/17834 [3:26:22<5:12:37,  1.71s/it] 38%|███▊      | 6837/17834 [3:26:24<5:19:42,  1.74s/it] 38%|███▊      | 6838/17834 [3:26:26<5:25:02,  1.77s/it] 38%|███▊      | 6839/17834 [3:26:28<5:21:46,  1.76s/it] 38%|███▊      | 6840/17834 [3:26:29<5:20:02,  1.75s/it] 38%|███▊      | 6841/17834 [3:26:31<5:20:55,  1.75s/it] 38%|███▊      | 6842/17834 [3:26:33<5:17:26,  1.73s/it] 38%|███▊      | 6843/17834 [3:26:34<5:18:51,  1.74s/it] 38%|███▊      | 6844/17834 [3:26:36<5:20:13,  1.75s/it] 38%|███▊      | 6845/17834 [3:26:38<5:24:27,  1.77s/it] 38%|███▊      | 6846/17834 [3:26:40<5:23:43,  1.77s/it] 38%|███▊      | 6847/17834 [3:26:42<5:22:16,  1.76s/it] 38%|███▊      | 6848/17834 [3:26:43<5:19:20,  1.74s/it] 38%|███▊      | 6849/17834 [3:26:45<5:22:41,  1.76s/it]08/30/2024 22:41:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0515129566192627, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03782092407345772, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2243409156799316, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3136749267578125}
 38%|███▊      | 6850/17834 [3:26:47<5:24:48,  1.77s/it] 38%|███▊      | 6851/17834 [3:26:49<5:21:08,  1.75s/it] 38%|███▊      | 6852/17834 [3:26:50<5:24:04,  1.77s/it] 38%|███▊      | 6853/17834 [3:26:52<5:23:55,  1.77s/it] 38%|███▊      | 6854/17834 [3:26:54<5:25:38,  1.78s/it] 38%|███▊      | 6855/17834 [3:26:56<5:24:55,  1.78s/it] 38%|███▊      | 6856/17834 [3:26:57<5:22:53,  1.76s/it] 38%|███▊      | 6857/17834 [3:26:59<5:21:32,  1.76s/it] 38%|███▊      | 6858/17834 [3:27:01<5:22:57,  1.77s/it] 38%|███▊      | 6859/17834 [3:27:03<5:25:47,  1.78s/it] 38%|███▊      | 6860/17834 [3:27:05<5:22:11,  1.76s/it] 38%|███▊      | 6861/17834 [3:27:06<5:21:01,  1.76s/it] 38%|███▊      | 6862/17834 [3:27:08<5:18:28,  1.74s/it] 38%|███▊      | 6863/17834 [3:27:10<5:17:17,  1.74s/it] 38%|███▊      | 6864/17834 [3:27:11<5:15:47,  1.73s/it] 38%|███▊      | 6865/17834 [3:27:13<5:15:08,  1.72s/it] 38%|███▊      | 6866/17834 [3:27:15<5:19:42,  1.75s/it] 39%|███▊      | 6867/17834 [3:27:17<5:21:13,  1.76s/it] 39%|███▊      | 6868/17834 [3:27:18<5:19:07,  1.75s/it] 39%|███▊      | 6869/17834 [3:27:20<5:19:28,  1.75s/it] 39%|███▊      | 6870/17834 [3:27:22<5:20:08,  1.75s/it] 39%|███▊      | 6871/17834 [3:27:24<5:28:23,  1.80s/it] 39%|███▊      | 6872/17834 [3:27:26<5:23:13,  1.77s/it] 39%|███▊      | 6873/17834 [3:27:27<5:21:31,  1.76s/it] 39%|███▊      | 6874/17834 [3:27:29<5:21:01,  1.76s/it] 39%|███▊      | 6875/17834 [3:27:31<5:18:55,  1.75s/it] 39%|███▊      | 6876/17834 [3:27:32<5:15:43,  1.73s/it] 39%|███▊      | 6877/17834 [3:27:34<5:16:18,  1.73s/it] 39%|███▊      | 6878/17834 [3:27:36<5:15:50,  1.73s/it] 39%|███▊      | 6879/17834 [3:27:38<5:22:47,  1.77s/it] 39%|███▊      | 6880/17834 [3:27:40<5:23:13,  1.77s/it] 39%|███▊      | 6881/17834 [3:27:41<5:20:07,  1.75s/it] 39%|███▊      | 6882/17834 [3:27:43<5:24:26,  1.78s/it] 39%|███▊      | 6883/17834 [3:27:45<5:20:11,  1.75s/it] 39%|███▊      | 6884/17834 [3:27:47<5:20:39,  1.76s/it] 39%|███▊      | 6885/17834 [3:27:48<5:23:57,  1.78s/it] 39%|███▊      | 6886/17834 [3:27:50<5:18:52,  1.75s/it] 39%|███▊      | 6887/17834 [3:27:52<5:17:38,  1.74s/it] 39%|███▊      | 6888/17834 [3:27:53<5:16:43,  1.74s/it] 39%|███▊      | 6889/17834 [3:27:55<5:16:12,  1.73s/it] 39%|███▊      | 6890/17834 [3:27:57<5:14:14,  1.72s/it] 39%|███▊      | 6891/17834 [3:27:59<5:20:03,  1.75s/it] 39%|███▊      | 6892/17834 [3:28:00<5:19:16,  1.75s/it] 39%|███▊      | 6893/17834 [3:28:02<5:19:23,  1.75s/it] 39%|███▊      | 6894/17834 [3:28:04<5:16:53,  1.74s/it] 39%|███▊      | 6895/17834 [3:28:06<5:16:09,  1.73s/it] 39%|███▊      | 6896/17834 [3:28:07<5:21:08,  1.76s/it] 39%|███▊      | 6897/17834 [3:28:09<5:19:20,  1.75s/it] 39%|███▊      | 6898/17834 [3:28:11<5:15:53,  1.73s/it] 39%|███▊      | 6899/17834 [3:28:13<5:14:25,  1.73s/it]08/30/2024 22:42:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4575252532958984, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03974467143416405, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.340247631072998, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.837517499923706}
 39%|███▊      | 6900/17834 [3:28:14<5:15:52,  1.73s/it] 39%|███▊      | 6901/17834 [3:28:16<5:14:10,  1.72s/it] 39%|███▊      | 6902/17834 [3:28:18<5:26:08,  1.79s/it] 39%|███▊      | 6903/17834 [3:28:20<5:18:51,  1.75s/it] 39%|███▊      | 6904/17834 [3:28:22<5:24:07,  1.78s/it] 39%|███▊      | 6905/17834 [3:28:23<5:23:18,  1.77s/it] 39%|███▊      | 6906/17834 [3:28:25<5:23:28,  1.78s/it] 39%|███▊      | 6907/17834 [3:28:27<5:21:39,  1.77s/it] 39%|███▊      | 6908/17834 [3:28:29<5:20:56,  1.76s/it] 39%|███▊      | 6909/17834 [3:28:30<5:20:37,  1.76s/it] 39%|███▊      | 6910/17834 [3:28:32<5:19:56,  1.76s/it] 39%|███▉      | 6911/17834 [3:28:34<5:17:36,  1.74s/it] 39%|███▉      | 6912/17834 [3:28:36<5:18:38,  1.75s/it] 39%|███▉      | 6913/17834 [3:28:37<5:21:26,  1.77s/it] 39%|███▉      | 6914/17834 [3:28:39<5:17:49,  1.75s/it] 39%|███▉      | 6915/17834 [3:28:41<5:16:44,  1.74s/it] 39%|███▉      | 6916/17834 [3:28:42<5:15:31,  1.73s/it] 39%|███▉      | 6917/17834 [3:28:44<5:16:52,  1.74s/it] 39%|███▉      | 6918/17834 [3:28:46<5:14:56,  1.73s/it] 39%|███▉      | 6919/17834 [3:28:48<5:14:50,  1.73s/it] 39%|███▉      | 6920/17834 [3:28:49<5:16:54,  1.74s/it] 39%|███▉      | 6921/17834 [3:28:51<5:17:46,  1.75s/it] 39%|███▉      | 6922/17834 [3:28:53<5:16:57,  1.74s/it] 39%|███▉      | 6923/17834 [3:28:55<5:15:32,  1.74s/it] 39%|███▉      | 6924/17834 [3:28:56<5:19:31,  1.76s/it] 39%|███▉      | 6925/17834 [3:28:58<5:22:13,  1.77s/it] 39%|███▉      | 6926/17834 [3:29:00<5:25:14,  1.79s/it] 39%|███▉      | 6927/17834 [3:29:02<5:19:21,  1.76s/it] 39%|███▉      | 6928/17834 [3:29:03<5:14:43,  1.73s/it] 39%|███▉      | 6929/17834 [3:29:05<5:12:32,  1.72s/it] 39%|███▉      | 6930/17834 [3:29:07<5:16:44,  1.74s/it] 39%|███▉      | 6931/17834 [3:29:09<5:13:06,  1.72s/it] 39%|███▉      | 6932/17834 [3:29:10<5:15:12,  1.73s/it] 39%|███▉      | 6933/17834 [3:29:12<5:17:55,  1.75s/it] 39%|███▉      | 6934/17834 [3:29:14<5:25:05,  1.79s/it] 39%|███▉      | 6935/17834 [3:29:16<5:19:58,  1.76s/it] 39%|███▉      | 6936/17834 [3:29:17<5:15:55,  1.74s/it] 39%|███▉      | 6937/17834 [3:29:19<5:20:13,  1.76s/it] 39%|███▉      | 6938/17834 [3:29:21<5:16:08,  1.74s/it] 39%|███▉      | 6939/17834 [3:29:23<5:20:07,  1.76s/it] 39%|███▉      | 6940/17834 [3:29:24<5:18:27,  1.75s/it] 39%|███▉      | 6941/17834 [3:29:26<5:22:16,  1.78s/it] 39%|███▉      | 6942/17834 [3:29:28<5:17:30,  1.75s/it] 39%|███▉      | 6943/17834 [3:29:30<5:16:05,  1.74s/it] 39%|███▉      | 6944/17834 [3:29:31<5:14:57,  1.74s/it] 39%|███▉      | 6945/17834 [3:29:33<5:16:56,  1.75s/it] 39%|███▉      | 6946/17834 [3:29:35<5:23:23,  1.78s/it] 39%|███▉      | 6947/17834 [3:29:37<5:21:11,  1.77s/it] 39%|███▉      | 6948/17834 [3:29:39<5:21:07,  1.77s/it] 39%|███▉      | 6949/17834 [3:29:40<5:18:01,  1.75s/it]08/30/2024 22:44:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3464479446411133, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045372769236564636, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3629953861236572, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7548160552978516}
 39%|███▉      | 6950/17834 [3:29:42<5:16:30,  1.74s/it] 39%|███▉      | 6951/17834 [3:29:44<5:17:19,  1.75s/it] 39%|███▉      | 6952/17834 [3:29:46<5:16:58,  1.75s/it] 39%|███▉      | 6953/17834 [3:29:47<5:13:51,  1.73s/it] 39%|███▉      | 6954/17834 [3:29:49<5:14:52,  1.74s/it] 39%|███▉      | 6955/17834 [3:29:51<5:16:02,  1.74s/it] 39%|███▉      | 6956/17834 [3:29:52<5:14:19,  1.73s/it] 39%|███▉      | 6957/17834 [3:29:54<5:20:13,  1.77s/it] 39%|███▉      | 6958/17834 [3:29:56<5:17:54,  1.75s/it] 39%|███▉      | 6959/17834 [3:29:58<5:13:54,  1.73s/it] 39%|███▉      | 6960/17834 [3:29:59<5:12:50,  1.73s/it] 39%|███▉      | 6961/17834 [3:30:01<5:16:48,  1.75s/it] 39%|███▉      | 6962/17834 [3:30:03<5:16:42,  1.75s/it] 39%|███▉      | 6963/17834 [3:30:05<5:15:01,  1.74s/it] 39%|███▉      | 6964/17834 [3:30:06<5:17:59,  1.76s/it] 39%|███▉      | 6965/17834 [3:30:08<5:20:04,  1.77s/it] 39%|███▉      | 6966/17834 [3:30:10<5:14:28,  1.74s/it] 39%|███▉      | 6967/17834 [3:30:12<5:18:33,  1.76s/it] 39%|███▉      | 6968/17834 [3:30:13<5:15:17,  1.74s/it] 39%|███▉      | 6969/17834 [3:30:15<5:16:24,  1.75s/it] 39%|███▉      | 6970/17834 [3:30:17<5:16:02,  1.75s/it] 39%|███▉      | 6971/17834 [3:30:19<5:17:16,  1.75s/it] 39%|███▉      | 6972/17834 [3:30:20<5:17:32,  1.75s/it] 39%|███▉      | 6973/17834 [3:30:22<5:17:34,  1.75s/it] 39%|███▉      | 6974/17834 [3:30:24<5:16:34,  1.75s/it] 39%|███▉      | 6975/17834 [3:30:26<5:23:32,  1.79s/it] 39%|███▉      | 6976/17834 [3:30:28<5:20:37,  1.77s/it] 39%|███▉      | 6977/17834 [3:30:29<5:23:08,  1.79s/it] 39%|███▉      | 6978/17834 [3:30:31<5:18:28,  1.76s/it] 39%|███▉      | 6979/17834 [3:30:33<5:16:39,  1.75s/it] 39%|███▉      | 6980/17834 [3:30:35<5:19:18,  1.77s/it] 39%|███▉      | 6981/17834 [3:30:36<5:18:46,  1.76s/it] 39%|███▉      | 6982/17834 [3:30:38<5:16:19,  1.75s/it] 39%|███▉      | 6983/17834 [3:30:40<5:18:00,  1.76s/it] 39%|███▉      | 6984/17834 [3:30:42<5:18:25,  1.76s/it] 39%|███▉      | 6985/17834 [3:30:43<5:18:09,  1.76s/it] 39%|███▉      | 6986/17834 [3:30:45<5:18:53,  1.76s/it] 39%|███▉      | 6987/17834 [3:30:47<5:17:57,  1.76s/it] 39%|███▉      | 6988/17834 [3:30:49<5:19:18,  1.77s/it] 39%|███▉      | 6989/17834 [3:30:50<5:18:16,  1.76s/it] 39%|███▉      | 6990/17834 [3:30:52<5:15:05,  1.74s/it] 39%|███▉      | 6991/17834 [3:30:54<5:16:17,  1.75s/it] 39%|███▉      | 6992/17834 [3:30:56<5:18:19,  1.76s/it] 39%|███▉      | 6993/17834 [3:30:57<5:17:01,  1.75s/it] 39%|███▉      | 6994/17834 [3:30:59<5:18:30,  1.76s/it] 39%|███▉      | 6995/17834 [3:31:01<5:16:29,  1.75s/it] 39%|███▉      | 6996/17834 [3:31:03<5:25:01,  1.80s/it] 39%|███▉      | 6997/17834 [3:31:05<5:16:48,  1.75s/it] 39%|███▉      | 6998/17834 [3:31:06<5:15:40,  1.75s/it] 39%|███▉      | 6999/17834 [3:31:08<5:15:49,  1.75s/it]08/30/2024 22:45:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1027849912643433, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024527231231331825, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.166752338409424, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.294064521789551}
 39%|███▉      | 7000/17834 [3:31:10<5:15:45,  1.75s/it] 39%|███▉      | 7001/17834 [3:31:12<5:17:28,  1.76s/it] 39%|███▉      | 7002/17834 [3:31:13<5:18:45,  1.77s/it] 39%|███▉      | 7003/17834 [3:31:15<5:20:08,  1.77s/it] 39%|███▉      | 7004/17834 [3:31:17<5:19:29,  1.77s/it] 39%|███▉      | 7005/17834 [3:31:19<5:17:47,  1.76s/it] 39%|███▉      | 7006/17834 [3:31:20<5:17:44,  1.76s/it] 39%|███▉      | 7007/17834 [3:31:22<5:17:50,  1.76s/it] 39%|███▉      | 7008/17834 [3:31:24<5:16:51,  1.76s/it] 39%|███▉      | 7009/17834 [3:31:26<5:19:58,  1.77s/it] 39%|███▉      | 7010/17834 [3:31:28<5:23:02,  1.79s/it] 39%|███▉      | 7011/17834 [3:31:29<5:19:41,  1.77s/it] 39%|███▉      | 7012/17834 [3:31:31<5:16:14,  1.75s/it] 39%|███▉      | 7013/17834 [3:31:33<5:13:34,  1.74s/it] 39%|███▉      | 7014/17834 [3:31:34<5:16:51,  1.76s/it] 39%|███▉      | 7015/17834 [3:31:36<5:20:26,  1.78s/it] 39%|███▉      | 7016/17834 [3:31:38<5:20:52,  1.78s/it] 39%|███▉      | 7017/17834 [3:31:40<5:21:12,  1.78s/it] 39%|███▉      | 7018/17834 [3:31:42<5:18:50,  1.77s/it] 39%|███▉      | 7019/17834 [3:31:43<5:17:24,  1.76s/it] 39%|███▉      | 7020/17834 [3:31:45<5:13:56,  1.74s/it] 39%|███▉      | 7021/17834 [3:31:47<5:18:41,  1.77s/it] 39%|███▉      | 7022/17834 [3:31:49<5:17:40,  1.76s/it] 39%|███▉      | 7023/17834 [3:31:50<5:16:13,  1.76s/it] 39%|███▉      | 7024/17834 [3:31:52<5:18:03,  1.77s/it] 39%|███▉      | 7025/17834 [3:31:54<5:18:11,  1.77s/it] 39%|███▉      | 7026/17834 [3:31:56<5:14:49,  1.75s/it] 39%|███▉      | 7027/17834 [3:31:57<5:17:13,  1.76s/it] 39%|███▉      | 7028/17834 [3:31:59<5:19:20,  1.77s/it] 39%|███▉      | 7029/17834 [3:32:01<5:20:43,  1.78s/it] 39%|███▉      | 7030/17834 [3:32:03<5:24:21,  1.80s/it] 39%|███▉      | 7031/17834 [3:32:05<5:20:59,  1.78s/it] 39%|███▉      | 7032/17834 [3:32:06<5:18:45,  1.77s/it] 39%|███▉      | 7033/17834 [3:32:08<5:20:12,  1.78s/it] 39%|███▉      | 7034/17834 [3:32:10<5:20:13,  1.78s/it] 39%|███▉      | 7035/17834 [3:32:12<5:20:44,  1.78s/it] 39%|███▉      | 7036/17834 [3:32:13<5:17:04,  1.76s/it] 39%|███▉      | 7037/17834 [3:32:15<5:16:44,  1.76s/it] 39%|███▉      | 7038/17834 [3:32:17<5:18:12,  1.77s/it] 39%|███▉      | 7039/17834 [3:32:19<5:16:28,  1.76s/it] 39%|███▉      | 7040/17834 [3:32:20<5:13:41,  1.74s/it] 39%|███▉      | 7041/17834 [3:32:22<5:15:31,  1.75s/it] 39%|███▉      | 7042/17834 [3:32:24<5:15:15,  1.75s/it] 39%|███▉      | 7043/17834 [3:32:26<5:15:01,  1.75s/it] 39%|███▉      | 7044/17834 [3:32:28<5:19:10,  1.77s/it] 40%|███▉      | 7045/17834 [3:32:29<5:14:03,  1.75s/it] 40%|███▉      | 7046/17834 [3:32:31<5:14:33,  1.75s/it] 40%|███▉      | 7047/17834 [3:32:33<5:16:06,  1.76s/it] 40%|███▉      | 7048/17834 [3:32:35<5:17:20,  1.77s/it] 40%|███▉      | 7049/17834 [3:32:36<5:16:51,  1.76s/it]08/30/2024 22:46:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.810366153717041, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04866091161966324, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.5476083755493164, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.406635284423828}
 40%|███▉      | 7050/17834 [3:32:38<5:14:10,  1.75s/it] 40%|███▉      | 7051/17834 [3:32:40<5:16:36,  1.76s/it] 40%|███▉      | 7052/17834 [3:32:42<5:15:56,  1.76s/it] 40%|███▉      | 7053/17834 [3:32:43<5:10:39,  1.73s/it] 40%|███▉      | 7054/17834 [3:32:45<5:14:25,  1.75s/it] 40%|███▉      | 7055/17834 [3:32:47<5:12:16,  1.74s/it] 40%|███▉      | 7056/17834 [3:32:48<5:11:39,  1.73s/it] 40%|███▉      | 7057/17834 [3:32:50<5:12:32,  1.74s/it] 40%|███▉      | 7058/17834 [3:32:52<5:13:55,  1.75s/it] 40%|███▉      | 7059/17834 [3:32:54<5:11:14,  1.73s/it] 40%|███▉      | 7060/17834 [3:32:55<5:12:58,  1.74s/it] 40%|███▉      | 7061/17834 [3:32:57<5:11:08,  1.73s/it] 40%|███▉      | 7062/17834 [3:32:59<5:10:13,  1.73s/it] 40%|███▉      | 7063/17834 [3:33:01<5:10:04,  1.73s/it] 40%|███▉      | 7064/17834 [3:33:02<5:12:51,  1.74s/it] 40%|███▉      | 7065/17834 [3:33:04<5:09:07,  1.72s/it] 40%|███▉      | 7066/17834 [3:33:06<5:13:30,  1.75s/it] 40%|███▉      | 7067/17834 [3:33:08<5:14:46,  1.75s/it] 40%|███▉      | 7068/17834 [3:33:09<5:16:50,  1.77s/it] 40%|███▉      | 7069/17834 [3:33:11<5:16:43,  1.77s/it] 40%|███▉      | 7070/17834 [3:33:13<5:14:13,  1.75s/it] 40%|███▉      | 7071/17834 [3:33:15<5:13:46,  1.75s/it] 40%|███▉      | 7072/17834 [3:33:16<5:11:39,  1.74s/it] 40%|███▉      | 7073/17834 [3:33:18<5:12:39,  1.74s/it] 40%|███▉      | 7074/17834 [3:33:20<5:11:40,  1.74s/it] 40%|███▉      | 7075/17834 [3:33:22<5:12:18,  1.74s/it] 40%|███▉      | 7076/17834 [3:33:23<5:09:24,  1.73s/it] 40%|███▉      | 7077/17834 [3:33:25<5:14:10,  1.75s/it] 40%|███▉      | 7078/17834 [3:33:27<5:10:16,  1.73s/it] 40%|███▉      | 7079/17834 [3:33:28<5:11:04,  1.74s/it] 40%|███▉      | 7080/17834 [3:33:30<5:14:43,  1.76s/it] 40%|███▉      | 7081/17834 [3:33:32<5:13:20,  1.75s/it] 40%|███▉      | 7082/17834 [3:33:34<5:12:30,  1.74s/it] 40%|███▉      | 7083/17834 [3:33:36<5:13:48,  1.75s/it] 40%|███▉      | 7084/17834 [3:33:37<5:18:16,  1.78s/it] 40%|███▉      | 7085/17834 [3:33:39<5:16:42,  1.77s/it] 40%|███▉      | 7086/17834 [3:33:41<5:16:17,  1.77s/it] 40%|███▉      | 7087/17834 [3:33:43<5:15:59,  1.76s/it] 40%|███▉      | 7088/17834 [3:33:44<5:14:27,  1.76s/it] 40%|███▉      | 7089/17834 [3:33:46<5:13:04,  1.75s/it] 40%|███▉      | 7090/17834 [3:33:48<5:14:17,  1.76s/it] 40%|███▉      | 7091/17834 [3:33:50<5:11:14,  1.74s/it] 40%|███▉      | 7092/17834 [3:33:51<5:11:56,  1.74s/it] 40%|███▉      | 7093/17834 [3:33:53<5:12:25,  1.75s/it] 40%|███▉      | 7094/17834 [3:33:55<5:13:08,  1.75s/it] 40%|███▉      | 7095/17834 [3:33:57<5:13:03,  1.75s/it] 40%|███▉      | 7096/17834 [3:33:58<5:11:53,  1.74s/it] 40%|███▉      | 7097/17834 [3:34:00<5:08:16,  1.72s/it] 40%|███▉      | 7098/17834 [3:34:02<5:12:49,  1.75s/it] 40%|███▉      | 7099/17834 [3:34:03<5:08:41,  1.73s/it]08/30/2024 22:48:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0166473388671875, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030605614185333252, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.181525230407715, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.228778123855591}
 40%|███▉      | 7100/17834 [3:34:05<5:15:41,  1.76s/it] 40%|███▉      | 7101/17834 [3:34:07<5:10:40,  1.74s/it] 40%|███▉      | 7102/17834 [3:34:09<5:15:04,  1.76s/it] 40%|███▉      | 7103/17834 [3:34:10<5:10:36,  1.74s/it] 40%|███▉      | 7104/17834 [3:34:12<5:13:05,  1.75s/it] 40%|███▉      | 7105/17834 [3:34:14<5:15:42,  1.77s/it] 40%|███▉      | 7106/17834 [3:34:16<5:17:56,  1.78s/it] 40%|███▉      | 7107/17834 [3:34:18<5:15:43,  1.77s/it] 40%|███▉      | 7108/17834 [3:34:19<5:14:29,  1.76s/it] 40%|███▉      | 7109/17834 [3:34:21<5:12:51,  1.75s/it] 40%|███▉      | 7110/17834 [3:34:23<5:16:12,  1.77s/it] 40%|███▉      | 7111/17834 [3:34:25<5:12:35,  1.75s/it] 40%|███▉      | 7112/17834 [3:34:26<5:13:55,  1.76s/it] 40%|███▉      | 7113/17834 [3:34:28<5:14:58,  1.76s/it] 40%|███▉      | 7114/17834 [3:34:30<5:12:24,  1.75s/it] 40%|███▉      | 7115/17834 [3:34:32<5:17:20,  1.78s/it] 40%|███▉      | 7116/17834 [3:34:33<5:17:32,  1.78s/it] 40%|███▉      | 7117/17834 [3:34:35<5:13:48,  1.76s/it] 40%|███▉      | 7118/17834 [3:34:37<5:16:09,  1.77s/it] 40%|███▉      | 7119/17834 [3:34:39<5:13:45,  1.76s/it] 40%|███▉      | 7120/17834 [3:34:40<5:13:07,  1.75s/it] 40%|███▉      | 7121/17834 [3:34:42<5:11:59,  1.75s/it] 40%|███▉      | 7122/17834 [3:34:44<5:13:29,  1.76s/it] 40%|███▉      | 7123/17834 [3:34:46<5:12:10,  1.75s/it] 40%|███▉      | 7124/17834 [3:34:47<5:12:43,  1.75s/it] 40%|███▉      | 7125/17834 [3:34:49<5:16:55,  1.78s/it] 40%|███▉      | 7126/17834 [3:34:51<5:14:41,  1.76s/it] 40%|███▉      | 7127/17834 [3:34:53<5:16:10,  1.77s/it]08/30/2024 22:49:12 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/30/2024 22:49:12 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:25,  8.80it/s][A
  1%|          | 2/221 [00:00<00:42,  5.17it/s][A
  1%|▏         | 3/221 [00:00<00:53,  4.08it/s][A
  2%|▏         | 4/221 [00:00<00:44,  4.83it/s][A
  2%|▏         | 5/221 [00:01<00:57,  3.73it/s][A
  3%|▎         | 6/221 [00:01<00:50,  4.23it/s][A
  3%|▎         | 7/221 [00:01<00:57,  3.69it/s][A
  4%|▎         | 8/221 [00:01<00:53,  4.00it/s][A
  4%|▍         | 9/221 [00:02<01:14,  2.85it/s][A
  5%|▍         | 10/221 [00:02<01:19,  2.66it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.21it/s][A
  5%|▌         | 12/221 [00:03<00:56,  3.72it/s][A
  6%|▌         | 13/221 [00:03<00:54,  3.81it/s][A
  6%|▋         | 14/221 [00:03<00:56,  3.68it/s][A
  7%|▋         | 15/221 [00:04<00:51,  3.98it/s][A
  7%|▋         | 16/221 [00:04<00:52,  3.93it/s][A
  8%|▊         | 17/221 [00:04<00:47,  4.28it/s][A
  8%|▊         | 18/221 [00:04<00:40,  5.02it/s][A
  9%|▊         | 19/221 [00:04<00:47,  4.21it/s][A
  9%|▉         | 20/221 [00:05<00:41,  4.81it/s][A
 10%|▉         | 21/221 [00:05<00:40,  4.98it/s][A
 10%|▉         | 22/221 [00:05<00:46,  4.24it/s][A
 10%|█         | 23/221 [00:05<00:47,  4.17it/s][A
 11%|█         | 24/221 [00:05<00:45,  4.37it/s][A
 11%|█▏        | 25/221 [00:06<00:46,  4.23it/s][A
 12%|█▏        | 26/221 [00:06<00:39,  4.99it/s][A
 12%|█▏        | 27/221 [00:06<00:53,  3.61it/s][A
 13%|█▎        | 28/221 [00:07<01:07,  2.86it/s][A
 13%|█▎        | 29/221 [00:07<01:10,  2.74it/s][A
 14%|█▎        | 30/221 [00:08<01:11,  2.67it/s][A
 14%|█▍        | 31/221 [00:08<01:11,  2.67it/s][A
 14%|█▍        | 32/221 [00:08<01:02,  3.02it/s][A
 15%|█▍        | 33/221 [00:08<00:55,  3.36it/s][A
 15%|█▌        | 34/221 [00:09<00:51,  3.61it/s][A
 16%|█▌        | 35/221 [00:09<00:54,  3.39it/s][A
 16%|█▋        | 36/221 [00:09<01:01,  3.02it/s][A
 17%|█▋        | 37/221 [00:10<00:54,  3.39it/s][A
 17%|█▋        | 38/221 [00:10<00:53,  3.44it/s][A
 18%|█▊        | 39/221 [00:10<00:44,  4.12it/s][A
 18%|█▊        | 40/221 [00:10<00:46,  3.88it/s][A
 19%|█▊        | 41/221 [00:11<00:46,  3.86it/s][A
 19%|█▉        | 43/221 [00:11<00:36,  4.87it/s][A
 20%|█▉        | 44/221 [00:11<00:41,  4.27it/s][A
 20%|██        | 45/221 [00:12<00:49,  3.55it/s][A
 21%|██        | 46/221 [00:12<00:47,  3.70it/s][A
 21%|██▏       | 47/221 [00:12<00:45,  3.86it/s][A
 22%|██▏       | 48/221 [00:12<00:40,  4.31it/s][A
 22%|██▏       | 49/221 [00:12<00:35,  4.88it/s][A
 23%|██▎       | 50/221 [00:13<00:40,  4.20it/s][A
 23%|██▎       | 51/221 [00:13<00:46,  3.64it/s][A
 24%|██▎       | 52/221 [00:13<00:41,  4.06it/s][A
 24%|██▍       | 53/221 [00:13<00:36,  4.60it/s][A
 24%|██▍       | 54/221 [00:14<00:38,  4.33it/s][A
 25%|██▍       | 55/221 [00:14<00:39,  4.18it/s][A
 25%|██▌       | 56/221 [00:14<00:41,  3.99it/s][A
 26%|██▌       | 57/221 [00:15<00:48,  3.38it/s][A
 26%|██▌       | 58/221 [00:15<00:41,  3.97it/s][A
 27%|██▋       | 59/221 [00:15<00:35,  4.61it/s][A
 27%|██▋       | 60/221 [00:15<00:34,  4.67it/s][A
 28%|██▊       | 61/221 [00:15<00:37,  4.26it/s][A
 28%|██▊       | 62/221 [00:16<00:34,  4.55it/s][A
 29%|██▊       | 63/221 [00:16<00:35,  4.47it/s][A
 29%|██▉       | 64/221 [00:16<00:50,  3.12it/s][A
 29%|██▉       | 65/221 [00:17<00:50,  3.07it/s][A
 30%|██▉       | 66/221 [00:17<00:51,  3.00it/s][A
 30%|███       | 67/221 [00:17<00:51,  2.98it/s][A
 31%|███       | 68/221 [00:18<00:43,  3.52it/s][A
 31%|███       | 69/221 [00:18<00:52,  2.88it/s][A
 32%|███▏      | 70/221 [00:18<00:54,  2.77it/s][A
 32%|███▏      | 71/221 [00:19<00:49,  3.02it/s][A
 33%|███▎      | 72/221 [00:19<00:57,  2.60it/s][A
 33%|███▎      | 73/221 [00:20<00:53,  2.74it/s][A
 33%|███▎      | 74/221 [00:20<00:43,  3.40it/s][A
 34%|███▍      | 75/221 [00:20<00:40,  3.64it/s][A
 34%|███▍      | 76/221 [00:20<00:47,  3.06it/s][A
 35%|███▍      | 77/221 [00:21<00:59,  2.44it/s][A
 35%|███▌      | 78/221 [00:21<01:00,  2.36it/s][A
 36%|███▌      | 79/221 [00:22<01:00,  2.36it/s][A
 36%|███▌      | 80/221 [00:22<00:52,  2.66it/s][A
 37%|███▋      | 82/221 [00:22<00:40,  3.45it/s][A
 38%|███▊      | 83/221 [00:23<00:45,  3.02it/s][A
 38%|███▊      | 84/221 [00:23<00:40,  3.34it/s][A
 38%|███▊      | 85/221 [00:23<00:35,  3.81it/s][A
 39%|███▉      | 86/221 [00:24<00:49,  2.74it/s][A
 39%|███▉      | 87/221 [00:24<00:53,  2.51it/s][A
 40%|███▉      | 88/221 [00:25<00:55,  2.39it/s][A
 40%|████      | 89/221 [00:25<00:50,  2.62it/s][A
 41%|████      | 90/221 [00:25<00:46,  2.83it/s][A
 41%|████      | 91/221 [00:26<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:26<00:40,  3.21it/s][A
 42%|████▏     | 93/221 [00:27<00:48,  2.62it/s][A
 43%|████▎     | 94/221 [00:27<00:43,  2.92it/s][A
 43%|████▎     | 95/221 [00:27<00:38,  3.26it/s][A
 43%|████▎     | 96/221 [00:27<00:38,  3.24it/s][A
 44%|████▍     | 97/221 [00:28<00:39,  3.12it/s][A
 44%|████▍     | 98/221 [00:28<00:43,  2.85it/s][A
 45%|████▍     | 99/221 [00:28<00:40,  3.03it/s][A
 45%|████▌     | 100/221 [00:29<00:40,  3.00it/s][A
 46%|████▌     | 101/221 [00:29<00:37,  3.24it/s][A
 46%|████▌     | 102/221 [00:29<00:35,  3.40it/s][A
 47%|████▋     | 103/221 [00:30<00:35,  3.33it/s][A
 47%|████▋     | 104/221 [00:30<00:41,  2.82it/s][A
 48%|████▊     | 105/221 [00:30<00:41,  2.78it/s][A
 48%|████▊     | 106/221 [00:31<00:37,  3.05it/s][A
 48%|████▊     | 107/221 [00:31<00:35,  3.20it/s][A
 49%|████▉     | 108/221 [00:31<00:38,  2.95it/s][A
 49%|████▉     | 109/221 [00:32<00:33,  3.30it/s][A
 50%|████▉     | 110/221 [00:32<00:31,  3.48it/s][A
 50%|█████     | 111/221 [00:32<00:30,  3.63it/s][A
 51%|█████     | 112/221 [00:32<00:30,  3.56it/s][A
 51%|█████     | 113/221 [00:33<00:30,  3.58it/s][A
 52%|█████▏    | 114/221 [00:33<00:26,  4.09it/s][A
 52%|█████▏    | 115/221 [00:33<00:36,  2.94it/s][A
 52%|█████▏    | 116/221 [00:34<00:30,  3.41it/s][A
 53%|█████▎    | 117/221 [00:34<00:28,  3.63it/s][A
 53%|█████▎    | 118/221 [00:34<00:26,  3.82it/s][A
 54%|█████▍    | 119/221 [00:34<00:29,  3.52it/s][A
 54%|█████▍    | 120/221 [00:35<00:31,  3.16it/s][A
 55%|█████▍    | 121/221 [00:35<00:28,  3.57it/s][A
 55%|█████▌    | 122/221 [00:35<00:29,  3.33it/s][A
 56%|█████▌    | 123/221 [00:35<00:26,  3.67it/s][A
 56%|█████▌    | 124/221 [00:36<00:28,  3.41it/s][A
 57%|█████▋    | 125/221 [00:36<00:29,  3.22it/s][A
 57%|█████▋    | 126/221 [00:36<00:23,  3.96it/s][A
 57%|█████▋    | 127/221 [00:37<00:26,  3.58it/s][A
 58%|█████▊    | 128/221 [00:37<00:27,  3.39it/s][A
 58%|█████▊    | 129/221 [00:37<00:26,  3.51it/s][A
 59%|█████▉    | 130/221 [00:38<00:27,  3.34it/s][A
 59%|█████▉    | 131/221 [00:38<00:27,  3.27it/s][A
 60%|█████▉    | 132/221 [00:38<00:31,  2.79it/s][A
 60%|██████    | 133/221 [00:39<00:32,  2.71it/s][A
 61%|██████    | 134/221 [00:39<00:33,  2.61it/s][A
 61%|██████    | 135/221 [00:39<00:30,  2.84it/s][A
 62%|██████▏   | 136/221 [00:40<00:26,  3.20it/s][A
 62%|██████▏   | 137/221 [00:40<00:21,  3.94it/s][A
 62%|██████▏   | 138/221 [00:40<00:19,  4.16it/s][A
 63%|██████▎   | 139/221 [00:40<00:20,  3.94it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.71it/s][A
 64%|██████▍   | 141/221 [00:41<00:24,  3.29it/s][A
 64%|██████▍   | 142/221 [00:41<00:21,  3.65it/s][A
 65%|██████▍   | 143/221 [00:42<00:32,  2.41it/s][A
 65%|██████▌   | 144/221 [00:42<00:28,  2.68it/s][A
 66%|██████▌   | 145/221 [00:42<00:26,  2.83it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.27it/s][A
 67%|██████▋   | 147/221 [00:43<00:22,  3.35it/s][A
 67%|██████▋   | 148/221 [00:43<00:19,  3.74it/s][A
 67%|██████▋   | 149/221 [00:43<00:20,  3.54it/s][A
 68%|██████▊   | 150/221 [00:44<00:16,  4.20it/s][A
 68%|██████▊   | 151/221 [00:44<00:18,  3.71it/s][A
 69%|██████▉   | 152/221 [00:44<00:22,  3.07it/s][A
 69%|██████▉   | 153/221 [00:45<00:18,  3.67it/s][A
 70%|██████▉   | 154/221 [00:45<00:22,  3.04it/s][A
 70%|███████   | 155/221 [00:45<00:23,  2.85it/s][A
 71%|███████   | 156/221 [00:46<00:22,  2.88it/s][A
 71%|███████   | 157/221 [00:46<00:21,  3.04it/s][A
 71%|███████▏  | 158/221 [00:47<00:28,  2.17it/s][A
 72%|███████▏  | 159/221 [00:47<00:23,  2.61it/s][A
 72%|███████▏  | 160/221 [00:47<00:23,  2.61it/s][A
 73%|███████▎  | 161/221 [00:48<00:26,  2.29it/s][A
 73%|███████▎  | 162/221 [00:48<00:23,  2.56it/s][A
 74%|███████▍  | 163/221 [00:49<00:23,  2.43it/s][A
 75%|███████▍  | 165/221 [00:49<00:16,  3.48it/s][A
 75%|███████▌  | 166/221 [00:49<00:14,  3.77it/s][A
 76%|███████▌  | 167/221 [00:50<00:15,  3.47it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.51it/s][A
 76%|███████▋  | 169/221 [00:50<00:14,  3.69it/s][A
 77%|███████▋  | 170/221 [00:50<00:15,  3.35it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.55it/s][A
 78%|███████▊  | 172/221 [00:51<00:13,  3.62it/s][A
 78%|███████▊  | 173/221 [00:51<00:15,  3.18it/s][A
 79%|███████▊  | 174/221 [00:52<00:15,  3.12it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.26it/s][A
 80%|███████▉  | 176/221 [00:52<00:12,  3.70it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.07it/s][A
 81%|████████  | 178/221 [00:53<00:11,  3.88it/s][A
 81%|████████  | 179/221 [00:53<00:10,  4.16it/s][A
 81%|████████▏ | 180/221 [00:53<00:09,  4.44it/s][A
 82%|████████▏ | 181/221 [00:53<00:08,  4.70it/s][A
 82%|████████▏ | 182/221 [00:53<00:08,  4.73it/s][A
 83%|████████▎ | 183/221 [00:54<00:11,  3.23it/s][A
 83%|████████▎ | 184/221 [00:54<00:12,  3.03it/s][A
 84%|████████▎ | 185/221 [00:54<00:10,  3.54it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.40it/s][A
 85%|████████▍ | 187/221 [00:55<00:10,  3.24it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.41it/s][A
 86%|████████▌ | 189/221 [00:56<00:08,  3.90it/s][A
 86%|████████▌ | 190/221 [00:56<00:07,  4.15it/s][A
 86%|████████▋ | 191/221 [00:56<00:08,  3.39it/s][A
 87%|████████▋ | 192/221 [00:56<00:07,  3.63it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.70it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.73it/s][A
 88%|████████▊ | 195/221 [00:57<00:07,  3.43it/s][A
 89%|████████▊ | 196/221 [00:58<00:08,  2.90it/s][A
 89%|████████▉ | 197/221 [00:58<00:08,  2.72it/s][A
 90%|████████▉ | 198/221 [00:58<00:08,  2.79it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.55it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.81it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.74it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.00it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.88it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.10it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.33it/s][A
 94%|█████████▎| 207/221 [01:00<00:03,  4.54it/s][A
 94%|█████████▍| 208/221 [01:01<00:02,  5.25it/s][A
 95%|█████████▍| 209/221 [01:01<00:02,  5.18it/s][A
 95%|█████████▌| 210/221 [01:01<00:01,  5.84it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  4.60it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.88it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.76it/s][A
 97%|█████████▋| 214/221 [01:02<00:02,  3.03it/s][A
 97%|█████████▋| 215/221 [01:02<00:01,  3.68it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.73it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.28it/s][A
 99%|█████████▊| 218/221 [01:03<00:00,  3.04it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  2.85it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.33it/s][A
100%|██████████| 221/221 [01:04<00:00,  3.76it/s][A100%|██████████| 221/221 [01:04<00:00,  3.41it/s]
08/30/2024 22:51:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 7127--===========

08/30/2024 22:51:36 - INFO - __main__ -   {'area_r1': 6.6, 'area_recall': '6.6/17.4/23.0', 'area_ravg': 15.6}
08/30/2024 22:51:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 7127--===========

08/30/2024 22:51:36 - INFO - __main__ -   {'area_r1': 41.7, 'area_recall': '41.7/71.3/84.0', 'area_ravg': 65.7}
08/30/2024 22:51:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 7127--===========

08/30/2024 22:51:36 - INFO - __main__ -   {'video_r1': 35.5, 'video_recall': '35.5/68.1/78.2', 'video_ravg': 60.6}
08/30/2024 22:51:36 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/30/2024 22:51:36 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/30/2024 22:51:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 7127--===========

08/30/2024 22:51:36 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
08/30/2024 22:51:36 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/30/2024 22:51:36 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
 40%|███▉      | 7128/17834 [3:37:44<156:10:31, 52.52s/it] 40%|███▉      | 7129/17834 [3:37:45<110:49:50, 37.27s/it] 40%|███▉      | 7130/17834 [3:37:47<79:13:38, 26.65s/it]  40%|███▉      | 7131/17834 [3:37:49<56:58:33, 19.16s/it] 40%|███▉      | 7132/17834 [3:37:51<41:25:53, 13.94s/it] 40%|███▉      | 7133/17834 [3:37:53<30:34:19, 10.28s/it] 40%|████      | 7134/17834 [3:37:54<22:56:42,  7.72s/it] 40%|████      | 7135/17834 [3:37:56<17:39:09,  5.94s/it] 40%|████      | 7136/17834 [3:37:58<13:53:43,  4.68s/it] 40%|████      | 7137/17834 [3:38:00<11:17:28,  3.80s/it] 40%|████      | 7138/17834 [3:38:01<9:26:11,  3.18s/it]  40%|████      | 7139/17834 [3:38:03<8:07:02,  2.73s/it] 40%|████      | 7140/17834 [3:38:05<7:14:49,  2.44s/it] 40%|████      | 7141/17834 [3:38:06<6:34:12,  2.21s/it] 40%|████      | 7142/17834 [3:38:08<6:08:46,  2.07s/it] 40%|████      | 7143/17834 [3:38:10<5:47:55,  1.95s/it] 40%|████      | 7144/17834 [3:38:12<5:41:28,  1.92s/it] 40%|████      | 7145/17834 [3:38:13<5:32:40,  1.87s/it] 40%|████      | 7146/17834 [3:38:15<5:27:26,  1.84s/it] 40%|████      | 7147/17834 [3:38:17<5:23:45,  1.82s/it] 40%|████      | 7148/17834 [3:38:19<5:24:58,  1.82s/it] 40%|████      | 7149/17834 [3:38:20<5:17:22,  1.78s/it]08/30/2024 22:52:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0108778476715088, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03488640487194061, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1326351165771484, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1783993244171143}
 40%|████      | 7150/17834 [3:38:22<5:13:32,  1.76s/it] 40%|████      | 7151/17834 [3:38:24<5:10:53,  1.75s/it] 40%|████      | 7152/17834 [3:38:26<5:15:43,  1.77s/it] 40%|████      | 7153/17834 [3:38:27<5:14:17,  1.77s/it] 40%|████      | 7154/17834 [3:38:29<5:12:14,  1.75s/it] 40%|████      | 7155/17834 [3:38:31<5:14:01,  1.76s/it] 40%|████      | 7156/17834 [3:38:33<5:14:55,  1.77s/it] 40%|████      | 7157/17834 [3:38:34<5:11:01,  1.75s/it] 40%|████      | 7158/17834 [3:38:36<5:09:56,  1.74s/it] 40%|████      | 7159/17834 [3:38:38<5:15:42,  1.77s/it] 40%|████      | 7160/17834 [3:38:40<5:13:16,  1.76s/it] 40%|████      | 7161/17834 [3:38:42<5:14:30,  1.77s/it] 40%|████      | 7162/17834 [3:38:43<5:13:19,  1.76s/it] 40%|████      | 7163/17834 [3:38:45<5:12:21,  1.76s/it] 40%|████      | 7164/17834 [3:38:47<5:12:22,  1.76s/it] 40%|████      | 7165/17834 [3:38:48<5:10:13,  1.74s/it] 40%|████      | 7166/17834 [3:38:50<5:07:03,  1.73s/it] 40%|████      | 7167/17834 [3:38:52<5:09:41,  1.74s/it] 40%|████      | 7168/17834 [3:38:54<5:11:00,  1.75s/it] 40%|████      | 7169/17834 [3:38:55<5:08:34,  1.74s/it] 40%|████      | 7170/17834 [3:38:57<5:07:06,  1.73s/it] 40%|████      | 7171/17834 [3:38:59<5:06:25,  1.72s/it] 40%|████      | 7172/17834 [3:39:01<5:13:17,  1.76s/it] 40%|████      | 7173/17834 [3:39:02<5:13:00,  1.76s/it] 40%|████      | 7174/17834 [3:39:04<5:13:30,  1.76s/it] 40%|████      | 7175/17834 [3:39:06<5:11:47,  1.76s/it] 40%|████      | 7176/17834 [3:39:08<5:11:13,  1.75s/it] 40%|████      | 7177/17834 [3:39:09<5:12:40,  1.76s/it] 40%|████      | 7178/17834 [3:39:11<5:08:48,  1.74s/it] 40%|████      | 7179/17834 [3:39:13<5:09:49,  1.74s/it] 40%|████      | 7180/17834 [3:39:15<5:07:57,  1.73s/it] 40%|████      | 7181/17834 [3:39:16<5:06:56,  1.73s/it] 40%|████      | 7182/17834 [3:39:18<5:10:49,  1.75s/it] 40%|████      | 7183/17834 [3:39:20<5:09:16,  1.74s/it] 40%|████      | 7184/17834 [3:39:22<5:08:20,  1.74s/it] 40%|████      | 7185/17834 [3:39:23<5:04:58,  1.72s/it] 40%|████      | 7186/17834 [3:39:25<5:07:01,  1.73s/it] 40%|████      | 7187/17834 [3:39:27<5:08:05,  1.74s/it] 40%|████      | 7188/17834 [3:39:29<5:08:57,  1.74s/it] 40%|████      | 7189/17834 [3:39:30<5:09:01,  1.74s/it] 40%|████      | 7190/17834 [3:39:32<5:06:43,  1.73s/it] 40%|████      | 7191/17834 [3:39:34<5:10:37,  1.75s/it] 40%|████      | 7192/17834 [3:39:36<5:10:03,  1.75s/it] 40%|████      | 7193/17834 [3:39:37<5:09:12,  1.74s/it] 40%|████      | 7194/17834 [3:39:39<5:11:16,  1.76s/it] 40%|████      | 7195/17834 [3:39:41<5:08:19,  1.74s/it] 40%|████      | 7196/17834 [3:39:43<5:12:54,  1.76s/it] 40%|████      | 7197/17834 [3:39:44<5:11:00,  1.75s/it] 40%|████      | 7198/17834 [3:39:46<5:09:32,  1.75s/it] 40%|████      | 7199/17834 [3:39:48<5:10:38,  1.75s/it]08/30/2024 22:54:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3806864023208618, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03333381563425064, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2765164375305176, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6905364990234375}
 40%|████      | 7200/17834 [3:39:50<5:12:24,  1.76s/it] 40%|████      | 7201/17834 [3:39:51<5:12:49,  1.77s/it] 40%|████      | 7202/17834 [3:39:53<5:17:01,  1.79s/it] 40%|████      | 7203/17834 [3:39:55<5:21:55,  1.82s/it] 40%|████      | 7204/17834 [3:39:57<5:16:31,  1.79s/it] 40%|████      | 7205/17834 [3:39:59<5:11:55,  1.76s/it] 40%|████      | 7206/17834 [3:40:00<5:13:02,  1.77s/it] 40%|████      | 7207/17834 [3:40:02<5:07:35,  1.74s/it] 40%|████      | 7208/17834 [3:40:04<5:11:27,  1.76s/it] 40%|████      | 7209/17834 [3:40:06<5:11:51,  1.76s/it] 40%|████      | 7210/17834 [3:40:07<5:12:31,  1.76s/it] 40%|████      | 7211/17834 [3:40:09<5:11:07,  1.76s/it] 40%|████      | 7212/17834 [3:40:11<5:12:15,  1.76s/it] 40%|████      | 7213/17834 [3:40:13<5:10:23,  1.75s/it] 40%|████      | 7214/17834 [3:40:14<5:09:01,  1.75s/it] 40%|████      | 7215/17834 [3:40:16<5:10:33,  1.75s/it] 40%|████      | 7216/17834 [3:40:18<5:12:27,  1.77s/it] 40%|████      | 7217/17834 [3:40:20<5:11:36,  1.76s/it] 40%|████      | 7218/17834 [3:40:21<5:13:15,  1.77s/it] 40%|████      | 7219/17834 [3:40:23<5:14:52,  1.78s/it] 40%|████      | 7220/17834 [3:40:25<5:16:13,  1.79s/it] 40%|████      | 7221/17834 [3:40:27<5:12:22,  1.77s/it] 40%|████      | 7222/17834 [3:40:28<5:11:07,  1.76s/it] 41%|████      | 7223/17834 [3:40:30<5:08:49,  1.75s/it] 41%|████      | 7224/17834 [3:40:32<5:06:39,  1.73s/it] 41%|████      | 7225/17834 [3:40:34<5:10:41,  1.76s/it] 41%|████      | 7226/17834 [3:40:35<5:09:48,  1.75s/it] 41%|████      | 7227/17834 [3:40:37<5:10:23,  1.76s/it] 41%|████      | 7228/17834 [3:40:39<5:10:23,  1.76s/it] 41%|████      | 7229/17834 [3:40:41<5:10:46,  1.76s/it] 41%|████      | 7230/17834 [3:40:43<5:12:27,  1.77s/it] 41%|████      | 7231/17834 [3:40:44<5:07:18,  1.74s/it] 41%|████      | 7232/17834 [3:40:46<5:06:32,  1.73s/it] 41%|████      | 7233/17834 [3:40:48<5:09:19,  1.75s/it] 41%|████      | 7234/17834 [3:40:49<5:10:27,  1.76s/it] 41%|████      | 7235/17834 [3:40:51<5:10:24,  1.76s/it] 41%|████      | 7236/17834 [3:40:53<5:06:01,  1.73s/it] 41%|████      | 7237/17834 [3:40:55<5:16:43,  1.79s/it] 41%|████      | 7238/17834 [3:40:57<5:11:17,  1.76s/it] 41%|████      | 7239/17834 [3:40:58<5:12:11,  1.77s/it] 41%|████      | 7240/17834 [3:41:00<5:11:11,  1.76s/it] 41%|████      | 7241/17834 [3:41:02<5:10:24,  1.76s/it] 41%|████      | 7242/17834 [3:41:04<5:09:09,  1.75s/it] 41%|████      | 7243/17834 [3:41:05<5:09:47,  1.76s/it] 41%|████      | 7244/17834 [3:41:07<5:10:31,  1.76s/it] 41%|████      | 7245/17834 [3:41:09<5:07:50,  1.74s/it] 41%|████      | 7246/17834 [3:41:11<5:07:56,  1.75s/it] 41%|████      | 7247/17834 [3:41:12<5:08:32,  1.75s/it] 41%|████      | 7248/17834 [3:41:14<5:11:27,  1.77s/it] 41%|████      | 7249/17834 [3:41:16<5:14:28,  1.78s/it]08/30/2024 22:55:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2540977001190186, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04313152655959129, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1687512397766113, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4659805297851562}
 41%|████      | 7250/17834 [3:41:18<5:10:57,  1.76s/it] 41%|████      | 7251/17834 [3:41:19<5:10:17,  1.76s/it] 41%|████      | 7252/17834 [3:41:21<5:11:23,  1.77s/it] 41%|████      | 7253/17834 [3:41:23<5:10:52,  1.76s/it] 41%|████      | 7254/17834 [3:41:25<5:11:36,  1.77s/it] 41%|████      | 7255/17834 [3:41:26<5:09:48,  1.76s/it] 41%|████      | 7256/17834 [3:41:28<5:06:34,  1.74s/it] 41%|████      | 7257/17834 [3:41:30<5:09:47,  1.76s/it] 41%|████      | 7258/17834 [3:41:32<5:07:08,  1.74s/it] 41%|████      | 7259/17834 [3:41:33<5:10:54,  1.76s/it] 41%|████      | 7260/17834 [3:41:35<5:12:25,  1.77s/it] 41%|████      | 7261/17834 [3:41:37<5:08:46,  1.75s/it] 41%|████      | 7262/17834 [3:41:39<5:11:09,  1.77s/it] 41%|████      | 7263/17834 [3:41:41<5:12:54,  1.78s/it] 41%|████      | 7264/17834 [3:41:42<5:11:11,  1.77s/it] 41%|████      | 7265/17834 [3:41:44<5:06:57,  1.74s/it] 41%|████      | 7266/17834 [3:41:46<5:09:17,  1.76s/it] 41%|████      | 7267/17834 [3:41:48<5:12:31,  1.77s/it] 41%|████      | 7268/17834 [3:41:49<5:10:06,  1.76s/it] 41%|████      | 7269/17834 [3:41:51<5:10:02,  1.76s/it] 41%|████      | 7270/17834 [3:41:53<5:11:52,  1.77s/it] 41%|████      | 7271/17834 [3:41:55<5:05:20,  1.73s/it] 41%|████      | 7272/17834 [3:41:56<5:05:21,  1.73s/it] 41%|████      | 7273/17834 [3:41:58<5:10:32,  1.76s/it] 41%|████      | 7274/17834 [3:42:00<5:09:00,  1.76s/it] 41%|████      | 7275/17834 [3:42:02<5:10:44,  1.77s/it] 41%|████      | 7276/17834 [3:42:03<5:10:41,  1.77s/it] 41%|████      | 7277/17834 [3:42:05<5:07:52,  1.75s/it] 41%|████      | 7278/17834 [3:42:07<5:08:31,  1.75s/it] 41%|████      | 7279/17834 [3:42:09<5:06:46,  1.74s/it] 41%|████      | 7280/17834 [3:42:10<5:03:49,  1.73s/it] 41%|████      | 7281/17834 [3:42:12<5:01:55,  1.72s/it] 41%|████      | 7282/17834 [3:42:14<5:02:25,  1.72s/it] 41%|████      | 7283/17834 [3:42:15<5:04:41,  1.73s/it] 41%|████      | 7284/17834 [3:42:17<5:04:41,  1.73s/it] 41%|████      | 7285/17834 [3:42:19<5:06:16,  1.74s/it] 41%|████      | 7286/17834 [3:42:21<5:08:57,  1.76s/it] 41%|████      | 7287/17834 [3:42:22<5:06:50,  1.75s/it] 41%|████      | 7288/17834 [3:42:24<5:09:20,  1.76s/it] 41%|████      | 7289/17834 [3:42:26<5:12:21,  1.78s/it] 41%|████      | 7290/17834 [3:42:28<5:10:17,  1.77s/it] 41%|████      | 7291/17834 [3:42:30<5:09:46,  1.76s/it] 41%|████      | 7292/17834 [3:42:31<5:11:14,  1.77s/it] 41%|████      | 7293/17834 [3:42:33<5:05:56,  1.74s/it] 41%|████      | 7294/17834 [3:42:35<5:07:22,  1.75s/it] 41%|████      | 7295/17834 [3:42:36<5:05:04,  1.74s/it] 41%|████      | 7296/17834 [3:42:38<5:02:07,  1.72s/it] 41%|████      | 7297/17834 [3:42:40<5:03:11,  1.73s/it] 41%|████      | 7298/17834 [3:42:42<5:01:34,  1.72s/it] 41%|████      | 7299/17834 [3:42:43<5:06:38,  1.75s/it]08/30/2024 22:57:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3083868026733398, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0373934768140316, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.191823720932007, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5376038551330566}
 41%|████      | 7300/17834 [3:42:45<5:06:34,  1.75s/it] 41%|████      | 7301/17834 [3:42:47<5:07:18,  1.75s/it] 41%|████      | 7302/17834 [3:42:49<5:04:16,  1.73s/it] 41%|████      | 7303/17834 [3:42:50<5:08:10,  1.76s/it] 41%|████      | 7304/17834 [3:42:52<5:07:15,  1.75s/it] 41%|████      | 7305/17834 [3:42:54<5:09:22,  1.76s/it] 41%|████      | 7306/17834 [3:42:56<5:09:05,  1.76s/it] 41%|████      | 7307/17834 [3:42:57<5:04:55,  1.74s/it] 41%|████      | 7308/17834 [3:42:59<5:05:31,  1.74s/it] 41%|████      | 7309/17834 [3:43:01<5:04:55,  1.74s/it] 41%|████      | 7310/17834 [3:43:03<5:11:16,  1.77s/it] 41%|████      | 7311/17834 [3:43:04<5:05:35,  1.74s/it] 41%|████      | 7312/17834 [3:43:06<5:06:40,  1.75s/it] 41%|████      | 7313/17834 [3:43:08<5:08:55,  1.76s/it] 41%|████      | 7314/17834 [3:43:10<5:10:06,  1.77s/it] 41%|████      | 7315/17834 [3:43:12<5:11:21,  1.78s/it] 41%|████      | 7316/17834 [3:43:13<5:08:12,  1.76s/it] 41%|████      | 7317/17834 [3:43:15<5:08:30,  1.76s/it] 41%|████      | 7318/17834 [3:43:17<5:10:24,  1.77s/it] 41%|████      | 7319/17834 [3:43:19<5:14:00,  1.79s/it] 41%|████      | 7320/17834 [3:43:20<5:07:19,  1.75s/it] 41%|████      | 7321/17834 [3:43:22<5:06:05,  1.75s/it] 41%|████      | 7322/17834 [3:43:24<5:06:49,  1.75s/it] 41%|████      | 7323/17834 [3:43:26<5:08:21,  1.76s/it] 41%|████      | 7324/17834 [3:43:27<5:07:01,  1.75s/it] 41%|████      | 7325/17834 [3:43:29<5:05:28,  1.74s/it] 41%|████      | 7326/17834 [3:43:31<5:04:03,  1.74s/it] 41%|████      | 7327/17834 [3:43:33<5:06:27,  1.75s/it] 41%|████      | 7328/17834 [3:43:34<5:06:50,  1.75s/it] 41%|████      | 7329/17834 [3:43:36<5:08:04,  1.76s/it] 41%|████      | 7330/17834 [3:43:38<5:04:45,  1.74s/it] 41%|████      | 7331/17834 [3:43:40<5:12:39,  1.79s/it] 41%|████      | 7332/17834 [3:43:41<5:11:41,  1.78s/it] 41%|████      | 7333/17834 [3:43:43<5:07:28,  1.76s/it] 41%|████      | 7334/17834 [3:43:45<5:04:56,  1.74s/it] 41%|████      | 7335/17834 [3:43:47<5:03:56,  1.74s/it] 41%|████      | 7336/17834 [3:43:48<5:00:56,  1.72s/it] 41%|████      | 7337/17834 [3:43:50<5:02:31,  1.73s/it] 41%|████      | 7338/17834 [3:43:52<5:06:03,  1.75s/it] 41%|████      | 7339/17834 [3:43:54<5:06:13,  1.75s/it] 41%|████      | 7340/17834 [3:43:55<5:04:29,  1.74s/it] 41%|████      | 7341/17834 [3:43:57<5:04:42,  1.74s/it] 41%|████      | 7342/17834 [3:43:59<5:02:01,  1.73s/it] 41%|████      | 7343/17834 [3:44:00<5:02:21,  1.73s/it] 41%|████      | 7344/17834 [3:44:02<5:06:36,  1.75s/it] 41%|████      | 7345/17834 [3:44:04<5:08:56,  1.77s/it] 41%|████      | 7346/17834 [3:44:06<5:06:45,  1.75s/it] 41%|████      | 7347/17834 [3:44:07<5:04:21,  1.74s/it] 41%|████      | 7348/17834 [3:44:09<5:06:16,  1.75s/it] 41%|████      | 7349/17834 [3:44:11<5:07:17,  1.76s/it]08/30/2024 22:58:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4969055652618408, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.06214897334575653, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3190252780914307, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.878079891204834}
 41%|████      | 7350/17834 [3:44:13<5:10:13,  1.78s/it] 41%|████      | 7351/17834 [3:44:15<5:10:23,  1.78s/it] 41%|████      | 7352/17834 [3:44:16<5:05:07,  1.75s/it] 41%|████      | 7353/17834 [3:44:18<5:02:07,  1.73s/it] 41%|████      | 7354/17834 [3:44:20<5:02:38,  1.73s/it] 41%|████      | 7355/17834 [3:44:22<5:05:44,  1.75s/it] 41%|████      | 7356/17834 [3:44:23<5:06:20,  1.75s/it] 41%|████▏     | 7357/17834 [3:44:25<5:02:22,  1.73s/it] 41%|████▏     | 7358/17834 [3:44:27<5:01:03,  1.72s/it] 41%|████▏     | 7359/17834 [3:44:28<5:05:47,  1.75s/it] 41%|████▏     | 7360/17834 [3:44:30<5:07:02,  1.76s/it] 41%|████▏     | 7361/17834 [3:44:32<5:13:44,  1.80s/it] 41%|████▏     | 7362/17834 [3:44:34<5:09:49,  1.78s/it] 41%|████▏     | 7363/17834 [3:44:36<5:09:31,  1.77s/it] 41%|████▏     | 7364/17834 [3:44:37<5:08:40,  1.77s/it] 41%|████▏     | 7365/17834 [3:44:39<5:09:56,  1.78s/it] 41%|████▏     | 7366/17834 [3:44:41<5:08:09,  1.77s/it] 41%|████▏     | 7367/17834 [3:44:43<5:05:41,  1.75s/it] 41%|████▏     | 7368/17834 [3:44:44<5:05:55,  1.75s/it] 41%|████▏     | 7369/17834 [3:44:46<5:07:56,  1.77s/it] 41%|████▏     | 7370/17834 [3:44:48<5:07:20,  1.76s/it] 41%|████▏     | 7371/17834 [3:44:50<5:08:43,  1.77s/it] 41%|████▏     | 7372/17834 [3:44:51<5:06:19,  1.76s/it] 41%|████▏     | 7373/17834 [3:44:53<5:07:56,  1.77s/it] 41%|████▏     | 7374/17834 [3:44:55<5:07:24,  1.76s/it] 41%|████▏     | 7375/17834 [3:44:57<5:10:17,  1.78s/it] 41%|████▏     | 7376/17834 [3:44:59<5:06:25,  1.76s/it] 41%|████▏     | 7377/17834 [3:45:00<5:08:19,  1.77s/it] 41%|████▏     | 7378/17834 [3:45:02<5:07:32,  1.76s/it] 41%|████▏     | 7379/17834 [3:45:04<5:04:36,  1.75s/it] 41%|████▏     | 7380/17834 [3:45:06<5:02:30,  1.74s/it] 41%|████▏     | 7381/17834 [3:45:07<5:09:37,  1.78s/it] 41%|████▏     | 7382/17834 [3:45:09<5:08:31,  1.77s/it] 41%|████▏     | 7383/17834 [3:45:11<5:03:40,  1.74s/it] 41%|████▏     | 7384/17834 [3:45:12<4:59:04,  1.72s/it] 41%|████▏     | 7385/17834 [3:45:14<5:00:35,  1.73s/it] 41%|████▏     | 7386/17834 [3:45:16<4:59:03,  1.72s/it] 41%|████▏     | 7387/17834 [3:45:18<5:01:09,  1.73s/it] 41%|████▏     | 7388/17834 [3:45:19<5:04:03,  1.75s/it] 41%|████▏     | 7389/17834 [3:45:21<5:07:11,  1.76s/it] 41%|████▏     | 7390/17834 [3:45:23<5:05:02,  1.75s/it] 41%|████▏     | 7391/17834 [3:45:25<5:05:04,  1.75s/it] 41%|████▏     | 7392/17834 [3:45:27<5:04:51,  1.75s/it] 41%|████▏     | 7393/17834 [3:45:28<5:03:07,  1.74s/it] 41%|████▏     | 7394/17834 [3:45:30<5:03:38,  1.75s/it] 41%|████▏     | 7395/17834 [3:45:32<5:01:28,  1.73s/it] 41%|████▏     | 7396/17834 [3:45:33<5:02:07,  1.74s/it] 41%|████▏     | 7397/17834 [3:45:35<5:04:03,  1.75s/it] 41%|████▏     | 7398/17834 [3:45:37<5:01:14,  1.73s/it] 41%|████▏     | 7399/17834 [3:45:39<5:00:37,  1.73s/it]08/30/2024 22:59:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0332034826278687, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.036183685064315796, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.169517755508423, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2389049530029297}
 41%|████▏     | 7400/17834 [3:45:40<5:01:55,  1.74s/it] 41%|████▏     | 7401/17834 [3:45:42<4:57:08,  1.71s/it] 42%|████▏     | 7402/17834 [3:45:44<5:02:46,  1.74s/it] 42%|████▏     | 7403/17834 [3:45:46<5:03:30,  1.75s/it] 42%|████▏     | 7404/17834 [3:45:47<5:05:57,  1.76s/it] 42%|████▏     | 7405/17834 [3:45:49<5:04:21,  1.75s/it] 42%|████▏     | 7406/17834 [3:45:51<5:00:49,  1.73s/it] 42%|████▏     | 7407/17834 [3:45:53<5:01:53,  1.74s/it] 42%|████▏     | 7408/17834 [3:45:54<5:04:16,  1.75s/it] 42%|████▏     | 7409/17834 [3:45:56<5:03:29,  1.75s/it] 42%|████▏     | 7410/17834 [3:45:58<5:08:02,  1.77s/it] 42%|████▏     | 7411/17834 [3:46:00<5:03:48,  1.75s/it] 42%|████▏     | 7412/17834 [3:46:01<5:01:55,  1.74s/it] 42%|████▏     | 7413/17834 [3:46:03<5:01:22,  1.74s/it] 42%|████▏     | 7414/17834 [3:46:05<5:00:24,  1.73s/it] 42%|████▏     | 7415/17834 [3:46:07<5:01:32,  1.74s/it] 42%|████▏     | 7416/17834 [3:46:08<5:02:18,  1.74s/it] 42%|████▏     | 7417/17834 [3:46:10<4:59:39,  1.73s/it] 42%|████▏     | 7418/17834 [3:46:12<5:08:30,  1.78s/it] 42%|████▏     | 7419/17834 [3:46:14<5:09:53,  1.79s/it] 42%|████▏     | 7420/17834 [3:46:15<5:12:25,  1.80s/it] 42%|████▏     | 7421/17834 [3:46:17<5:06:07,  1.76s/it] 42%|████▏     | 7422/17834 [3:46:19<5:08:23,  1.78s/it] 42%|████▏     | 7423/17834 [3:46:21<5:06:16,  1.77s/it] 42%|████▏     | 7424/17834 [3:46:22<5:04:56,  1.76s/it] 42%|████▏     | 7425/17834 [3:46:24<5:03:48,  1.75s/it] 42%|████▏     | 7426/17834 [3:46:26<5:05:34,  1.76s/it] 42%|████▏     | 7427/17834 [3:46:28<5:05:42,  1.76s/it] 42%|████▏     | 7428/17834 [3:46:29<5:03:42,  1.75s/it] 42%|████▏     | 7429/17834 [3:46:31<5:05:58,  1.76s/it] 42%|████▏     | 7430/17834 [3:46:33<5:05:01,  1.76s/it] 42%|████▏     | 7431/17834 [3:46:35<5:01:11,  1.74s/it] 42%|████▏     | 7432/17834 [3:46:36<5:00:25,  1.73s/it] 42%|████▏     | 7433/17834 [3:46:38<5:04:50,  1.76s/it] 42%|████▏     | 7434/17834 [3:46:40<5:05:58,  1.77s/it] 42%|████▏     | 7435/17834 [3:46:42<5:07:14,  1.77s/it] 42%|████▏     | 7436/17834 [3:46:44<5:03:01,  1.75s/it] 42%|████▏     | 7437/17834 [3:46:45<5:03:16,  1.75s/it] 42%|████▏     | 7438/17834 [3:46:47<5:03:32,  1.75s/it] 42%|████▏     | 7439/17834 [3:46:49<5:02:57,  1.75s/it] 42%|████▏     | 7440/17834 [3:46:51<5:03:04,  1.75s/it] 42%|████▏     | 7441/17834 [3:46:52<5:03:30,  1.75s/it] 42%|████▏     | 7442/17834 [3:46:54<5:00:22,  1.73s/it] 42%|████▏     | 7443/17834 [3:46:56<4:59:01,  1.73s/it] 42%|████▏     | 7444/17834 [3:46:57<4:57:39,  1.72s/it] 42%|████▏     | 7445/17834 [3:46:59<4:59:37,  1.73s/it] 42%|████▏     | 7446/17834 [3:47:01<5:03:25,  1.75s/it] 42%|████▏     | 7447/17834 [3:47:03<5:05:24,  1.76s/it] 42%|████▏     | 7448/17834 [3:47:05<5:09:32,  1.79s/it] 42%|████▏     | 7449/17834 [3:47:06<5:03:19,  1.75s/it]08/30/2024 23:01:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0970492362976074, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03257206827402115, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.235305070877075, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.364926338195801}
 42%|████▏     | 7450/17834 [3:47:08<5:03:24,  1.75s/it] 42%|████▏     | 7451/17834 [3:47:10<5:00:55,  1.74s/it] 42%|████▏     | 7452/17834 [3:47:12<5:08:27,  1.78s/it] 42%|████▏     | 7453/17834 [3:47:13<5:04:49,  1.76s/it] 42%|████▏     | 7454/17834 [3:47:15<5:07:12,  1.78s/it] 42%|████▏     | 7455/17834 [3:47:17<5:07:34,  1.78s/it] 42%|████▏     | 7456/17834 [3:47:19<5:02:09,  1.75s/it] 42%|████▏     | 7457/17834 [3:47:20<5:05:45,  1.77s/it] 42%|████▏     | 7458/17834 [3:47:22<5:05:18,  1.77s/it] 42%|████▏     | 7459/17834 [3:47:24<5:02:57,  1.75s/it] 42%|████▏     | 7460/17834 [3:47:26<5:01:11,  1.74s/it] 42%|████▏     | 7461/17834 [3:47:27<5:06:29,  1.77s/it] 42%|████▏     | 7462/17834 [3:47:29<5:06:35,  1.77s/it] 42%|████▏     | 7463/17834 [3:47:31<5:04:22,  1.76s/it] 42%|████▏     | 7464/17834 [3:47:33<5:04:32,  1.76s/it] 42%|████▏     | 7465/17834 [3:47:34<5:00:53,  1.74s/it] 42%|████▏     | 7466/17834 [3:47:36<4:59:58,  1.74s/it] 42%|████▏     | 7467/17834 [3:47:38<5:00:40,  1.74s/it] 42%|████▏     | 7468/17834 [3:47:40<4:59:35,  1.73s/it] 42%|████▏     | 7469/17834 [3:47:41<5:00:16,  1.74s/it] 42%|████▏     | 7470/17834 [3:47:43<4:56:48,  1.72s/it] 42%|████▏     | 7471/17834 [3:47:45<5:01:06,  1.74s/it] 42%|████▏     | 7472/17834 [3:47:47<5:04:25,  1.76s/it] 42%|████▏     | 7473/17834 [3:47:48<5:03:05,  1.76s/it] 42%|████▏     | 7474/17834 [3:47:50<4:59:15,  1.73s/it] 42%|████▏     | 7475/17834 [3:47:52<4:56:09,  1.72s/it] 42%|████▏     | 7476/17834 [3:47:53<4:57:38,  1.72s/it] 42%|████▏     | 7477/17834 [3:47:55<4:54:54,  1.71s/it] 42%|████▏     | 7478/17834 [3:47:57<4:56:53,  1.72s/it] 42%|████▏     | 7479/17834 [3:47:59<5:01:26,  1.75s/it] 42%|████▏     | 7480/17834 [3:48:00<4:59:27,  1.74s/it] 42%|████▏     | 7481/17834 [3:48:02<5:01:03,  1.74s/it] 42%|████▏     | 7482/17834 [3:48:04<4:57:05,  1.72s/it] 42%|████▏     | 7483/17834 [3:48:06<5:00:49,  1.74s/it] 42%|████▏     | 7484/17834 [3:48:07<4:59:46,  1.74s/it] 42%|████▏     | 7485/17834 [3:48:09<5:00:29,  1.74s/it] 42%|████▏     | 7486/17834 [3:48:11<5:02:15,  1.75s/it] 42%|████▏     | 7487/17834 [3:48:13<5:04:25,  1.77s/it] 42%|████▏     | 7488/17834 [3:48:14<4:59:52,  1.74s/it] 42%|████▏     | 7489/17834 [3:48:16<5:00:30,  1.74s/it] 42%|████▏     | 7490/17834 [3:48:18<5:01:37,  1.75s/it] 42%|████▏     | 7491/17834 [3:48:20<5:01:46,  1.75s/it] 42%|████▏     | 7492/17834 [3:48:21<4:59:28,  1.74s/it] 42%|████▏     | 7493/17834 [3:48:23<4:59:09,  1.74s/it] 42%|████▏     | 7494/17834 [3:48:25<4:58:19,  1.73s/it] 42%|████▏     | 7495/17834 [3:48:26<4:58:42,  1.73s/it] 42%|████▏     | 7496/17834 [3:48:28<4:58:47,  1.73s/it] 42%|████▏     | 7497/17834 [3:48:30<4:59:27,  1.74s/it] 42%|████▏     | 7498/17834 [3:48:32<4:56:41,  1.72s/it] 42%|████▏     | 7499/17834 [3:48:33<4:57:56,  1.73s/it]08/30/2024 23:02:53 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3713440895080566, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02218431420624256, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.322871685028076, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.716400146484375}
 42%|████▏     | 7500/17834 [3:48:35<4:58:37,  1.73s/it] 42%|████▏     | 7501/17834 [3:48:37<5:03:05,  1.76s/it] 42%|████▏     | 7502/17834 [3:48:39<5:02:04,  1.75s/it] 42%|████▏     | 7503/17834 [3:48:40<5:01:12,  1.75s/it] 42%|████▏     | 7504/17834 [3:48:42<5:02:06,  1.75s/it] 42%|████▏     | 7505/17834 [3:48:44<5:02:32,  1.76s/it] 42%|████▏     | 7506/17834 [3:48:46<5:00:31,  1.75s/it] 42%|████▏     | 7507/17834 [3:48:47<4:57:08,  1.73s/it] 42%|████▏     | 7508/17834 [3:48:49<5:00:08,  1.74s/it] 42%|████▏     | 7509/17834 [3:48:51<5:05:03,  1.77s/it] 42%|████▏     | 7510/17834 [3:48:53<5:06:23,  1.78s/it] 42%|████▏     | 7511/17834 [3:48:55<5:07:23,  1.79s/it] 42%|████▏     | 7512/17834 [3:48:56<5:01:47,  1.75s/it] 42%|████▏     | 7513/17834 [3:48:58<5:00:02,  1.74s/it] 42%|████▏     | 7514/17834 [3:49:00<5:00:47,  1.75s/it] 42%|████▏     | 7515/17834 [3:49:02<5:01:35,  1.75s/it] 42%|████▏     | 7516/17834 [3:49:03<5:00:33,  1.75s/it] 42%|████▏     | 7517/17834 [3:49:05<5:02:10,  1.76s/it] 42%|████▏     | 7518/17834 [3:49:07<4:59:40,  1.74s/it] 42%|████▏     | 7519/17834 [3:49:09<5:00:30,  1.75s/it] 42%|████▏     | 7520/17834 [3:49:10<5:03:25,  1.77s/it] 42%|████▏     | 7521/17834 [3:49:12<5:02:25,  1.76s/it] 42%|████▏     | 7522/17834 [3:49:14<5:05:47,  1.78s/it] 42%|████▏     | 7523/17834 [3:49:16<5:05:24,  1.78s/it] 42%|████▏     | 7524/17834 [3:49:17<5:02:19,  1.76s/it] 42%|████▏     | 7525/17834 [3:49:19<4:58:27,  1.74s/it] 42%|████▏     | 7526/17834 [3:49:21<4:59:44,  1.74s/it] 42%|████▏     | 7527/17834 [3:49:23<4:59:04,  1.74s/it] 42%|████▏     | 7528/17834 [3:49:24<5:01:18,  1.75s/it] 42%|████▏     | 7529/17834 [3:49:26<5:03:11,  1.77s/it] 42%|████▏     | 7530/17834 [3:49:28<5:01:56,  1.76s/it] 42%|████▏     | 7531/17834 [3:49:30<5:01:32,  1.76s/it] 42%|████▏     | 7532/17834 [3:49:31<5:03:34,  1.77s/it] 42%|████▏     | 7533/17834 [3:49:33<5:01:50,  1.76s/it] 42%|████▏     | 7534/17834 [3:49:35<5:01:53,  1.76s/it] 42%|████▏     | 7535/17834 [3:49:37<4:58:06,  1.74s/it] 42%|████▏     | 7536/17834 [3:49:38<4:59:51,  1.75s/it] 42%|████▏     | 7537/17834 [3:49:40<5:00:08,  1.75s/it] 42%|████▏     | 7538/17834 [3:49:42<5:02:39,  1.76s/it] 42%|████▏     | 7539/17834 [3:49:44<4:59:42,  1.75s/it] 42%|████▏     | 7540/17834 [3:49:45<5:03:54,  1.77s/it] 42%|████▏     | 7541/17834 [3:49:47<5:00:57,  1.75s/it] 42%|████▏     | 7542/17834 [3:49:49<5:03:17,  1.77s/it] 42%|████▏     | 7543/17834 [3:49:51<5:05:18,  1.78s/it] 42%|████▏     | 7544/17834 [3:49:53<5:01:57,  1.76s/it] 42%|████▏     | 7545/17834 [3:49:54<5:01:22,  1.76s/it] 42%|████▏     | 7546/17834 [3:49:56<5:01:01,  1.76s/it] 42%|████▏     | 7547/17834 [3:49:58<5:03:08,  1.77s/it] 42%|████▏     | 7548/17834 [3:50:00<5:02:38,  1.77s/it] 42%|████▏     | 7549/17834 [3:50:01<5:00:05,  1.75s/it]08/30/2024 23:04:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4569205045700073, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05018123984336853, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2885384559631348, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.795640230178833}
 42%|████▏     | 7550/17834 [3:50:03<5:04:27,  1.78s/it] 42%|████▏     | 7551/17834 [3:50:05<5:08:59,  1.80s/it] 42%|████▏     | 7552/17834 [3:50:07<5:01:44,  1.76s/it] 42%|████▏     | 7553/17834 [3:50:08<4:56:34,  1.73s/it] 42%|████▏     | 7554/17834 [3:50:10<4:56:13,  1.73s/it] 42%|████▏     | 7555/17834 [3:50:12<4:55:54,  1.73s/it] 42%|████▏     | 7556/17834 [3:50:14<5:00:58,  1.76s/it] 42%|████▏     | 7557/17834 [3:50:15<5:01:59,  1.76s/it] 42%|████▏     | 7558/17834 [3:50:17<5:00:39,  1.76s/it] 42%|████▏     | 7559/17834 [3:50:19<4:58:38,  1.74s/it] 42%|████▏     | 7560/17834 [3:50:21<5:02:39,  1.77s/it] 42%|████▏     | 7561/17834 [3:50:22<5:00:28,  1.75s/it] 42%|████▏     | 7562/17834 [3:50:24<5:02:50,  1.77s/it] 42%|████▏     | 7563/17834 [3:50:26<4:59:39,  1.75s/it] 42%|████▏     | 7564/17834 [3:50:28<4:59:06,  1.75s/it] 42%|████▏     | 7565/17834 [3:50:29<5:05:38,  1.79s/it] 42%|████▏     | 7566/17834 [3:50:31<5:02:09,  1.77s/it] 42%|████▏     | 7567/17834 [3:50:33<5:03:48,  1.78s/it] 42%|████▏     | 7568/17834 [3:50:35<4:57:35,  1.74s/it] 42%|████▏     | 7569/17834 [3:50:36<4:57:05,  1.74s/it] 42%|████▏     | 7570/17834 [3:50:38<4:58:48,  1.75s/it] 42%|████▏     | 7571/17834 [3:50:40<5:02:31,  1.77s/it] 42%|████▏     | 7572/17834 [3:50:42<5:00:35,  1.76s/it] 42%|████▏     | 7573/17834 [3:50:43<4:58:21,  1.74s/it] 42%|████▏     | 7574/17834 [3:50:45<4:58:00,  1.74s/it] 42%|████▏     | 7575/17834 [3:50:47<4:57:44,  1.74s/it] 42%|████▏     | 7576/17834 [3:50:49<4:59:35,  1.75s/it] 42%|████▏     | 7577/17834 [3:50:50<5:01:04,  1.76s/it] 42%|████▏     | 7578/17834 [3:50:52<5:00:53,  1.76s/it] 42%|████▏     | 7579/17834 [3:50:54<4:59:50,  1.75s/it] 43%|████▎     | 7580/17834 [3:50:56<5:02:32,  1.77s/it] 43%|████▎     | 7581/17834 [3:50:58<5:02:11,  1.77s/it] 43%|████▎     | 7582/17834 [3:50:59<5:01:06,  1.76s/it] 43%|████▎     | 7583/17834 [3:51:01<4:57:12,  1.74s/it] 43%|████▎     | 7584/17834 [3:51:03<4:59:10,  1.75s/it] 43%|████▎     | 7585/17834 [3:51:05<5:00:02,  1.76s/it] 43%|████▎     | 7586/17834 [3:51:06<5:02:27,  1.77s/it] 43%|████▎     | 7587/17834 [3:51:08<5:02:32,  1.77s/it] 43%|████▎     | 7588/17834 [3:51:10<5:00:20,  1.76s/it] 43%|████▎     | 7589/17834 [3:51:12<4:58:01,  1.75s/it] 43%|████▎     | 7590/17834 [3:51:13<5:00:24,  1.76s/it] 43%|████▎     | 7591/17834 [3:51:15<4:58:28,  1.75s/it] 43%|████▎     | 7592/17834 [3:51:17<5:01:34,  1.77s/it] 43%|████▎     | 7593/17834 [3:51:19<4:57:27,  1.74s/it] 43%|████▎     | 7594/17834 [3:51:20<4:57:09,  1.74s/it] 43%|████▎     | 7595/17834 [3:51:22<4:55:16,  1.73s/it] 43%|████▎     | 7596/17834 [3:51:24<4:55:11,  1.73s/it] 43%|████▎     | 7597/17834 [3:51:25<4:56:25,  1.74s/it] 43%|████▎     | 7598/17834 [3:51:27<4:58:39,  1.75s/it] 43%|████▎     | 7599/17834 [3:51:29<5:01:41,  1.77s/it]08/30/2024 23:05:48 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.278477430343628, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03793906792998314, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.188201665878296, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5046181678771973}
 43%|████▎     | 7600/17834 [3:51:31<5:00:30,  1.76s/it] 43%|████▎     | 7601/17834 [3:51:33<5:00:44,  1.76s/it] 43%|████▎     | 7602/17834 [3:51:34<4:59:54,  1.76s/it] 43%|████▎     | 7603/17834 [3:51:36<5:01:29,  1.77s/it] 43%|████▎     | 7604/17834 [3:51:38<5:01:02,  1.77s/it] 43%|████▎     | 7605/17834 [3:51:40<5:00:28,  1.76s/it] 43%|████▎     | 7606/17834 [3:51:41<5:02:58,  1.78s/it] 43%|████▎     | 7607/17834 [3:51:43<5:02:36,  1.78s/it] 43%|████▎     | 7608/17834 [3:51:45<5:05:38,  1.79s/it] 43%|████▎     | 7609/17834 [3:51:47<5:06:34,  1.80s/it] 43%|████▎     | 7610/17834 [3:51:49<5:02:02,  1.77s/it] 43%|████▎     | 7611/17834 [3:51:50<5:00:47,  1.77s/it] 43%|████▎     | 7612/17834 [3:51:52<4:58:17,  1.75s/it] 43%|████▎     | 7613/17834 [3:51:54<4:58:24,  1.75s/it] 43%|████▎     | 7614/17834 [3:51:56<5:00:17,  1.76s/it] 43%|████▎     | 7615/17834 [3:51:57<4:59:54,  1.76s/it] 43%|████▎     | 7616/17834 [3:51:59<4:58:05,  1.75s/it] 43%|████▎     | 7617/17834 [3:52:01<4:59:50,  1.76s/it] 43%|████▎     | 7618/17834 [3:52:03<4:57:32,  1.75s/it] 43%|████▎     | 7619/17834 [3:52:04<4:56:35,  1.74s/it] 43%|████▎     | 7620/17834 [3:52:06<4:55:34,  1.74s/it] 43%|████▎     | 7621/17834 [3:52:08<5:01:03,  1.77s/it] 43%|████▎     | 7622/17834 [3:52:10<5:00:59,  1.77s/it] 43%|████▎     | 7623/17834 [3:52:11<4:57:16,  1.75s/it] 43%|████▎     | 7624/17834 [3:52:13<4:57:34,  1.75s/it] 43%|████▎     | 7625/17834 [3:52:15<4:57:30,  1.75s/it] 43%|████▎     | 7626/17834 [3:52:17<4:56:26,  1.74s/it] 43%|████▎     | 7627/17834 [3:52:18<5:00:49,  1.77s/it] 43%|████▎     | 7628/17834 [3:52:20<5:01:17,  1.77s/it] 43%|████▎     | 7629/17834 [3:52:22<5:00:50,  1.77s/it] 43%|████▎     | 7630/17834 [3:52:24<5:00:33,  1.77s/it] 43%|████▎     | 7631/17834 [3:52:25<4:57:29,  1.75s/it] 43%|████▎     | 7632/17834 [3:52:27<4:56:22,  1.74s/it] 43%|████▎     | 7633/17834 [3:52:29<5:00:11,  1.77s/it] 43%|████▎     | 7634/17834 [3:52:31<5:04:12,  1.79s/it] 43%|████▎     | 7635/17834 [3:52:33<5:00:26,  1.77s/it] 43%|████▎     | 7636/17834 [3:52:34<4:59:34,  1.76s/it] 43%|████▎     | 7637/17834 [3:52:36<5:01:53,  1.78s/it] 43%|████▎     | 7638/17834 [3:52:38<5:01:18,  1.77s/it] 43%|████▎     | 7639/17834 [3:52:40<5:00:25,  1.77s/it] 43%|████▎     | 7640/17834 [3:52:41<5:01:47,  1.78s/it] 43%|████▎     | 7641/17834 [3:52:43<5:02:18,  1.78s/it] 43%|████▎     | 7642/17834 [3:52:45<4:58:42,  1.76s/it] 43%|████▎     | 7643/17834 [3:52:47<4:59:11,  1.76s/it] 43%|████▎     | 7644/17834 [3:52:48<4:56:37,  1.75s/it] 43%|████▎     | 7645/17834 [3:52:50<4:55:15,  1.74s/it] 43%|████▎     | 7646/17834 [3:52:52<4:55:38,  1.74s/it] 43%|████▎     | 7647/17834 [3:52:54<4:57:57,  1.75s/it] 43%|████▎     | 7648/17834 [3:52:55<4:56:51,  1.75s/it] 43%|████▎     | 7649/17834 [3:52:57<5:02:00,  1.78s/it]08/30/2024 23:07:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4582947492599487, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04567941278219223, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2977540493011475, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8017282485961914}
 43%|████▎     | 7650/17834 [3:52:59<4:57:53,  1.76s/it] 43%|████▎     | 7651/17834 [3:53:01<4:52:51,  1.73s/it] 43%|████▎     | 7652/17834 [3:53:02<4:52:51,  1.73s/it] 43%|████▎     | 7653/17834 [3:53:04<4:55:12,  1.74s/it] 43%|████▎     | 7654/17834 [3:53:06<5:00:57,  1.77s/it] 43%|████▎     | 7655/17834 [3:53:08<4:56:05,  1.75s/it] 43%|████▎     | 7656/17834 [3:53:09<4:55:50,  1.74s/it] 43%|████▎     | 7657/17834 [3:53:11<4:57:46,  1.76s/it] 43%|████▎     | 7658/17834 [3:53:13<4:56:50,  1.75s/it] 43%|████▎     | 7659/17834 [3:53:15<4:59:39,  1.77s/it] 43%|████▎     | 7660/17834 [3:53:16<4:56:19,  1.75s/it] 43%|████▎     | 7661/17834 [3:53:18<4:59:03,  1.76s/it] 43%|████▎     | 7662/17834 [3:53:20<4:58:38,  1.76s/it] 43%|████▎     | 7663/17834 [3:53:22<4:57:40,  1.76s/it] 43%|████▎     | 7664/17834 [3:53:23<4:56:24,  1.75s/it] 43%|████▎     | 7665/17834 [3:53:25<4:58:14,  1.76s/it] 43%|████▎     | 7666/17834 [3:53:27<4:59:26,  1.77s/it] 43%|████▎     | 7667/17834 [3:53:29<5:01:43,  1.78s/it] 43%|████▎     | 7668/17834 [3:53:30<4:57:59,  1.76s/it] 43%|████▎     | 7669/17834 [3:53:32<5:00:55,  1.78s/it] 43%|████▎     | 7670/17834 [3:53:34<4:57:31,  1.76s/it] 43%|████▎     | 7671/17834 [3:53:36<4:57:50,  1.76s/it] 43%|████▎     | 7672/17834 [3:53:38<4:57:52,  1.76s/it] 43%|████▎     | 7673/17834 [3:53:39<4:59:45,  1.77s/it] 43%|████▎     | 7674/17834 [3:53:41<4:57:32,  1.76s/it] 43%|████▎     | 7675/17834 [3:53:43<4:57:53,  1.76s/it] 43%|████▎     | 7676/17834 [3:53:45<4:55:23,  1.74s/it] 43%|████▎     | 7677/17834 [3:53:46<4:52:10,  1.73s/it] 43%|████▎     | 7678/17834 [3:53:48<4:53:03,  1.73s/it] 43%|████▎     | 7679/17834 [3:53:50<4:52:14,  1.73s/it] 43%|████▎     | 7680/17834 [3:53:51<4:53:15,  1.73s/it] 43%|████▎     | 7681/17834 [3:53:53<5:00:27,  1.78s/it] 43%|████▎     | 7682/17834 [3:53:55<4:59:36,  1.77s/it] 43%|████▎     | 7683/17834 [3:53:57<4:55:19,  1.75s/it] 43%|████▎     | 7684/17834 [3:53:59<4:57:23,  1.76s/it] 43%|████▎     | 7685/17834 [3:54:00<4:54:59,  1.74s/it] 43%|████▎     | 7686/17834 [3:54:02<4:58:23,  1.76s/it] 43%|████▎     | 7687/17834 [3:54:04<4:56:29,  1.75s/it] 43%|████▎     | 7688/17834 [3:54:06<4:57:40,  1.76s/it] 43%|████▎     | 7689/17834 [3:54:07<4:55:58,  1.75s/it] 43%|████▎     | 7690/17834 [3:54:09<4:53:00,  1.73s/it] 43%|████▎     | 7691/17834 [3:54:11<4:53:51,  1.74s/it] 43%|████▎     | 7692/17834 [3:54:12<4:55:36,  1.75s/it] 43%|████▎     | 7693/17834 [3:54:14<4:53:53,  1.74s/it] 43%|████▎     | 7694/17834 [3:54:16<4:57:08,  1.76s/it] 43%|████▎     | 7695/17834 [3:54:18<4:53:33,  1.74s/it] 43%|████▎     | 7696/17834 [3:54:20<4:58:25,  1.77s/it] 43%|████▎     | 7697/17834 [3:54:21<4:57:11,  1.76s/it] 43%|████▎     | 7698/17834 [3:54:23<4:59:12,  1.77s/it] 43%|████▎     | 7699/17834 [3:54:25<4:57:16,  1.76s/it]08/30/2024 23:08:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5479907989501953, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04144985228776932, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2503037452697754, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8397445678710938}
 43%|████▎     | 7700/17834 [3:54:27<4:58:17,  1.77s/it] 43%|████▎     | 7701/17834 [3:54:28<4:57:23,  1.76s/it] 43%|████▎     | 7702/17834 [3:54:30<4:56:15,  1.75s/it] 43%|████▎     | 7703/17834 [3:54:32<4:54:11,  1.74s/it] 43%|████▎     | 7704/17834 [3:54:34<4:54:41,  1.75s/it] 43%|████▎     | 7705/17834 [3:54:35<4:55:51,  1.75s/it] 43%|████▎     | 7706/17834 [3:54:37<4:56:20,  1.76s/it] 43%|████▎     | 7707/17834 [3:54:39<4:58:42,  1.77s/it] 43%|████▎     | 7708/17834 [3:54:41<4:56:10,  1.75s/it] 43%|████▎     | 7709/17834 [3:54:42<4:58:27,  1.77s/it] 43%|████▎     | 7710/17834 [3:54:44<4:58:10,  1.77s/it] 43%|████▎     | 7711/17834 [3:54:46<4:59:11,  1.77s/it] 43%|████▎     | 7712/17834 [3:54:48<4:54:35,  1.75s/it] 43%|████▎     | 7713/17834 [3:54:49<4:58:44,  1.77s/it] 43%|████▎     | 7714/17834 [3:54:51<4:55:32,  1.75s/it] 43%|████▎     | 7715/17834 [3:54:53<4:57:47,  1.77s/it] 43%|████▎     | 7716/17834 [3:54:55<4:57:00,  1.76s/it] 43%|████▎     | 7717/17834 [3:54:56<4:54:25,  1.75s/it] 43%|████▎     | 7718/17834 [3:54:58<4:52:54,  1.74s/it] 43%|████▎     | 7719/17834 [3:55:00<4:52:17,  1.73s/it] 43%|████▎     | 7720/17834 [3:55:02<4:56:05,  1.76s/it] 43%|████▎     | 7721/17834 [3:55:03<4:53:34,  1.74s/it] 43%|████▎     | 7722/17834 [3:55:05<5:01:45,  1.79s/it] 43%|████▎     | 7723/17834 [3:55:07<4:57:05,  1.76s/it] 43%|████▎     | 7724/17834 [3:55:09<4:55:46,  1.76s/it] 43%|████▎     | 7725/17834 [3:55:10<4:53:17,  1.74s/it] 43%|████▎     | 7726/17834 [3:55:12<4:53:50,  1.74s/it] 43%|████▎     | 7727/17834 [3:55:14<4:52:59,  1.74s/it] 43%|████▎     | 7728/17834 [3:55:16<4:53:11,  1.74s/it] 43%|████▎     | 7729/17834 [3:55:18<5:01:36,  1.79s/it] 43%|████▎     | 7730/17834 [3:55:19<4:59:04,  1.78s/it] 43%|████▎     | 7731/17834 [3:55:21<4:56:28,  1.76s/it] 43%|████▎     | 7732/17834 [3:55:23<4:59:29,  1.78s/it] 43%|████▎     | 7733/17834 [3:55:25<4:58:54,  1.78s/it] 43%|████▎     | 7734/17834 [3:55:26<4:57:20,  1.77s/it] 43%|████▎     | 7735/17834 [3:55:28<4:58:20,  1.77s/it] 43%|████▎     | 7736/17834 [3:55:30<5:00:44,  1.79s/it] 43%|████▎     | 7737/17834 [3:55:32<4:58:56,  1.78s/it] 43%|████▎     | 7738/17834 [3:55:33<4:57:16,  1.77s/it] 43%|████▎     | 7739/17834 [3:55:35<4:53:14,  1.74s/it] 43%|████▎     | 7740/17834 [3:55:37<4:55:45,  1.76s/it] 43%|████▎     | 7741/17834 [3:55:39<4:56:19,  1.76s/it] 43%|████▎     | 7742/17834 [3:55:40<4:56:10,  1.76s/it] 43%|████▎     | 7743/17834 [3:55:42<4:57:20,  1.77s/it] 43%|████▎     | 7744/17834 [3:55:44<4:55:12,  1.76s/it] 43%|████▎     | 7745/17834 [3:55:46<4:55:13,  1.76s/it] 43%|████▎     | 7746/17834 [3:55:48<4:57:17,  1.77s/it] 43%|████▎     | 7747/17834 [3:55:49<4:54:21,  1.75s/it] 43%|████▎     | 7748/17834 [3:55:51<4:55:17,  1.76s/it] 43%|████▎     | 7749/17834 [3:55:53<4:53:53,  1.75s/it]08/30/2024 23:10:12 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2873355150222778, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029995068907737732, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2821898460388184, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.599520444869995}
 43%|████▎     | 7750/17834 [3:55:54<4:53:33,  1.75s/it] 43%|████▎     | 7751/17834 [3:55:56<4:55:47,  1.76s/it] 43%|████▎     | 7752/17834 [3:55:58<4:51:44,  1.74s/it] 43%|████▎     | 7753/17834 [3:56:00<4:52:02,  1.74s/it] 43%|████▎     | 7754/17834 [3:56:01<4:53:47,  1.75s/it] 43%|████▎     | 7755/17834 [3:56:03<4:56:49,  1.77s/it] 43%|████▎     | 7756/17834 [3:56:05<4:54:33,  1.75s/it] 43%|████▎     | 7757/17834 [3:56:07<4:52:05,  1.74s/it] 44%|████▎     | 7758/17834 [3:56:08<4:53:17,  1.75s/it] 44%|████▎     | 7759/17834 [3:56:10<4:57:42,  1.77s/it] 44%|████▎     | 7760/17834 [3:56:12<4:53:48,  1.75s/it] 44%|████▎     | 7761/17834 [3:56:14<4:51:51,  1.74s/it] 44%|████▎     | 7762/17834 [3:56:15<4:52:11,  1.74s/it] 44%|████▎     | 7763/17834 [3:56:17<4:54:05,  1.75s/it] 44%|████▎     | 7764/17834 [3:56:19<4:54:41,  1.76s/it] 44%|████▎     | 7765/17834 [3:56:21<4:52:37,  1.74s/it] 44%|████▎     | 7766/17834 [3:56:23<4:54:50,  1.76s/it] 44%|████▎     | 7767/17834 [3:56:24<4:55:00,  1.76s/it] 44%|████▎     | 7768/17834 [3:56:26<4:57:35,  1.77s/it] 44%|████▎     | 7769/17834 [3:56:28<4:53:16,  1.75s/it] 44%|████▎     | 7770/17834 [3:56:30<4:55:56,  1.76s/it] 44%|████▎     | 7771/17834 [3:56:31<4:52:36,  1.74s/it] 44%|████▎     | 7772/17834 [3:56:33<4:55:41,  1.76s/it] 44%|████▎     | 7773/17834 [3:56:35<4:51:35,  1.74s/it] 44%|████▎     | 7774/17834 [3:56:36<4:50:37,  1.73s/it] 44%|████▎     | 7775/17834 [3:56:38<4:53:44,  1.75s/it] 44%|████▎     | 7776/17834 [3:56:40<4:50:56,  1.74s/it] 44%|████▎     | 7777/17834 [3:56:42<4:53:16,  1.75s/it] 44%|████▎     | 7778/17834 [3:56:43<4:50:17,  1.73s/it] 44%|████▎     | 7779/17834 [3:56:45<4:53:43,  1.75s/it] 44%|████▎     | 7780/17834 [3:56:47<4:53:10,  1.75s/it] 44%|████▎     | 7781/17834 [3:56:49<4:51:43,  1.74s/it] 44%|████▎     | 7782/17834 [3:56:50<4:50:06,  1.73s/it] 44%|████▎     | 7783/17834 [3:56:52<4:50:26,  1.73s/it] 44%|████▎     | 7784/17834 [3:56:54<4:52:06,  1.74s/it] 44%|████▎     | 7785/17834 [3:56:56<4:52:58,  1.75s/it] 44%|████▎     | 7786/17834 [3:56:57<4:54:05,  1.76s/it] 44%|████▎     | 7787/17834 [3:56:59<4:52:58,  1.75s/it] 44%|████▎     | 7788/17834 [3:57:01<4:53:26,  1.75s/it] 44%|████▎     | 7789/17834 [3:57:03<4:50:40,  1.74s/it] 44%|████▎     | 7790/17834 [3:57:04<4:50:26,  1.74s/it] 44%|████▎     | 7791/17834 [3:57:06<4:52:46,  1.75s/it] 44%|████▎     | 7792/17834 [3:57:08<4:56:33,  1.77s/it] 44%|████▎     | 7793/17834 [3:57:10<4:53:55,  1.76s/it] 44%|████▎     | 7794/17834 [3:57:12<4:59:38,  1.79s/it] 44%|████▎     | 7795/17834 [3:57:13<4:56:10,  1.77s/it] 44%|████▎     | 7796/17834 [3:57:15<4:54:04,  1.76s/it] 44%|████▎     | 7797/17834 [3:57:17<4:54:27,  1.76s/it] 44%|████▎     | 7798/17834 [3:57:19<4:56:37,  1.77s/it] 44%|████▎     | 7799/17834 [3:57:20<4:55:16,  1.77s/it]08/30/2024 23:11:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.100297212600708, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.027948372066020966, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1901652812957764, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.318410873413086}
 44%|████▎     | 7800/17834 [3:57:22<4:57:14,  1.78s/it] 44%|████▎     | 7801/17834 [3:57:24<4:54:19,  1.76s/it] 44%|████▎     | 7802/17834 [3:57:26<4:54:29,  1.76s/it] 44%|████▍     | 7803/17834 [3:57:27<4:56:28,  1.77s/it] 44%|████▍     | 7804/17834 [3:57:29<4:57:24,  1.78s/it] 44%|████▍     | 7805/17834 [3:57:31<4:58:03,  1.78s/it] 44%|████▍     | 7806/17834 [3:57:33<4:57:04,  1.78s/it] 44%|████▍     | 7807/17834 [3:57:35<4:54:44,  1.76s/it] 44%|████▍     | 7808/17834 [3:57:36<4:56:13,  1.77s/it] 44%|████▍     | 7809/17834 [3:57:38<4:52:37,  1.75s/it] 44%|████▍     | 7810/17834 [3:57:40<4:56:42,  1.78s/it] 44%|████▍     | 7811/17834 [3:57:42<4:51:32,  1.75s/it] 44%|████▍     | 7812/17834 [3:57:43<4:51:38,  1.75s/it] 44%|████▍     | 7813/17834 [3:57:45<4:53:48,  1.76s/it] 44%|████▍     | 7814/17834 [3:57:47<4:53:00,  1.75s/it] 44%|████▍     | 7815/17834 [3:57:49<4:54:05,  1.76s/it] 44%|████▍     | 7816/17834 [3:57:50<4:51:22,  1.75s/it] 44%|████▍     | 7817/17834 [3:57:52<4:47:02,  1.72s/it] 44%|████▍     | 7818/17834 [3:57:54<4:48:37,  1.73s/it] 44%|████▍     | 7819/17834 [3:57:55<4:47:17,  1.72s/it] 44%|████▍     | 7820/17834 [3:57:57<4:46:32,  1.72s/it] 44%|████▍     | 7821/17834 [3:57:59<4:50:53,  1.74s/it] 44%|████▍     | 7822/17834 [3:58:01<4:54:52,  1.77s/it] 44%|████▍     | 7823/17834 [3:58:02<4:51:00,  1.74s/it] 44%|████▍     | 7824/17834 [3:58:04<4:48:23,  1.73s/it] 44%|████▍     | 7825/17834 [3:58:06<4:46:07,  1.72s/it] 44%|████▍     | 7826/17834 [3:58:08<4:48:05,  1.73s/it] 44%|████▍     | 7827/17834 [3:58:09<4:50:17,  1.74s/it] 44%|████▍     | 7828/17834 [3:58:11<4:52:24,  1.75s/it] 44%|████▍     | 7829/17834 [3:58:13<4:51:46,  1.75s/it] 44%|████▍     | 7830/17834 [3:58:15<4:53:43,  1.76s/it] 44%|████▍     | 7831/17834 [3:58:16<4:55:15,  1.77s/it] 44%|████▍     | 7832/17834 [3:58:18<4:53:22,  1.76s/it] 44%|████▍     | 7833/17834 [3:58:20<5:02:49,  1.82s/it] 44%|████▍     | 7834/17834 [3:58:22<4:57:04,  1.78s/it] 44%|████▍     | 7835/17834 [3:58:24<4:53:57,  1.76s/it] 44%|████▍     | 7836/17834 [3:58:25<4:53:45,  1.76s/it] 44%|████▍     | 7837/17834 [3:58:27<4:52:10,  1.75s/it] 44%|████▍     | 7838/17834 [3:58:29<4:50:15,  1.74s/it] 44%|████▍     | 7839/17834 [3:58:30<4:49:20,  1.74s/it] 44%|████▍     | 7840/17834 [3:58:32<4:51:06,  1.75s/it] 44%|████▍     | 7841/17834 [3:58:34<4:52:48,  1.76s/it] 44%|████▍     | 7842/17834 [3:58:36<4:55:10,  1.77s/it] 44%|████▍     | 7843/17834 [3:58:38<4:50:14,  1.74s/it] 44%|████▍     | 7844/17834 [3:58:39<4:49:41,  1.74s/it] 44%|████▍     | 7845/17834 [3:58:41<4:47:37,  1.73s/it] 44%|████▍     | 7846/17834 [3:58:43<4:49:44,  1.74s/it] 44%|████▍     | 7847/17834 [3:58:44<4:51:11,  1.75s/it] 44%|████▍     | 7848/17834 [3:58:46<4:50:18,  1.74s/it] 44%|████▍     | 7849/17834 [3:58:48<4:51:24,  1.75s/it]08/30/2024 23:13:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9991343021392822, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.031456392258405685, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1318576335906982, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1624484062194824}
 44%|████▍     | 7850/17834 [3:58:50<4:52:48,  1.76s/it] 44%|████▍     | 7851/17834 [3:58:51<4:49:59,  1.74s/it] 44%|████▍     | 7852/17834 [3:58:53<4:49:43,  1.74s/it] 44%|████▍     | 7853/17834 [3:58:55<4:50:59,  1.75s/it] 44%|████▍     | 7854/17834 [3:58:57<4:47:35,  1.73s/it] 44%|████▍     | 7855/17834 [3:58:58<4:47:24,  1.73s/it] 44%|████▍     | 7856/17834 [3:59:00<4:51:52,  1.76s/it] 44%|████▍     | 7857/17834 [3:59:02<4:49:07,  1.74s/it] 44%|████▍     | 7858/17834 [3:59:04<4:45:52,  1.72s/it] 44%|████▍     | 7859/17834 [3:59:05<4:47:38,  1.73s/it] 44%|████▍     | 7860/17834 [3:59:07<4:46:11,  1.72s/it] 44%|████▍     | 7861/17834 [3:59:09<4:44:32,  1.71s/it] 44%|████▍     | 7862/17834 [3:59:11<4:49:50,  1.74s/it] 44%|████▍     | 7863/17834 [3:59:12<4:49:18,  1.74s/it] 44%|████▍     | 7864/17834 [3:59:14<4:49:05,  1.74s/it] 44%|████▍     | 7865/17834 [3:59:16<4:51:36,  1.76s/it] 44%|████▍     | 7866/17834 [3:59:18<4:58:12,  1.79s/it] 44%|████▍     | 7867/17834 [3:59:19<4:53:48,  1.77s/it] 44%|████▍     | 7868/17834 [3:59:21<4:51:09,  1.75s/it] 44%|████▍     | 7869/17834 [3:59:23<4:51:17,  1.75s/it] 44%|████▍     | 7870/17834 [3:59:25<4:54:23,  1.77s/it] 44%|████▍     | 7871/17834 [3:59:26<4:53:29,  1.77s/it] 44%|████▍     | 7872/17834 [3:59:28<4:51:32,  1.76s/it] 44%|████▍     | 7873/17834 [3:59:30<4:50:31,  1.75s/it] 44%|████▍     | 7874/17834 [3:59:32<4:52:18,  1.76s/it] 44%|████▍     | 7875/17834 [3:59:33<4:46:36,  1.73s/it] 44%|████▍     | 7876/17834 [3:59:35<4:49:39,  1.75s/it] 44%|████▍     | 7877/17834 [3:59:37<4:49:01,  1.74s/it] 44%|████▍     | 7878/17834 [3:59:39<4:47:28,  1.73s/it] 44%|████▍     | 7879/17834 [3:59:40<4:47:19,  1.73s/it] 44%|████▍     | 7880/17834 [3:59:42<4:47:42,  1.73s/it] 44%|████▍     | 7881/17834 [3:59:44<4:46:52,  1.73s/it] 44%|████▍     | 7882/17834 [3:59:45<4:46:11,  1.73s/it] 44%|████▍     | 7883/17834 [3:59:47<4:49:35,  1.75s/it] 44%|████▍     | 7884/17834 [3:59:49<4:51:20,  1.76s/it] 44%|████▍     | 7885/17834 [3:59:51<4:48:42,  1.74s/it] 44%|████▍     | 7886/17834 [3:59:52<4:47:33,  1.73s/it] 44%|████▍     | 7887/17834 [3:59:54<4:49:42,  1.75s/it] 44%|████▍     | 7888/17834 [3:59:56<4:49:35,  1.75s/it] 44%|████▍     | 7889/17834 [3:59:58<4:50:18,  1.75s/it] 44%|████▍     | 7890/17834 [4:00:00<4:52:22,  1.76s/it] 44%|████▍     | 7891/17834 [4:00:01<4:57:43,  1.80s/it] 44%|████▍     | 7892/17834 [4:00:03<4:54:31,  1.78s/it] 44%|████▍     | 7893/17834 [4:00:05<4:48:21,  1.74s/it] 44%|████▍     | 7894/17834 [4:00:07<4:49:37,  1.75s/it] 44%|████▍     | 7895/17834 [4:00:08<4:57:07,  1.79s/it] 44%|████▍     | 7896/17834 [4:00:10<4:54:41,  1.78s/it] 44%|████▍     | 7897/17834 [4:00:12<4:50:38,  1.75s/it] 44%|████▍     | 7898/17834 [4:00:14<4:53:12,  1.77s/it] 44%|████▍     | 7899/17834 [4:00:15<4:49:47,  1.75s/it]08/30/2024 23:14:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0013147592544556, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02829565666615963, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1375274658203125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.167137861251831}
 44%|████▍     | 7900/17834 [4:00:17<4:52:02,  1.76s/it] 44%|████▍     | 7901/17834 [4:00:19<4:50:02,  1.75s/it] 44%|████▍     | 7902/17834 [4:00:21<4:54:14,  1.78s/it] 44%|████▍     | 7903/17834 [4:00:23<4:54:40,  1.78s/it] 44%|████▍     | 7904/17834 [4:00:24<4:53:35,  1.77s/it] 44%|████▍     | 7905/17834 [4:00:26<4:52:31,  1.77s/it] 44%|████▍     | 7906/17834 [4:00:28<4:50:43,  1.76s/it] 44%|████▍     | 7907/17834 [4:00:30<4:49:34,  1.75s/it] 44%|████▍     | 7908/17834 [4:00:31<4:50:05,  1.75s/it] 44%|████▍     | 7909/17834 [4:00:33<4:49:11,  1.75s/it] 44%|████▍     | 7910/17834 [4:00:35<4:50:46,  1.76s/it] 44%|████▍     | 7911/17834 [4:00:37<4:55:00,  1.78s/it] 44%|████▍     | 7912/17834 [4:00:38<4:54:18,  1.78s/it] 44%|████▍     | 7913/17834 [4:00:40<4:54:25,  1.78s/it] 44%|████▍     | 7914/17834 [4:00:42<4:56:04,  1.79s/it] 44%|████▍     | 7915/17834 [4:00:44<4:54:37,  1.78s/it] 44%|████▍     | 7916/17834 [4:00:46<4:52:42,  1.77s/it] 44%|████▍     | 7917/17834 [4:00:47<4:54:25,  1.78s/it] 44%|████▍     | 7918/17834 [4:00:49<4:52:31,  1.77s/it] 44%|████▍     | 7919/17834 [4:00:51<4:52:21,  1.77s/it] 44%|████▍     | 7920/17834 [4:00:53<4:52:18,  1.77s/it] 44%|████▍     | 7921/17834 [4:00:55<4:56:53,  1.80s/it] 44%|████▍     | 7922/17834 [4:00:56<4:50:28,  1.76s/it] 44%|████▍     | 7923/17834 [4:00:58<4:55:18,  1.79s/it] 44%|████▍     | 7924/17834 [4:01:00<4:55:41,  1.79s/it] 44%|████▍     | 7925/17834 [4:01:02<4:55:48,  1.79s/it] 44%|████▍     | 7926/17834 [4:01:03<4:57:10,  1.80s/it] 44%|████▍     | 7927/17834 [4:01:05<4:56:58,  1.80s/it] 44%|████▍     | 7928/17834 [4:01:07<4:52:31,  1.77s/it] 44%|████▍     | 7929/17834 [4:01:09<4:53:52,  1.78s/it] 44%|████▍     | 7930/17834 [4:01:10<4:51:25,  1.77s/it] 44%|████▍     | 7931/17834 [4:01:12<4:48:23,  1.75s/it] 44%|████▍     | 7932/17834 [4:01:14<4:49:34,  1.75s/it] 44%|████▍     | 7933/17834 [4:01:16<4:49:44,  1.76s/it] 44%|████▍     | 7934/17834 [4:01:17<4:48:33,  1.75s/it] 44%|████▍     | 7935/17834 [4:01:19<4:45:51,  1.73s/it] 44%|████▍     | 7936/17834 [4:01:21<4:47:32,  1.74s/it] 45%|████▍     | 7937/17834 [4:01:23<4:48:29,  1.75s/it] 45%|████▍     | 7938/17834 [4:01:24<4:50:19,  1.76s/it] 45%|████▍     | 7939/17834 [4:01:26<4:47:47,  1.75s/it] 45%|████▍     | 7940/17834 [4:01:28<4:49:10,  1.75s/it] 45%|████▍     | 7941/17834 [4:01:30<4:48:57,  1.75s/it] 45%|████▍     | 7942/17834 [4:01:31<4:46:34,  1.74s/it] 45%|████▍     | 7943/17834 [4:01:33<4:48:56,  1.75s/it] 45%|████▍     | 7944/17834 [4:01:35<4:47:19,  1.74s/it] 45%|████▍     | 7945/17834 [4:01:37<4:45:40,  1.73s/it] 45%|████▍     | 7946/17834 [4:01:38<4:46:50,  1.74s/it] 45%|████▍     | 7947/17834 [4:01:40<4:44:00,  1.72s/it] 45%|████▍     | 7948/17834 [4:01:42<4:42:16,  1.71s/it] 45%|████▍     | 7949/17834 [4:01:43<4:43:01,  1.72s/it]08/30/2024 23:16:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.233716368675232, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03596170246601105, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2362923622131348, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5059704780578613}
 45%|████▍     | 7950/17834 [4:01:45<4:43:18,  1.72s/it] 45%|████▍     | 7951/17834 [4:01:47<4:43:20,  1.72s/it] 45%|████▍     | 7952/17834 [4:01:49<4:42:07,  1.71s/it] 45%|████▍     | 7953/17834 [4:01:50<4:43:56,  1.72s/it] 45%|████▍     | 7954/17834 [4:01:52<4:45:15,  1.73s/it] 45%|████▍     | 7955/17834 [4:01:54<4:46:51,  1.74s/it] 45%|████▍     | 7956/17834 [4:01:56<4:51:47,  1.77s/it] 45%|████▍     | 7957/17834 [4:01:57<4:47:50,  1.75s/it] 45%|████▍     | 7958/17834 [4:01:59<4:48:38,  1.75s/it] 45%|████▍     | 7959/17834 [4:02:01<4:50:04,  1.76s/it] 45%|████▍     | 7960/17834 [4:02:03<4:47:41,  1.75s/it] 45%|████▍     | 7961/17834 [4:02:04<4:46:40,  1.74s/it] 45%|████▍     | 7962/17834 [4:02:06<4:45:14,  1.73s/it] 45%|████▍     | 7963/17834 [4:02:08<4:44:44,  1.73s/it] 45%|████▍     | 7964/17834 [4:02:10<4:46:05,  1.74s/it] 45%|████▍     | 7965/17834 [4:02:11<4:46:57,  1.74s/it] 45%|████▍     | 7966/17834 [4:02:13<4:46:49,  1.74s/it] 45%|████▍     | 7967/17834 [4:02:15<4:46:49,  1.74s/it] 45%|████▍     | 7968/17834 [4:02:17<4:45:07,  1.73s/it] 45%|████▍     | 7969/17834 [4:02:18<4:50:12,  1.77s/it] 45%|████▍     | 7970/17834 [4:02:20<4:46:27,  1.74s/it] 45%|████▍     | 7971/17834 [4:02:22<4:45:56,  1.74s/it] 45%|████▍     | 7972/17834 [4:02:24<4:51:54,  1.78s/it] 45%|████▍     | 7973/17834 [4:02:25<4:50:30,  1.77s/it] 45%|████▍     | 7974/17834 [4:02:27<4:48:23,  1.75s/it] 45%|████▍     | 7975/17834 [4:02:29<4:45:07,  1.74s/it] 45%|████▍     | 7976/17834 [4:02:31<4:44:25,  1.73s/it] 45%|████▍     | 7977/17834 [4:02:32<4:45:33,  1.74s/it] 45%|████▍     | 7978/17834 [4:02:34<4:45:34,  1.74s/it] 45%|████▍     | 7979/17834 [4:02:36<4:44:04,  1.73s/it] 45%|████▍     | 7980/17834 [4:02:37<4:42:27,  1.72s/it] 45%|████▍     | 7981/17834 [4:02:39<4:43:05,  1.72s/it] 45%|████▍     | 7982/17834 [4:02:41<4:46:46,  1.75s/it] 45%|████▍     | 7983/17834 [4:02:43<4:47:07,  1.75s/it] 45%|████▍     | 7984/17834 [4:02:44<4:44:22,  1.73s/it] 45%|████▍     | 7985/17834 [4:02:46<4:42:15,  1.72s/it] 45%|████▍     | 7986/17834 [4:02:48<4:43:50,  1.73s/it] 45%|████▍     | 7987/17834 [4:02:50<4:46:42,  1.75s/it] 45%|████▍     | 7988/17834 [4:02:51<4:49:31,  1.76s/it] 45%|████▍     | 7989/17834 [4:02:53<4:43:28,  1.73s/it] 45%|████▍     | 7990/17834 [4:02:55<4:43:07,  1.73s/it] 45%|████▍     | 7991/17834 [4:02:57<4:44:10,  1.73s/it] 45%|████▍     | 7992/17834 [4:02:58<4:44:11,  1.73s/it] 45%|████▍     | 7993/17834 [4:03:00<4:47:47,  1.75s/it] 45%|████▍     | 7994/17834 [4:03:02<4:49:16,  1.76s/it] 45%|████▍     | 7995/17834 [4:03:04<4:47:25,  1.75s/it] 45%|████▍     | 7996/17834 [4:03:05<4:49:12,  1.76s/it] 45%|████▍     | 7997/17834 [4:03:07<4:47:02,  1.75s/it] 45%|████▍     | 7998/17834 [4:03:09<4:46:54,  1.75s/it] 45%|████▍     | 7999/17834 [4:03:11<4:49:43,  1.77s/it]08/30/2024 23:17:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2670059204101562, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038087017834186554, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2817413806915283, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.58683443069458}
 45%|████▍     | 8000/17834 [4:03:12<4:46:36,  1.75s/it] 45%|████▍     | 8001/17834 [4:03:14<4:45:21,  1.74s/it] 45%|████▍     | 8002/17834 [4:03:16<4:44:00,  1.73s/it] 45%|████▍     | 8003/17834 [4:03:18<4:44:58,  1.74s/it] 45%|████▍     | 8004/17834 [4:03:19<4:44:21,  1.74s/it] 45%|████▍     | 8005/17834 [4:03:21<4:45:11,  1.74s/it] 45%|████▍     | 8006/17834 [4:03:23<4:46:26,  1.75s/it] 45%|████▍     | 8007/17834 [4:03:25<4:47:43,  1.76s/it] 45%|████▍     | 8008/17834 [4:03:26<4:43:35,  1.73s/it] 45%|████▍     | 8009/17834 [4:03:28<4:42:43,  1.73s/it] 45%|████▍     | 8010/17834 [4:03:30<4:46:21,  1.75s/it] 45%|████▍     | 8011/17834 [4:03:32<4:45:52,  1.75s/it] 45%|████▍     | 8012/17834 [4:03:33<4:43:08,  1.73s/it] 45%|████▍     | 8013/17834 [4:03:35<4:43:22,  1.73s/it] 45%|████▍     | 8014/17834 [4:03:37<4:42:32,  1.73s/it] 45%|████▍     | 8015/17834 [4:03:38<4:43:22,  1.73s/it] 45%|████▍     | 8016/17834 [4:03:40<4:46:58,  1.75s/it] 45%|████▍     | 8017/17834 [4:03:42<4:48:10,  1.76s/it] 45%|████▍     | 8018/17834 [4:03:44<4:46:05,  1.75s/it] 45%|████▍     | 8019/17834 [4:03:45<4:44:38,  1.74s/it] 45%|████▍     | 8020/17834 [4:03:47<4:44:37,  1.74s/it] 45%|████▍     | 8021/17834 [4:03:49<4:44:38,  1.74s/it] 45%|████▍     | 8022/17834 [4:03:51<4:44:22,  1.74s/it] 45%|████▍     | 8023/17834 [4:03:52<4:47:38,  1.76s/it] 45%|████▍     | 8024/17834 [4:03:54<4:44:42,  1.74s/it] 45%|████▍     | 8025/17834 [4:03:56<4:43:06,  1.73s/it] 45%|████▌     | 8026/17834 [4:03:58<4:43:37,  1.74s/it] 45%|████▌     | 8027/17834 [4:03:59<4:40:25,  1.72s/it] 45%|████▌     | 8028/17834 [4:04:01<4:42:52,  1.73s/it] 45%|████▌     | 8029/17834 [4:04:03<4:40:17,  1.72s/it] 45%|████▌     | 8030/17834 [4:04:04<4:41:05,  1.72s/it] 45%|████▌     | 8031/17834 [4:04:06<4:40:28,  1.72s/it] 45%|████▌     | 8032/17834 [4:04:08<4:43:11,  1.73s/it] 45%|████▌     | 8033/17834 [4:04:10<4:43:36,  1.74s/it] 45%|████▌     | 8034/17834 [4:04:11<4:41:58,  1.73s/it] 45%|████▌     | 8035/17834 [4:04:13<4:42:57,  1.73s/it] 45%|████▌     | 8036/17834 [4:04:15<4:41:56,  1.73s/it] 45%|████▌     | 8037/17834 [4:04:17<4:43:17,  1.73s/it] 45%|████▌     | 8038/17834 [4:04:18<4:41:53,  1.73s/it] 45%|████▌     | 8039/17834 [4:04:20<4:41:54,  1.73s/it] 45%|████▌     | 8040/17834 [4:04:22<4:44:42,  1.74s/it] 45%|████▌     | 8041/17834 [4:04:24<4:46:23,  1.75s/it] 45%|████▌     | 8042/17834 [4:04:25<4:47:15,  1.76s/it] 45%|████▌     | 8043/17834 [4:04:27<4:47:33,  1.76s/it] 45%|████▌     | 8044/17834 [4:04:29<4:49:18,  1.77s/it] 45%|████▌     | 8045/17834 [4:04:31<4:49:58,  1.78s/it] 45%|████▌     | 8046/17834 [4:04:32<4:45:24,  1.75s/it] 45%|████▌     | 8047/17834 [4:04:34<4:52:23,  1.79s/it] 45%|████▌     | 8048/17834 [4:04:36<4:46:51,  1.76s/it] 45%|████▌     | 8049/17834 [4:04:38<4:47:36,  1.76s/it]08/30/2024 23:18:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3422670364379883, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04282978177070618, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.20696759223938, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.592064380645752}
 45%|████▌     | 8050/17834 [4:04:40<4:47:02,  1.76s/it] 45%|████▌     | 8051/17834 [4:04:41<4:48:53,  1.77s/it] 45%|████▌     | 8052/17834 [4:04:43<4:44:34,  1.75s/it] 45%|████▌     | 8053/17834 [4:04:45<4:45:15,  1.75s/it] 45%|████▌     | 8054/17834 [4:04:47<4:45:51,  1.75s/it] 45%|████▌     | 8055/17834 [4:04:48<4:47:56,  1.77s/it] 45%|████▌     | 8056/17834 [4:04:50<4:46:29,  1.76s/it] 45%|████▌     | 8057/17834 [4:04:52<4:43:58,  1.74s/it] 45%|████▌     | 8058/17834 [4:04:54<4:43:15,  1.74s/it] 45%|████▌     | 8059/17834 [4:04:55<4:46:36,  1.76s/it] 45%|████▌     | 8060/17834 [4:04:57<4:43:48,  1.74s/it] 45%|████▌     | 8061/17834 [4:04:59<4:43:05,  1.74s/it] 45%|████▌     | 8062/17834 [4:05:01<4:48:15,  1.77s/it] 45%|████▌     | 8063/17834 [4:05:02<4:47:06,  1.76s/it] 45%|████▌     | 8064/17834 [4:05:04<4:47:28,  1.77s/it] 45%|████▌     | 8065/17834 [4:05:06<4:48:15,  1.77s/it] 45%|████▌     | 8066/17834 [4:05:08<4:50:50,  1.79s/it] 45%|████▌     | 8067/17834 [4:05:10<4:52:24,  1.80s/it] 45%|████▌     | 8068/17834 [4:05:11<4:50:40,  1.79s/it] 45%|████▌     | 8069/17834 [4:05:13<4:50:33,  1.79s/it] 45%|████▌     | 8070/17834 [4:05:15<4:47:37,  1.77s/it] 45%|████▌     | 8071/17834 [4:05:17<4:46:13,  1.76s/it] 45%|████▌     | 8072/17834 [4:05:18<4:44:19,  1.75s/it] 45%|████▌     | 8073/17834 [4:05:20<4:43:51,  1.74s/it] 45%|████▌     | 8074/17834 [4:05:22<4:42:55,  1.74s/it] 45%|████▌     | 8075/17834 [4:05:24<4:45:33,  1.76s/it] 45%|████▌     | 8076/17834 [4:05:25<4:44:59,  1.75s/it] 45%|████▌     | 8077/17834 [4:05:27<4:44:16,  1.75s/it] 45%|████▌     | 8078/17834 [4:05:29<4:47:01,  1.77s/it] 45%|████▌     | 8079/17834 [4:05:31<4:45:46,  1.76s/it] 45%|████▌     | 8080/17834 [4:05:32<4:43:48,  1.75s/it] 45%|████▌     | 8081/17834 [4:05:34<4:41:28,  1.73s/it] 45%|████▌     | 8082/17834 [4:05:36<4:40:16,  1.72s/it] 45%|████▌     | 8083/17834 [4:05:37<4:40:49,  1.73s/it] 45%|████▌     | 8084/17834 [4:05:39<4:38:45,  1.72s/it] 45%|████▌     | 8085/17834 [4:05:41<4:40:19,  1.73s/it] 45%|████▌     | 8086/17834 [4:05:43<4:37:37,  1.71s/it] 45%|████▌     | 8087/17834 [4:05:44<4:41:14,  1.73s/it] 45%|████▌     | 8088/17834 [4:05:46<4:40:31,  1.73s/it] 45%|████▌     | 8089/17834 [4:05:48<4:41:38,  1.73s/it] 45%|████▌     | 8090/17834 [4:05:50<4:50:53,  1.79s/it] 45%|████▌     | 8091/17834 [4:05:51<4:45:59,  1.76s/it] 45%|████▌     | 8092/17834 [4:05:53<4:42:46,  1.74s/it] 45%|████▌     | 8093/17834 [4:05:55<4:40:25,  1.73s/it] 45%|████▌     | 8094/17834 [4:05:57<4:40:06,  1.73s/it] 45%|████▌     | 8095/17834 [4:05:58<4:43:58,  1.75s/it] 45%|████▌     | 8096/17834 [4:06:00<4:42:22,  1.74s/it] 45%|████▌     | 8097/17834 [4:06:02<4:42:42,  1.74s/it] 45%|████▌     | 8098/17834 [4:06:04<4:43:38,  1.75s/it] 45%|████▌     | 8099/17834 [4:06:05<4:49:24,  1.78s/it]08/30/2024 23:20:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2272346019744873, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.041651058942079544, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.132166624069214, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.401052236557007}
 45%|████▌     | 8100/17834 [4:06:07<4:52:52,  1.81s/it] 45%|████▌     | 8101/17834 [4:06:09<4:49:19,  1.78s/it] 45%|████▌     | 8102/17834 [4:06:11<4:47:40,  1.77s/it] 45%|████▌     | 8103/17834 [4:06:13<4:47:47,  1.77s/it] 45%|████▌     | 8104/17834 [4:06:14<4:46:03,  1.76s/it] 45%|████▌     | 8105/17834 [4:06:16<4:44:32,  1.75s/it] 45%|████▌     | 8106/17834 [4:06:18<4:48:15,  1.78s/it] 45%|████▌     | 8107/17834 [4:06:20<4:47:18,  1.77s/it] 45%|████▌     | 8108/17834 [4:06:21<4:44:28,  1.75s/it] 45%|████▌     | 8109/17834 [4:06:23<4:41:28,  1.74s/it] 45%|████▌     | 8110/17834 [4:06:25<4:40:18,  1.73s/it] 45%|████▌     | 8111/17834 [4:06:27<4:45:48,  1.76s/it] 45%|████▌     | 8112/17834 [4:06:28<4:44:38,  1.76s/it] 45%|████▌     | 8113/17834 [4:06:30<4:53:31,  1.81s/it] 45%|████▌     | 8114/17834 [4:06:32<4:44:02,  1.75s/it] 46%|████▌     | 8115/17834 [4:06:34<4:45:28,  1.76s/it] 46%|████▌     | 8116/17834 [4:06:35<4:43:33,  1.75s/it] 46%|████▌     | 8117/17834 [4:06:37<4:42:14,  1.74s/it] 46%|████▌     | 8118/17834 [4:06:39<4:40:01,  1.73s/it] 46%|████▌     | 8119/17834 [4:06:41<4:44:25,  1.76s/it] 46%|████▌     | 8120/17834 [4:06:42<4:45:02,  1.76s/it] 46%|████▌     | 8121/17834 [4:06:44<4:43:04,  1.75s/it] 46%|████▌     | 8122/17834 [4:06:46<4:43:17,  1.75s/it] 46%|████▌     | 8123/17834 [4:06:48<4:40:54,  1.74s/it] 46%|████▌     | 8124/17834 [4:06:49<4:39:28,  1.73s/it] 46%|████▌     | 8125/17834 [4:06:51<4:39:10,  1.73s/it] 46%|████▌     | 8126/17834 [4:06:53<4:39:10,  1.73s/it] 46%|████▌     | 8127/17834 [4:06:54<4:38:24,  1.72s/it] 46%|████▌     | 8128/17834 [4:06:56<4:37:44,  1.72s/it] 46%|████▌     | 8129/17834 [4:06:58<4:37:14,  1.71s/it] 46%|████▌     | 8130/17834 [4:07:00<4:39:53,  1.73s/it] 46%|████▌     | 8131/17834 [4:07:01<4:42:57,  1.75s/it] 46%|████▌     | 8132/17834 [4:07:03<4:43:26,  1.75s/it] 46%|████▌     | 8133/17834 [4:07:05<4:43:42,  1.75s/it] 46%|████▌     | 8134/17834 [4:07:07<4:41:45,  1.74s/it] 46%|████▌     | 8135/17834 [4:07:08<4:42:42,  1.75s/it] 46%|████▌     | 8136/17834 [4:07:10<4:41:26,  1.74s/it] 46%|████▌     | 8137/17834 [4:07:12<4:44:41,  1.76s/it] 46%|████▌     | 8138/17834 [4:07:14<4:43:50,  1.76s/it] 46%|████▌     | 8139/17834 [4:07:15<4:44:19,  1.76s/it] 46%|████▌     | 8140/17834 [4:07:17<4:44:31,  1.76s/it] 46%|████▌     | 8141/17834 [4:07:19<4:43:25,  1.75s/it] 46%|████▌     | 8142/17834 [4:07:21<4:42:51,  1.75s/it] 46%|████▌     | 8143/17834 [4:07:22<4:39:23,  1.73s/it] 46%|████▌     | 8144/17834 [4:07:24<4:39:19,  1.73s/it] 46%|████▌     | 8145/17834 [4:07:26<4:36:52,  1.71s/it] 46%|████▌     | 8146/17834 [4:07:28<4:38:20,  1.72s/it] 46%|████▌     | 8147/17834 [4:07:29<4:41:56,  1.75s/it] 46%|████▌     | 8148/17834 [4:07:31<4:40:32,  1.74s/it] 46%|████▌     | 8149/17834 [4:07:33<4:40:18,  1.74s/it]08/30/2024 23:21:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.5400629043579102, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05461554974317551, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.371577262878418, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.9662556648254395}
 46%|████▌     | 8150/17834 [4:07:35<4:45:01,  1.77s/it] 46%|████▌     | 8151/17834 [4:07:36<4:42:47,  1.75s/it] 46%|████▌     | 8152/17834 [4:07:38<4:41:41,  1.75s/it] 46%|████▌     | 8153/17834 [4:07:40<4:40:06,  1.74s/it] 46%|████▌     | 8154/17834 [4:07:42<4:43:18,  1.76s/it] 46%|████▌     | 8155/17834 [4:07:43<4:41:13,  1.74s/it] 46%|████▌     | 8156/17834 [4:07:45<4:47:52,  1.78s/it] 46%|████▌     | 8157/17834 [4:07:47<4:45:56,  1.77s/it] 46%|████▌     | 8158/17834 [4:07:49<4:45:11,  1.77s/it] 46%|████▌     | 8159/17834 [4:07:50<4:44:47,  1.77s/it] 46%|████▌     | 8160/17834 [4:07:52<4:40:57,  1.74s/it] 46%|████▌     | 8161/17834 [4:07:54<4:43:50,  1.76s/it] 46%|████▌     | 8162/17834 [4:07:56<4:43:43,  1.76s/it] 46%|████▌     | 8163/17834 [4:07:57<4:46:56,  1.78s/it] 46%|████▌     | 8164/17834 [4:07:59<4:42:34,  1.75s/it] 46%|████▌     | 8165/17834 [4:08:01<4:46:15,  1.78s/it] 46%|████▌     | 8166/17834 [4:08:03<4:44:32,  1.77s/it] 46%|████▌     | 8167/17834 [4:08:05<4:47:59,  1.79s/it] 46%|████▌     | 8168/17834 [4:08:06<4:47:08,  1.78s/it] 46%|████▌     | 8169/17834 [4:08:08<4:45:27,  1.77s/it] 46%|████▌     | 8170/17834 [4:08:10<4:45:01,  1.77s/it] 46%|████▌     | 8171/17834 [4:08:12<4:44:50,  1.77s/it] 46%|████▌     | 8172/17834 [4:08:13<4:45:43,  1.77s/it] 46%|████▌     | 8173/17834 [4:08:15<4:42:19,  1.75s/it] 46%|████▌     | 8174/17834 [4:08:17<4:44:05,  1.76s/it] 46%|████▌     | 8175/17834 [4:08:19<4:43:40,  1.76s/it] 46%|████▌     | 8176/17834 [4:08:20<4:43:50,  1.76s/it] 46%|████▌     | 8177/17834 [4:08:22<4:42:17,  1.75s/it] 46%|████▌     | 8178/17834 [4:08:24<4:40:40,  1.74s/it] 46%|████▌     | 8179/17834 [4:08:26<4:41:02,  1.75s/it] 46%|████▌     | 8180/17834 [4:08:27<4:43:25,  1.76s/it] 46%|████▌     | 8181/17834 [4:08:29<4:41:35,  1.75s/it] 46%|████▌     | 8182/17834 [4:08:31<4:40:15,  1.74s/it] 46%|████▌     | 8183/17834 [4:08:33<4:42:50,  1.76s/it] 46%|████▌     | 8184/17834 [4:08:34<4:39:34,  1.74s/it] 46%|████▌     | 8185/17834 [4:08:36<4:41:31,  1.75s/it] 46%|████▌     | 8186/17834 [4:08:38<4:40:59,  1.75s/it] 46%|████▌     | 8187/17834 [4:08:40<4:39:26,  1.74s/it] 46%|████▌     | 8188/17834 [4:08:41<4:41:59,  1.75s/it] 46%|████▌     | 8189/17834 [4:08:43<4:39:26,  1.74s/it] 46%|████▌     | 8190/17834 [4:08:45<4:37:38,  1.73s/it] 46%|████▌     | 8191/17834 [4:08:47<4:44:20,  1.77s/it] 46%|████▌     | 8192/17834 [4:08:48<4:41:05,  1.75s/it] 46%|████▌     | 8193/17834 [4:08:50<4:43:11,  1.76s/it] 46%|████▌     | 8194/17834 [4:08:52<4:44:52,  1.77s/it] 46%|████▌     | 8195/17834 [4:08:54<4:42:16,  1.76s/it] 46%|████▌     | 8196/17834 [4:08:56<4:47:53,  1.79s/it] 46%|████▌     | 8197/17834 [4:08:57<4:43:31,  1.77s/it] 46%|████▌     | 8198/17834 [4:08:59<4:44:50,  1.77s/it] 46%|████▌     | 8199/17834 [4:09:01<4:44:16,  1.77s/it]08/30/2024 23:23:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.076393723487854, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026531755924224854, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1708407402038574, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.273766279220581}
 46%|████▌     | 8200/17834 [4:09:03<4:43:22,  1.76s/it] 46%|████▌     | 8201/17834 [4:09:04<4:42:35,  1.76s/it] 46%|████▌     | 8202/17834 [4:09:06<4:41:48,  1.76s/it] 46%|████▌     | 8203/17834 [4:09:08<4:38:08,  1.73s/it] 46%|████▌     | 8204/17834 [4:09:10<4:39:23,  1.74s/it] 46%|████▌     | 8205/17834 [4:09:11<4:41:15,  1.75s/it] 46%|████▌     | 8206/17834 [4:09:13<4:37:08,  1.73s/it] 46%|████▌     | 8207/17834 [4:09:15<4:37:01,  1.73s/it] 46%|████▌     | 8208/17834 [4:09:16<4:36:46,  1.73s/it] 46%|████▌     | 8209/17834 [4:09:18<4:36:05,  1.72s/it] 46%|████▌     | 8210/17834 [4:09:20<4:36:16,  1.72s/it] 46%|████▌     | 8211/17834 [4:09:22<4:37:13,  1.73s/it] 46%|████▌     | 8212/17834 [4:09:23<4:36:24,  1.72s/it] 46%|████▌     | 8213/17834 [4:09:25<4:33:28,  1.71s/it] 46%|████▌     | 8214/17834 [4:09:27<4:34:34,  1.71s/it] 46%|████▌     | 8215/17834 [4:09:28<4:35:49,  1.72s/it] 46%|████▌     | 8216/17834 [4:09:30<4:39:39,  1.74s/it] 46%|████▌     | 8217/17834 [4:09:32<4:41:16,  1.75s/it] 46%|████▌     | 8218/17834 [4:09:34<4:41:39,  1.76s/it] 46%|████▌     | 8219/17834 [4:09:35<4:39:37,  1.74s/it] 46%|████▌     | 8220/17834 [4:09:37<4:42:01,  1.76s/it] 46%|████▌     | 8221/17834 [4:09:39<4:40:25,  1.75s/it] 46%|████▌     | 8222/17834 [4:09:41<4:40:25,  1.75s/it] 46%|████▌     | 8223/17834 [4:09:43<4:40:36,  1.75s/it] 46%|████▌     | 8224/17834 [4:09:44<4:40:41,  1.75s/it] 46%|████▌     | 8225/17834 [4:09:46<4:38:57,  1.74s/it] 46%|████▌     | 8226/17834 [4:09:48<4:40:04,  1.75s/it] 46%|████▌     | 8227/17834 [4:09:49<4:39:23,  1.74s/it] 46%|████▌     | 8228/17834 [4:09:51<4:42:38,  1.77s/it] 46%|████▌     | 8229/17834 [4:09:53<4:43:50,  1.77s/it] 46%|████▌     | 8230/17834 [4:09:55<4:41:38,  1.76s/it] 46%|████▌     | 8231/17834 [4:09:57<4:38:50,  1.74s/it] 46%|████▌     | 8232/17834 [4:09:58<4:40:40,  1.75s/it] 46%|████▌     | 8233/17834 [4:10:00<4:38:13,  1.74s/it] 46%|████▌     | 8234/17834 [4:10:02<4:39:36,  1.75s/it] 46%|████▌     | 8235/17834 [4:10:03<4:37:57,  1.74s/it] 46%|████▌     | 8236/17834 [4:10:05<4:40:21,  1.75s/it] 46%|████▌     | 8237/17834 [4:10:07<4:40:56,  1.76s/it] 46%|████▌     | 8238/17834 [4:10:09<4:37:52,  1.74s/it] 46%|████▌     | 8239/17834 [4:10:10<4:38:18,  1.74s/it] 46%|████▌     | 8240/17834 [4:10:12<4:40:32,  1.75s/it] 46%|████▌     | 8241/17834 [4:10:14<4:41:36,  1.76s/it] 46%|████▌     | 8242/17834 [4:10:16<4:41:57,  1.76s/it] 46%|████▌     | 8243/17834 [4:10:18<4:38:47,  1.74s/it] 46%|████▌     | 8244/17834 [4:10:19<4:46:33,  1.79s/it] 46%|████▌     | 8245/17834 [4:10:21<4:45:42,  1.79s/it] 46%|████▌     | 8246/17834 [4:10:23<4:41:27,  1.76s/it] 46%|████▌     | 8247/17834 [4:10:25<4:39:47,  1.75s/it] 46%|████▌     | 8248/17834 [4:10:26<4:39:48,  1.75s/it] 46%|████▋     | 8249/17834 [4:10:28<4:36:48,  1.73s/it]08/30/2024 23:24:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3046090602874756, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03556015342473984, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2708330154418945, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.611002206802368}
 46%|████▋     | 8250/17834 [4:10:30<4:37:44,  1.74s/it] 46%|████▋     | 8251/17834 [4:10:32<4:35:05,  1.72s/it] 46%|████▋     | 8252/17834 [4:10:33<4:38:24,  1.74s/it] 46%|████▋     | 8253/17834 [4:10:35<4:38:55,  1.75s/it] 46%|████▋     | 8254/17834 [4:10:37<4:40:42,  1.76s/it] 46%|████▋     | 8255/17834 [4:10:39<4:44:55,  1.78s/it] 46%|████▋     | 8256/17834 [4:10:40<4:41:54,  1.77s/it] 46%|████▋     | 8257/17834 [4:10:42<4:39:49,  1.75s/it] 46%|████▋     | 8258/17834 [4:10:44<4:45:42,  1.79s/it] 46%|████▋     | 8259/17834 [4:10:46<4:41:02,  1.76s/it] 46%|████▋     | 8260/17834 [4:10:48<4:44:57,  1.79s/it] 46%|████▋     | 8261/17834 [4:10:49<4:43:48,  1.78s/it] 46%|████▋     | 8262/17834 [4:10:51<4:41:05,  1.76s/it] 46%|████▋     | 8263/17834 [4:10:53<4:38:01,  1.74s/it] 46%|████▋     | 8264/17834 [4:10:55<4:40:37,  1.76s/it] 46%|████▋     | 8265/17834 [4:10:56<4:44:24,  1.78s/it] 46%|████▋     | 8266/17834 [4:10:58<4:40:55,  1.76s/it] 46%|████▋     | 8267/17834 [4:11:00<4:39:51,  1.76s/it] 46%|████▋     | 8268/17834 [4:11:02<4:38:37,  1.75s/it] 46%|████▋     | 8269/17834 [4:11:03<4:40:41,  1.76s/it] 46%|████▋     | 8270/17834 [4:11:05<4:41:08,  1.76s/it] 46%|████▋     | 8271/17834 [4:11:07<4:39:43,  1.76s/it] 46%|████▋     | 8272/17834 [4:11:09<4:41:38,  1.77s/it] 46%|████▋     | 8273/17834 [4:11:10<4:37:58,  1.74s/it] 46%|████▋     | 8274/17834 [4:11:12<4:40:53,  1.76s/it] 46%|████▋     | 8275/17834 [4:11:14<4:39:21,  1.75s/it] 46%|████▋     | 8276/17834 [4:11:16<4:38:32,  1.75s/it] 46%|████▋     | 8277/17834 [4:11:17<4:40:19,  1.76s/it] 46%|████▋     | 8278/17834 [4:11:19<4:40:11,  1.76s/it] 46%|████▋     | 8279/17834 [4:11:21<4:40:34,  1.76s/it] 46%|████▋     | 8280/17834 [4:11:23<4:38:14,  1.75s/it] 46%|████▋     | 8281/17834 [4:11:24<4:34:48,  1.73s/it] 46%|████▋     | 8282/17834 [4:11:26<4:37:17,  1.74s/it] 46%|████▋     | 8283/17834 [4:11:28<4:42:39,  1.78s/it] 46%|████▋     | 8284/17834 [4:11:30<4:42:24,  1.77s/it] 46%|████▋     | 8285/17834 [4:11:31<4:41:03,  1.77s/it] 46%|████▋     | 8286/17834 [4:11:33<4:40:14,  1.76s/it] 46%|████▋     | 8287/17834 [4:11:35<4:36:59,  1.74s/it] 46%|████▋     | 8288/17834 [4:11:37<4:36:08,  1.74s/it] 46%|████▋     | 8289/17834 [4:11:38<4:35:39,  1.73s/it] 46%|████▋     | 8290/17834 [4:11:40<4:39:00,  1.75s/it] 46%|████▋     | 8291/17834 [4:11:42<4:39:34,  1.76s/it] 46%|████▋     | 8292/17834 [4:11:44<4:35:05,  1.73s/it] 47%|████▋     | 8293/17834 [4:11:45<4:36:08,  1.74s/it] 47%|████▋     | 8294/17834 [4:11:47<4:39:59,  1.76s/it] 47%|████▋     | 8295/17834 [4:11:49<4:38:14,  1.75s/it] 47%|████▋     | 8296/17834 [4:11:51<4:45:50,  1.80s/it] 47%|████▋     | 8297/17834 [4:11:53<4:44:26,  1.79s/it] 47%|████▋     | 8298/17834 [4:11:54<4:42:34,  1.78s/it] 47%|████▋     | 8299/17834 [4:11:56<4:38:03,  1.75s/it]08/30/2024 23:26:15 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1611990928649902, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03124164789915085, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1977038383483887, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3901445865631104}
 47%|████▋     | 8300/17834 [4:11:58<4:39:37,  1.76s/it] 47%|████▋     | 8301/17834 [4:12:00<4:39:31,  1.76s/it] 47%|████▋     | 8302/17834 [4:12:01<4:37:29,  1.75s/it] 47%|████▋     | 8303/17834 [4:12:03<4:37:49,  1.75s/it] 47%|████▋     | 8304/17834 [4:12:05<4:37:40,  1.75s/it] 47%|████▋     | 8305/17834 [4:12:06<4:37:34,  1.75s/it] 47%|████▋     | 8306/17834 [4:12:08<4:38:14,  1.75s/it] 47%|████▋     | 8307/17834 [4:12:10<4:38:13,  1.75s/it] 47%|████▋     | 8308/17834 [4:12:12<4:40:03,  1.76s/it] 47%|████▋     | 8309/17834 [4:12:14<4:40:02,  1.76s/it] 47%|████▋     | 8310/17834 [4:12:15<4:36:46,  1.74s/it] 47%|████▋     | 8311/17834 [4:12:17<4:33:30,  1.72s/it] 47%|████▋     | 8312/17834 [4:12:19<4:33:34,  1.72s/it] 47%|████▋     | 8313/17834 [4:12:20<4:35:50,  1.74s/it] 47%|████▋     | 8314/17834 [4:12:22<4:33:04,  1.72s/it] 47%|████▋     | 8315/17834 [4:12:24<4:33:47,  1.73s/it] 47%|████▋     | 8316/17834 [4:12:26<4:39:22,  1.76s/it] 47%|████▋     | 8317/17834 [4:12:27<4:37:34,  1.75s/it] 47%|████▋     | 8318/17834 [4:12:29<4:36:37,  1.74s/it] 47%|████▋     | 8319/17834 [4:12:31<4:38:49,  1.76s/it] 47%|████▋     | 8320/17834 [4:12:33<4:36:35,  1.74s/it] 47%|████▋     | 8321/17834 [4:12:34<4:35:17,  1.74s/it] 47%|████▋     | 8322/17834 [4:12:36<4:35:04,  1.74s/it] 47%|████▋     | 8323/17834 [4:12:38<4:34:45,  1.73s/it] 47%|████▋     | 8324/17834 [4:12:40<4:37:32,  1.75s/it] 47%|████▋     | 8325/17834 [4:12:41<4:37:13,  1.75s/it] 47%|████▋     | 8326/17834 [4:12:43<4:36:13,  1.74s/it] 47%|████▋     | 8327/17834 [4:12:45<4:37:38,  1.75s/it] 47%|████▋     | 8328/17834 [4:12:47<4:35:56,  1.74s/it] 47%|████▋     | 8329/17834 [4:12:48<4:37:26,  1.75s/it] 47%|████▋     | 8330/17834 [4:12:50<4:36:09,  1.74s/it] 47%|████▋     | 8331/17834 [4:12:52<4:36:09,  1.74s/it] 47%|████▋     | 8332/17834 [4:12:54<4:40:19,  1.77s/it] 47%|████▋     | 8333/17834 [4:12:55<4:39:49,  1.77s/it] 47%|████▋     | 8334/17834 [4:12:57<4:42:33,  1.78s/it] 47%|████▋     | 8335/17834 [4:12:59<4:39:22,  1.76s/it] 47%|████▋     | 8336/17834 [4:13:01<4:37:09,  1.75s/it] 47%|████▋     | 8337/17834 [4:13:02<4:38:48,  1.76s/it] 47%|████▋     | 8338/17834 [4:13:04<4:40:07,  1.77s/it] 47%|████▋     | 8339/17834 [4:13:06<4:48:10,  1.82s/it] 47%|████▋     | 8340/17834 [4:13:08<4:40:16,  1.77s/it] 47%|████▋     | 8341/17834 [4:13:10<4:41:53,  1.78s/it] 47%|████▋     | 8342/17834 [4:13:12<4:46:05,  1.81s/it] 47%|████▋     | 8343/17834 [4:13:13<4:42:17,  1.78s/it] 47%|████▋     | 8344/17834 [4:13:15<4:44:35,  1.80s/it] 47%|████▋     | 8345/17834 [4:13:17<4:44:58,  1.80s/it] 47%|████▋     | 8346/17834 [4:13:19<4:40:00,  1.77s/it] 47%|████▋     | 8347/17834 [4:13:20<4:41:23,  1.78s/it] 47%|████▋     | 8348/17834 [4:13:22<4:39:29,  1.77s/it] 47%|████▋     | 8349/17834 [4:13:24<4:38:11,  1.76s/it]08/30/2024 23:27:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1273829936981201, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029436152428388596, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1782681941986084, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.335087299346924}
 47%|████▋     | 8350/17834 [4:13:26<4:39:58,  1.77s/it] 47%|████▋     | 8351/17834 [4:13:27<4:38:19,  1.76s/it] 47%|████▋     | 8352/17834 [4:13:29<4:35:28,  1.74s/it] 47%|████▋     | 8353/17834 [4:13:31<4:37:00,  1.75s/it] 47%|████▋     | 8354/17834 [4:13:33<4:37:54,  1.76s/it] 47%|████▋     | 8355/17834 [4:13:34<4:38:25,  1.76s/it] 47%|████▋     | 8356/17834 [4:13:36<4:35:27,  1.74s/it] 47%|████▋     | 8357/17834 [4:13:38<4:36:20,  1.75s/it] 47%|████▋     | 8358/17834 [4:13:40<4:37:18,  1.76s/it] 47%|████▋     | 8359/17834 [4:13:41<4:35:39,  1.75s/it] 47%|████▋     | 8360/17834 [4:13:43<4:34:37,  1.74s/it] 47%|████▋     | 8361/17834 [4:13:45<4:33:23,  1.73s/it] 47%|████▋     | 8362/17834 [4:13:47<4:32:06,  1.72s/it] 47%|████▋     | 8363/17834 [4:13:48<4:34:30,  1.74s/it] 47%|████▋     | 8364/17834 [4:13:50<4:31:45,  1.72s/it] 47%|████▋     | 8365/17834 [4:13:52<4:33:16,  1.73s/it] 47%|████▋     | 8366/17834 [4:13:54<4:35:23,  1.75s/it] 47%|████▋     | 8367/17834 [4:13:55<4:35:32,  1.75s/it] 47%|████▋     | 8368/17834 [4:13:57<4:35:41,  1.75s/it] 47%|████▋     | 8369/17834 [4:13:59<4:33:52,  1.74s/it] 47%|████▋     | 8370/17834 [4:14:01<4:37:25,  1.76s/it] 47%|████▋     | 8371/17834 [4:14:02<4:37:15,  1.76s/it] 47%|████▋     | 8372/17834 [4:14:04<4:37:20,  1.76s/it] 47%|████▋     | 8373/17834 [4:14:06<4:36:07,  1.75s/it] 47%|████▋     | 8374/17834 [4:14:07<4:32:41,  1.73s/it] 47%|████▋     | 8375/17834 [4:14:09<4:34:18,  1.74s/it] 47%|████▋     | 8376/17834 [4:14:11<4:37:28,  1.76s/it] 47%|████▋     | 8377/17834 [4:14:13<4:44:37,  1.81s/it] 47%|████▋     | 8378/17834 [4:14:15<4:38:39,  1.77s/it] 47%|████▋     | 8379/17834 [4:14:16<4:35:37,  1.75s/it] 47%|████▋     | 8380/17834 [4:14:18<4:33:28,  1.74s/it] 47%|████▋     | 8381/17834 [4:14:20<4:34:42,  1.74s/it] 47%|████▋     | 8382/17834 [4:14:22<4:38:47,  1.77s/it] 47%|████▋     | 8383/17834 [4:14:23<4:38:13,  1.77s/it] 47%|████▋     | 8384/17834 [4:14:25<4:42:28,  1.79s/it] 47%|████▋     | 8385/17834 [4:14:27<4:36:46,  1.76s/it] 47%|████▋     | 8386/17834 [4:14:29<4:36:37,  1.76s/it] 47%|████▋     | 8387/17834 [4:14:30<4:37:21,  1.76s/it] 47%|████▋     | 8388/17834 [4:14:32<4:36:52,  1.76s/it] 47%|████▋     | 8389/17834 [4:14:34<4:37:22,  1.76s/it] 47%|████▋     | 8390/17834 [4:14:36<4:39:29,  1.78s/it] 47%|████▋     | 8391/17834 [4:14:38<4:36:27,  1.76s/it] 47%|████▋     | 8392/17834 [4:14:39<4:34:35,  1.74s/it] 47%|████▋     | 8393/17834 [4:14:41<4:36:36,  1.76s/it] 47%|████▋     | 8394/17834 [4:14:43<4:40:57,  1.79s/it] 47%|████▋     | 8395/17834 [4:14:45<4:38:49,  1.77s/it] 47%|████▋     | 8396/17834 [4:14:46<4:37:39,  1.77s/it] 47%|████▋     | 8397/17834 [4:14:48<4:35:03,  1.75s/it] 47%|████▋     | 8398/17834 [4:14:50<4:35:43,  1.75s/it] 47%|████▋     | 8399/17834 [4:14:52<4:35:24,  1.75s/it]08/30/2024 23:29:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1323249340057373, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032908570021390915, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.214433193206787, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.379666805267334}
 47%|████▋     | 8400/17834 [4:14:53<4:37:16,  1.76s/it] 47%|████▋     | 8401/17834 [4:14:55<4:32:04,  1.73s/it] 47%|████▋     | 8402/17834 [4:14:57<4:35:02,  1.75s/it] 47%|████▋     | 8403/17834 [4:14:59<4:35:08,  1.75s/it] 47%|████▋     | 8404/17834 [4:15:00<4:38:22,  1.77s/it] 47%|████▋     | 8405/17834 [4:15:02<4:36:34,  1.76s/it] 47%|████▋     | 8406/17834 [4:15:04<4:34:47,  1.75s/it] 47%|████▋     | 8407/17834 [4:15:06<4:30:58,  1.72s/it] 47%|████▋     | 8408/17834 [4:15:07<4:33:26,  1.74s/it] 47%|████▋     | 8409/17834 [4:15:09<4:36:45,  1.76s/it] 47%|████▋     | 8410/17834 [4:15:11<4:36:54,  1.76s/it] 47%|████▋     | 8411/17834 [4:15:13<4:39:15,  1.78s/it] 47%|████▋     | 8412/17834 [4:15:14<4:38:30,  1.77s/it] 47%|████▋     | 8413/17834 [4:15:16<4:36:24,  1.76s/it] 47%|████▋     | 8414/17834 [4:15:18<4:37:31,  1.77s/it] 47%|████▋     | 8415/17834 [4:15:20<4:37:39,  1.77s/it] 47%|████▋     | 8416/17834 [4:15:22<4:39:40,  1.78s/it] 47%|████▋     | 8417/17834 [4:15:23<4:37:03,  1.77s/it] 47%|████▋     | 8418/17834 [4:15:25<4:37:43,  1.77s/it] 47%|████▋     | 8419/17834 [4:15:27<4:35:54,  1.76s/it] 47%|████▋     | 8420/17834 [4:15:29<4:37:59,  1.77s/it] 47%|████▋     | 8421/17834 [4:15:30<4:35:45,  1.76s/it] 47%|████▋     | 8422/17834 [4:15:32<4:35:07,  1.75s/it] 47%|████▋     | 8423/17834 [4:15:34<4:33:49,  1.75s/it] 47%|████▋     | 8424/17834 [4:15:36<4:33:40,  1.74s/it] 47%|████▋     | 8425/17834 [4:15:37<4:34:33,  1.75s/it] 47%|████▋     | 8426/17834 [4:15:39<4:34:11,  1.75s/it] 47%|████▋     | 8427/17834 [4:15:41<4:35:51,  1.76s/it] 47%|████▋     | 8428/17834 [4:15:43<4:38:45,  1.78s/it] 47%|████▋     | 8429/17834 [4:15:44<4:34:45,  1.75s/it] 47%|████▋     | 8430/17834 [4:15:46<4:37:53,  1.77s/it] 47%|████▋     | 8431/17834 [4:15:48<4:38:41,  1.78s/it] 47%|████▋     | 8432/17834 [4:15:50<4:35:17,  1.76s/it] 47%|████▋     | 8433/17834 [4:15:51<4:33:23,  1.74s/it] 47%|████▋     | 8434/17834 [4:15:53<4:32:22,  1.74s/it] 47%|████▋     | 8435/17834 [4:15:55<4:31:23,  1.73s/it] 47%|████▋     | 8436/17834 [4:15:57<4:36:42,  1.77s/it] 47%|████▋     | 8437/17834 [4:15:58<4:38:23,  1.78s/it] 47%|████▋     | 8438/17834 [4:16:00<4:35:52,  1.76s/it] 47%|████▋     | 8439/17834 [4:16:02<4:35:21,  1.76s/it] 47%|████▋     | 8440/17834 [4:16:04<4:35:18,  1.76s/it] 47%|████▋     | 8441/17834 [4:16:05<4:32:35,  1.74s/it] 47%|████▋     | 8442/17834 [4:16:07<4:34:14,  1.75s/it] 47%|████▋     | 8443/17834 [4:16:09<4:44:43,  1.82s/it] 47%|████▋     | 8444/17834 [4:16:11<4:38:03,  1.78s/it] 47%|████▋     | 8445/17834 [4:16:13<4:33:51,  1.75s/it] 47%|████▋     | 8446/17834 [4:16:14<4:33:21,  1.75s/it] 47%|████▋     | 8447/17834 [4:16:16<4:31:22,  1.73s/it] 47%|████▋     | 8448/17834 [4:16:18<4:32:14,  1.74s/it] 47%|████▋     | 8449/17834 [4:16:19<4:34:49,  1.76s/it]08/30/2024 23:30:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3594236373901367, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03813256323337555, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.230203151702881, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6277594566345215}
 47%|████▋     | 8450/17834 [4:16:21<4:34:25,  1.75s/it] 47%|████▋     | 8451/17834 [4:16:23<4:33:37,  1.75s/it] 47%|████▋     | 8452/17834 [4:16:25<4:33:09,  1.75s/it] 47%|████▋     | 8453/17834 [4:16:26<4:30:23,  1.73s/it] 47%|████▋     | 8454/17834 [4:16:28<4:28:52,  1.72s/it] 47%|████▋     | 8455/17834 [4:16:30<4:37:03,  1.77s/it] 47%|████▋     | 8456/17834 [4:16:32<4:34:26,  1.76s/it] 47%|████▋     | 8457/17834 [4:16:33<4:33:21,  1.75s/it] 47%|████▋     | 8458/17834 [4:16:35<4:31:39,  1.74s/it] 47%|████▋     | 8459/17834 [4:16:37<4:34:26,  1.76s/it] 47%|████▋     | 8460/17834 [4:16:39<4:33:25,  1.75s/it] 47%|████▋     | 8461/17834 [4:16:41<4:36:16,  1.77s/it] 47%|████▋     | 8462/17834 [4:16:42<4:32:34,  1.75s/it] 47%|████▋     | 8463/17834 [4:16:44<4:35:28,  1.76s/it] 47%|████▋     | 8464/17834 [4:16:46<4:33:34,  1.75s/it] 47%|████▋     | 8465/17834 [4:16:47<4:33:03,  1.75s/it] 47%|████▋     | 8466/17834 [4:16:49<4:31:58,  1.74s/it] 47%|████▋     | 8467/17834 [4:16:51<4:29:35,  1.73s/it] 47%|████▋     | 8468/17834 [4:16:53<4:36:29,  1.77s/it] 47%|████▋     | 8469/17834 [4:16:55<4:34:41,  1.76s/it] 47%|████▋     | 8470/17834 [4:16:56<4:34:25,  1.76s/it] 47%|████▋     | 8471/17834 [4:16:58<4:35:51,  1.77s/it] 48%|████▊     | 8472/17834 [4:17:00<4:36:39,  1.77s/it] 48%|████▊     | 8473/17834 [4:17:02<4:35:02,  1.76s/it] 48%|████▊     | 8474/17834 [4:17:03<4:33:36,  1.75s/it] 48%|████▊     | 8475/17834 [4:17:05<4:34:51,  1.76s/it] 48%|████▊     | 8476/17834 [4:17:07<4:37:19,  1.78s/it] 48%|████▊     | 8477/17834 [4:17:09<4:33:54,  1.76s/it] 48%|████▊     | 8478/17834 [4:17:10<4:39:06,  1.79s/it] 48%|████▊     | 8479/17834 [4:17:12<4:38:33,  1.79s/it] 48%|████▊     | 8480/17834 [4:17:14<4:36:06,  1.77s/it] 48%|████▊     | 8481/17834 [4:17:16<4:34:55,  1.76s/it] 48%|████▊     | 8482/17834 [4:17:17<4:33:51,  1.76s/it] 48%|████▊     | 8483/17834 [4:17:19<4:33:14,  1.75s/it] 48%|████▊     | 8484/17834 [4:17:21<4:33:49,  1.76s/it] 48%|████▊     | 8485/17834 [4:17:23<4:34:27,  1.76s/it] 48%|████▊     | 8486/17834 [4:17:25<4:33:40,  1.76s/it] 48%|████▊     | 8487/17834 [4:17:26<4:32:54,  1.75s/it] 48%|████▊     | 8488/17834 [4:17:28<4:29:25,  1.73s/it] 48%|████▊     | 8489/17834 [4:17:30<4:30:01,  1.73s/it] 48%|████▊     | 8490/17834 [4:17:31<4:29:01,  1.73s/it] 48%|████▊     | 8491/17834 [4:17:33<4:32:13,  1.75s/it] 48%|████▊     | 8492/17834 [4:17:35<4:37:05,  1.78s/it] 48%|████▊     | 8493/17834 [4:17:37<4:33:31,  1.76s/it] 48%|████▊     | 8494/17834 [4:17:38<4:30:57,  1.74s/it] 48%|████▊     | 8495/17834 [4:17:40<4:31:30,  1.74s/it] 48%|████▊     | 8496/17834 [4:17:42<4:31:56,  1.75s/it] 48%|████▊     | 8497/17834 [4:17:44<4:34:09,  1.76s/it] 48%|████▊     | 8498/17834 [4:17:46<4:37:27,  1.78s/it] 48%|████▊     | 8499/17834 [4:17:47<4:32:36,  1.75s/it]08/30/2024 23:32:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9663724899291992, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0231175534427166, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2388103008270264, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2283003330230713}
 48%|████▊     | 8500/17834 [4:17:49<4:32:09,  1.75s/it] 48%|████▊     | 8501/17834 [4:17:51<4:31:47,  1.75s/it] 48%|████▊     | 8502/17834 [4:17:53<4:37:49,  1.79s/it] 48%|████▊     | 8503/17834 [4:17:54<4:34:05,  1.76s/it] 48%|████▊     | 8504/17834 [4:17:56<4:34:38,  1.77s/it] 48%|████▊     | 8505/17834 [4:17:58<4:31:29,  1.75s/it] 48%|████▊     | 8506/17834 [4:18:00<4:31:59,  1.75s/it] 48%|████▊     | 8507/17834 [4:18:01<4:30:53,  1.74s/it] 48%|████▊     | 8508/17834 [4:18:03<4:32:16,  1.75s/it] 48%|████▊     | 8509/17834 [4:18:05<4:31:05,  1.74s/it] 48%|████▊     | 8510/17834 [4:18:06<4:29:20,  1.73s/it] 48%|████▊     | 8511/17834 [4:18:08<4:30:38,  1.74s/it] 48%|████▊     | 8512/17834 [4:18:10<4:34:45,  1.77s/it] 48%|████▊     | 8513/17834 [4:18:12<4:31:44,  1.75s/it] 48%|████▊     | 8514/17834 [4:18:14<4:32:27,  1.75s/it] 48%|████▊     | 8515/17834 [4:18:15<4:34:43,  1.77s/it] 48%|████▊     | 8516/17834 [4:18:17<4:32:15,  1.75s/it] 48%|████▊     | 8517/17834 [4:18:19<4:33:22,  1.76s/it] 48%|████▊     | 8518/17834 [4:18:21<4:31:11,  1.75s/it] 48%|████▊     | 8519/17834 [4:18:22<4:31:22,  1.75s/it] 48%|████▊     | 8520/17834 [4:18:24<4:29:37,  1.74s/it] 48%|████▊     | 8521/17834 [4:18:26<4:30:46,  1.74s/it] 48%|████▊     | 8522/17834 [4:18:27<4:28:38,  1.73s/it] 48%|████▊     | 8523/17834 [4:18:29<4:26:51,  1.72s/it] 48%|████▊     | 8524/17834 [4:18:31<4:26:10,  1.72s/it] 48%|████▊     | 8525/17834 [4:18:33<4:26:49,  1.72s/it] 48%|████▊     | 8526/17834 [4:18:34<4:30:44,  1.75s/it] 48%|████▊     | 8527/17834 [4:18:36<4:33:16,  1.76s/it] 48%|████▊     | 8528/17834 [4:18:38<4:28:31,  1.73s/it] 48%|████▊     | 8529/17834 [4:18:40<4:33:05,  1.76s/it] 48%|████▊     | 8530/17834 [4:18:41<4:32:50,  1.76s/it] 48%|████▊     | 8531/17834 [4:18:43<4:27:25,  1.72s/it] 48%|████▊     | 8532/17834 [4:18:45<4:27:42,  1.73s/it] 48%|████▊     | 8533/17834 [4:18:47<4:28:04,  1.73s/it] 48%|████▊     | 8534/17834 [4:18:48<4:31:47,  1.75s/it] 48%|████▊     | 8535/17834 [4:18:50<4:27:39,  1.73s/it] 48%|████▊     | 8536/17834 [4:18:52<4:28:00,  1.73s/it] 48%|████▊     | 8537/17834 [4:18:54<4:32:42,  1.76s/it] 48%|████▊     | 8538/17834 [4:18:55<4:28:57,  1.74s/it] 48%|████▊     | 8539/17834 [4:18:57<4:26:36,  1.72s/it] 48%|████▊     | 8540/17834 [4:18:59<4:28:49,  1.74s/it] 48%|████▊     | 8541/17834 [4:19:01<4:30:13,  1.74s/it] 48%|████▊     | 8542/17834 [4:19:02<4:30:26,  1.75s/it] 48%|████▊     | 8543/17834 [4:19:04<4:34:45,  1.77s/it] 48%|████▊     | 8544/17834 [4:19:06<4:28:50,  1.74s/it] 48%|████▊     | 8545/17834 [4:19:07<4:28:06,  1.73s/it] 48%|████▊     | 8546/17834 [4:19:09<4:35:03,  1.78s/it] 48%|████▊     | 8547/17834 [4:19:11<4:36:39,  1.79s/it] 48%|████▊     | 8548/17834 [4:19:13<4:32:28,  1.76s/it] 48%|████▊     | 8549/17834 [4:19:15<4:33:12,  1.77s/it]08/30/2024 23:33:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3707973957061768, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04298946261405945, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.276080846786499, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6898677349090576}
 48%|████▊     | 8550/17834 [4:19:16<4:32:11,  1.76s/it] 48%|████▊     | 8551/17834 [4:19:18<4:32:04,  1.76s/it] 48%|████▊     | 8552/17834 [4:19:20<4:29:08,  1.74s/it] 48%|████▊     | 8553/17834 [4:19:22<4:32:24,  1.76s/it] 48%|████▊     | 8554/17834 [4:19:23<4:30:21,  1.75s/it] 48%|████▊     | 8555/17834 [4:19:25<4:32:35,  1.76s/it] 48%|████▊     | 8556/17834 [4:19:27<4:30:25,  1.75s/it] 48%|████▊     | 8557/17834 [4:19:29<4:29:52,  1.75s/it] 48%|████▊     | 8558/17834 [4:19:30<4:27:34,  1.73s/it] 48%|████▊     | 8559/17834 [4:19:32<4:26:51,  1.73s/it] 48%|████▊     | 8560/17834 [4:19:34<4:28:38,  1.74s/it] 48%|████▊     | 8561/17834 [4:19:36<4:27:24,  1.73s/it] 48%|████▊     | 8562/17834 [4:19:37<4:28:32,  1.74s/it] 48%|████▊     | 8563/17834 [4:19:39<4:26:21,  1.72s/it] 48%|████▊     | 8564/17834 [4:19:41<4:26:20,  1.72s/it] 48%|████▊     | 8565/17834 [4:19:42<4:29:09,  1.74s/it] 48%|████▊     | 8566/17834 [4:19:44<4:30:57,  1.75s/it] 48%|████▊     | 8567/17834 [4:19:46<4:26:57,  1.73s/it] 48%|████▊     | 8568/17834 [4:19:48<4:27:35,  1.73s/it] 48%|████▊     | 8569/17834 [4:19:49<4:26:45,  1.73s/it] 48%|████▊     | 8570/17834 [4:19:51<4:25:06,  1.72s/it] 48%|████▊     | 8571/17834 [4:19:53<4:28:29,  1.74s/it] 48%|████▊     | 8572/17834 [4:19:55<4:29:33,  1.75s/it] 48%|████▊     | 8573/17834 [4:19:56<4:27:50,  1.74s/it] 48%|████▊     | 8574/17834 [4:19:58<4:25:11,  1.72s/it] 48%|████▊     | 8575/17834 [4:20:00<4:25:17,  1.72s/it] 48%|████▊     | 8576/17834 [4:20:01<4:24:15,  1.71s/it] 48%|████▊     | 8577/17834 [4:20:03<4:27:36,  1.73s/it] 48%|████▊     | 8578/17834 [4:20:05<4:27:09,  1.73s/it] 48%|████▊     | 8579/17834 [4:20:07<4:28:27,  1.74s/it] 48%|████▊     | 8580/17834 [4:20:09<4:31:34,  1.76s/it] 48%|████▊     | 8581/17834 [4:20:10<4:35:14,  1.78s/it] 48%|████▊     | 8582/17834 [4:20:12<4:31:03,  1.76s/it] 48%|████▊     | 8583/17834 [4:20:14<4:30:03,  1.75s/it] 48%|████▊     | 8584/17834 [4:20:16<4:28:53,  1.74s/it] 48%|████▊     | 8585/17834 [4:20:17<4:30:29,  1.75s/it] 48%|████▊     | 8586/17834 [4:20:19<4:28:48,  1.74s/it] 48%|████▊     | 8587/17834 [4:20:21<4:27:58,  1.74s/it] 48%|████▊     | 8588/17834 [4:20:22<4:27:31,  1.74s/it] 48%|████▊     | 8589/17834 [4:20:24<4:26:27,  1.73s/it] 48%|████▊     | 8590/17834 [4:20:26<4:25:04,  1.72s/it] 48%|████▊     | 8591/17834 [4:20:28<4:27:34,  1.74s/it] 48%|████▊     | 8592/17834 [4:20:29<4:27:05,  1.73s/it] 48%|████▊     | 8593/17834 [4:20:31<4:30:11,  1.75s/it] 48%|████▊     | 8594/17834 [4:20:33<4:26:14,  1.73s/it] 48%|████▊     | 8595/17834 [4:20:35<4:28:27,  1.74s/it] 48%|████▊     | 8596/17834 [4:20:36<4:31:07,  1.76s/it] 48%|████▊     | 8597/17834 [4:20:38<4:30:38,  1.76s/it] 48%|████▊     | 8598/17834 [4:20:40<4:27:09,  1.74s/it] 48%|████▊     | 8599/17834 [4:20:42<4:28:56,  1.75s/it]08/30/2024 23:35:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3440113067626953, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032794155180454254, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3767800331115723, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7535853385925293}
 48%|████▊     | 8600/17834 [4:20:43<4:29:08,  1.75s/it] 48%|████▊     | 8601/17834 [4:20:45<4:26:00,  1.73s/it] 48%|████▊     | 8602/17834 [4:20:47<4:25:14,  1.72s/it] 48%|████▊     | 8603/17834 [4:20:49<4:25:24,  1.73s/it] 48%|████▊     | 8604/17834 [4:20:50<4:22:05,  1.70s/it] 48%|████▊     | 8605/17834 [4:20:52<4:26:39,  1.73s/it] 48%|████▊     | 8606/17834 [4:20:54<4:26:22,  1.73s/it] 48%|████▊     | 8607/17834 [4:20:55<4:25:41,  1.73s/it] 48%|████▊     | 8608/17834 [4:20:57<4:26:28,  1.73s/it] 48%|████▊     | 8609/17834 [4:20:59<4:27:05,  1.74s/it] 48%|████▊     | 8610/17834 [4:21:01<4:26:57,  1.74s/it] 48%|████▊     | 8611/17834 [4:21:02<4:27:43,  1.74s/it] 48%|████▊     | 8612/17834 [4:21:04<4:27:39,  1.74s/it] 48%|████▊     | 8613/17834 [4:21:06<4:26:32,  1.73s/it] 48%|████▊     | 8614/17834 [4:21:08<4:33:06,  1.78s/it] 48%|████▊     | 8615/17834 [4:21:09<4:30:41,  1.76s/it] 48%|████▊     | 8616/17834 [4:21:11<4:27:57,  1.74s/it] 48%|████▊     | 8617/17834 [4:21:13<4:27:43,  1.74s/it] 48%|████▊     | 8618/17834 [4:21:15<4:24:36,  1.72s/it] 48%|████▊     | 8619/17834 [4:21:16<4:31:25,  1.77s/it] 48%|████▊     | 8620/17834 [4:21:18<4:29:39,  1.76s/it] 48%|████▊     | 8621/17834 [4:21:20<4:29:32,  1.76s/it] 48%|████▊     | 8622/17834 [4:21:22<4:33:12,  1.78s/it] 48%|████▊     | 8623/17834 [4:21:23<4:29:27,  1.76s/it] 48%|████▊     | 8624/17834 [4:21:25<4:27:51,  1.74s/it] 48%|████▊     | 8625/17834 [4:21:27<4:28:22,  1.75s/it] 48%|████▊     | 8626/17834 [4:21:29<4:28:57,  1.75s/it] 48%|████▊     | 8627/17834 [4:21:30<4:29:06,  1.75s/it] 48%|████▊     | 8628/17834 [4:21:32<4:27:44,  1.75s/it] 48%|████▊     | 8629/17834 [4:21:34<4:25:59,  1.73s/it] 48%|████▊     | 8630/17834 [4:21:36<4:23:32,  1.72s/it] 48%|████▊     | 8631/17834 [4:21:37<4:22:07,  1.71s/it] 48%|████▊     | 8632/17834 [4:21:39<4:23:59,  1.72s/it] 48%|████▊     | 8633/17834 [4:21:41<4:24:28,  1.72s/it] 48%|████▊     | 8634/17834 [4:21:42<4:25:21,  1.73s/it] 48%|████▊     | 8635/17834 [4:21:44<4:27:21,  1.74s/it] 48%|████▊     | 8636/17834 [4:21:46<4:29:57,  1.76s/it] 48%|████▊     | 8637/17834 [4:21:48<4:28:50,  1.75s/it] 48%|████▊     | 8638/17834 [4:21:50<4:28:07,  1.75s/it] 48%|████▊     | 8639/17834 [4:21:51<4:34:01,  1.79s/it] 48%|████▊     | 8640/17834 [4:21:53<4:32:20,  1.78s/it] 48%|████▊     | 8641/17834 [4:21:55<4:28:29,  1.75s/it] 48%|████▊     | 8642/17834 [4:21:57<4:31:32,  1.77s/it] 48%|████▊     | 8643/17834 [4:21:58<4:29:59,  1.76s/it] 48%|████▊     | 8644/17834 [4:22:00<4:29:28,  1.76s/it] 48%|████▊     | 8645/17834 [4:22:02<4:31:18,  1.77s/it] 48%|████▊     | 8646/17834 [4:22:04<4:28:38,  1.75s/it] 48%|████▊     | 8647/17834 [4:22:05<4:29:15,  1.76s/it] 48%|████▊     | 8648/17834 [4:22:07<4:25:37,  1.74s/it] 48%|████▊     | 8649/17834 [4:22:09<4:25:41,  1.74s/it]08/30/2024 23:36:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6060826778411865, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03881238028407097, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.362457036972046, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.007351875305176}
 49%|████▊     | 8650/17834 [4:22:11<4:25:06,  1.73s/it] 49%|████▊     | 8651/17834 [4:22:12<4:25:55,  1.74s/it] 49%|████▊     | 8652/17834 [4:22:14<4:24:48,  1.73s/it] 49%|████▊     | 8653/17834 [4:22:16<4:21:31,  1.71s/it] 49%|████▊     | 8654/17834 [4:22:18<4:27:10,  1.75s/it] 49%|████▊     | 8655/17834 [4:22:19<4:24:45,  1.73s/it] 49%|████▊     | 8656/17834 [4:22:21<4:22:30,  1.72s/it] 49%|████▊     | 8657/17834 [4:22:23<4:27:50,  1.75s/it] 49%|████▊     | 8658/17834 [4:22:24<4:26:34,  1.74s/it] 49%|████▊     | 8659/17834 [4:22:26<4:27:59,  1.75s/it] 49%|████▊     | 8660/17834 [4:22:28<4:31:05,  1.77s/it] 49%|████▊     | 8661/17834 [4:22:30<4:28:07,  1.75s/it] 49%|████▊     | 8662/17834 [4:22:32<4:30:11,  1.77s/it] 49%|████▊     | 8663/17834 [4:22:33<4:27:39,  1.75s/it] 49%|████▊     | 8664/17834 [4:22:35<4:25:52,  1.74s/it] 49%|████▊     | 8665/17834 [4:22:37<4:24:48,  1.73s/it] 49%|████▊     | 8666/17834 [4:22:39<4:28:29,  1.76s/it] 49%|████▊     | 8667/17834 [4:22:40<4:26:23,  1.74s/it] 49%|████▊     | 8668/17834 [4:22:42<4:25:39,  1.74s/it] 49%|████▊     | 8669/17834 [4:22:44<4:24:24,  1.73s/it] 49%|████▊     | 8670/17834 [4:22:45<4:23:19,  1.72s/it] 49%|████▊     | 8671/17834 [4:22:47<4:23:46,  1.73s/it] 49%|████▊     | 8672/17834 [4:22:49<4:24:57,  1.74s/it] 49%|████▊     | 8673/17834 [4:22:51<4:24:34,  1.73s/it] 49%|████▊     | 8674/17834 [4:22:52<4:24:06,  1.73s/it] 49%|████▊     | 8675/17834 [4:22:54<4:21:30,  1.71s/it] 49%|████▊     | 8676/17834 [4:22:56<4:20:20,  1.71s/it] 49%|████▊     | 8677/17834 [4:22:57<4:22:22,  1.72s/it] 49%|████▊     | 8678/17834 [4:22:59<4:28:39,  1.76s/it] 49%|████▊     | 8679/17834 [4:23:01<4:28:49,  1.76s/it] 49%|████▊     | 8680/17834 [4:23:03<4:29:25,  1.77s/it] 49%|████▊     | 8681/17834 [4:23:05<4:27:28,  1.75s/it] 49%|████▊     | 8682/17834 [4:23:06<4:30:08,  1.77s/it] 49%|████▊     | 8683/17834 [4:23:08<4:29:01,  1.76s/it] 49%|████▊     | 8684/17834 [4:23:10<4:24:29,  1.73s/it] 49%|████▊     | 8685/17834 [4:23:12<4:23:42,  1.73s/it] 49%|████▊     | 8686/17834 [4:23:13<4:24:08,  1.73s/it] 49%|████▊     | 8687/17834 [4:23:15<4:24:03,  1.73s/it] 49%|████▊     | 8688/17834 [4:23:17<4:26:12,  1.75s/it] 49%|████▊     | 8689/17834 [4:23:19<4:26:30,  1.75s/it] 49%|████▊     | 8690/17834 [4:23:20<4:23:50,  1.73s/it] 49%|████▊     | 8691/17834 [4:23:22<4:26:09,  1.75s/it] 49%|████▊     | 8692/17834 [4:23:24<4:26:56,  1.75s/it] 49%|████▊     | 8693/17834 [4:23:26<4:28:26,  1.76s/it] 49%|████▊     | 8694/17834 [4:23:27<4:25:45,  1.74s/it] 49%|████▉     | 8695/17834 [4:23:29<4:27:04,  1.75s/it] 49%|████▉     | 8696/17834 [4:23:31<4:24:30,  1.74s/it] 49%|████▉     | 8697/17834 [4:23:32<4:23:10,  1.73s/it] 49%|████▉     | 8698/17834 [4:23:34<4:21:16,  1.72s/it] 49%|████▉     | 8699/17834 [4:23:36<4:21:56,  1.72s/it]08/30/2024 23:37:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3869857788085938, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0333625003695488, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.310976505279541, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7313246726989746}
 49%|████▉     | 8700/17834 [4:23:38<4:25:26,  1.74s/it] 49%|████▉     | 8701/17834 [4:23:39<4:22:20,  1.72s/it] 49%|████▉     | 8702/17834 [4:23:41<4:21:07,  1.72s/it] 49%|████▉     | 8703/17834 [4:23:43<4:23:38,  1.73s/it] 49%|████▉     | 8704/17834 [4:23:45<4:22:54,  1.73s/it] 49%|████▉     | 8705/17834 [4:23:46<4:26:38,  1.75s/it] 49%|████▉     | 8706/17834 [4:23:48<4:23:11,  1.73s/it] 49%|████▉     | 8707/17834 [4:23:50<4:26:06,  1.75s/it] 49%|████▉     | 8708/17834 [4:23:52<4:25:19,  1.74s/it] 49%|████▉     | 8709/17834 [4:23:53<4:27:21,  1.76s/it] 49%|████▉     | 8710/17834 [4:23:55<4:27:42,  1.76s/it] 49%|████▉     | 8711/17834 [4:23:57<4:26:56,  1.76s/it] 49%|████▉     | 8712/17834 [4:23:59<4:27:54,  1.76s/it] 49%|████▉     | 8713/17834 [4:24:00<4:27:39,  1.76s/it] 49%|████▉     | 8714/17834 [4:24:02<4:27:02,  1.76s/it] 49%|████▉     | 8715/17834 [4:24:04<4:28:53,  1.77s/it] 49%|████▉     | 8716/17834 [4:24:06<4:23:29,  1.73s/it] 49%|████▉     | 8717/17834 [4:24:07<4:24:39,  1.74s/it] 49%|████▉     | 8718/17834 [4:24:09<4:26:30,  1.75s/it] 49%|████▉     | 8719/17834 [4:24:11<4:26:53,  1.76s/it] 49%|████▉     | 8720/17834 [4:24:13<4:27:23,  1.76s/it] 49%|████▉     | 8721/17834 [4:24:14<4:29:12,  1.77s/it] 49%|████▉     | 8722/17834 [4:24:16<4:26:24,  1.75s/it] 49%|████▉     | 8723/17834 [4:24:18<4:25:42,  1.75s/it] 49%|████▉     | 8724/17834 [4:24:20<4:25:09,  1.75s/it] 49%|████▉     | 8725/17834 [4:24:21<4:26:32,  1.76s/it] 49%|████▉     | 8726/17834 [4:24:23<4:26:31,  1.76s/it] 49%|████▉     | 8727/17834 [4:24:25<4:28:29,  1.77s/it] 49%|████▉     | 8728/17834 [4:24:27<4:25:05,  1.75s/it] 49%|████▉     | 8729/17834 [4:24:28<4:25:56,  1.75s/it] 49%|████▉     | 8730/17834 [4:24:30<4:24:11,  1.74s/it] 49%|████▉     | 8731/17834 [4:24:32<4:22:41,  1.73s/it] 49%|████▉     | 8732/17834 [4:24:34<4:25:51,  1.75s/it] 49%|████▉     | 8733/17834 [4:24:35<4:22:58,  1.73s/it] 49%|████▉     | 8734/17834 [4:24:37<4:24:06,  1.74s/it] 49%|████▉     | 8735/17834 [4:24:39<4:29:20,  1.78s/it] 49%|████▉     | 8736/17834 [4:24:41<4:26:19,  1.76s/it] 49%|████▉     | 8737/17834 [4:24:42<4:27:51,  1.77s/it] 49%|████▉     | 8738/17834 [4:24:44<4:29:37,  1.78s/it] 49%|████▉     | 8739/17834 [4:24:46<4:27:20,  1.76s/it] 49%|████▉     | 8740/17834 [4:24:48<4:27:11,  1.76s/it] 49%|████▉     | 8741/17834 [4:24:50<4:28:17,  1.77s/it] 49%|████▉     | 8742/17834 [4:24:51<4:27:56,  1.77s/it] 49%|████▉     | 8743/17834 [4:24:53<4:32:23,  1.80s/it] 49%|████▉     | 8744/17834 [4:24:55<4:29:03,  1.78s/it] 49%|████▉     | 8745/17834 [4:24:57<4:28:50,  1.77s/it] 49%|████▉     | 8746/17834 [4:24:58<4:30:53,  1.79s/it] 49%|████▉     | 8747/17834 [4:25:00<4:28:33,  1.77s/it] 49%|████▉     | 8748/17834 [4:25:02<4:30:06,  1.78s/it] 49%|████▉     | 8749/17834 [4:25:04<4:27:23,  1.77s/it]08/30/2024 23:39:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0133402347564697, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.031188305467367172, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.200732946395874, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2452614307403564}
 49%|████▉     | 8750/17834 [4:25:06<4:29:31,  1.78s/it] 49%|████▉     | 8751/17834 [4:25:07<4:27:00,  1.76s/it] 49%|████▉     | 8752/17834 [4:25:09<4:23:50,  1.74s/it] 49%|████▉     | 8753/17834 [4:25:11<4:25:43,  1.76s/it] 49%|████▉     | 8754/17834 [4:25:13<4:25:59,  1.76s/it] 49%|████▉     | 8755/17834 [4:25:14<4:27:25,  1.77s/it] 49%|████▉     | 8756/17834 [4:25:16<4:29:58,  1.78s/it] 49%|████▉     | 8757/17834 [4:25:18<4:28:44,  1.78s/it] 49%|████▉     | 8758/17834 [4:25:20<4:29:01,  1.78s/it] 49%|████▉     | 8759/17834 [4:25:21<4:26:47,  1.76s/it] 49%|████▉     | 8760/17834 [4:25:23<4:24:50,  1.75s/it] 49%|████▉     | 8761/17834 [4:25:25<4:27:42,  1.77s/it] 49%|████▉     | 8762/17834 [4:25:27<4:27:31,  1.77s/it] 49%|████▉     | 8763/17834 [4:25:28<4:25:05,  1.75s/it] 49%|████▉     | 8764/17834 [4:25:30<4:28:17,  1.77s/it] 49%|████▉     | 8765/17834 [4:25:32<4:24:19,  1.75s/it] 49%|████▉     | 8766/17834 [4:25:34<4:24:58,  1.75s/it] 49%|████▉     | 8767/17834 [4:25:35<4:24:36,  1.75s/it] 49%|████▉     | 8768/17834 [4:25:37<4:22:55,  1.74s/it] 49%|████▉     | 8769/17834 [4:25:39<4:23:34,  1.74s/it] 49%|████▉     | 8770/17834 [4:25:41<4:20:46,  1.73s/it] 49%|████▉     | 8771/17834 [4:25:42<4:21:21,  1.73s/it] 49%|████▉     | 8772/17834 [4:25:44<4:24:53,  1.75s/it] 49%|████▉     | 8773/17834 [4:25:46<4:24:00,  1.75s/it] 49%|████▉     | 8774/17834 [4:25:48<4:25:02,  1.76s/it] 49%|████▉     | 8775/17834 [4:25:49<4:21:17,  1.73s/it] 49%|████▉     | 8776/17834 [4:25:51<4:21:52,  1.73s/it] 49%|████▉     | 8777/17834 [4:25:53<4:21:06,  1.73s/it] 49%|████▉     | 8778/17834 [4:25:55<4:23:29,  1.75s/it] 49%|████▉     | 8779/17834 [4:25:56<4:23:00,  1.74s/it] 49%|████▉     | 8780/17834 [4:25:58<4:24:31,  1.75s/it] 49%|████▉     | 8781/17834 [4:26:00<4:24:19,  1.75s/it] 49%|████▉     | 8782/17834 [4:26:02<4:25:13,  1.76s/it] 49%|████▉     | 8783/17834 [4:26:03<4:23:12,  1.74s/it] 49%|████▉     | 8784/17834 [4:26:05<4:24:53,  1.76s/it] 49%|████▉     | 8785/17834 [4:26:07<4:26:25,  1.77s/it] 49%|████▉     | 8786/17834 [4:26:09<4:28:15,  1.78s/it] 49%|████▉     | 8787/17834 [4:26:10<4:26:33,  1.77s/it] 49%|████▉     | 8788/17834 [4:26:12<4:30:08,  1.79s/it] 49%|████▉     | 8789/17834 [4:26:14<4:29:38,  1.79s/it] 49%|████▉     | 8790/17834 [4:26:16<4:24:14,  1.75s/it] 49%|████▉     | 8791/17834 [4:26:18<4:27:42,  1.78s/it] 49%|████▉     | 8792/17834 [4:26:19<4:27:42,  1.78s/it] 49%|████▉     | 8793/17834 [4:26:21<4:25:04,  1.76s/it] 49%|████▉     | 8794/17834 [4:26:23<4:24:36,  1.76s/it] 49%|████▉     | 8795/17834 [4:26:25<4:27:58,  1.78s/it] 49%|████▉     | 8796/17834 [4:26:27<4:30:45,  1.80s/it] 49%|████▉     | 8797/17834 [4:26:28<4:30:39,  1.80s/it] 49%|████▉     | 8798/17834 [4:26:30<4:28:39,  1.78s/it] 49%|████▉     | 8799/17834 [4:26:32<4:25:20,  1.76s/it]08/30/2024 23:40:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0040652751922607, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03836176544427872, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1588802337646484, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2013072967529297}
 49%|████▉     | 8800/17834 [4:26:33<4:22:40,  1.74s/it] 49%|████▉     | 8801/17834 [4:26:35<4:24:32,  1.76s/it] 49%|████▉     | 8802/17834 [4:26:37<4:24:00,  1.75s/it] 49%|████▉     | 8803/17834 [4:26:39<4:29:01,  1.79s/it] 49%|████▉     | 8804/17834 [4:26:41<4:29:18,  1.79s/it] 49%|████▉     | 8805/17834 [4:26:42<4:25:49,  1.77s/it] 49%|████▉     | 8806/17834 [4:26:44<4:23:22,  1.75s/it] 49%|████▉     | 8807/17834 [4:26:46<4:26:54,  1.77s/it] 49%|████▉     | 8808/17834 [4:26:48<4:27:12,  1.78s/it] 49%|████▉     | 8809/17834 [4:26:49<4:27:12,  1.78s/it] 49%|████▉     | 8810/17834 [4:26:51<4:25:43,  1.77s/it] 49%|████▉     | 8811/17834 [4:26:53<4:25:45,  1.77s/it] 49%|████▉     | 8812/17834 [4:26:55<4:23:10,  1.75s/it] 49%|████▉     | 8813/17834 [4:26:56<4:23:45,  1.75s/it] 49%|████▉     | 8814/17834 [4:26:58<4:21:23,  1.74s/it] 49%|████▉     | 8815/17834 [4:27:00<4:20:57,  1.74s/it] 49%|████▉     | 8816/17834 [4:27:02<4:21:46,  1.74s/it] 49%|████▉     | 8817/17834 [4:27:03<4:21:40,  1.74s/it] 49%|████▉     | 8818/17834 [4:27:05<4:25:34,  1.77s/it] 49%|████▉     | 8819/17834 [4:27:07<4:22:44,  1.75s/it] 49%|████▉     | 8820/17834 [4:27:09<4:28:13,  1.79s/it] 49%|████▉     | 8821/17834 [4:27:11<4:25:36,  1.77s/it] 49%|████▉     | 8822/17834 [4:27:12<4:23:10,  1.75s/it] 49%|████▉     | 8823/17834 [4:27:14<4:24:31,  1.76s/it] 49%|████▉     | 8824/17834 [4:27:16<4:23:27,  1.75s/it] 49%|████▉     | 8825/17834 [4:27:18<4:24:28,  1.76s/it] 49%|████▉     | 8826/17834 [4:27:19<4:22:39,  1.75s/it] 49%|████▉     | 8827/17834 [4:27:21<4:24:52,  1.76s/it] 50%|████▉     | 8828/17834 [4:27:23<4:22:55,  1.75s/it] 50%|████▉     | 8829/17834 [4:27:24<4:19:57,  1.73s/it] 50%|████▉     | 8830/17834 [4:27:26<4:20:47,  1.74s/it] 50%|████▉     | 8831/17834 [4:27:28<4:19:29,  1.73s/it] 50%|████▉     | 8832/17834 [4:27:30<4:23:44,  1.76s/it] 50%|████▉     | 8833/17834 [4:27:32<4:24:47,  1.77s/it] 50%|████▉     | 8834/17834 [4:27:33<4:24:03,  1.76s/it] 50%|████▉     | 8835/17834 [4:27:35<4:23:36,  1.76s/it] 50%|████▉     | 8836/17834 [4:27:37<4:22:59,  1.75s/it] 50%|████▉     | 8837/17834 [4:27:39<4:22:01,  1.75s/it] 50%|████▉     | 8838/17834 [4:27:40<4:21:23,  1.74s/it] 50%|████▉     | 8839/17834 [4:27:42<4:23:12,  1.76s/it] 50%|████▉     | 8840/17834 [4:27:44<4:20:02,  1.73s/it] 50%|████▉     | 8841/17834 [4:27:45<4:21:15,  1.74s/it] 50%|████▉     | 8842/17834 [4:27:47<4:22:09,  1.75s/it] 50%|████▉     | 8843/17834 [4:27:49<4:24:53,  1.77s/it] 50%|████▉     | 8844/17834 [4:27:51<4:25:52,  1.77s/it] 50%|████▉     | 8845/17834 [4:27:53<4:22:17,  1.75s/it] 50%|████▉     | 8846/17834 [4:27:54<4:21:10,  1.74s/it] 50%|████▉     | 8847/17834 [4:27:56<4:22:35,  1.75s/it] 50%|████▉     | 8848/17834 [4:27:58<4:21:00,  1.74s/it] 50%|████▉     | 8849/17834 [4:27:59<4:18:39,  1.73s/it]08/30/2024 23:42:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2141435146331787, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028453703969717026, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2560982704162598, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4986953735351562}
 50%|████▉     | 8850/17834 [4:28:01<4:23:58,  1.76s/it] 50%|████▉     | 8851/17834 [4:28:03<4:19:46,  1.74s/it] 50%|████▉     | 8852/17834 [4:28:05<4:23:07,  1.76s/it] 50%|████▉     | 8853/17834 [4:28:07<4:21:28,  1.75s/it] 50%|████▉     | 8854/17834 [4:28:08<4:19:49,  1.74s/it] 50%|████▉     | 8855/17834 [4:28:10<4:16:25,  1.71s/it] 50%|████▉     | 8856/17834 [4:28:12<4:19:34,  1.73s/it] 50%|████▉     | 8857/17834 [4:28:13<4:20:58,  1.74s/it] 50%|████▉     | 8858/17834 [4:28:15<4:19:28,  1.73s/it] 50%|████▉     | 8859/17834 [4:28:17<4:19:00,  1.73s/it] 50%|████▉     | 8860/17834 [4:28:19<4:20:32,  1.74s/it] 50%|████▉     | 8861/17834 [4:28:20<4:20:49,  1.74s/it] 50%|████▉     | 8862/17834 [4:28:22<4:19:13,  1.73s/it] 50%|████▉     | 8863/17834 [4:28:24<4:20:10,  1.74s/it] 50%|████▉     | 8864/17834 [4:28:26<4:21:20,  1.75s/it] 50%|████▉     | 8865/17834 [4:28:27<4:17:53,  1.73s/it] 50%|████▉     | 8866/17834 [4:28:29<4:16:27,  1.72s/it] 50%|████▉     | 8867/17834 [4:28:31<4:18:20,  1.73s/it] 50%|████▉     | 8868/17834 [4:28:33<4:21:20,  1.75s/it] 50%|████▉     | 8869/17834 [4:28:34<4:20:50,  1.75s/it] 50%|████▉     | 8870/17834 [4:28:36<4:19:00,  1.73s/it] 50%|████▉     | 8871/17834 [4:28:38<4:15:56,  1.71s/it] 50%|████▉     | 8872/17834 [4:28:39<4:15:20,  1.71s/it] 50%|████▉     | 8873/17834 [4:28:41<4:16:01,  1.71s/it] 50%|████▉     | 8874/17834 [4:28:43<4:18:17,  1.73s/it] 50%|████▉     | 8875/17834 [4:28:45<4:19:53,  1.74s/it] 50%|████▉     | 8876/17834 [4:28:46<4:20:54,  1.75s/it] 50%|████▉     | 8877/17834 [4:28:48<4:22:21,  1.76s/it] 50%|████▉     | 8878/17834 [4:28:50<4:22:37,  1.76s/it] 50%|████▉     | 8879/17834 [4:28:52<4:25:54,  1.78s/it] 50%|████▉     | 8880/17834 [4:28:53<4:22:11,  1.76s/it] 50%|████▉     | 8881/17834 [4:28:55<4:19:49,  1.74s/it] 50%|████▉     | 8882/17834 [4:28:57<4:22:35,  1.76s/it] 50%|████▉     | 8883/17834 [4:28:59<4:21:02,  1.75s/it] 50%|████▉     | 8884/17834 [4:29:00<4:23:17,  1.77s/it] 50%|████▉     | 8885/17834 [4:29:02<4:19:10,  1.74s/it] 50%|████▉     | 8886/17834 [4:29:04<4:21:15,  1.75s/it] 50%|████▉     | 8887/17834 [4:29:06<4:21:34,  1.75s/it] 50%|████▉     | 8888/17834 [4:29:08<4:25:08,  1.78s/it] 50%|████▉     | 8889/17834 [4:29:09<4:26:20,  1.79s/it] 50%|████▉     | 8890/17834 [4:29:11<4:25:06,  1.78s/it] 50%|████▉     | 8891/17834 [4:29:13<4:22:52,  1.76s/it] 50%|████▉     | 8892/17834 [4:29:15<4:20:47,  1.75s/it] 50%|████▉     | 8893/17834 [4:29:16<4:21:24,  1.75s/it] 50%|████▉     | 8894/17834 [4:29:18<4:24:57,  1.78s/it] 50%|████▉     | 8895/17834 [4:29:20<4:25:41,  1.78s/it] 50%|████▉     | 8896/17834 [4:29:22<4:30:17,  1.81s/it] 50%|████▉     | 8897/17834 [4:29:23<4:23:11,  1.77s/it] 50%|████▉     | 8898/17834 [4:29:25<4:19:36,  1.74s/it] 50%|████▉     | 8899/17834 [4:29:27<4:21:10,  1.75s/it]08/30/2024 23:43:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.060096025466919, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03691866621375084, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.256930112838745, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.353944778442383}
 50%|████▉     | 8900/17834 [4:29:29<4:19:09,  1.74s/it] 50%|████▉     | 8901/17834 [4:29:30<4:17:12,  1.73s/it] 50%|████▉     | 8902/17834 [4:29:32<4:19:56,  1.75s/it] 50%|████▉     | 8903/17834 [4:29:34<4:19:47,  1.75s/it] 50%|████▉     | 8904/17834 [4:29:36<4:19:32,  1.74s/it] 50%|████▉     | 8905/17834 [4:29:37<4:22:49,  1.77s/it] 50%|████▉     | 8906/17834 [4:29:39<4:20:05,  1.75s/it] 50%|████▉     | 8907/17834 [4:29:41<4:18:16,  1.74s/it] 50%|████▉     | 8908/17834 [4:29:43<4:19:34,  1.74s/it] 50%|████▉     | 8909/17834 [4:29:44<4:22:15,  1.76s/it]08/30/2024 23:44:03 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/30/2024 23:44:03 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:40,  5.36it/s][A
  1%|▏         | 3/221 [00:00<00:47,  4.58it/s][A
  2%|▏         | 4/221 [00:00<00:41,  5.26it/s][A
  2%|▏         | 5/221 [00:01<00:46,  4.65it/s][A
  3%|▎         | 6/221 [00:01<00:43,  4.96it/s][A
  3%|▎         | 7/221 [00:01<00:51,  4.19it/s][A
  4%|▎         | 8/221 [00:01<00:51,  4.13it/s][A
  4%|▍         | 9/221 [00:02<01:03,  3.33it/s][A
  5%|▍         | 10/221 [00:02<01:07,  3.13it/s][A
  5%|▍         | 11/221 [00:02<00:53,  3.89it/s][A
  5%|▌         | 12/221 [00:02<00:48,  4.33it/s][A
  6%|▌         | 13/221 [00:03<00:44,  4.70it/s][A
  6%|▋         | 14/221 [00:03<00:46,  4.47it/s][A
  7%|▋         | 15/221 [00:03<00:45,  4.56it/s][A
  7%|▋         | 16/221 [00:03<00:51,  4.00it/s][A
  8%|▊         | 17/221 [00:04<00:50,  4.08it/s][A
  8%|▊         | 18/221 [00:04<00:45,  4.49it/s][A
  9%|▊         | 19/221 [00:04<00:46,  4.38it/s][A
  9%|▉         | 20/221 [00:04<00:42,  4.72it/s][A
 10%|▉         | 21/221 [00:04<00:46,  4.34it/s][A
 10%|▉         | 22/221 [00:05<00:47,  4.19it/s][A
 10%|█         | 23/221 [00:05<00:44,  4.50it/s][A
 11%|█         | 24/221 [00:05<00:38,  5.10it/s][A
 11%|█▏        | 25/221 [00:05<00:41,  4.77it/s][A
 12%|█▏        | 26/221 [00:05<00:36,  5.31it/s][A
 12%|█▏        | 27/221 [00:06<00:48,  3.96it/s][A
 13%|█▎        | 28/221 [00:06<01:00,  3.19it/s][A
 13%|█▎        | 29/221 [00:07<01:06,  2.90it/s][A
 14%|█▎        | 30/221 [00:07<01:07,  2.83it/s][A
 14%|█▍        | 31/221 [00:07<01:05,  2.88it/s][A
 14%|█▍        | 32/221 [00:08<01:00,  3.11it/s][A
 15%|█▍        | 33/221 [00:08<00:54,  3.48it/s][A
 15%|█▌        | 34/221 [00:08<00:52,  3.58it/s][A
 16%|█▌        | 35/221 [00:08<00:52,  3.56it/s][A
 16%|█▋        | 36/221 [00:09<00:58,  3.15it/s][A
 17%|█▋        | 37/221 [00:09<00:51,  3.61it/s][A
 17%|█▋        | 38/221 [00:09<00:52,  3.50it/s][A
 18%|█▊        | 40/221 [00:10<00:45,  3.95it/s][A
 19%|█▊        | 41/221 [00:10<00:52,  3.44it/s][A
 19%|█▉        | 42/221 [00:10<00:43,  4.10it/s][A
 19%|█▉        | 43/221 [00:10<00:38,  4.58it/s][A
 20%|█▉        | 44/221 [00:11<00:44,  3.98it/s][A
 20%|██        | 45/221 [00:11<00:48,  3.64it/s][A
 21%|██        | 46/221 [00:11<00:51,  3.42it/s][A
 21%|██▏       | 47/221 [00:12<00:47,  3.64it/s][A
 22%|██▏       | 48/221 [00:12<00:42,  4.06it/s][A
 22%|██▏       | 49/221 [00:12<00:43,  3.98it/s][A
 23%|██▎       | 50/221 [00:12<00:47,  3.61it/s][A
 23%|██▎       | 51/221 [00:13<00:46,  3.63it/s][A
 24%|██▎       | 52/221 [00:13<00:41,  4.03it/s][A
 24%|██▍       | 53/221 [00:13<00:37,  4.49it/s][A
 24%|██▍       | 54/221 [00:13<00:37,  4.45it/s][A
 25%|██▍       | 55/221 [00:14<00:42,  3.91it/s][A
 25%|██▌       | 56/221 [00:14<00:46,  3.53it/s][A
 26%|██▌       | 57/221 [00:14<00:55,  2.96it/s][A
 26%|██▌       | 58/221 [00:15<00:47,  3.43it/s][A
 27%|██▋       | 59/221 [00:15<00:40,  4.02it/s][A
 27%|██▋       | 60/221 [00:15<00:38,  4.14it/s][A
 28%|██▊       | 61/221 [00:15<00:44,  3.60it/s][A
 28%|██▊       | 62/221 [00:15<00:41,  3.84it/s][A
 29%|██▊       | 63/221 [00:16<00:47,  3.29it/s][A
 29%|██▉       | 64/221 [00:16<00:57,  2.71it/s][A
 29%|██▉       | 65/221 [00:17<00:53,  2.93it/s][A
 30%|██▉       | 66/221 [00:17<00:49,  3.16it/s][A
 30%|███       | 67/221 [00:17<00:48,  3.17it/s][A
 31%|███       | 68/221 [00:17<00:40,  3.79it/s][A
 31%|███       | 69/221 [00:18<00:54,  2.81it/s][A
 32%|███▏      | 70/221 [00:18<00:56,  2.66it/s][A
 32%|███▏      | 71/221 [00:19<00:50,  2.99it/s][A
 33%|███▎      | 72/221 [00:19<00:59,  2.52it/s][A
 33%|███▎      | 73/221 [00:19<00:53,  2.77it/s][A
 33%|███▎      | 74/221 [00:20<00:42,  3.48it/s][A
 34%|███▍      | 75/221 [00:20<00:40,  3.63it/s][A
 34%|███▍      | 76/221 [00:20<00:52,  2.77it/s][A
 35%|███▍      | 77/221 [00:21<01:02,  2.31it/s][A
 35%|███▌      | 78/221 [00:21<00:59,  2.39it/s][A
 36%|███▌      | 79/221 [00:22<00:59,  2.39it/s][A
 36%|███▌      | 80/221 [00:22<00:54,  2.57it/s][A
 37%|███▋      | 81/221 [00:22<00:43,  3.24it/s][A
 37%|███▋      | 82/221 [00:22<00:40,  3.44it/s][A
 38%|███▊      | 83/221 [00:23<00:43,  3.15it/s][A
 38%|███▊      | 84/221 [00:23<00:39,  3.51it/s][A
 38%|███▊      | 85/221 [00:23<00:36,  3.77it/s][A
 39%|███▉      | 86/221 [00:24<00:49,  2.73it/s][A
 39%|███▉      | 87/221 [00:24<00:47,  2.80it/s][A
 40%|███▉      | 88/221 [00:25<00:52,  2.54it/s][A
 40%|████      | 89/221 [00:25<00:47,  2.78it/s][A
 41%|████      | 90/221 [00:25<00:40,  3.21it/s][A
 41%|████      | 91/221 [00:25<00:36,  3.58it/s][A
 42%|████▏     | 92/221 [00:26<00:39,  3.24it/s][A
 42%|████▏     | 93/221 [00:26<00:50,  2.53it/s][A
 43%|████▎     | 94/221 [00:27<00:46,  2.74it/s][A
 43%|████▎     | 95/221 [00:27<00:41,  3.05it/s][A
 43%|████▎     | 96/221 [00:27<00:41,  2.99it/s][A
 44%|████▍     | 97/221 [00:28<00:40,  3.04it/s][A
 44%|████▍     | 98/221 [00:28<00:46,  2.67it/s][A
 45%|████▍     | 99/221 [00:28<00:39,  3.10it/s][A
 45%|████▌     | 100/221 [00:29<00:42,  2.82it/s][A
 46%|████▌     | 101/221 [00:29<00:36,  3.28it/s][A
 46%|████▌     | 102/221 [00:29<00:31,  3.72it/s][A
 47%|████▋     | 103/221 [00:29<00:33,  3.54it/s][A
 47%|████▋     | 104/221 [00:30<00:41,  2.83it/s][A
 48%|████▊     | 105/221 [00:30<00:43,  2.66it/s][A
 48%|████▊     | 106/221 [00:31<00:39,  2.92it/s][A
 48%|████▊     | 107/221 [00:31<00:37,  3.08it/s][A
 49%|████▉     | 108/221 [00:31<00:39,  2.88it/s][A
 49%|████▉     | 109/221 [00:31<00:34,  3.23it/s][A
 50%|████▉     | 110/221 [00:32<00:33,  3.35it/s][A
 50%|█████     | 111/221 [00:32<00:32,  3.44it/s][A
 51%|█████     | 112/221 [00:32<00:32,  3.36it/s][A
 51%|█████     | 113/221 [00:33<00:31,  3.48it/s][A
 52%|█████▏    | 114/221 [00:33<00:28,  3.69it/s][A
 52%|█████▏    | 115/221 [00:33<00:33,  3.14it/s][A
 52%|█████▏    | 116/221 [00:33<00:30,  3.40it/s][A
 53%|█████▎    | 117/221 [00:34<00:27,  3.78it/s][A
 53%|█████▎    | 118/221 [00:34<00:26,  3.94it/s][A
 54%|█████▍    | 119/221 [00:34<00:24,  4.08it/s][A
 54%|█████▍    | 120/221 [00:35<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:35<00:28,  3.49it/s][A
 55%|█████▌    | 122/221 [00:35<00:28,  3.47it/s][A
 56%|█████▌    | 123/221 [00:35<00:27,  3.62it/s][A
 56%|█████▌    | 124/221 [00:36<00:31,  3.11it/s][A
 57%|█████▋    | 125/221 [00:36<00:35,  2.71it/s][A
 57%|█████▋    | 126/221 [00:36<00:28,  3.29it/s][A
 57%|█████▋    | 127/221 [00:37<00:28,  3.29it/s][A
 58%|█████▊    | 128/221 [00:37<00:27,  3.43it/s][A
 58%|█████▊    | 129/221 [00:37<00:22,  4.05it/s][A
 59%|█████▉    | 130/221 [00:37<00:22,  4.04it/s][A
 59%|█████▉    | 131/221 [00:38<00:24,  3.62it/s][A
 60%|█████▉    | 132/221 [00:38<00:32,  2.77it/s][A
 60%|██████    | 133/221 [00:39<00:33,  2.62it/s][A
 61%|██████    | 134/221 [00:39<00:31,  2.73it/s][A
 61%|██████    | 135/221 [00:39<00:30,  2.79it/s][A
 62%|██████▏   | 136/221 [00:40<00:26,  3.19it/s][A
 62%|██████▏   | 137/221 [00:40<00:23,  3.60it/s][A
 62%|██████▏   | 138/221 [00:40<00:21,  3.78it/s][A
 63%|██████▎   | 139/221 [00:40<00:21,  3.79it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.80it/s][A
 64%|██████▍   | 141/221 [00:41<00:24,  3.30it/s][A
 64%|██████▍   | 142/221 [00:41<00:21,  3.76it/s][A
 65%|██████▍   | 143/221 [00:42<00:27,  2.80it/s][A
 65%|██████▌   | 144/221 [00:42<00:26,  2.88it/s][A
 66%|██████▌   | 145/221 [00:42<00:26,  2.91it/s][A
 66%|██████▌   | 146/221 [00:43<00:21,  3.42it/s][A
 67%|██████▋   | 147/221 [00:43<00:21,  3.38it/s][A
 67%|██████▋   | 148/221 [00:43<00:21,  3.34it/s][A
 67%|██████▋   | 149/221 [00:44<00:23,  3.06it/s][A
 68%|██████▊   | 150/221 [00:44<00:18,  3.78it/s][A
 68%|██████▊   | 151/221 [00:44<00:21,  3.26it/s][A
 69%|██████▉   | 152/221 [00:44<00:21,  3.15it/s][A
 69%|██████▉   | 153/221 [00:45<00:17,  3.88it/s][A
 70%|██████▉   | 154/221 [00:45<00:21,  3.14it/s][A
 70%|███████   | 155/221 [00:45<00:21,  3.07it/s][A
 71%|███████   | 156/221 [00:46<00:22,  2.84it/s][A
 71%|███████   | 157/221 [00:46<00:21,  2.91it/s][A
 71%|███████▏  | 158/221 [00:47<00:25,  2.49it/s][A
 72%|███████▏  | 159/221 [00:47<00:22,  2.81it/s][A
 72%|███████▏  | 160/221 [00:47<00:21,  2.85it/s][A
 73%|███████▎  | 161/221 [00:48<00:25,  2.40it/s][A
 73%|███████▎  | 162/221 [00:48<00:22,  2.65it/s][A
 74%|███████▍  | 163/221 [00:48<00:21,  2.67it/s][A
 74%|███████▍  | 164/221 [00:49<00:17,  3.26it/s][A
 75%|███████▍  | 165/221 [00:49<00:14,  3.74it/s][A
 75%|███████▌  | 166/221 [00:49<00:13,  4.18it/s][A
 76%|███████▌  | 167/221 [00:49<00:15,  3.38it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.50it/s][A
 76%|███████▋  | 169/221 [00:50<00:13,  3.72it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.40it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.29it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.44it/s][A
 78%|███████▊  | 173/221 [00:51<00:14,  3.24it/s][A
 79%|███████▊  | 174/221 [00:51<00:12,  3.66it/s][A
 79%|███████▉  | 175/221 [00:52<00:13,  3.32it/s][A
 80%|███████▉  | 176/221 [00:52<00:11,  4.07it/s][A
 80%|████████  | 177/221 [00:52<00:09,  4.71it/s][A
 81%|████████  | 178/221 [00:52<00:09,  4.58it/s][A
 81%|████████  | 179/221 [00:52<00:09,  4.51it/s][A
 81%|████████▏ | 180/221 [00:53<00:08,  4.82it/s][A
 82%|████████▏ | 181/221 [00:53<00:08,  4.81it/s][A
 82%|████████▏ | 182/221 [00:53<00:08,  4.40it/s][A
 83%|████████▎ | 183/221 [00:53<00:10,  3.71it/s][A
 83%|████████▎ | 184/221 [00:54<00:12,  3.06it/s][A
 84%|████████▎ | 185/221 [00:54<00:10,  3.35it/s][A
 84%|████████▍ | 186/221 [00:54<00:10,  3.39it/s][A
 85%|████████▍ | 187/221 [00:55<00:10,  3.22it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.51it/s][A
 86%|████████▌ | 189/221 [00:55<00:07,  4.15it/s][A
 86%|████████▌ | 190/221 [00:55<00:07,  3.90it/s][A
 86%|████████▋ | 191/221 [00:56<00:09,  3.29it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.55it/s][A
 87%|████████▋ | 193/221 [00:56<00:07,  3.89it/s][A
 88%|████████▊ | 194/221 [00:56<00:06,  4.07it/s][A
 88%|████████▊ | 195/221 [00:57<00:06,  4.18it/s][A
 89%|████████▊ | 196/221 [00:57<00:08,  2.93it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.12it/s][A
 90%|████████▉ | 198/221 [00:58<00:07,  3.03it/s][A
 90%|█████████ | 199/221 [00:58<00:05,  3.71it/s][A
 90%|█████████ | 200/221 [00:58<00:05,  3.64it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.83it/s][A
 91%|█████████▏| 202/221 [00:59<00:04,  3.92it/s][A
 92%|█████████▏| 203/221 [00:59<00:04,  4.31it/s][A
 92%|█████████▏| 204/221 [00:59<00:03,  4.66it/s][A
 93%|█████████▎| 205/221 [00:59<00:03,  4.94it/s][A
 93%|█████████▎| 206/221 [00:59<00:02,  5.76it/s][A
 94%|█████████▎| 207/221 [01:00<00:02,  4.69it/s][A
 94%|█████████▍| 208/221 [01:00<00:02,  4.93it/s][A
 95%|█████████▍| 209/221 [01:00<00:02,  5.18it/s][A
 95%|█████████▌| 210/221 [01:00<00:02,  5.43it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  4.32it/s][A
 96%|█████████▌| 212/221 [01:01<00:02,  3.42it/s][A
 96%|█████████▋| 213/221 [01:01<00:02,  3.34it/s][A
 97%|█████████▋| 214/221 [01:02<00:02,  2.98it/s][A
 97%|█████████▋| 215/221 [01:02<00:01,  3.70it/s][A
 98%|█████████▊| 216/221 [01:02<00:01,  3.65it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.28it/s][A
 99%|█████████▊| 218/221 [01:03<00:01,  2.85it/s][A
 99%|█████████▉| 219/221 [01:03<00:00,  2.73it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.09it/s][A
100%|██████████| 221/221 [01:04<00:00,  3.24it/s][A100%|██████████| 221/221 [01:04<00:00,  3.43it/s]
08/30/2024 23:46:28 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 8909--===========

08/30/2024 23:46:28 - INFO - __main__ -   {'area_r1': 7.1, 'area_recall': '7.1/16.3/21.0', 'area_ravg': 14.8}
08/30/2024 23:46:28 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 8909--===========

08/30/2024 23:46:28 - INFO - __main__ -   {'area_r1': 38.0, 'area_recall': '38.0/71.8/82.5', 'area_ravg': 64.1}
08/30/2024 23:46:28 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 8909--===========

08/30/2024 23:46:28 - INFO - __main__ -   {'video_r1': 35.6, 'video_recall': '35.6/67.4/78.1', 'video_ravg': 60.4}
08/30/2024 23:46:28 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/30/2024 23:46:28 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/30/2024 23:46:28 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 8909--===========

08/30/2024 23:46:28 - INFO - __main__ -   {'video_r1': 56.7, 'video_recall': '56.7/79.4/86.5', 'video_ravg': 74.2}
08/30/2024 23:46:28 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/30/2024 23:46:28 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
 50%|████▉     | 8910/17834 [4:32:30<126:21:00, 50.97s/it] 50%|████▉     | 8911/17834 [4:32:32<89:45:35, 36.21s/it]  50%|████▉     | 8912/17834 [4:32:34<64:04:22, 25.85s/it] 50%|████▉     | 8913/17834 [4:32:35<46:11:19, 18.64s/it] 50%|████▉     | 8914/17834 [4:32:37<33:38:42, 13.58s/it] 50%|████▉     | 8915/17834 [4:32:39<24:50:40, 10.03s/it] 50%|████▉     | 8916/17834 [4:32:41<18:43:17,  7.56s/it] 50%|█████     | 8917/17834 [4:32:43<14:22:59,  5.81s/it] 50%|█████     | 8918/17834 [4:32:44<11:21:37,  4.59s/it] 50%|█████     | 8919/17834 [4:32:46<9:13:49,  3.73s/it]  50%|█████     | 8920/17834 [4:32:48<7:50:24,  3.17s/it] 50%|█████     | 8921/17834 [4:32:49<6:42:45,  2.71s/it] 50%|█████     | 8922/17834 [4:32:51<6:00:16,  2.43s/it] 50%|█████     | 8923/17834 [4:32:53<5:31:02,  2.23s/it] 50%|█████     | 8924/17834 [4:32:55<5:11:59,  2.10s/it] 50%|█████     | 8925/17834 [4:32:57<4:57:42,  2.00s/it] 50%|█████     | 8926/17834 [4:32:58<4:44:46,  1.92s/it] 50%|█████     | 8927/17834 [4:33:00<4:38:04,  1.87s/it] 50%|█████     | 8928/17834 [4:33:02<4:30:43,  1.82s/it] 50%|█████     | 8929/17834 [4:33:04<4:27:08,  1.80s/it] 50%|█████     | 8930/17834 [4:33:05<4:22:13,  1.77s/it] 50%|█████     | 8931/17834 [4:33:07<4:24:14,  1.78s/it] 50%|█████     | 8932/17834 [4:33:09<4:21:26,  1.76s/it] 50%|█████     | 8933/17834 [4:33:11<4:21:44,  1.76s/it] 50%|█████     | 8934/17834 [4:33:12<4:21:55,  1.77s/it] 50%|█████     | 8935/17834 [4:33:14<4:22:51,  1.77s/it] 50%|█████     | 8936/17834 [4:33:16<4:21:02,  1.76s/it] 50%|█████     | 8937/17834 [4:33:18<4:22:48,  1.77s/it] 50%|█████     | 8938/17834 [4:33:19<4:21:22,  1.76s/it] 50%|█████     | 8939/17834 [4:33:21<4:23:18,  1.78s/it] 50%|█████     | 8940/17834 [4:33:23<4:21:33,  1.76s/it] 50%|█████     | 8941/17834 [4:33:25<4:22:31,  1.77s/it] 50%|█████     | 8942/17834 [4:33:26<4:22:34,  1.77s/it] 50%|█████     | 8943/17834 [4:33:28<4:19:09,  1.75s/it] 50%|█████     | 8944/17834 [4:33:30<4:20:49,  1.76s/it] 50%|█████     | 8945/17834 [4:33:32<4:21:15,  1.76s/it] 50%|█████     | 8946/17834 [4:33:33<4:19:28,  1.75s/it] 50%|█████     | 8947/17834 [4:33:35<4:22:17,  1.77s/it] 50%|█████     | 8948/17834 [4:33:37<4:19:16,  1.75s/it] 50%|█████     | 8949/17834 [4:33:39<4:21:28,  1.77s/it]08/30/2024 23:47:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.027970790863037, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0315760001540184, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1928536891937256, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2524003982543945}
 50%|█████     | 8950/17834 [4:33:40<4:18:39,  1.75s/it] 50%|█████     | 8951/17834 [4:33:42<4:20:08,  1.76s/it] 50%|█████     | 8952/17834 [4:33:44<4:18:37,  1.75s/it] 50%|█████     | 8953/17834 [4:33:46<4:17:29,  1.74s/it] 50%|█████     | 8954/17834 [4:33:47<4:18:31,  1.75s/it] 50%|█████     | 8955/17834 [4:33:49<4:19:00,  1.75s/it] 50%|█████     | 8956/17834 [4:33:51<4:19:24,  1.75s/it] 50%|█████     | 8957/17834 [4:33:53<4:23:53,  1.78s/it] 50%|█████     | 8958/17834 [4:33:55<4:22:10,  1.77s/it] 50%|█████     | 8959/17834 [4:33:56<4:20:07,  1.76s/it] 50%|█████     | 8960/17834 [4:33:58<4:16:00,  1.73s/it] 50%|█████     | 8961/17834 [4:34:00<4:16:44,  1.74s/it] 50%|█████     | 8962/17834 [4:34:02<4:19:52,  1.76s/it] 50%|█████     | 8963/17834 [4:34:03<4:19:43,  1.76s/it] 50%|█████     | 8964/17834 [4:34:05<4:21:35,  1.77s/it] 50%|█████     | 8965/17834 [4:34:07<4:17:35,  1.74s/it] 50%|█████     | 8966/17834 [4:34:08<4:15:39,  1.73s/it] 50%|█████     | 8967/17834 [4:34:10<4:16:24,  1.73s/it] 50%|█████     | 8968/17834 [4:34:12<4:16:37,  1.74s/it] 50%|█████     | 8969/17834 [4:34:14<4:13:56,  1.72s/it] 50%|█████     | 8970/17834 [4:34:15<4:14:41,  1.72s/it] 50%|█████     | 8971/17834 [4:34:17<4:16:13,  1.73s/it] 50%|█████     | 8972/17834 [4:34:19<4:15:31,  1.73s/it] 50%|█████     | 8973/17834 [4:34:21<4:17:46,  1.75s/it] 50%|█████     | 8974/17834 [4:34:22<4:14:23,  1.72s/it] 50%|█████     | 8975/17834 [4:34:24<4:14:38,  1.72s/it] 50%|█████     | 8976/17834 [4:34:26<4:16:02,  1.73s/it] 50%|█████     | 8977/17834 [4:34:27<4:14:49,  1.73s/it] 50%|█████     | 8978/17834 [4:34:29<4:13:34,  1.72s/it] 50%|█████     | 8979/17834 [4:34:31<4:17:19,  1.74s/it] 50%|█████     | 8980/17834 [4:34:33<4:19:18,  1.76s/it] 50%|█████     | 8981/17834 [4:34:34<4:17:23,  1.74s/it] 50%|█████     | 8982/17834 [4:34:36<4:17:56,  1.75s/it] 50%|█████     | 8983/17834 [4:34:38<4:17:14,  1.74s/it] 50%|█████     | 8984/17834 [4:34:40<4:14:28,  1.73s/it] 50%|█████     | 8985/17834 [4:34:41<4:12:37,  1.71s/it] 50%|█████     | 8986/17834 [4:34:43<4:14:45,  1.73s/it] 50%|█████     | 8987/17834 [4:34:45<4:12:55,  1.72s/it] 50%|█████     | 8988/17834 [4:34:47<4:15:14,  1.73s/it] 50%|█████     | 8989/17834 [4:34:48<4:17:57,  1.75s/it] 50%|█████     | 8990/17834 [4:34:50<4:17:44,  1.75s/it] 50%|█████     | 8991/17834 [4:34:52<4:16:14,  1.74s/it] 50%|█████     | 8992/17834 [4:34:54<4:16:16,  1.74s/it] 50%|█████     | 8993/17834 [4:34:55<4:14:53,  1.73s/it] 50%|█████     | 8994/17834 [4:34:57<4:16:01,  1.74s/it] 50%|█████     | 8995/17834 [4:34:59<4:15:35,  1.73s/it] 50%|█████     | 8996/17834 [4:35:01<4:19:35,  1.76s/it] 50%|█████     | 8997/17834 [4:35:02<4:20:14,  1.77s/it] 50%|█████     | 8998/17834 [4:35:04<4:16:40,  1.74s/it] 50%|█████     | 8999/17834 [4:35:06<4:15:39,  1.74s/it]08/30/2024 23:49:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0449912548065186, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03403506800532341, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.243046760559082, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.322072982788086}
 50%|█████     | 9000/17834 [4:35:08<4:16:36,  1.74s/it] 50%|█████     | 9001/17834 [4:35:09<4:15:45,  1.74s/it] 50%|█████     | 9002/17834 [4:35:11<4:15:33,  1.74s/it] 50%|█████     | 9003/17834 [4:35:13<4:14:38,  1.73s/it] 50%|█████     | 9004/17834 [4:35:14<4:13:08,  1.72s/it] 50%|█████     | 9005/17834 [4:35:16<4:13:02,  1.72s/it] 50%|█████     | 9006/17834 [4:35:18<4:15:48,  1.74s/it] 51%|█████     | 9007/17834 [4:35:20<4:17:50,  1.75s/it] 51%|█████     | 9008/17834 [4:35:21<4:15:16,  1.74s/it] 51%|█████     | 9009/17834 [4:35:23<4:14:45,  1.73s/it] 51%|█████     | 9010/17834 [4:35:25<4:16:17,  1.74s/it] 51%|█████     | 9011/17834 [4:35:27<4:19:57,  1.77s/it] 51%|█████     | 9012/17834 [4:35:28<4:16:45,  1.75s/it] 51%|█████     | 9013/17834 [4:35:30<4:17:45,  1.75s/it] 51%|█████     | 9014/17834 [4:35:32<4:19:33,  1.77s/it] 51%|█████     | 9015/17834 [4:35:34<4:23:22,  1.79s/it] 51%|█████     | 9016/17834 [4:35:36<4:21:22,  1.78s/it] 51%|█████     | 9017/17834 [4:35:37<4:18:56,  1.76s/it] 51%|█████     | 9018/17834 [4:35:39<4:17:14,  1.75s/it] 51%|█████     | 9019/17834 [4:35:41<4:17:39,  1.75s/it] 51%|█████     | 9020/17834 [4:35:43<4:19:29,  1.77s/it] 51%|█████     | 9021/17834 [4:35:44<4:20:22,  1.77s/it] 51%|█████     | 9022/17834 [4:35:46<4:20:44,  1.78s/it] 51%|█████     | 9023/17834 [4:35:48<4:19:19,  1.77s/it] 51%|█████     | 9024/17834 [4:35:50<4:17:40,  1.75s/it] 51%|█████     | 9025/17834 [4:35:51<4:21:05,  1.78s/it] 51%|█████     | 9026/17834 [4:35:53<4:22:44,  1.79s/it] 51%|█████     | 9027/17834 [4:35:55<4:21:40,  1.78s/it] 51%|█████     | 9028/17834 [4:35:57<4:20:57,  1.78s/it] 51%|█████     | 9029/17834 [4:35:59<4:22:25,  1.79s/it] 51%|█████     | 9030/17834 [4:36:00<4:21:13,  1.78s/it] 51%|█████     | 9031/17834 [4:36:02<4:22:39,  1.79s/it] 51%|█████     | 9032/17834 [4:36:04<4:20:21,  1.77s/it] 51%|█████     | 9033/17834 [4:36:06<4:15:48,  1.74s/it] 51%|█████     | 9034/17834 [4:36:07<4:15:34,  1.74s/it] 51%|█████     | 9035/17834 [4:36:09<4:14:24,  1.73s/it] 51%|█████     | 9036/17834 [4:36:11<4:12:17,  1.72s/it] 51%|█████     | 9037/17834 [4:36:12<4:12:43,  1.72s/it] 51%|█████     | 9038/17834 [4:36:14<4:14:26,  1.74s/it] 51%|█████     | 9039/17834 [4:36:16<4:16:13,  1.75s/it] 51%|█████     | 9040/17834 [4:36:18<4:16:10,  1.75s/it] 51%|█████     | 9041/17834 [4:36:20<4:21:12,  1.78s/it] 51%|█████     | 9042/17834 [4:36:21<4:21:52,  1.79s/it] 51%|█████     | 9043/17834 [4:36:23<4:26:04,  1.82s/it] 51%|█████     | 9044/17834 [4:36:25<4:23:31,  1.80s/it] 51%|█████     | 9045/17834 [4:36:27<4:18:04,  1.76s/it] 51%|█████     | 9046/17834 [4:36:28<4:17:05,  1.76s/it] 51%|█████     | 9047/17834 [4:36:30<4:15:22,  1.74s/it] 51%|█████     | 9048/17834 [4:36:32<4:12:28,  1.72s/it] 51%|█████     | 9049/17834 [4:36:34<4:14:56,  1.74s/it]08/30/2024 23:50:53 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3255860805511475, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03482656553387642, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2976255416870117, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6580381393432617}
 51%|█████     | 9050/17834 [4:36:35<4:17:37,  1.76s/it] 51%|█████     | 9051/17834 [4:36:37<4:17:30,  1.76s/it] 51%|█████     | 9052/17834 [4:36:39<4:18:48,  1.77s/it] 51%|█████     | 9053/17834 [4:36:41<4:16:40,  1.75s/it] 51%|█████     | 9054/17834 [4:36:42<4:15:36,  1.75s/it] 51%|█████     | 9055/17834 [4:36:44<4:14:08,  1.74s/it] 51%|█████     | 9056/17834 [4:36:46<4:12:39,  1.73s/it] 51%|█████     | 9057/17834 [4:36:48<4:11:48,  1.72s/it] 51%|█████     | 9058/17834 [4:36:49<4:15:32,  1.75s/it] 51%|█████     | 9059/17834 [4:36:51<4:21:10,  1.79s/it] 51%|█████     | 9060/17834 [4:36:53<4:22:05,  1.79s/it] 51%|█████     | 9061/17834 [4:36:55<4:19:16,  1.77s/it] 51%|█████     | 9062/17834 [4:36:57<4:23:01,  1.80s/it] 51%|█████     | 9063/17834 [4:36:58<4:20:44,  1.78s/it] 51%|█████     | 9064/17834 [4:37:00<4:20:13,  1.78s/it] 51%|█████     | 9065/17834 [4:37:02<4:17:36,  1.76s/it] 51%|█████     | 9066/17834 [4:37:04<4:15:21,  1.75s/it] 51%|█████     | 9067/17834 [4:37:05<4:16:55,  1.76s/it] 51%|█████     | 9068/17834 [4:37:07<4:17:09,  1.76s/it] 51%|█████     | 9069/17834 [4:37:09<4:17:25,  1.76s/it] 51%|█████     | 9070/17834 [4:37:11<4:17:40,  1.76s/it] 51%|█████     | 9071/17834 [4:37:12<4:16:40,  1.76s/it] 51%|█████     | 9072/17834 [4:37:14<4:17:13,  1.76s/it] 51%|█████     | 9073/17834 [4:37:16<4:14:30,  1.74s/it] 51%|█████     | 9074/17834 [4:37:18<4:12:55,  1.73s/it] 51%|█████     | 9075/17834 [4:37:19<4:12:41,  1.73s/it] 51%|█████     | 9076/17834 [4:37:21<4:15:45,  1.75s/it] 51%|█████     | 9077/17834 [4:37:23<4:18:09,  1.77s/it] 51%|█████     | 9078/17834 [4:37:25<4:17:50,  1.77s/it] 51%|█████     | 9079/17834 [4:37:26<4:17:24,  1.76s/it] 51%|█████     | 9080/17834 [4:37:28<4:19:38,  1.78s/it] 51%|█████     | 9081/17834 [4:37:30<4:19:57,  1.78s/it] 51%|█████     | 9082/17834 [4:37:32<4:18:09,  1.77s/it] 51%|█████     | 9083/17834 [4:37:34<4:21:13,  1.79s/it] 51%|█████     | 9084/17834 [4:37:36<4:29:56,  1.85s/it] 51%|█████     | 9085/17834 [4:37:37<4:24:16,  1.81s/it] 51%|█████     | 9086/17834 [4:37:39<4:23:21,  1.81s/it] 51%|█████     | 9087/17834 [4:37:41<4:18:43,  1.77s/it] 51%|█████     | 9088/17834 [4:37:43<4:20:12,  1.79s/it] 51%|█████     | 9089/17834 [4:37:44<4:16:11,  1.76s/it] 51%|█████     | 9090/17834 [4:37:46<4:15:28,  1.75s/it] 51%|█████     | 9091/17834 [4:37:48<4:20:08,  1.79s/it] 51%|█████     | 9092/17834 [4:37:50<4:21:13,  1.79s/it] 51%|█████     | 9093/17834 [4:37:51<4:17:25,  1.77s/it] 51%|█████     | 9094/17834 [4:37:53<4:16:51,  1.76s/it] 51%|█████     | 9095/17834 [4:37:55<4:13:24,  1.74s/it] 51%|█████     | 9096/17834 [4:37:57<4:13:23,  1.74s/it] 51%|█████     | 9097/17834 [4:37:58<4:16:38,  1.76s/it] 51%|█████     | 9098/17834 [4:38:00<4:17:06,  1.77s/it] 51%|█████     | 9099/17834 [4:38:02<4:18:41,  1.78s/it]08/30/2024 23:52:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1687278747558594, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03698264807462692, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2022924423217773, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4080028533935547}
 51%|█████     | 9100/17834 [4:38:04<4:15:45,  1.76s/it] 51%|█████     | 9101/17834 [4:38:05<4:15:38,  1.76s/it] 51%|█████     | 9102/17834 [4:38:07<4:15:28,  1.76s/it] 51%|█████     | 9103/17834 [4:38:09<4:14:58,  1.75s/it] 51%|█████     | 9104/17834 [4:38:11<4:17:44,  1.77s/it] 51%|█████     | 9105/17834 [4:38:13<4:20:09,  1.79s/it] 51%|█████     | 9106/17834 [4:38:14<4:18:07,  1.77s/it] 51%|█████     | 9107/17834 [4:38:16<4:14:58,  1.75s/it] 51%|█████     | 9108/17834 [4:38:18<4:13:17,  1.74s/it] 51%|█████     | 9109/17834 [4:38:20<4:14:15,  1.75s/it] 51%|█████     | 9110/17834 [4:38:21<4:13:22,  1.74s/it] 51%|█████     | 9111/17834 [4:38:23<4:16:16,  1.76s/it] 51%|█████     | 9112/17834 [4:38:25<4:15:15,  1.76s/it] 51%|█████     | 9113/17834 [4:38:27<4:16:06,  1.76s/it] 51%|█████     | 9114/17834 [4:38:28<4:19:20,  1.78s/it] 51%|█████     | 9115/17834 [4:38:30<4:19:56,  1.79s/it] 51%|█████     | 9116/17834 [4:38:32<4:16:47,  1.77s/it] 51%|█████     | 9117/17834 [4:38:34<4:14:06,  1.75s/it] 51%|█████     | 9118/17834 [4:38:35<4:13:26,  1.74s/it] 51%|█████     | 9119/17834 [4:38:37<4:19:32,  1.79s/it] 51%|█████     | 9120/17834 [4:38:39<4:18:17,  1.78s/it] 51%|█████     | 9121/17834 [4:38:41<4:14:50,  1.75s/it] 51%|█████     | 9122/17834 [4:38:43<4:17:45,  1.78s/it] 51%|█████     | 9123/17834 [4:38:44<4:15:28,  1.76s/it] 51%|█████     | 9124/17834 [4:38:46<4:13:36,  1.75s/it] 51%|█████     | 9125/17834 [4:38:48<4:13:35,  1.75s/it] 51%|█████     | 9126/17834 [4:38:49<4:10:50,  1.73s/it] 51%|█████     | 9127/17834 [4:38:51<4:12:32,  1.74s/it] 51%|█████     | 9128/17834 [4:38:53<4:14:27,  1.75s/it] 51%|█████     | 9129/17834 [4:38:55<4:16:38,  1.77s/it] 51%|█████     | 9130/17834 [4:38:57<4:19:54,  1.79s/it] 51%|█████     | 9131/17834 [4:38:58<4:16:46,  1.77s/it] 51%|█████     | 9132/17834 [4:39:00<4:17:03,  1.77s/it] 51%|█████     | 9133/17834 [4:39:02<4:15:30,  1.76s/it] 51%|█████     | 9134/17834 [4:39:04<4:14:03,  1.75s/it] 51%|█████     | 9135/17834 [4:39:05<4:12:43,  1.74s/it] 51%|█████     | 9136/17834 [4:39:07<4:17:51,  1.78s/it] 51%|█████     | 9137/17834 [4:39:09<4:15:35,  1.76s/it] 51%|█████     | 9138/17834 [4:39:11<4:14:36,  1.76s/it] 51%|█████     | 9139/17834 [4:39:12<4:14:12,  1.75s/it] 51%|█████▏    | 9140/17834 [4:39:14<4:11:55,  1.74s/it] 51%|█████▏    | 9141/17834 [4:39:16<4:11:14,  1.73s/it] 51%|█████▏    | 9142/17834 [4:39:18<4:16:56,  1.77s/it] 51%|█████▏    | 9143/17834 [4:39:19<4:12:42,  1.74s/it] 51%|█████▏    | 9144/17834 [4:39:21<4:12:42,  1.74s/it] 51%|█████▏    | 9145/17834 [4:39:23<4:10:58,  1.73s/it] 51%|█████▏    | 9146/17834 [4:39:25<4:13:13,  1.75s/it] 51%|█████▏    | 9147/17834 [4:39:26<4:14:55,  1.76s/it] 51%|█████▏    | 9148/17834 [4:39:28<4:13:21,  1.75s/it] 51%|█████▏    | 9149/17834 [4:39:30<4:11:59,  1.74s/it]08/30/2024 23:53:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.111757755279541, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03588927164673805, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.196596145629883, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.344243049621582}
 51%|█████▏    | 9150/17834 [4:39:32<4:12:04,  1.74s/it] 51%|█████▏    | 9151/17834 [4:39:33<4:12:23,  1.74s/it] 51%|█████▏    | 9152/17834 [4:39:35<4:16:09,  1.77s/it] 51%|█████▏    | 9153/17834 [4:39:37<4:13:25,  1.75s/it] 51%|█████▏    | 9154/17834 [4:39:39<4:16:07,  1.77s/it] 51%|█████▏    | 9155/17834 [4:39:40<4:13:11,  1.75s/it] 51%|█████▏    | 9156/17834 [4:39:42<4:13:34,  1.75s/it] 51%|█████▏    | 9157/17834 [4:39:44<4:16:18,  1.77s/it] 51%|█████▏    | 9158/17834 [4:39:46<4:14:14,  1.76s/it] 51%|█████▏    | 9159/17834 [4:39:47<4:14:04,  1.76s/it] 51%|█████▏    | 9160/17834 [4:39:49<4:19:10,  1.79s/it] 51%|█████▏    | 9161/17834 [4:39:51<4:16:39,  1.78s/it] 51%|█████▏    | 9162/17834 [4:39:53<4:12:43,  1.75s/it] 51%|█████▏    | 9163/17834 [4:39:54<4:09:52,  1.73s/it] 51%|█████▏    | 9164/17834 [4:39:56<4:12:17,  1.75s/it] 51%|█████▏    | 9165/17834 [4:39:58<4:13:31,  1.75s/it] 51%|█████▏    | 9166/17834 [4:40:00<4:17:48,  1.78s/it] 51%|█████▏    | 9167/17834 [4:40:02<4:15:04,  1.77s/it] 51%|█████▏    | 9168/17834 [4:40:03<4:15:29,  1.77s/it] 51%|█████▏    | 9169/17834 [4:40:05<4:13:37,  1.76s/it] 51%|█████▏    | 9170/17834 [4:40:07<4:12:25,  1.75s/it] 51%|█████▏    | 9171/17834 [4:40:09<4:15:16,  1.77s/it] 51%|█████▏    | 9172/17834 [4:40:10<4:12:10,  1.75s/it] 51%|█████▏    | 9173/17834 [4:40:12<4:12:35,  1.75s/it] 51%|█████▏    | 9174/17834 [4:40:14<4:13:57,  1.76s/it] 51%|█████▏    | 9175/17834 [4:40:16<4:12:39,  1.75s/it] 51%|█████▏    | 9176/17834 [4:40:17<4:15:47,  1.77s/it] 51%|█████▏    | 9177/17834 [4:40:19<4:17:02,  1.78s/it] 51%|█████▏    | 9178/17834 [4:40:21<4:12:36,  1.75s/it] 51%|█████▏    | 9179/17834 [4:40:23<4:15:55,  1.77s/it] 51%|█████▏    | 9180/17834 [4:40:24<4:14:45,  1.77s/it] 51%|█████▏    | 9181/17834 [4:40:26<4:13:40,  1.76s/it] 51%|█████▏    | 9182/17834 [4:40:28<4:19:36,  1.80s/it] 51%|█████▏    | 9183/17834 [4:40:30<4:18:13,  1.79s/it] 51%|█████▏    | 9184/17834 [4:40:32<4:16:10,  1.78s/it] 52%|█████▏    | 9185/17834 [4:40:33<4:15:10,  1.77s/it] 52%|█████▏    | 9186/17834 [4:40:35<4:11:23,  1.74s/it] 52%|█████▏    | 9187/17834 [4:40:37<4:11:18,  1.74s/it] 52%|█████▏    | 9188/17834 [4:40:39<4:13:52,  1.76s/it] 52%|█████▏    | 9189/17834 [4:40:40<4:17:21,  1.79s/it] 52%|█████▏    | 9190/17834 [4:40:42<4:15:04,  1.77s/it] 52%|█████▏    | 9191/17834 [4:40:44<4:14:14,  1.76s/it] 52%|█████▏    | 9192/17834 [4:40:46<4:11:36,  1.75s/it] 52%|█████▏    | 9193/17834 [4:40:47<4:11:19,  1.75s/it] 52%|█████▏    | 9194/17834 [4:40:49<4:12:12,  1.75s/it] 52%|█████▏    | 9195/17834 [4:40:51<4:11:02,  1.74s/it] 52%|█████▏    | 9196/17834 [4:40:53<4:13:00,  1.76s/it] 52%|█████▏    | 9197/17834 [4:40:54<4:13:20,  1.76s/it] 52%|█████▏    | 9198/17834 [4:40:56<4:11:34,  1.75s/it] 52%|█████▏    | 9199/17834 [4:40:58<4:11:25,  1.75s/it]08/30/2024 23:55:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0975136756896973, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03325912356376648, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.157287359237671, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.288060188293457}
 52%|█████▏    | 9200/17834 [4:41:00<4:08:42,  1.73s/it] 52%|█████▏    | 9201/17834 [4:41:01<4:09:37,  1.73s/it] 52%|█████▏    | 9202/17834 [4:41:03<4:12:10,  1.75s/it] 52%|█████▏    | 9203/17834 [4:41:05<4:11:00,  1.74s/it] 52%|█████▏    | 9204/17834 [4:41:07<4:10:56,  1.74s/it] 52%|█████▏    | 9205/17834 [4:41:08<4:09:27,  1.73s/it] 52%|█████▏    | 9206/17834 [4:41:10<4:15:59,  1.78s/it] 52%|█████▏    | 9207/17834 [4:41:12<4:13:22,  1.76s/it] 52%|█████▏    | 9208/17834 [4:41:14<4:18:39,  1.80s/it] 52%|█████▏    | 9209/17834 [4:41:16<4:15:02,  1.77s/it] 52%|█████▏    | 9210/17834 [4:41:17<4:15:44,  1.78s/it] 52%|█████▏    | 9211/17834 [4:41:19<4:13:27,  1.76s/it] 52%|█████▏    | 9212/17834 [4:41:21<4:12:56,  1.76s/it] 52%|█████▏    | 9213/17834 [4:41:22<4:10:33,  1.74s/it] 52%|█████▏    | 9214/17834 [4:41:24<4:11:40,  1.75s/it] 52%|█████▏    | 9215/17834 [4:41:26<4:12:31,  1.76s/it] 52%|█████▏    | 9216/17834 [4:41:28<4:09:09,  1.73s/it] 52%|█████▏    | 9217/17834 [4:41:29<4:11:07,  1.75s/it] 52%|█████▏    | 9218/17834 [4:41:31<4:09:30,  1.74s/it] 52%|█████▏    | 9219/17834 [4:41:33<4:06:33,  1.72s/it] 52%|█████▏    | 9220/17834 [4:41:35<4:08:08,  1.73s/it] 52%|█████▏    | 9221/17834 [4:41:36<4:07:07,  1.72s/it] 52%|█████▏    | 9222/17834 [4:41:38<4:07:19,  1.72s/it] 52%|█████▏    | 9223/17834 [4:41:40<4:08:10,  1.73s/it] 52%|█████▏    | 9224/17834 [4:41:42<4:09:30,  1.74s/it] 52%|█████▏    | 9225/17834 [4:41:43<4:10:18,  1.74s/it] 52%|█████▏    | 9226/17834 [4:41:45<4:12:07,  1.76s/it] 52%|█████▏    | 9227/17834 [4:41:47<4:09:54,  1.74s/it] 52%|█████▏    | 9228/17834 [4:41:49<4:09:31,  1.74s/it] 52%|█████▏    | 9229/17834 [4:41:50<4:08:10,  1.73s/it] 52%|█████▏    | 9230/17834 [4:41:52<4:12:30,  1.76s/it] 52%|█████▏    | 9231/17834 [4:41:54<4:14:20,  1.77s/it] 52%|█████▏    | 9232/17834 [4:41:56<4:14:46,  1.78s/it] 52%|█████▏    | 9233/17834 [4:41:57<4:11:34,  1.76s/it] 52%|█████▏    | 9234/17834 [4:41:59<4:11:01,  1.75s/it] 52%|█████▏    | 9235/17834 [4:42:01<4:12:42,  1.76s/it] 52%|█████▏    | 9236/17834 [4:42:03<4:10:46,  1.75s/it] 52%|█████▏    | 9237/17834 [4:42:04<4:07:32,  1.73s/it] 52%|█████▏    | 9238/17834 [4:42:06<4:10:04,  1.75s/it] 52%|█████▏    | 9239/17834 [4:42:08<4:08:23,  1.73s/it] 52%|█████▏    | 9240/17834 [4:42:10<4:10:19,  1.75s/it] 52%|█████▏    | 9241/17834 [4:42:11<4:10:07,  1.75s/it] 52%|█████▏    | 9242/17834 [4:42:13<4:12:31,  1.76s/it] 52%|█████▏    | 9243/17834 [4:42:15<4:15:24,  1.78s/it] 52%|█████▏    | 9244/17834 [4:42:17<4:15:03,  1.78s/it] 52%|█████▏    | 9245/17834 [4:42:18<4:12:09,  1.76s/it] 52%|█████▏    | 9246/17834 [4:42:20<4:09:28,  1.74s/it] 52%|█████▏    | 9247/17834 [4:42:22<4:10:22,  1.75s/it] 52%|█████▏    | 9248/17834 [4:42:24<4:08:01,  1.73s/it] 52%|█████▏    | 9249/17834 [4:42:25<4:11:32,  1.76s/it]08/30/2024 23:56:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3524456024169922, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03624961897730827, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.210026502609253, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.598721742630005}
 52%|█████▏    | 9250/17834 [4:42:27<4:13:04,  1.77s/it] 52%|█████▏    | 9251/17834 [4:42:29<4:12:35,  1.77s/it] 52%|█████▏    | 9252/17834 [4:42:31<4:10:41,  1.75s/it] 52%|█████▏    | 9253/17834 [4:42:33<4:16:02,  1.79s/it] 52%|█████▏    | 9254/17834 [4:42:34<4:14:20,  1.78s/it] 52%|█████▏    | 9255/17834 [4:42:36<4:11:36,  1.76s/it] 52%|█████▏    | 9256/17834 [4:42:38<4:09:48,  1.75s/it] 52%|█████▏    | 9257/17834 [4:42:40<4:08:55,  1.74s/it] 52%|█████▏    | 9258/17834 [4:42:41<4:10:04,  1.75s/it] 52%|█████▏    | 9259/17834 [4:42:43<4:10:50,  1.76s/it] 52%|█████▏    | 9260/17834 [4:42:45<4:10:37,  1.75s/it] 52%|█████▏    | 9261/17834 [4:42:46<4:08:27,  1.74s/it] 52%|█████▏    | 9262/17834 [4:42:48<4:14:32,  1.78s/it] 52%|█████▏    | 9263/17834 [4:42:50<4:14:04,  1.78s/it] 52%|█████▏    | 9264/17834 [4:42:52<4:10:27,  1.75s/it] 52%|█████▏    | 9265/17834 [4:42:54<4:12:04,  1.76s/it] 52%|█████▏    | 9266/17834 [4:42:55<4:08:58,  1.74s/it] 52%|█████▏    | 9267/17834 [4:42:57<4:07:30,  1.73s/it] 52%|█████▏    | 9268/17834 [4:42:59<4:09:37,  1.75s/it] 52%|█████▏    | 9269/17834 [4:43:01<4:11:29,  1.76s/it] 52%|█████▏    | 9270/17834 [4:43:02<4:11:31,  1.76s/it] 52%|█████▏    | 9271/17834 [4:43:04<4:12:27,  1.77s/it] 52%|█████▏    | 9272/17834 [4:43:06<4:14:23,  1.78s/it] 52%|█████▏    | 9273/17834 [4:43:08<4:13:36,  1.78s/it] 52%|█████▏    | 9274/17834 [4:43:09<4:12:15,  1.77s/it] 52%|█████▏    | 9275/17834 [4:43:11<4:17:15,  1.80s/it] 52%|█████▏    | 9276/17834 [4:43:13<4:17:46,  1.81s/it] 52%|█████▏    | 9277/17834 [4:43:15<4:11:59,  1.77s/it] 52%|█████▏    | 9278/17834 [4:43:17<4:11:04,  1.76s/it] 52%|█████▏    | 9279/17834 [4:43:18<4:07:16,  1.73s/it] 52%|█████▏    | 9280/17834 [4:43:20<4:07:19,  1.73s/it] 52%|█████▏    | 9281/17834 [4:43:22<4:08:31,  1.74s/it] 52%|█████▏    | 9282/17834 [4:43:23<4:07:01,  1.73s/it] 52%|█████▏    | 9283/17834 [4:43:25<4:05:10,  1.72s/it] 52%|█████▏    | 9284/17834 [4:43:27<4:06:43,  1.73s/it] 52%|█████▏    | 9285/17834 [4:43:29<4:07:31,  1.74s/it] 52%|█████▏    | 9286/17834 [4:43:30<4:04:30,  1.72s/it] 52%|█████▏    | 9287/17834 [4:43:32<4:04:30,  1.72s/it] 52%|█████▏    | 9288/17834 [4:43:34<4:02:38,  1.70s/it] 52%|█████▏    | 9289/17834 [4:43:36<4:09:20,  1.75s/it] 52%|█████▏    | 9290/17834 [4:43:37<4:10:13,  1.76s/it] 52%|█████▏    | 9291/17834 [4:43:39<4:11:02,  1.76s/it] 52%|█████▏    | 9292/17834 [4:43:41<4:08:49,  1.75s/it] 52%|█████▏    | 9293/17834 [4:43:43<4:08:48,  1.75s/it] 52%|█████▏    | 9294/17834 [4:43:44<4:11:36,  1.77s/it] 52%|█████▏    | 9295/17834 [4:43:46<4:10:14,  1.76s/it] 52%|█████▏    | 9296/17834 [4:43:48<4:10:02,  1.76s/it] 52%|█████▏    | 9297/17834 [4:43:50<4:11:03,  1.76s/it] 52%|█████▏    | 9298/17834 [4:43:51<4:11:25,  1.77s/it] 52%|█████▏    | 9299/17834 [4:43:53<4:12:05,  1.77s/it]08/30/2024 23:58:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9999593496322632, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03895201534032822, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1558289527893066, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1947402954101562}
 52%|█████▏    | 9300/17834 [4:43:55<4:12:44,  1.78s/it] 52%|█████▏    | 9301/17834 [4:43:57<4:10:24,  1.76s/it] 52%|█████▏    | 9302/17834 [4:43:59<4:09:33,  1.75s/it] 52%|█████▏    | 9303/17834 [4:44:00<4:09:04,  1.75s/it] 52%|█████▏    | 9304/17834 [4:44:02<4:08:55,  1.75s/it] 52%|█████▏    | 9305/17834 [4:44:04<4:10:01,  1.76s/it] 52%|█████▏    | 9306/17834 [4:44:06<4:09:12,  1.75s/it] 52%|█████▏    | 9307/17834 [4:44:07<4:05:42,  1.73s/it] 52%|█████▏    | 9308/17834 [4:44:09<4:06:28,  1.73s/it] 52%|█████▏    | 9309/17834 [4:44:11<4:08:47,  1.75s/it] 52%|█████▏    | 9310/17834 [4:44:13<4:12:31,  1.78s/it] 52%|█████▏    | 9311/17834 [4:44:14<4:08:30,  1.75s/it] 52%|█████▏    | 9312/17834 [4:44:16<4:08:11,  1.75s/it] 52%|█████▏    | 9313/17834 [4:44:18<4:06:56,  1.74s/it] 52%|█████▏    | 9314/17834 [4:44:19<4:08:20,  1.75s/it] 52%|█████▏    | 9315/17834 [4:44:21<4:06:24,  1.74s/it] 52%|█████▏    | 9316/17834 [4:44:23<4:08:14,  1.75s/it] 52%|█████▏    | 9317/17834 [4:44:25<4:08:19,  1.75s/it] 52%|█████▏    | 9318/17834 [4:44:27<4:09:28,  1.76s/it] 52%|█████▏    | 9319/17834 [4:44:28<4:09:31,  1.76s/it] 52%|█████▏    | 9320/17834 [4:44:30<4:09:46,  1.76s/it] 52%|█████▏    | 9321/17834 [4:44:32<4:07:35,  1.75s/it] 52%|█████▏    | 9322/17834 [4:44:33<4:08:16,  1.75s/it] 52%|█████▏    | 9323/17834 [4:44:35<4:09:15,  1.76s/it] 52%|█████▏    | 9324/17834 [4:44:37<4:09:05,  1.76s/it] 52%|█████▏    | 9325/17834 [4:44:39<4:13:15,  1.79s/it] 52%|█████▏    | 9326/17834 [4:44:41<4:14:03,  1.79s/it] 52%|█████▏    | 9327/17834 [4:44:42<4:11:08,  1.77s/it] 52%|█████▏    | 9328/17834 [4:44:44<4:12:07,  1.78s/it] 52%|█████▏    | 9329/17834 [4:44:46<4:10:15,  1.77s/it] 52%|█████▏    | 9330/17834 [4:44:48<4:10:12,  1.77s/it] 52%|█████▏    | 9331/17834 [4:44:49<4:09:10,  1.76s/it] 52%|█████▏    | 9332/17834 [4:44:51<4:10:45,  1.77s/it] 52%|█████▏    | 9333/17834 [4:44:53<4:09:48,  1.76s/it] 52%|█████▏    | 9334/17834 [4:44:55<4:07:13,  1.75s/it] 52%|█████▏    | 9335/17834 [4:44:56<4:04:50,  1.73s/it] 52%|█████▏    | 9336/17834 [4:44:58<4:06:37,  1.74s/it] 52%|█████▏    | 9337/17834 [4:45:00<4:06:25,  1.74s/it] 52%|█████▏    | 9338/17834 [4:45:02<4:07:19,  1.75s/it] 52%|█████▏    | 9339/17834 [4:45:03<4:07:57,  1.75s/it] 52%|█████▏    | 9340/17834 [4:45:05<4:05:49,  1.74s/it] 52%|█████▏    | 9341/17834 [4:45:07<4:08:00,  1.75s/it] 52%|█████▏    | 9342/17834 [4:45:09<4:09:26,  1.76s/it] 52%|█████▏    | 9343/17834 [4:45:10<4:10:29,  1.77s/it] 52%|█████▏    | 9344/17834 [4:45:12<4:12:30,  1.78s/it] 52%|█████▏    | 9345/17834 [4:45:14<4:13:52,  1.79s/it] 52%|█████▏    | 9346/17834 [4:45:16<4:15:11,  1.80s/it] 52%|█████▏    | 9347/17834 [4:45:18<4:12:55,  1.79s/it] 52%|█████▏    | 9348/17834 [4:45:19<4:10:30,  1.77s/it] 52%|█████▏    | 9349/17834 [4:45:21<4:09:08,  1.76s/it]08/30/2024 23:59:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3124799728393555, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.040302716195583344, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.250040054321289, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.602822780609131}
 52%|█████▏    | 9350/17834 [4:45:23<4:12:18,  1.78s/it] 52%|█████▏    | 9351/17834 [4:45:25<4:13:13,  1.79s/it] 52%|█████▏    | 9352/17834 [4:45:27<4:10:11,  1.77s/it] 52%|█████▏    | 9353/17834 [4:45:28<4:10:52,  1.77s/it] 52%|█████▏    | 9354/17834 [4:45:30<4:09:40,  1.77s/it] 52%|█████▏    | 9355/17834 [4:45:32<4:08:12,  1.76s/it] 52%|█████▏    | 9356/17834 [4:45:34<4:07:58,  1.75s/it] 52%|█████▏    | 9357/17834 [4:45:35<4:04:49,  1.73s/it] 52%|█████▏    | 9358/17834 [4:45:37<4:07:20,  1.75s/it] 52%|█████▏    | 9359/17834 [4:45:39<4:10:21,  1.77s/it] 52%|█████▏    | 9360/17834 [4:45:41<4:05:37,  1.74s/it] 52%|█████▏    | 9361/17834 [4:45:42<4:03:45,  1.73s/it] 52%|█████▏    | 9362/17834 [4:45:44<4:07:26,  1.75s/it] 53%|█████▎    | 9363/17834 [4:45:46<4:04:59,  1.74s/it] 53%|█████▎    | 9364/17834 [4:45:47<4:03:00,  1.72s/it] 53%|█████▎    | 9365/17834 [4:45:49<4:10:48,  1.78s/it] 53%|█████▎    | 9366/17834 [4:45:51<4:07:49,  1.76s/it] 53%|█████▎    | 9367/17834 [4:45:53<4:07:54,  1.76s/it] 53%|█████▎    | 9368/17834 [4:45:55<4:08:49,  1.76s/it] 53%|█████▎    | 9369/17834 [4:45:56<4:07:40,  1.76s/it] 53%|█████▎    | 9370/17834 [4:45:58<4:04:51,  1.74s/it] 53%|█████▎    | 9371/17834 [4:46:00<4:06:01,  1.74s/it] 53%|█████▎    | 9372/17834 [4:46:02<4:09:00,  1.77s/it] 53%|█████▎    | 9373/17834 [4:46:03<4:07:41,  1.76s/it] 53%|█████▎    | 9374/17834 [4:46:05<4:09:37,  1.77s/it] 53%|█████▎    | 9375/17834 [4:46:07<4:06:51,  1.75s/it] 53%|█████▎    | 9376/17834 [4:46:09<4:06:11,  1.75s/it] 53%|█████▎    | 9377/17834 [4:46:10<4:03:51,  1.73s/it] 53%|█████▎    | 9378/17834 [4:46:12<4:04:12,  1.73s/it] 53%|█████▎    | 9379/17834 [4:46:14<4:06:30,  1.75s/it] 53%|█████▎    | 9380/17834 [4:46:16<4:10:00,  1.77s/it] 53%|█████▎    | 9381/17834 [4:46:17<4:06:31,  1.75s/it] 53%|█████▎    | 9382/17834 [4:46:19<4:09:18,  1.77s/it] 53%|█████▎    | 9383/17834 [4:46:21<4:10:22,  1.78s/it] 53%|█████▎    | 9384/17834 [4:46:23<4:10:51,  1.78s/it] 53%|█████▎    | 9385/17834 [4:46:24<4:10:27,  1.78s/it] 53%|█████▎    | 9386/17834 [4:46:26<4:09:13,  1.77s/it] 53%|█████▎    | 9387/17834 [4:46:28<4:06:39,  1.75s/it] 53%|█████▎    | 9388/17834 [4:46:30<4:04:18,  1.74s/it] 53%|█████▎    | 9389/17834 [4:46:31<4:09:32,  1.77s/it] 53%|█████▎    | 9390/17834 [4:46:33<4:06:15,  1.75s/it] 53%|█████▎    | 9391/17834 [4:46:35<4:05:03,  1.74s/it] 53%|█████▎    | 9392/17834 [4:46:37<4:04:41,  1.74s/it] 53%|█████▎    | 9393/17834 [4:46:38<4:07:40,  1.76s/it] 53%|█████▎    | 9394/17834 [4:46:40<4:05:44,  1.75s/it] 53%|█████▎    | 9395/17834 [4:46:42<4:07:01,  1.76s/it] 53%|█████▎    | 9396/17834 [4:46:44<4:05:17,  1.74s/it] 53%|█████▎    | 9397/17834 [4:46:45<4:05:42,  1.75s/it] 53%|█████▎    | 9398/17834 [4:46:47<4:06:48,  1.76s/it] 53%|█████▎    | 9399/17834 [4:46:49<4:04:40,  1.74s/it]08/31/2024 00:01:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.170154333114624, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.047437116503715515, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2041707038879395, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.421761989593506}
 53%|█████▎    | 9400/17834 [4:46:51<4:05:16,  1.74s/it] 53%|█████▎    | 9401/17834 [4:46:53<4:10:47,  1.78s/it] 53%|█████▎    | 9402/17834 [4:46:54<4:04:53,  1.74s/it] 53%|█████▎    | 9403/17834 [4:46:56<4:06:57,  1.76s/it] 53%|█████▎    | 9404/17834 [4:46:58<4:11:40,  1.79s/it] 53%|█████▎    | 9405/17834 [4:47:00<4:07:17,  1.76s/it] 53%|█████▎    | 9406/17834 [4:47:01<4:05:44,  1.75s/it] 53%|█████▎    | 9407/17834 [4:47:03<4:06:10,  1.75s/it] 53%|█████▎    | 9408/17834 [4:47:05<4:07:21,  1.76s/it] 53%|█████▎    | 9409/17834 [4:47:06<4:05:54,  1.75s/it] 53%|█████▎    | 9410/17834 [4:47:08<4:06:09,  1.75s/it] 53%|█████▎    | 9411/17834 [4:47:10<4:06:34,  1.76s/it] 53%|█████▎    | 9412/17834 [4:47:12<4:08:59,  1.77s/it] 53%|█████▎    | 9413/17834 [4:47:14<4:09:21,  1.78s/it] 53%|█████▎    | 9414/17834 [4:47:15<4:07:08,  1.76s/it] 53%|█████▎    | 9415/17834 [4:47:17<4:09:38,  1.78s/it] 53%|█████▎    | 9416/17834 [4:47:19<4:06:43,  1.76s/it] 53%|█████▎    | 9417/17834 [4:47:21<4:04:25,  1.74s/it] 53%|█████▎    | 9418/17834 [4:47:22<4:03:21,  1.73s/it] 53%|█████▎    | 9419/17834 [4:47:24<4:04:04,  1.74s/it] 53%|█████▎    | 9420/17834 [4:47:26<4:02:45,  1.73s/it] 53%|█████▎    | 9421/17834 [4:47:28<4:06:18,  1.76s/it] 53%|█████▎    | 9422/17834 [4:47:29<4:05:43,  1.75s/it] 53%|█████▎    | 9423/17834 [4:47:31<4:09:30,  1.78s/it] 53%|█████▎    | 9424/17834 [4:47:33<4:06:37,  1.76s/it] 53%|█████▎    | 9425/17834 [4:47:35<4:04:30,  1.74s/it] 53%|█████▎    | 9426/17834 [4:47:36<4:05:02,  1.75s/it] 53%|█████▎    | 9427/17834 [4:47:38<4:05:36,  1.75s/it] 53%|█████▎    | 9428/17834 [4:47:40<4:04:21,  1.74s/it] 53%|█████▎    | 9429/17834 [4:47:42<4:04:31,  1.75s/it] 53%|█████▎    | 9430/17834 [4:47:43<4:03:08,  1.74s/it] 53%|█████▎    | 9431/17834 [4:47:45<4:02:19,  1.73s/it] 53%|█████▎    | 9432/17834 [4:47:47<4:02:40,  1.73s/it] 53%|█████▎    | 9433/17834 [4:47:49<4:09:45,  1.78s/it] 53%|█████▎    | 9434/17834 [4:47:50<4:05:44,  1.76s/it] 53%|█████▎    | 9435/17834 [4:47:52<4:02:27,  1.73s/it] 53%|█████▎    | 9436/17834 [4:47:54<3:59:32,  1.71s/it] 53%|█████▎    | 9437/17834 [4:47:56<4:05:14,  1.75s/it] 53%|█████▎    | 9438/17834 [4:47:57<4:03:12,  1.74s/it] 53%|█████▎    | 9439/17834 [4:47:59<4:04:16,  1.75s/it] 53%|█████▎    | 9440/17834 [4:48:01<4:06:19,  1.76s/it] 53%|█████▎    | 9441/17834 [4:48:03<4:04:57,  1.75s/it] 53%|█████▎    | 9442/17834 [4:48:04<4:05:48,  1.76s/it] 53%|█████▎    | 9443/17834 [4:48:06<4:06:38,  1.76s/it] 53%|█████▎    | 9444/17834 [4:48:08<4:03:26,  1.74s/it] 53%|█████▎    | 9445/17834 [4:48:10<4:03:32,  1.74s/it] 53%|█████▎    | 9446/17834 [4:48:11<4:04:52,  1.75s/it] 53%|█████▎    | 9447/17834 [4:48:13<4:04:17,  1.75s/it] 53%|█████▎    | 9448/17834 [4:48:15<4:02:01,  1.73s/it] 53%|█████▎    | 9449/17834 [4:48:16<4:03:04,  1.74s/it]08/31/2024 00:02:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9757866859436035, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02479786053299904, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1679635047912598, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.168548107147217}
 53%|█████▎    | 9450/17834 [4:48:18<4:05:06,  1.75s/it] 53%|█████▎    | 9451/17834 [4:48:20<4:04:35,  1.75s/it] 53%|█████▎    | 9452/17834 [4:48:22<4:03:57,  1.75s/it] 53%|█████▎    | 9453/17834 [4:48:23<4:02:49,  1.74s/it] 53%|█████▎    | 9454/17834 [4:48:25<4:02:42,  1.74s/it] 53%|█████▎    | 9455/17834 [4:48:27<4:05:24,  1.76s/it] 53%|█████▎    | 9456/17834 [4:48:29<4:05:09,  1.76s/it] 53%|█████▎    | 9457/17834 [4:48:31<4:05:58,  1.76s/it] 53%|█████▎    | 9458/17834 [4:48:32<4:03:39,  1.75s/it] 53%|█████▎    | 9459/17834 [4:48:34<4:04:39,  1.75s/it] 53%|█████▎    | 9460/17834 [4:48:36<4:08:15,  1.78s/it] 53%|█████▎    | 9461/17834 [4:48:38<4:08:04,  1.78s/it] 53%|█████▎    | 9462/17834 [4:48:39<4:04:49,  1.75s/it] 53%|█████▎    | 9463/17834 [4:48:41<4:03:08,  1.74s/it] 53%|█████▎    | 9464/17834 [4:48:43<4:02:46,  1.74s/it] 53%|█████▎    | 9465/17834 [4:48:45<4:03:11,  1.74s/it] 53%|█████▎    | 9466/17834 [4:48:46<4:04:28,  1.75s/it] 53%|█████▎    | 9467/17834 [4:48:48<4:06:45,  1.77s/it] 53%|█████▎    | 9468/17834 [4:48:50<4:11:11,  1.80s/it] 53%|█████▎    | 9469/17834 [4:48:52<4:08:48,  1.78s/it] 53%|█████▎    | 9470/17834 [4:48:53<4:05:58,  1.76s/it] 53%|█████▎    | 9471/17834 [4:48:55<4:06:05,  1.77s/it] 53%|█████▎    | 9472/17834 [4:48:57<4:04:33,  1.75s/it] 53%|█████▎    | 9473/17834 [4:48:59<4:03:33,  1.75s/it] 53%|█████▎    | 9474/17834 [4:49:00<4:06:25,  1.77s/it] 53%|█████▎    | 9475/17834 [4:49:02<4:06:37,  1.77s/it] 53%|█████▎    | 9476/17834 [4:49:04<4:05:26,  1.76s/it] 53%|█████▎    | 9477/17834 [4:49:06<4:03:45,  1.75s/it] 53%|█████▎    | 9478/17834 [4:49:07<4:04:06,  1.75s/it] 53%|█████▎    | 9479/17834 [4:49:09<4:02:43,  1.74s/it] 53%|█████▎    | 9480/17834 [4:49:11<4:00:37,  1.73s/it] 53%|█████▎    | 9481/17834 [4:49:13<3:59:31,  1.72s/it] 53%|█████▎    | 9482/17834 [4:49:14<4:00:46,  1.73s/it] 53%|█████▎    | 9483/17834 [4:49:16<4:01:26,  1.73s/it] 53%|█████▎    | 9484/17834 [4:49:18<4:02:29,  1.74s/it] 53%|█████▎    | 9485/17834 [4:49:20<4:03:50,  1.75s/it] 53%|█████▎    | 9486/17834 [4:49:21<4:02:03,  1.74s/it] 53%|█████▎    | 9487/17834 [4:49:23<4:03:59,  1.75s/it] 53%|█████▎    | 9488/17834 [4:49:25<4:02:48,  1.75s/it] 53%|█████▎    | 9489/17834 [4:49:27<4:03:22,  1.75s/it] 53%|█████▎    | 9490/17834 [4:49:28<4:01:51,  1.74s/it] 53%|█████▎    | 9491/17834 [4:49:30<3:58:33,  1.72s/it] 53%|█████▎    | 9492/17834 [4:49:32<4:03:25,  1.75s/it] 53%|█████▎    | 9493/17834 [4:49:34<4:00:50,  1.73s/it] 53%|█████▎    | 9494/17834 [4:49:35<4:02:09,  1.74s/it] 53%|█████▎    | 9495/17834 [4:49:37<4:03:19,  1.75s/it] 53%|█████▎    | 9496/17834 [4:49:39<4:02:14,  1.74s/it] 53%|█████▎    | 9497/17834 [4:49:40<3:59:35,  1.72s/it] 53%|█████▎    | 9498/17834 [4:49:42<3:58:35,  1.72s/it] 53%|█████▎    | 9499/17834 [4:49:44<3:59:59,  1.73s/it]08/31/2024 00:04:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8902977705001831, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02805502526462078, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1579811573028564, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.076333999633789}
 53%|█████▎    | 9500/17834 [4:49:46<4:05:02,  1.76s/it] 53%|█████▎    | 9501/17834 [4:49:48<4:08:38,  1.79s/it] 53%|█████▎    | 9502/17834 [4:49:49<4:04:23,  1.76s/it] 53%|█████▎    | 9503/17834 [4:49:51<4:03:57,  1.76s/it] 53%|█████▎    | 9504/17834 [4:49:53<4:01:26,  1.74s/it] 53%|█████▎    | 9505/17834 [4:49:54<4:01:22,  1.74s/it] 53%|█████▎    | 9506/17834 [4:49:56<3:59:27,  1.73s/it] 53%|█████▎    | 9507/17834 [4:49:58<4:00:25,  1.73s/it] 53%|█████▎    | 9508/17834 [4:50:00<4:10:27,  1.80s/it] 53%|█████▎    | 9509/17834 [4:50:02<4:04:16,  1.76s/it] 53%|█████▎    | 9510/17834 [4:50:03<4:03:22,  1.75s/it] 53%|█████▎    | 9511/17834 [4:50:05<4:04:17,  1.76s/it] 53%|█████▎    | 9512/17834 [4:50:07<4:04:07,  1.76s/it] 53%|█████▎    | 9513/17834 [4:50:09<4:04:08,  1.76s/it] 53%|█████▎    | 9514/17834 [4:50:10<4:04:32,  1.76s/it] 53%|█████▎    | 9515/17834 [4:50:12<4:01:38,  1.74s/it] 53%|█████▎    | 9516/17834 [4:50:14<4:02:43,  1.75s/it] 53%|█████▎    | 9517/17834 [4:50:15<3:58:50,  1.72s/it] 53%|█████▎    | 9518/17834 [4:50:17<3:58:51,  1.72s/it] 53%|█████▎    | 9519/17834 [4:50:19<3:57:23,  1.71s/it] 53%|█████▎    | 9520/17834 [4:50:21<4:00:10,  1.73s/it] 53%|█████▎    | 9521/17834 [4:50:23<4:10:28,  1.81s/it] 53%|█████▎    | 9522/17834 [4:50:24<4:04:46,  1.77s/it] 53%|█████▎    | 9523/17834 [4:50:26<4:05:39,  1.77s/it] 53%|█████▎    | 9524/17834 [4:50:28<4:02:42,  1.75s/it] 53%|█████▎    | 9525/17834 [4:50:30<4:01:04,  1.74s/it] 53%|█████▎    | 9526/17834 [4:50:31<3:59:38,  1.73s/it] 53%|█████▎    | 9527/17834 [4:50:33<3:58:37,  1.72s/it] 53%|█████▎    | 9528/17834 [4:50:35<3:59:28,  1.73s/it] 53%|█████▎    | 9529/17834 [4:50:36<3:59:41,  1.73s/it] 53%|█████▎    | 9530/17834 [4:50:38<4:00:35,  1.74s/it] 53%|█████▎    | 9531/17834 [4:50:40<4:00:40,  1.74s/it] 53%|█████▎    | 9532/17834 [4:50:42<3:58:39,  1.72s/it] 53%|█████▎    | 9533/17834 [4:50:43<3:55:41,  1.70s/it] 53%|█████▎    | 9534/17834 [4:50:45<3:57:20,  1.72s/it] 53%|█████▎    | 9535/17834 [4:50:47<3:58:10,  1.72s/it] 53%|█████▎    | 9536/17834 [4:50:48<3:58:04,  1.72s/it] 53%|█████▎    | 9537/17834 [4:50:50<3:58:51,  1.73s/it] 53%|█████▎    | 9538/17834 [4:50:52<4:00:41,  1.74s/it] 53%|█████▎    | 9539/17834 [4:50:54<4:03:31,  1.76s/it] 53%|█████▎    | 9540/17834 [4:50:56<4:02:45,  1.76s/it] 53%|█████▎    | 9541/17834 [4:50:57<4:02:55,  1.76s/it] 54%|█████▎    | 9542/17834 [4:50:59<4:00:56,  1.74s/it] 54%|█████▎    | 9543/17834 [4:51:01<3:59:26,  1.73s/it] 54%|█████▎    | 9544/17834 [4:51:02<3:58:37,  1.73s/it] 54%|█████▎    | 9545/17834 [4:51:04<3:56:59,  1.72s/it] 54%|█████▎    | 9546/17834 [4:51:06<3:59:35,  1.73s/it] 54%|█████▎    | 9547/17834 [4:51:08<3:59:34,  1.73s/it] 54%|█████▎    | 9548/17834 [4:51:09<4:00:09,  1.74s/it] 54%|█████▎    | 9549/17834 [4:51:11<4:00:45,  1.74s/it]08/31/2024 00:05:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1473486423492432, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030643951147794724, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.232011079788208, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.410003662109375}
 54%|█████▎    | 9550/17834 [4:51:13<4:00:25,  1.74s/it] 54%|█████▎    | 9551/17834 [4:51:15<4:00:34,  1.74s/it] 54%|█████▎    | 9552/17834 [4:51:16<3:59:32,  1.74s/it] 54%|█████▎    | 9553/17834 [4:51:18<3:57:24,  1.72s/it] 54%|█████▎    | 9554/17834 [4:51:20<4:04:29,  1.77s/it] 54%|█████▎    | 9555/17834 [4:51:22<4:04:07,  1.77s/it] 54%|█████▎    | 9556/17834 [4:51:23<4:04:05,  1.77s/it] 54%|█████▎    | 9557/17834 [4:51:25<4:00:12,  1.74s/it] 54%|█████▎    | 9558/17834 [4:51:27<4:01:27,  1.75s/it] 54%|█████▎    | 9559/17834 [4:51:29<4:02:20,  1.76s/it] 54%|█████▎    | 9560/17834 [4:51:30<4:01:37,  1.75s/it] 54%|█████▎    | 9561/17834 [4:51:32<4:01:24,  1.75s/it] 54%|█████▎    | 9562/17834 [4:51:34<4:08:32,  1.80s/it] 54%|█████▎    | 9563/17834 [4:51:36<4:03:30,  1.77s/it] 54%|█████▎    | 9564/17834 [4:51:37<4:01:14,  1.75s/it] 54%|█████▎    | 9565/17834 [4:51:39<4:02:15,  1.76s/it] 54%|█████▎    | 9566/17834 [4:51:41<4:01:00,  1.75s/it] 54%|█████▎    | 9567/17834 [4:51:43<4:01:32,  1.75s/it] 54%|█████▎    | 9568/17834 [4:51:45<4:02:42,  1.76s/it] 54%|█████▎    | 9569/17834 [4:51:46<4:03:36,  1.77s/it] 54%|█████▎    | 9570/17834 [4:51:48<4:02:15,  1.76s/it] 54%|█████▎    | 9571/17834 [4:51:50<4:01:13,  1.75s/it] 54%|█████▎    | 9572/17834 [4:51:52<4:02:27,  1.76s/it] 54%|█████▎    | 9573/17834 [4:51:53<4:03:25,  1.77s/it] 54%|█████▎    | 9574/17834 [4:51:55<4:02:17,  1.76s/it] 54%|█████▎    | 9575/17834 [4:51:57<3:59:56,  1.74s/it] 54%|█████▎    | 9576/17834 [4:51:59<4:00:17,  1.75s/it] 54%|█████▎    | 9577/17834 [4:52:00<3:58:58,  1.74s/it] 54%|█████▎    | 9578/17834 [4:52:02<3:57:49,  1.73s/it] 54%|█████▎    | 9579/17834 [4:52:04<3:59:44,  1.74s/it] 54%|█████▎    | 9580/17834 [4:52:06<4:03:22,  1.77s/it] 54%|█████▎    | 9581/17834 [4:52:07<4:00:59,  1.75s/it] 54%|█████▎    | 9582/17834 [4:52:09<4:02:58,  1.77s/it] 54%|█████▎    | 9583/17834 [4:52:11<4:00:56,  1.75s/it] 54%|█████▎    | 9584/17834 [4:52:12<3:58:05,  1.73s/it] 54%|█████▎    | 9585/17834 [4:52:14<3:58:40,  1.74s/it] 54%|█████▍    | 9586/17834 [4:52:16<4:00:11,  1.75s/it] 54%|█████▍    | 9587/17834 [4:52:18<3:58:31,  1.74s/it] 54%|█████▍    | 9588/17834 [4:52:19<3:59:30,  1.74s/it] 54%|█████▍    | 9589/17834 [4:52:21<4:02:29,  1.76s/it] 54%|█████▍    | 9590/17834 [4:52:23<3:58:59,  1.74s/it] 54%|█████▍    | 9591/17834 [4:52:25<4:02:20,  1.76s/it] 54%|█████▍    | 9592/17834 [4:52:27<4:01:25,  1.76s/it] 54%|█████▍    | 9593/17834 [4:52:28<4:01:50,  1.76s/it] 54%|█████▍    | 9594/17834 [4:52:30<4:01:39,  1.76s/it] 54%|█████▍    | 9595/17834 [4:52:32<4:01:13,  1.76s/it] 54%|█████▍    | 9596/17834 [4:52:34<4:03:32,  1.77s/it] 54%|█████▍    | 9597/17834 [4:52:35<4:01:25,  1.76s/it] 54%|█████▍    | 9598/17834 [4:52:37<4:03:47,  1.78s/it] 54%|█████▍    | 9599/17834 [4:52:39<4:05:29,  1.79s/it]08/31/2024 00:06:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3871179819107056, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.047870367765426636, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.356962203979492, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7919507026672363}
 54%|█████▍    | 9600/17834 [4:52:41<4:00:09,  1.75s/it] 54%|█████▍    | 9601/17834 [4:52:42<3:58:12,  1.74s/it] 54%|█████▍    | 9602/17834 [4:52:44<4:00:47,  1.76s/it] 54%|█████▍    | 9603/17834 [4:52:46<4:03:56,  1.78s/it] 54%|█████▍    | 9604/17834 [4:52:48<4:03:38,  1.78s/it] 54%|█████▍    | 9605/17834 [4:52:50<4:08:03,  1.81s/it] 54%|█████▍    | 9606/17834 [4:52:51<4:04:51,  1.79s/it] 54%|█████▍    | 9607/17834 [4:52:53<4:01:55,  1.76s/it] 54%|█████▍    | 9608/17834 [4:52:55<4:02:01,  1.77s/it] 54%|█████▍    | 9609/17834 [4:52:57<3:59:39,  1.75s/it] 54%|█████▍    | 9610/17834 [4:52:58<3:59:41,  1.75s/it] 54%|█████▍    | 9611/17834 [4:53:00<4:00:07,  1.75s/it] 54%|█████▍    | 9612/17834 [4:53:02<3:58:59,  1.74s/it] 54%|█████▍    | 9613/17834 [4:53:04<3:58:26,  1.74s/it] 54%|█████▍    | 9614/17834 [4:53:05<3:58:22,  1.74s/it] 54%|█████▍    | 9615/17834 [4:53:07<3:57:17,  1.73s/it] 54%|█████▍    | 9616/17834 [4:53:09<4:02:23,  1.77s/it] 54%|█████▍    | 9617/17834 [4:53:11<4:02:19,  1.77s/it] 54%|█████▍    | 9618/17834 [4:53:12<3:59:48,  1.75s/it] 54%|█████▍    | 9619/17834 [4:53:14<4:00:42,  1.76s/it] 54%|█████▍    | 9620/17834 [4:53:16<4:00:02,  1.75s/it] 54%|█████▍    | 9621/17834 [4:53:18<4:02:43,  1.77s/it] 54%|█████▍    | 9622/17834 [4:53:19<3:58:58,  1.75s/it] 54%|█████▍    | 9623/17834 [4:53:21<4:00:27,  1.76s/it] 54%|█████▍    | 9624/17834 [4:53:23<4:00:52,  1.76s/it] 54%|█████▍    | 9625/17834 [4:53:25<4:04:24,  1.79s/it] 54%|█████▍    | 9626/17834 [4:53:26<3:59:52,  1.75s/it] 54%|█████▍    | 9627/17834 [4:53:28<4:01:43,  1.77s/it] 54%|█████▍    | 9628/17834 [4:53:30<4:07:58,  1.81s/it] 54%|█████▍    | 9629/17834 [4:53:32<4:06:13,  1.80s/it] 54%|█████▍    | 9630/17834 [4:53:34<4:06:15,  1.80s/it] 54%|█████▍    | 9631/17834 [4:53:35<4:03:11,  1.78s/it] 54%|█████▍    | 9632/17834 [4:53:37<4:02:30,  1.77s/it] 54%|█████▍    | 9633/17834 [4:53:39<3:59:16,  1.75s/it] 54%|█████▍    | 9634/17834 [4:53:41<4:03:40,  1.78s/it] 54%|█████▍    | 9635/17834 [4:53:43<4:04:32,  1.79s/it] 54%|█████▍    | 9636/17834 [4:53:44<4:05:24,  1.80s/it] 54%|█████▍    | 9637/17834 [4:53:46<4:11:19,  1.84s/it] 54%|█████▍    | 9638/17834 [4:53:48<4:06:08,  1.80s/it] 54%|█████▍    | 9639/17834 [4:53:50<4:03:02,  1.78s/it] 54%|█████▍    | 9640/17834 [4:53:52<4:04:16,  1.79s/it] 54%|█████▍    | 9641/17834 [4:53:53<4:02:59,  1.78s/it] 54%|█████▍    | 9642/17834 [4:53:55<3:58:58,  1.75s/it] 54%|█████▍    | 9643/17834 [4:53:57<3:58:38,  1.75s/it] 54%|█████▍    | 9644/17834 [4:53:58<3:57:27,  1.74s/it] 54%|█████▍    | 9645/17834 [4:54:00<3:58:44,  1.75s/it] 54%|█████▍    | 9646/17834 [4:54:02<4:02:01,  1.77s/it] 54%|█████▍    | 9647/17834 [4:54:04<4:02:34,  1.78s/it] 54%|█████▍    | 9648/17834 [4:54:06<3:59:14,  1.75s/it] 54%|█████▍    | 9649/17834 [4:54:07<3:59:15,  1.75s/it]08/31/2024 00:08:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1325719356536865, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02581818401813507, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2358477115631104, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.394237995147705}
 54%|█████▍    | 9650/17834 [4:54:09<3:57:26,  1.74s/it] 54%|█████▍    | 9651/17834 [4:54:11<3:56:03,  1.73s/it] 54%|█████▍    | 9652/17834 [4:54:12<3:57:41,  1.74s/it] 54%|█████▍    | 9653/17834 [4:54:14<3:57:08,  1.74s/it] 54%|█████▍    | 9654/17834 [4:54:16<3:55:43,  1.73s/it] 54%|█████▍    | 9655/17834 [4:54:18<3:57:53,  1.75s/it] 54%|█████▍    | 9656/17834 [4:54:19<3:58:20,  1.75s/it] 54%|█████▍    | 9657/17834 [4:54:21<3:58:08,  1.75s/it] 54%|█████▍    | 9658/17834 [4:54:23<3:56:23,  1.73s/it] 54%|█████▍    | 9659/17834 [4:54:25<3:59:40,  1.76s/it] 54%|█████▍    | 9660/17834 [4:54:26<4:00:05,  1.76s/it] 54%|█████▍    | 9661/17834 [4:54:28<3:58:13,  1.75s/it] 54%|█████▍    | 9662/17834 [4:54:30<3:57:24,  1.74s/it] 54%|█████▍    | 9663/17834 [4:54:32<3:57:13,  1.74s/it] 54%|█████▍    | 9664/17834 [4:54:34<4:00:26,  1.77s/it] 54%|█████▍    | 9665/17834 [4:54:35<3:58:13,  1.75s/it] 54%|█████▍    | 9666/17834 [4:54:37<3:59:04,  1.76s/it] 54%|█████▍    | 9667/17834 [4:54:39<3:57:46,  1.75s/it] 54%|█████▍    | 9668/17834 [4:54:40<3:58:57,  1.76s/it] 54%|█████▍    | 9669/17834 [4:54:42<3:59:32,  1.76s/it] 54%|█████▍    | 9670/17834 [4:54:44<4:01:30,  1.77s/it] 54%|█████▍    | 9671/17834 [4:54:46<4:01:10,  1.77s/it] 54%|█████▍    | 9672/17834 [4:54:48<3:59:04,  1.76s/it] 54%|█████▍    | 9673/17834 [4:54:49<3:59:39,  1.76s/it] 54%|█████▍    | 9674/17834 [4:54:51<3:58:12,  1.75s/it] 54%|█████▍    | 9675/17834 [4:54:53<3:56:35,  1.74s/it] 54%|█████▍    | 9676/17834 [4:54:55<3:59:36,  1.76s/it] 54%|█████▍    | 9677/17834 [4:54:56<3:56:06,  1.74s/it] 54%|█████▍    | 9678/17834 [4:54:58<3:58:42,  1.76s/it] 54%|█████▍    | 9679/17834 [4:55:00<3:54:47,  1.73s/it] 54%|█████▍    | 9680/17834 [4:55:01<3:55:01,  1.73s/it] 54%|█████▍    | 9681/17834 [4:55:03<3:56:00,  1.74s/it] 54%|█████▍    | 9682/17834 [4:55:05<3:56:22,  1.74s/it] 54%|█████▍    | 9683/17834 [4:55:07<3:58:02,  1.75s/it] 54%|█████▍    | 9684/17834 [4:55:09<3:58:43,  1.76s/it] 54%|█████▍    | 9685/17834 [4:55:10<3:56:16,  1.74s/it] 54%|█████▍    | 9686/17834 [4:55:12<3:56:35,  1.74s/it] 54%|█████▍    | 9687/17834 [4:55:14<3:55:29,  1.73s/it] 54%|█████▍    | 9688/17834 [4:55:15<3:57:44,  1.75s/it] 54%|█████▍    | 9689/17834 [4:55:17<3:56:09,  1.74s/it] 54%|█████▍    | 9690/17834 [4:55:19<3:59:04,  1.76s/it] 54%|█████▍    | 9691/17834 [4:55:21<3:56:07,  1.74s/it] 54%|█████▍    | 9692/17834 [4:55:22<3:54:38,  1.73s/it] 54%|█████▍    | 9693/17834 [4:55:24<3:55:14,  1.73s/it] 54%|█████▍    | 9694/17834 [4:55:26<3:54:38,  1.73s/it] 54%|█████▍    | 9695/17834 [4:55:28<3:57:15,  1.75s/it] 54%|█████▍    | 9696/17834 [4:55:29<3:56:22,  1.74s/it] 54%|█████▍    | 9697/17834 [4:55:31<3:53:20,  1.72s/it] 54%|█████▍    | 9698/17834 [4:55:33<3:54:51,  1.73s/it] 54%|█████▍    | 9699/17834 [4:55:35<3:57:43,  1.75s/it]08/31/2024 00:09:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0056769847869873, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.033118173480033875, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.18086314201355, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.219658374786377}
 54%|█████▍    | 9700/17834 [4:55:36<3:59:01,  1.76s/it] 54%|█████▍    | 9701/17834 [4:55:38<3:57:32,  1.75s/it] 54%|█████▍    | 9702/17834 [4:55:40<3:55:53,  1.74s/it] 54%|█████▍    | 9703/17834 [4:55:42<3:57:11,  1.75s/it] 54%|█████▍    | 9704/17834 [4:55:43<3:58:31,  1.76s/it] 54%|█████▍    | 9705/17834 [4:55:45<3:55:40,  1.74s/it] 54%|█████▍    | 9706/17834 [4:55:47<3:52:59,  1.72s/it] 54%|█████▍    | 9707/17834 [4:55:49<3:55:41,  1.74s/it] 54%|█████▍    | 9708/17834 [4:55:50<3:56:42,  1.75s/it] 54%|█████▍    | 9709/17834 [4:55:52<3:59:50,  1.77s/it] 54%|█████▍    | 9710/17834 [4:55:54<3:58:02,  1.76s/it] 54%|█████▍    | 9711/17834 [4:55:56<3:59:23,  1.77s/it] 54%|█████▍    | 9712/17834 [4:55:57<3:57:12,  1.75s/it] 54%|█████▍    | 9713/17834 [4:55:59<3:55:55,  1.74s/it] 54%|█████▍    | 9714/17834 [4:56:01<3:57:23,  1.75s/it] 54%|█████▍    | 9715/17834 [4:56:03<3:56:39,  1.75s/it] 54%|█████▍    | 9716/17834 [4:56:04<3:56:56,  1.75s/it] 54%|█████▍    | 9717/17834 [4:56:06<3:58:08,  1.76s/it] 54%|█████▍    | 9718/17834 [4:56:08<3:56:40,  1.75s/it] 54%|█████▍    | 9719/17834 [4:56:10<3:57:17,  1.75s/it] 55%|█████▍    | 9720/17834 [4:56:11<3:58:24,  1.76s/it] 55%|█████▍    | 9721/17834 [4:56:13<3:55:41,  1.74s/it] 55%|█████▍    | 9722/17834 [4:56:15<3:58:22,  1.76s/it] 55%|█████▍    | 9723/17834 [4:56:17<3:55:16,  1.74s/it] 55%|█████▍    | 9724/17834 [4:56:18<3:55:40,  1.74s/it] 55%|█████▍    | 9725/17834 [4:56:20<3:54:26,  1.73s/it] 55%|█████▍    | 9726/17834 [4:56:22<3:55:28,  1.74s/it] 55%|█████▍    | 9727/17834 [4:56:24<3:55:53,  1.75s/it] 55%|█████▍    | 9728/17834 [4:56:25<3:57:07,  1.76s/it] 55%|█████▍    | 9729/17834 [4:56:27<3:54:41,  1.74s/it] 55%|█████▍    | 9730/17834 [4:56:29<3:55:32,  1.74s/it] 55%|█████▍    | 9731/17834 [4:56:31<3:54:46,  1.74s/it] 55%|█████▍    | 9732/17834 [4:56:32<3:54:31,  1.74s/it] 55%|█████▍    | 9733/17834 [4:56:34<3:53:56,  1.73s/it] 55%|█████▍    | 9734/17834 [4:56:36<3:53:55,  1.73s/it] 55%|█████▍    | 9735/17834 [4:56:38<3:55:41,  1.75s/it] 55%|█████▍    | 9736/17834 [4:56:39<3:54:00,  1.73s/it] 55%|█████▍    | 9737/17834 [4:56:41<3:57:31,  1.76s/it] 55%|█████▍    | 9738/17834 [4:56:43<3:54:41,  1.74s/it] 55%|█████▍    | 9739/17834 [4:56:45<4:02:47,  1.80s/it] 55%|█████▍    | 9740/17834 [4:56:46<3:57:51,  1.76s/it] 55%|█████▍    | 9741/17834 [4:56:48<3:56:07,  1.75s/it] 55%|█████▍    | 9742/17834 [4:56:50<3:55:11,  1.74s/it] 55%|█████▍    | 9743/17834 [4:56:52<3:55:40,  1.75s/it] 55%|█████▍    | 9744/17834 [4:56:53<3:53:15,  1.73s/it] 55%|█████▍    | 9745/17834 [4:56:55<3:54:01,  1.74s/it] 55%|█████▍    | 9746/17834 [4:56:57<3:57:15,  1.76s/it] 55%|█████▍    | 9747/17834 [4:56:59<3:56:26,  1.75s/it] 55%|█████▍    | 9748/17834 [4:57:00<3:55:00,  1.74s/it] 55%|█████▍    | 9749/17834 [4:57:02<3:56:14,  1.75s/it]08/31/2024 00:11:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0561188459396362, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02928975038230419, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2372055053710938, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3226141929626465}
 55%|█████▍    | 9750/17834 [4:57:04<3:56:53,  1.76s/it] 55%|█████▍    | 9751/17834 [4:57:06<3:58:06,  1.77s/it] 55%|█████▍    | 9752/17834 [4:57:07<3:58:59,  1.77s/it] 55%|█████▍    | 9753/17834 [4:57:09<3:56:53,  1.76s/it] 55%|█████▍    | 9754/17834 [4:57:11<3:54:59,  1.74s/it] 55%|█████▍    | 9755/17834 [4:57:13<3:55:52,  1.75s/it] 55%|█████▍    | 9756/17834 [4:57:14<3:53:24,  1.73s/it] 55%|█████▍    | 9757/17834 [4:57:16<3:55:08,  1.75s/it] 55%|█████▍    | 9758/17834 [4:57:18<3:56:41,  1.76s/it] 55%|█████▍    | 9759/17834 [4:57:20<3:57:31,  1.76s/it] 55%|█████▍    | 9760/17834 [4:57:21<3:57:34,  1.77s/it] 55%|█████▍    | 9761/17834 [4:57:23<3:57:11,  1.76s/it] 55%|█████▍    | 9762/17834 [4:57:25<3:54:56,  1.75s/it] 55%|█████▍    | 9763/17834 [4:57:27<3:53:22,  1.73s/it] 55%|█████▍    | 9764/17834 [4:57:28<3:55:57,  1.75s/it] 55%|█████▍    | 9765/17834 [4:57:30<3:53:00,  1.73s/it] 55%|█████▍    | 9766/17834 [4:57:32<3:52:30,  1.73s/it] 55%|█████▍    | 9767/17834 [4:57:33<3:52:26,  1.73s/it] 55%|█████▍    | 9768/17834 [4:57:35<3:57:24,  1.77s/it] 55%|█████▍    | 9769/17834 [4:57:37<3:59:06,  1.78s/it] 55%|█████▍    | 9770/17834 [4:57:39<3:59:08,  1.78s/it] 55%|█████▍    | 9771/17834 [4:57:41<3:56:08,  1.76s/it] 55%|█████▍    | 9772/17834 [4:57:42<3:56:46,  1.76s/it] 55%|█████▍    | 9773/17834 [4:57:44<3:55:26,  1.75s/it] 55%|█████▍    | 9774/17834 [4:57:46<3:53:50,  1.74s/it] 55%|█████▍    | 9775/17834 [4:57:48<3:53:21,  1.74s/it] 55%|█████▍    | 9776/17834 [4:57:49<3:54:16,  1.74s/it] 55%|█████▍    | 9777/17834 [4:57:51<3:54:34,  1.75s/it] 55%|█████▍    | 9778/17834 [4:57:53<3:57:45,  1.77s/it] 55%|█████▍    | 9779/17834 [4:57:55<3:56:40,  1.76s/it] 55%|█████▍    | 9780/17834 [4:57:56<3:56:23,  1.76s/it] 55%|█████▍    | 9781/17834 [4:57:58<3:56:11,  1.76s/it] 55%|█████▍    | 9782/17834 [4:58:00<3:57:49,  1.77s/it] 55%|█████▍    | 9783/17834 [4:58:02<3:58:21,  1.78s/it] 55%|█████▍    | 9784/17834 [4:58:03<3:55:53,  1.76s/it] 55%|█████▍    | 9785/17834 [4:58:05<3:55:02,  1.75s/it] 55%|█████▍    | 9786/17834 [4:58:07<3:52:59,  1.74s/it] 55%|█████▍    | 9787/17834 [4:58:09<3:51:47,  1.73s/it] 55%|█████▍    | 9788/17834 [4:58:10<3:51:36,  1.73s/it] 55%|█████▍    | 9789/17834 [4:58:12<3:51:32,  1.73s/it] 55%|█████▍    | 9790/17834 [4:58:14<3:56:31,  1.76s/it] 55%|█████▍    | 9791/17834 [4:58:16<3:52:57,  1.74s/it] 55%|█████▍    | 9792/17834 [4:58:17<3:55:12,  1.75s/it] 55%|█████▍    | 9793/17834 [4:58:19<3:51:58,  1.73s/it] 55%|█████▍    | 9794/17834 [4:58:21<3:53:32,  1.74s/it] 55%|█████▍    | 9795/17834 [4:58:23<3:54:16,  1.75s/it] 55%|█████▍    | 9796/17834 [4:58:24<3:54:22,  1.75s/it] 55%|█████▍    | 9797/17834 [4:58:26<3:58:12,  1.78s/it] 55%|█████▍    | 9798/17834 [4:58:28<3:56:13,  1.76s/it] 55%|█████▍    | 9799/17834 [4:58:30<3:52:23,  1.74s/it]08/31/2024 00:12:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3027387857437134, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.035484783351421356, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.264136552810669, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.602360248565674}
 55%|█████▍    | 9800/17834 [4:58:31<3:53:14,  1.74s/it] 55%|█████▍    | 9801/17834 [4:58:33<3:51:38,  1.73s/it] 55%|█████▍    | 9802/17834 [4:58:35<3:50:21,  1.72s/it] 55%|█████▍    | 9803/17834 [4:58:37<3:51:11,  1.73s/it] 55%|█████▍    | 9804/17834 [4:58:38<3:51:24,  1.73s/it] 55%|█████▍    | 9805/17834 [4:58:40<3:51:36,  1.73s/it] 55%|█████▍    | 9806/17834 [4:58:42<3:52:04,  1.73s/it] 55%|█████▍    | 9807/17834 [4:58:43<3:51:26,  1.73s/it] 55%|█████▍    | 9808/17834 [4:58:45<3:53:25,  1.75s/it] 55%|█████▌    | 9809/17834 [4:58:47<3:56:05,  1.77s/it] 55%|█████▌    | 9810/17834 [4:58:49<3:54:31,  1.75s/it] 55%|█████▌    | 9811/17834 [4:58:51<3:54:59,  1.76s/it] 55%|█████▌    | 9812/17834 [4:58:52<3:57:16,  1.77s/it] 55%|█████▌    | 9813/17834 [4:58:54<3:54:15,  1.75s/it] 55%|█████▌    | 9814/17834 [4:58:56<3:57:02,  1.77s/it] 55%|█████▌    | 9815/17834 [4:58:58<3:57:31,  1.78s/it] 55%|█████▌    | 9816/17834 [4:58:59<3:54:23,  1.75s/it] 55%|█████▌    | 9817/17834 [4:59:01<3:57:48,  1.78s/it] 55%|█████▌    | 9818/17834 [4:59:03<3:55:50,  1.77s/it] 55%|█████▌    | 9819/17834 [4:59:05<3:55:49,  1.77s/it] 55%|█████▌    | 9820/17834 [4:59:07<4:01:27,  1.81s/it] 55%|█████▌    | 9821/17834 [4:59:08<4:00:47,  1.80s/it] 55%|█████▌    | 9822/17834 [4:59:10<3:59:26,  1.79s/it] 55%|█████▌    | 9823/17834 [4:59:12<3:54:59,  1.76s/it] 55%|█████▌    | 9824/17834 [4:59:14<3:55:50,  1.77s/it] 55%|█████▌    | 9825/17834 [4:59:15<3:54:46,  1.76s/it] 55%|█████▌    | 9826/17834 [4:59:17<3:57:23,  1.78s/it] 55%|█████▌    | 9827/17834 [4:59:19<3:52:48,  1.74s/it] 55%|█████▌    | 9828/17834 [4:59:21<3:54:29,  1.76s/it] 55%|█████▌    | 9829/17834 [4:59:22<3:56:12,  1.77s/it] 55%|█████▌    | 9830/17834 [4:59:24<3:58:15,  1.79s/it] 55%|█████▌    | 9831/17834 [4:59:26<3:57:56,  1.78s/it] 55%|█████▌    | 9832/17834 [4:59:28<3:54:51,  1.76s/it] 55%|█████▌    | 9833/17834 [4:59:29<3:54:05,  1.76s/it] 55%|█████▌    | 9834/17834 [4:59:31<3:50:53,  1.73s/it] 55%|█████▌    | 9835/17834 [4:59:33<3:53:32,  1.75s/it] 55%|█████▌    | 9836/17834 [4:59:35<3:50:52,  1.73s/it] 55%|█████▌    | 9837/17834 [4:59:36<3:52:03,  1.74s/it] 55%|█████▌    | 9838/17834 [4:59:38<3:53:33,  1.75s/it] 55%|█████▌    | 9839/17834 [4:59:40<3:52:17,  1.74s/it] 55%|█████▌    | 9840/17834 [4:59:42<3:53:32,  1.75s/it] 55%|█████▌    | 9841/17834 [4:59:43<3:53:46,  1.75s/it] 55%|█████▌    | 9842/17834 [4:59:45<3:53:29,  1.75s/it] 55%|█████▌    | 9843/17834 [4:59:47<4:02:21,  1.82s/it] 55%|█████▌    | 9844/17834 [4:59:49<4:00:33,  1.81s/it] 55%|█████▌    | 9845/17834 [4:59:51<3:56:23,  1.78s/it] 55%|█████▌    | 9846/17834 [4:59:52<3:52:42,  1.75s/it] 55%|█████▌    | 9847/17834 [4:59:54<3:55:39,  1.77s/it] 55%|█████▌    | 9848/17834 [4:59:56<3:58:12,  1.79s/it] 55%|█████▌    | 9849/17834 [4:59:58<3:57:01,  1.78s/it]08/31/2024 00:14:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4032056331634521, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04082047939300537, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.281327724456787, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.725353717803955}
 55%|█████▌    | 9850/17834 [4:59:59<3:54:36,  1.76s/it] 55%|█████▌    | 9851/17834 [5:00:01<3:54:23,  1.76s/it] 55%|█████▌    | 9852/17834 [5:00:03<3:55:18,  1.77s/it] 55%|█████▌    | 9853/17834 [5:00:05<3:53:19,  1.75s/it] 55%|█████▌    | 9854/17834 [5:00:07<3:53:36,  1.76s/it] 55%|█████▌    | 9855/17834 [5:00:08<3:55:19,  1.77s/it] 55%|█████▌    | 9856/17834 [5:00:10<3:52:14,  1.75s/it] 55%|█████▌    | 9857/17834 [5:00:12<3:55:38,  1.77s/it] 55%|█████▌    | 9858/17834 [5:00:14<3:54:30,  1.76s/it] 55%|█████▌    | 9859/17834 [5:00:15<3:55:10,  1.77s/it] 55%|█████▌    | 9860/17834 [5:00:17<3:53:34,  1.76s/it] 55%|█████▌    | 9861/17834 [5:00:19<3:51:59,  1.75s/it] 55%|█████▌    | 9862/17834 [5:00:21<3:50:46,  1.74s/it] 55%|█████▌    | 9863/17834 [5:00:22<3:51:07,  1.74s/it] 55%|█████▌    | 9864/17834 [5:00:24<3:51:48,  1.75s/it] 55%|█████▌    | 9865/17834 [5:00:26<3:54:07,  1.76s/it] 55%|█████▌    | 9866/17834 [5:00:28<3:58:07,  1.79s/it] 55%|█████▌    | 9867/17834 [5:00:29<3:55:51,  1.78s/it] 55%|█████▌    | 9868/17834 [5:00:31<3:53:33,  1.76s/it] 55%|█████▌    | 9869/17834 [5:00:33<3:52:19,  1.75s/it] 55%|█████▌    | 9870/17834 [5:00:35<3:52:41,  1.75s/it] 55%|█████▌    | 9871/17834 [5:00:36<3:51:10,  1.74s/it] 55%|█████▌    | 9872/17834 [5:00:38<3:51:37,  1.75s/it] 55%|█████▌    | 9873/17834 [5:00:40<3:53:06,  1.76s/it] 55%|█████▌    | 9874/17834 [5:00:42<3:51:23,  1.74s/it] 55%|█████▌    | 9875/17834 [5:00:43<3:52:55,  1.76s/it] 55%|█████▌    | 9876/17834 [5:00:45<3:51:05,  1.74s/it] 55%|█████▌    | 9877/17834 [5:00:47<3:52:54,  1.76s/it] 55%|█████▌    | 9878/17834 [5:00:49<3:58:16,  1.80s/it] 55%|█████▌    | 9879/17834 [5:00:51<3:56:50,  1.79s/it] 55%|█████▌    | 9880/17834 [5:00:52<3:55:26,  1.78s/it] 55%|█████▌    | 9881/17834 [5:00:54<3:52:50,  1.76s/it] 55%|█████▌    | 9882/17834 [5:00:56<3:54:39,  1.77s/it] 55%|█████▌    | 9883/17834 [5:00:58<3:52:36,  1.76s/it] 55%|█████▌    | 9884/17834 [5:00:59<3:54:36,  1.77s/it] 55%|█████▌    | 9885/17834 [5:01:01<3:56:27,  1.78s/it] 55%|█████▌    | 9886/17834 [5:01:03<3:53:37,  1.76s/it] 55%|█████▌    | 9887/17834 [5:01:05<3:51:34,  1.75s/it] 55%|█████▌    | 9888/17834 [5:01:06<3:53:12,  1.76s/it] 55%|█████▌    | 9889/17834 [5:01:08<3:53:40,  1.76s/it] 55%|█████▌    | 9890/17834 [5:01:10<3:52:49,  1.76s/it] 55%|█████▌    | 9891/17834 [5:01:12<3:53:26,  1.76s/it] 55%|█████▌    | 9892/17834 [5:01:13<3:47:19,  1.72s/it] 55%|█████▌    | 9893/17834 [5:01:15<3:42:41,  1.68s/it] 55%|█████▌    | 9894/17834 [5:01:16<3:39:03,  1.66s/it] 55%|█████▌    | 9895/17834 [5:01:18<3:36:53,  1.64s/it] 55%|█████▌    | 9896/17834 [5:01:20<3:35:09,  1.63s/it] 55%|█████▌    | 9897/17834 [5:01:21<3:34:10,  1.62s/it] 56%|█████▌    | 9898/17834 [5:01:23<3:33:22,  1.61s/it] 56%|█████▌    | 9899/17834 [5:01:24<3:32:43,  1.61s/it]08/31/2024 00:15:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.175417423248291, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02768765762448311, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2780117988586426, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.481116771697998}
 56%|█████▌    | 9900/17834 [5:01:26<3:32:13,  1.60s/it] 56%|█████▌    | 9901/17834 [5:01:28<3:32:02,  1.60s/it] 56%|█████▌    | 9902/17834 [5:01:29<3:31:45,  1.60s/it] 56%|█████▌    | 9903/17834 [5:01:31<3:31:45,  1.60s/it] 56%|█████▌    | 9904/17834 [5:01:32<3:31:31,  1.60s/it] 56%|█████▌    | 9905/17834 [5:01:34<3:31:08,  1.60s/it] 56%|█████▌    | 9906/17834 [5:01:36<3:31:07,  1.60s/it] 56%|█████▌    | 9907/17834 [5:01:37<3:31:13,  1.60s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
 56%|█████▌    | 9908/17834 [5:02:02<19:05:24,  8.67s/it] 56%|█████▌    | 9909/17834 [5:02:05<14:56:35,  6.79s/it] 56%|█████▌    | 9910/17834 [5:02:07<11:51:52,  5.39s/it] 56%|█████▌    | 9911/17834 [5:02:09<9:31:04,  4.32s/it]  56%|█████▌    | 9912/17834 [5:02:11<7:49:18,  3.55s/it] 56%|█████▌    | 9913/17834 [5:02:12<6:41:06,  3.04s/it] 56%|█████▌    | 9914/17834 [5:02:14<5:49:06,  2.64s/it] 56%|█████▌    | 9915/17834 [5:02:16<5:12:54,  2.37s/it] 56%|█████▌    | 9916/17834 [5:02:18<4:49:53,  2.20s/it] 56%|█████▌    | 9917/17834 [5:02:19<4:36:54,  2.10s/it] 56%|█████▌    | 9918/17834 [5:02:21<4:28:23,  2.03s/it] 56%|█████▌    | 9919/17834 [5:02:23<4:17:31,  1.95s/it] 56%|█████▌    | 9920/17834 [5:02:25<4:08:53,  1.89s/it] 56%|█████▌    | 9921/17834 [5:02:27<4:00:41,  1.82s/it] 56%|█████▌    | 9922/17834 [5:02:28<3:55:54,  1.79s/it] 56%|█████▌    | 9923/17834 [5:02:30<3:54:06,  1.78s/it] 56%|█████▌    | 9924/17834 [5:02:32<3:53:03,  1.77s/it] 56%|█████▌    | 9925/17834 [5:02:34<3:52:54,  1.77s/it] 56%|█████▌    | 9926/17834 [5:02:35<3:52:20,  1.76s/it] 56%|█████▌    | 9927/17834 [5:02:37<3:52:21,  1.76s/it] 56%|█████▌    | 9928/17834 [5:02:39<3:51:21,  1.76s/it] 56%|█████▌    | 9929/17834 [5:02:41<3:52:41,  1.77s/it] 56%|█████▌    | 9930/17834 [5:02:42<3:51:17,  1.76s/it] 56%|█████▌    | 9931/17834 [5:02:44<3:50:45,  1.75s/it] 56%|█████▌    | 9932/17834 [5:02:46<3:49:42,  1.74s/it] 56%|█████▌    | 9933/17834 [5:02:47<3:49:11,  1.74s/it] 56%|█████▌    | 9934/17834 [5:02:49<3:53:45,  1.78s/it] 56%|█████▌    | 9935/17834 [5:02:51<3:50:08,  1.75s/it] 56%|█████▌    | 9936/17834 [5:02:53<3:49:17,  1.74s/it] 56%|█████▌    | 9937/17834 [5:02:55<3:50:52,  1.75s/it] 56%|█████▌    | 9938/17834 [5:02:56<3:49:48,  1.75s/it] 56%|█████▌    | 9939/17834 [5:02:58<3:50:22,  1.75s/it] 56%|█████▌    | 9940/17834 [5:03:00<3:50:07,  1.75s/it] 56%|█████▌    | 9941/17834 [5:03:02<3:50:56,  1.76s/it] 56%|█████▌    | 9942/17834 [5:03:03<3:51:16,  1.76s/it] 56%|█████▌    | 9943/17834 [5:03:05<3:49:53,  1.75s/it] 56%|█████▌    | 9944/17834 [5:03:07<3:48:22,  1.74s/it] 56%|█████▌    | 9945/17834 [5:03:08<3:46:58,  1.73s/it] 56%|█████▌    | 9946/17834 [5:03:10<3:47:22,  1.73s/it] 56%|█████▌    | 9947/17834 [5:03:12<3:47:25,  1.73s/it] 56%|█████▌    | 9948/17834 [5:03:14<3:50:47,  1.76s/it] 56%|█████▌    | 9949/17834 [5:03:15<3:48:46,  1.74s/it]08/31/2024 00:17:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.209497094154358, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029658690094947815, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.161947011947632, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4011027812957764}
 56%|█████▌    | 9950/17834 [5:03:17<3:48:54,  1.74s/it] 56%|█████▌    | 9951/17834 [5:03:19<3:48:52,  1.74s/it] 56%|█████▌    | 9952/17834 [5:03:21<3:47:23,  1.73s/it] 56%|█████▌    | 9953/17834 [5:03:22<3:47:55,  1.74s/it] 56%|█████▌    | 9954/17834 [5:03:24<3:49:29,  1.75s/it] 56%|█████▌    | 9955/17834 [5:03:26<3:46:49,  1.73s/it] 56%|█████▌    | 9956/17834 [5:03:28<3:46:54,  1.73s/it] 56%|█████▌    | 9957/17834 [5:03:29<3:47:54,  1.74s/it] 56%|█████▌    | 9958/17834 [5:03:31<3:47:01,  1.73s/it] 56%|█████▌    | 9959/17834 [5:03:33<3:48:57,  1.74s/it] 56%|█████▌    | 9960/17834 [5:03:35<3:48:10,  1.74s/it] 56%|█████▌    | 9961/17834 [5:03:36<3:51:03,  1.76s/it] 56%|█████▌    | 9962/17834 [5:03:38<3:52:01,  1.77s/it] 56%|█████▌    | 9963/17834 [5:03:40<3:50:19,  1.76s/it] 56%|█████▌    | 9964/17834 [5:03:42<3:50:13,  1.76s/it] 56%|█████▌    | 9965/17834 [5:03:43<3:51:29,  1.77s/it] 56%|█████▌    | 9966/17834 [5:03:45<3:52:05,  1.77s/it] 56%|█████▌    | 9967/17834 [5:03:47<3:50:11,  1.76s/it] 56%|█████▌    | 9968/17834 [5:03:49<3:49:26,  1.75s/it] 56%|█████▌    | 9969/17834 [5:03:50<3:49:08,  1.75s/it] 56%|█████▌    | 9970/17834 [5:03:52<3:50:22,  1.76s/it] 56%|█████▌    | 9971/17834 [5:03:54<3:48:44,  1.75s/it] 56%|█████▌    | 9972/17834 [5:03:56<3:46:13,  1.73s/it] 56%|█████▌    | 9973/17834 [5:03:57<3:51:33,  1.77s/it] 56%|█████▌    | 9974/17834 [5:03:59<3:49:32,  1.75s/it] 56%|█████▌    | 9975/17834 [5:04:01<3:51:35,  1.77s/it] 56%|█████▌    | 9976/17834 [5:04:03<3:49:37,  1.75s/it] 56%|█████▌    | 9977/17834 [5:04:04<3:50:47,  1.76s/it] 56%|█████▌    | 9978/17834 [5:04:06<3:53:42,  1.78s/it] 56%|█████▌    | 9979/17834 [5:04:08<3:54:27,  1.79s/it] 56%|█████▌    | 9980/17834 [5:04:10<3:49:49,  1.76s/it] 56%|█████▌    | 9981/17834 [5:04:12<3:49:01,  1.75s/it] 56%|█████▌    | 9982/17834 [5:04:13<3:50:13,  1.76s/it] 56%|█████▌    | 9983/17834 [5:04:15<3:51:33,  1.77s/it] 56%|█████▌    | 9984/17834 [5:04:17<3:50:32,  1.76s/it] 56%|█████▌    | 9985/17834 [5:04:19<3:53:22,  1.78s/it] 56%|█████▌    | 9986/17834 [5:04:20<3:47:28,  1.74s/it] 56%|█████▌    | 9987/17834 [5:04:22<3:44:40,  1.72s/it] 56%|█████▌    | 9988/17834 [5:04:24<3:47:06,  1.74s/it] 56%|█████▌    | 9989/17834 [5:04:25<3:46:34,  1.73s/it] 56%|█████▌    | 9990/17834 [5:04:27<3:47:39,  1.74s/it] 56%|█████▌    | 9991/17834 [5:04:29<3:49:17,  1.75s/it] 56%|█████▌    | 9992/17834 [5:04:31<3:47:11,  1.74s/it] 56%|█████▌    | 9993/17834 [5:04:32<3:46:44,  1.73s/it] 56%|█████▌    | 9994/17834 [5:04:34<3:45:41,  1.73s/it] 56%|█████▌    | 9995/17834 [5:04:36<3:49:03,  1.75s/it] 56%|█████▌    | 9996/17834 [5:04:38<3:49:06,  1.75s/it] 56%|█████▌    | 9997/17834 [5:04:39<3:48:00,  1.75s/it] 56%|█████▌    | 9998/17834 [5:04:41<3:52:13,  1.78s/it] 56%|█████▌    | 9999/17834 [5:04:43<3:52:12,  1.78s/it]08/31/2024 00:19:02 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0728211402893066, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02348453924059868, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.205415725708008, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3017215728759766}
 56%|█████▌    | 10000/17834 [5:04:45<3:54:05,  1.79s/it] 56%|█████▌    | 10001/17834 [5:04:47<3:49:53,  1.76s/it] 56%|█████▌    | 10002/17834 [5:04:48<3:47:15,  1.74s/it] 56%|█████▌    | 10003/17834 [5:04:50<3:51:13,  1.77s/it] 56%|█████▌    | 10004/17834 [5:04:52<3:51:22,  1.77s/it] 56%|█████▌    | 10005/17834 [5:04:54<3:52:59,  1.79s/it] 56%|█████▌    | 10006/17834 [5:04:55<3:50:00,  1.76s/it] 56%|█████▌    | 10007/17834 [5:04:57<3:49:05,  1.76s/it] 56%|█████▌    | 10008/17834 [5:04:59<3:47:11,  1.74s/it] 56%|█████▌    | 10009/17834 [5:05:01<3:46:59,  1.74s/it] 56%|█████▌    | 10010/17834 [5:05:02<3:46:40,  1.74s/it] 56%|█████▌    | 10011/17834 [5:05:04<3:47:35,  1.75s/it] 56%|█████▌    | 10012/17834 [5:05:06<3:48:13,  1.75s/it] 56%|█████▌    | 10013/17834 [5:05:08<3:50:39,  1.77s/it] 56%|█████▌    | 10014/17834 [5:05:09<3:48:26,  1.75s/it] 56%|█████▌    | 10015/17834 [5:05:11<3:48:10,  1.75s/it] 56%|█████▌    | 10016/17834 [5:05:13<3:46:43,  1.74s/it] 56%|█████▌    | 10017/17834 [5:05:15<3:45:44,  1.73s/it] 56%|█████▌    | 10018/17834 [5:05:16<3:50:53,  1.77s/it] 56%|█████▌    | 10019/17834 [5:05:18<3:51:16,  1.78s/it] 56%|█████▌    | 10020/17834 [5:05:20<3:48:41,  1.76s/it] 56%|█████▌    | 10021/17834 [5:05:22<3:51:12,  1.78s/it] 56%|█████▌    | 10022/17834 [5:05:24<3:53:24,  1.79s/it] 56%|█████▌    | 10023/17834 [5:05:25<3:52:15,  1.78s/it] 56%|█████▌    | 10024/17834 [5:05:27<3:49:48,  1.77s/it] 56%|█████▌    | 10025/17834 [5:05:29<3:46:23,  1.74s/it] 56%|█████▌    | 10026/17834 [5:05:31<3:47:07,  1.75s/it] 56%|█████▌    | 10027/17834 [5:05:32<3:48:51,  1.76s/it] 56%|█████▌    | 10028/17834 [5:05:34<3:47:15,  1.75s/it] 56%|█████▌    | 10029/17834 [5:05:36<3:46:16,  1.74s/it] 56%|█████▌    | 10030/17834 [5:05:37<3:45:45,  1.74s/it] 56%|█████▌    | 10031/17834 [5:05:39<3:47:06,  1.75s/it] 56%|█████▋    | 10032/17834 [5:05:41<3:49:57,  1.77s/it] 56%|█████▋    | 10033/17834 [5:05:43<3:48:22,  1.76s/it] 56%|█████▋    | 10034/17834 [5:05:45<3:48:19,  1.76s/it] 56%|█████▋    | 10035/17834 [5:05:46<3:47:05,  1.75s/it] 56%|█████▋    | 10036/17834 [5:05:48<3:47:12,  1.75s/it] 56%|█████▋    | 10037/17834 [5:05:50<3:47:09,  1.75s/it] 56%|█████▋    | 10038/17834 [5:05:52<3:51:49,  1.78s/it] 56%|█████▋    | 10039/17834 [5:05:53<3:51:49,  1.78s/it] 56%|█████▋    | 10040/17834 [5:05:55<3:51:19,  1.78s/it] 56%|█████▋    | 10041/17834 [5:05:57<3:51:22,  1.78s/it] 56%|█████▋    | 10042/17834 [5:05:59<3:50:09,  1.77s/it] 56%|█████▋    | 10043/17834 [5:06:00<3:48:16,  1.76s/it] 56%|█████▋    | 10044/17834 [5:06:02<3:46:54,  1.75s/it] 56%|█████▋    | 10045/17834 [5:06:04<3:44:53,  1.73s/it] 56%|█████▋    | 10046/17834 [5:06:06<3:45:31,  1.74s/it] 56%|█████▋    | 10047/17834 [5:06:07<3:45:50,  1.74s/it] 56%|█████▋    | 10048/17834 [5:06:09<3:48:23,  1.76s/it] 56%|█████▋    | 10049/17834 [5:06:11<3:46:30,  1.75s/it]08/31/2024 00:20:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9249781966209412, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02542654238641262, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.141174793243408, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0915794372558594}
 56%|█████▋    | 10050/17834 [5:06:13<3:46:40,  1.75s/it] 56%|█████▋    | 10051/17834 [5:06:14<3:48:46,  1.76s/it] 56%|█████▋    | 10052/17834 [5:06:16<3:45:38,  1.74s/it] 56%|█████▋    | 10053/17834 [5:06:18<3:46:26,  1.75s/it] 56%|█████▋    | 10054/17834 [5:06:20<3:51:12,  1.78s/it] 56%|█████▋    | 10055/17834 [5:06:21<3:45:26,  1.74s/it] 56%|█████▋    | 10056/17834 [5:06:23<3:45:12,  1.74s/it] 56%|█████▋    | 10057/17834 [5:06:25<3:50:42,  1.78s/it] 56%|█████▋    | 10058/17834 [5:06:27<3:48:45,  1.77s/it] 56%|█████▋    | 10059/17834 [5:06:28<3:45:27,  1.74s/it] 56%|█████▋    | 10060/17834 [5:06:30<3:45:25,  1.74s/it] 56%|█████▋    | 10061/17834 [5:06:32<3:47:48,  1.76s/it] 56%|█████▋    | 10062/17834 [5:06:34<3:47:54,  1.76s/it] 56%|█████▋    | 10063/17834 [5:06:35<3:44:15,  1.73s/it] 56%|█████▋    | 10064/17834 [5:06:37<3:45:30,  1.74s/it] 56%|█████▋    | 10065/17834 [5:06:39<3:46:34,  1.75s/it] 56%|█████▋    | 10066/17834 [5:06:41<3:45:03,  1.74s/it] 56%|█████▋    | 10067/17834 [5:06:42<3:41:38,  1.71s/it] 56%|█████▋    | 10068/17834 [5:06:44<3:45:17,  1.74s/it] 56%|█████▋    | 10069/17834 [5:06:46<3:44:03,  1.73s/it] 56%|█████▋    | 10070/17834 [5:06:48<3:43:16,  1.73s/it] 56%|█████▋    | 10071/17834 [5:06:49<3:42:59,  1.72s/it] 56%|█████▋    | 10072/17834 [5:06:51<3:42:10,  1.72s/it] 56%|█████▋    | 10073/17834 [5:06:53<3:43:40,  1.73s/it] 56%|█████▋    | 10074/17834 [5:06:54<3:43:42,  1.73s/it] 56%|█████▋    | 10075/17834 [5:06:56<3:44:59,  1.74s/it] 56%|█████▋    | 10076/17834 [5:06:58<3:46:28,  1.75s/it] 57%|█████▋    | 10077/17834 [5:07:00<3:44:54,  1.74s/it] 57%|█████▋    | 10078/17834 [5:07:01<3:44:10,  1.73s/it] 57%|█████▋    | 10079/17834 [5:07:03<3:45:22,  1.74s/it] 57%|█████▋    | 10080/17834 [5:07:05<3:44:13,  1.74s/it] 57%|█████▋    | 10081/17834 [5:07:07<3:42:26,  1.72s/it] 57%|█████▋    | 10082/17834 [5:07:08<3:44:09,  1.73s/it] 57%|█████▋    | 10083/17834 [5:07:10<3:43:01,  1.73s/it] 57%|█████▋    | 10084/17834 [5:07:12<3:41:27,  1.71s/it] 57%|█████▋    | 10085/17834 [5:07:13<3:40:34,  1.71s/it] 57%|█████▋    | 10086/17834 [5:07:15<3:43:44,  1.73s/it] 57%|█████▋    | 10087/17834 [5:07:17<3:42:53,  1.73s/it] 57%|█████▋    | 10088/17834 [5:07:19<3:43:29,  1.73s/it] 57%|█████▋    | 10089/17834 [5:07:20<3:46:22,  1.75s/it] 57%|█████▋    | 10090/17834 [5:07:22<3:47:24,  1.76s/it] 57%|█████▋    | 10091/17834 [5:07:24<3:47:15,  1.76s/it] 57%|█████▋    | 10092/17834 [5:07:26<3:47:07,  1.76s/it] 57%|█████▋    | 10093/17834 [5:07:28<3:48:21,  1.77s/it] 57%|█████▋    | 10094/17834 [5:07:29<3:46:51,  1.76s/it] 57%|█████▋    | 10095/17834 [5:07:31<3:48:06,  1.77s/it] 57%|█████▋    | 10096/17834 [5:07:33<3:46:03,  1.75s/it] 57%|█████▋    | 10097/17834 [5:07:35<3:44:38,  1.74s/it] 57%|█████▋    | 10098/17834 [5:07:36<3:43:24,  1.73s/it] 57%|█████▋    | 10099/17834 [5:07:38<3:44:11,  1.74s/it]08/31/2024 00:21:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0240370035171509, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032075490802526474, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1618094444274902, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2179219722747803}
 57%|█████▋    | 10100/17834 [5:07:40<3:45:17,  1.75s/it] 57%|█████▋    | 10101/17834 [5:07:42<3:47:26,  1.76s/it] 57%|█████▋    | 10102/17834 [5:07:43<3:46:26,  1.76s/it] 57%|█████▋    | 10103/17834 [5:07:45<3:46:04,  1.75s/it] 57%|█████▋    | 10104/17834 [5:07:47<3:44:30,  1.74s/it] 57%|█████▋    | 10105/17834 [5:07:49<3:45:34,  1.75s/it] 57%|█████▋    | 10106/17834 [5:07:50<3:45:10,  1.75s/it] 57%|█████▋    | 10107/17834 [5:07:52<3:48:36,  1.78s/it] 57%|█████▋    | 10108/17834 [5:07:54<3:49:25,  1.78s/it] 57%|█████▋    | 10109/17834 [5:07:56<3:47:38,  1.77s/it] 57%|█████▋    | 10110/17834 [5:07:57<3:44:47,  1.75s/it] 57%|█████▋    | 10111/17834 [5:07:59<3:45:10,  1.75s/it] 57%|█████▋    | 10112/17834 [5:08:01<3:45:17,  1.75s/it] 57%|█████▋    | 10113/17834 [5:08:03<3:47:10,  1.77s/it] 57%|█████▋    | 10114/17834 [5:08:04<3:43:10,  1.73s/it] 57%|█████▋    | 10115/17834 [5:08:06<3:45:14,  1.75s/it] 57%|█████▋    | 10116/17834 [5:08:08<3:45:59,  1.76s/it] 57%|█████▋    | 10117/17834 [5:08:10<3:47:00,  1.76s/it] 57%|█████▋    | 10118/17834 [5:08:11<3:44:21,  1.74s/it] 57%|█████▋    | 10119/17834 [5:08:13<3:44:52,  1.75s/it] 57%|█████▋    | 10120/17834 [5:08:15<3:45:48,  1.76s/it] 57%|█████▋    | 10121/17834 [5:08:17<3:47:15,  1.77s/it] 57%|█████▋    | 10122/17834 [5:08:19<3:51:50,  1.80s/it] 57%|█████▋    | 10123/17834 [5:08:20<3:48:58,  1.78s/it] 57%|█████▋    | 10124/17834 [5:08:22<3:47:41,  1.77s/it] 57%|█████▋    | 10125/17834 [5:08:24<3:46:15,  1.76s/it] 57%|█████▋    | 10126/17834 [5:08:26<3:48:18,  1.78s/it] 57%|█████▋    | 10127/17834 [5:08:27<3:48:10,  1.78s/it] 57%|█████▋    | 10128/17834 [5:08:29<3:46:10,  1.76s/it] 57%|█████▋    | 10129/17834 [5:08:31<3:46:21,  1.76s/it] 57%|█████▋    | 10130/17834 [5:08:33<3:45:08,  1.75s/it] 57%|█████▋    | 10131/17834 [5:08:34<3:44:18,  1.75s/it] 57%|█████▋    | 10132/17834 [5:08:36<3:46:33,  1.76s/it] 57%|█████▋    | 10133/17834 [5:08:38<3:45:11,  1.75s/it] 57%|█████▋    | 10134/17834 [5:08:40<3:43:57,  1.75s/it] 57%|█████▋    | 10135/17834 [5:08:41<3:42:34,  1.73s/it] 57%|█████▋    | 10136/17834 [5:08:43<3:44:47,  1.75s/it] 57%|█████▋    | 10137/17834 [5:08:45<3:43:16,  1.74s/it] 57%|█████▋    | 10138/17834 [5:08:47<3:42:16,  1.73s/it] 57%|█████▋    | 10139/17834 [5:08:48<3:45:30,  1.76s/it] 57%|█████▋    | 10140/17834 [5:08:50<3:42:47,  1.74s/it] 57%|█████▋    | 10141/17834 [5:08:52<3:44:43,  1.75s/it] 57%|█████▋    | 10142/17834 [5:08:54<3:43:05,  1.74s/it] 57%|█████▋    | 10143/17834 [5:08:55<3:44:32,  1.75s/it] 57%|█████▋    | 10144/17834 [5:08:57<3:42:30,  1.74s/it] 57%|█████▋    | 10145/17834 [5:08:59<3:40:31,  1.72s/it] 57%|█████▋    | 10146/17834 [5:09:00<3:40:26,  1.72s/it] 57%|█████▋    | 10147/17834 [5:09:02<3:40:55,  1.72s/it] 57%|█████▋    | 10148/17834 [5:09:04<3:43:09,  1.74s/it] 57%|█████▋    | 10149/17834 [5:09:06<3:42:22,  1.74s/it]08/31/2024 00:23:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8247122764587402, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.016712050884962082, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.111358404159546, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.95278263092041}
 57%|█████▋    | 10150/17834 [5:09:07<3:45:34,  1.76s/it] 57%|█████▋    | 10151/17834 [5:09:09<3:47:41,  1.78s/it] 57%|█████▋    | 10152/17834 [5:09:11<3:45:34,  1.76s/it] 57%|█████▋    | 10153/17834 [5:09:13<3:42:13,  1.74s/it] 57%|█████▋    | 10154/17834 [5:09:14<3:44:43,  1.76s/it] 57%|█████▋    | 10155/17834 [5:09:16<3:43:18,  1.74s/it] 57%|█████▋    | 10156/17834 [5:09:18<3:42:18,  1.74s/it] 57%|█████▋    | 10157/17834 [5:09:20<3:41:45,  1.73s/it] 57%|█████▋    | 10158/17834 [5:09:21<3:41:13,  1.73s/it] 57%|█████▋    | 10159/17834 [5:09:23<3:40:01,  1.72s/it] 57%|█████▋    | 10160/17834 [5:09:25<3:39:02,  1.71s/it] 57%|█████▋    | 10161/17834 [5:09:27<3:41:29,  1.73s/it] 57%|█████▋    | 10162/17834 [5:09:28<3:43:35,  1.75s/it] 57%|█████▋    | 10163/17834 [5:09:30<3:40:56,  1.73s/it] 57%|█████▋    | 10164/17834 [5:09:32<3:43:26,  1.75s/it] 57%|█████▋    | 10165/17834 [5:09:33<3:41:16,  1.73s/it] 57%|█████▋    | 10166/17834 [5:09:35<3:43:32,  1.75s/it] 57%|█████▋    | 10167/17834 [5:09:37<3:45:01,  1.76s/it] 57%|█████▋    | 10168/17834 [5:09:39<3:44:14,  1.76s/it] 57%|█████▋    | 10169/17834 [5:09:41<3:45:42,  1.77s/it] 57%|█████▋    | 10170/17834 [5:09:42<3:43:45,  1.75s/it] 57%|█████▋    | 10171/17834 [5:09:44<3:46:02,  1.77s/it] 57%|█████▋    | 10172/17834 [5:09:46<3:45:34,  1.77s/it] 57%|█████▋    | 10173/17834 [5:09:48<3:46:16,  1.77s/it] 57%|█████▋    | 10174/17834 [5:09:49<3:47:21,  1.78s/it] 57%|█████▋    | 10175/17834 [5:09:51<3:47:30,  1.78s/it] 57%|█████▋    | 10176/17834 [5:09:53<3:45:22,  1.77s/it] 57%|█████▋    | 10177/17834 [5:09:55<3:46:29,  1.77s/it] 57%|█████▋    | 10178/17834 [5:09:57<3:46:40,  1.78s/it] 57%|█████▋    | 10179/17834 [5:09:58<3:44:41,  1.76s/it] 57%|█████▋    | 10180/17834 [5:10:00<3:43:13,  1.75s/it] 57%|█████▋    | 10181/17834 [5:10:02<3:41:55,  1.74s/it] 57%|█████▋    | 10182/17834 [5:10:03<3:39:02,  1.72s/it] 57%|█████▋    | 10183/17834 [5:10:05<3:43:35,  1.75s/it] 57%|█████▋    | 10184/17834 [5:10:07<3:40:46,  1.73s/it] 57%|█████▋    | 10185/17834 [5:10:09<3:38:57,  1.72s/it] 57%|█████▋    | 10186/17834 [5:10:10<3:41:38,  1.74s/it] 57%|█████▋    | 10187/17834 [5:10:12<3:42:41,  1.75s/it] 57%|█████▋    | 10188/17834 [5:10:14<3:43:50,  1.76s/it] 57%|█████▋    | 10189/17834 [5:10:16<3:42:14,  1.74s/it] 57%|█████▋    | 10190/17834 [5:10:17<3:44:13,  1.76s/it] 57%|█████▋    | 10191/17834 [5:10:19<3:44:14,  1.76s/it] 57%|█████▋    | 10192/17834 [5:10:21<3:45:12,  1.77s/it] 57%|█████▋    | 10193/17834 [5:10:23<3:43:09,  1.75s/it] 57%|█████▋    | 10194/17834 [5:10:24<3:40:50,  1.73s/it] 57%|█████▋    | 10195/17834 [5:10:26<3:43:47,  1.76s/it] 57%|█████▋    | 10196/17834 [5:10:28<3:41:13,  1.74s/it] 57%|█████▋    | 10197/17834 [5:10:30<3:39:56,  1.73s/it] 57%|█████▋    | 10198/17834 [5:10:31<3:43:17,  1.75s/it] 57%|█████▋    | 10199/17834 [5:10:33<3:41:19,  1.74s/it]08/31/2024 00:24:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4496406316757202, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03369053453207016, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.288562536239624, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7718937397003174}
 57%|█████▋    | 10200/17834 [5:10:35<3:41:17,  1.74s/it] 57%|█████▋    | 10201/17834 [5:10:37<3:41:47,  1.74s/it] 57%|█████▋    | 10202/17834 [5:10:38<3:41:40,  1.74s/it] 57%|█████▋    | 10203/17834 [5:10:40<3:44:07,  1.76s/it] 57%|█████▋    | 10204/17834 [5:10:42<3:42:25,  1.75s/it] 57%|█████▋    | 10205/17834 [5:10:44<3:40:56,  1.74s/it] 57%|█████▋    | 10206/17834 [5:10:45<3:40:17,  1.73s/it] 57%|█████▋    | 10207/17834 [5:10:47<3:42:32,  1.75s/it] 57%|█████▋    | 10208/17834 [5:10:49<3:40:24,  1.73s/it] 57%|█████▋    | 10209/17834 [5:10:51<3:41:11,  1.74s/it] 57%|█████▋    | 10210/17834 [5:10:52<3:41:53,  1.75s/it] 57%|█████▋    | 10211/17834 [5:10:54<3:40:28,  1.74s/it] 57%|█████▋    | 10212/17834 [5:10:56<3:41:16,  1.74s/it] 57%|█████▋    | 10213/17834 [5:10:57<3:39:07,  1.73s/it] 57%|█████▋    | 10214/17834 [5:10:59<3:40:23,  1.74s/it] 57%|█████▋    | 10215/17834 [5:11:01<3:42:55,  1.76s/it] 57%|█████▋    | 10216/17834 [5:11:03<3:42:00,  1.75s/it] 57%|█████▋    | 10217/17834 [5:11:05<3:43:38,  1.76s/it] 57%|█████▋    | 10218/17834 [5:11:06<3:39:35,  1.73s/it] 57%|█████▋    | 10219/17834 [5:11:08<3:38:26,  1.72s/it] 57%|█████▋    | 10220/17834 [5:11:10<3:40:59,  1.74s/it] 57%|█████▋    | 10221/17834 [5:11:11<3:38:35,  1.72s/it] 57%|█████▋    | 10222/17834 [5:11:13<3:41:12,  1.74s/it] 57%|█████▋    | 10223/17834 [5:11:15<3:39:48,  1.73s/it] 57%|█████▋    | 10224/17834 [5:11:17<3:38:43,  1.72s/it] 57%|█████▋    | 10225/17834 [5:11:18<3:40:27,  1.74s/it] 57%|█████▋    | 10226/17834 [5:11:20<3:39:56,  1.73s/it] 57%|█████▋    | 10227/17834 [5:11:22<3:42:43,  1.76s/it] 57%|█████▋    | 10228/17834 [5:11:24<3:42:53,  1.76s/it] 57%|█████▋    | 10229/17834 [5:11:25<3:42:46,  1.76s/it] 57%|█████▋    | 10230/17834 [5:11:27<3:41:11,  1.75s/it] 57%|█████▋    | 10231/17834 [5:11:29<3:42:25,  1.76s/it] 57%|█████▋    | 10232/17834 [5:11:31<3:42:20,  1.75s/it] 57%|█████▋    | 10233/17834 [5:11:32<3:41:17,  1.75s/it] 57%|█████▋    | 10234/17834 [5:11:34<3:38:49,  1.73s/it] 57%|█████▋    | 10235/17834 [5:11:36<3:37:48,  1.72s/it] 57%|█████▋    | 10236/17834 [5:11:38<3:37:55,  1.72s/it] 57%|█████▋    | 10237/17834 [5:11:39<3:40:04,  1.74s/it] 57%|█████▋    | 10238/17834 [5:11:41<3:44:58,  1.78s/it] 57%|█████▋    | 10239/17834 [5:11:43<3:50:55,  1.82s/it] 57%|█████▋    | 10240/17834 [5:11:45<3:47:06,  1.79s/it] 57%|█████▋    | 10241/17834 [5:11:46<3:42:57,  1.76s/it] 57%|█████▋    | 10242/17834 [5:11:48<3:42:52,  1.76s/it] 57%|█████▋    | 10243/17834 [5:11:50<3:41:08,  1.75s/it] 57%|█████▋    | 10244/17834 [5:11:52<3:42:31,  1.76s/it] 57%|█████▋    | 10245/17834 [5:11:54<3:46:00,  1.79s/it] 57%|█████▋    | 10246/17834 [5:11:55<3:43:03,  1.76s/it] 57%|█████▋    | 10247/17834 [5:11:57<3:40:23,  1.74s/it] 57%|█████▋    | 10248/17834 [5:11:59<3:37:45,  1.72s/it] 57%|█████▋    | 10249/17834 [5:12:00<3:40:57,  1.75s/it]08/31/2024 00:26:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3401398658752441, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.035129472613334656, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3253939151763916, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7006630897521973}
 57%|█████▋    | 10250/17834 [5:12:02<3:42:19,  1.76s/it] 57%|█████▋    | 10251/17834 [5:12:04<3:41:20,  1.75s/it] 57%|█████▋    | 10252/17834 [5:12:06<3:37:58,  1.72s/it] 57%|█████▋    | 10253/17834 [5:12:07<3:35:47,  1.71s/it] 57%|█████▋    | 10254/17834 [5:12:09<3:36:04,  1.71s/it] 58%|█████▊    | 10255/17834 [5:12:11<3:39:08,  1.73s/it] 58%|█████▊    | 10256/17834 [5:12:13<3:40:56,  1.75s/it] 58%|█████▊    | 10257/17834 [5:12:14<3:41:46,  1.76s/it] 58%|█████▊    | 10258/17834 [5:12:16<3:41:14,  1.75s/it] 58%|█████▊    | 10259/17834 [5:12:18<3:43:33,  1.77s/it] 58%|█████▊    | 10260/17834 [5:12:20<3:43:46,  1.77s/it] 58%|█████▊    | 10261/17834 [5:12:22<3:43:20,  1.77s/it] 58%|█████▊    | 10262/17834 [5:12:23<3:38:26,  1.73s/it] 58%|█████▊    | 10263/17834 [5:12:25<3:42:56,  1.77s/it] 58%|█████▊    | 10264/17834 [5:12:27<3:40:31,  1.75s/it] 58%|█████▊    | 10265/17834 [5:12:28<3:40:33,  1.75s/it] 58%|█████▊    | 10266/17834 [5:12:30<3:44:13,  1.78s/it] 58%|█████▊    | 10267/17834 [5:12:32<3:40:39,  1.75s/it] 58%|█████▊    | 10268/17834 [5:12:34<3:38:16,  1.73s/it] 58%|█████▊    | 10269/17834 [5:12:35<3:40:23,  1.75s/it] 58%|█████▊    | 10270/17834 [5:12:37<3:37:59,  1.73s/it] 58%|█████▊    | 10271/17834 [5:12:39<3:37:22,  1.72s/it] 58%|█████▊    | 10272/17834 [5:12:41<3:36:08,  1.71s/it] 58%|█████▊    | 10273/17834 [5:12:42<3:39:24,  1.74s/it] 58%|█████▊    | 10274/17834 [5:12:44<3:39:04,  1.74s/it] 58%|█████▊    | 10275/17834 [5:12:46<3:42:20,  1.76s/it] 58%|█████▊    | 10276/17834 [5:12:48<3:40:06,  1.75s/it] 58%|█████▊    | 10277/17834 [5:12:49<3:38:26,  1.73s/it] 58%|█████▊    | 10278/17834 [5:12:51<3:35:40,  1.71s/it] 58%|█████▊    | 10279/17834 [5:12:53<3:38:02,  1.73s/it] 58%|█████▊    | 10280/17834 [5:12:54<3:37:51,  1.73s/it] 58%|█████▊    | 10281/17834 [5:12:56<3:39:50,  1.75s/it] 58%|█████▊    | 10282/17834 [5:12:58<3:42:09,  1.77s/it] 58%|█████▊    | 10283/17834 [5:13:00<3:39:27,  1.74s/it] 58%|█████▊    | 10284/17834 [5:13:02<3:40:03,  1.75s/it] 58%|█████▊    | 10285/17834 [5:13:03<3:39:50,  1.75s/it] 58%|█████▊    | 10286/17834 [5:13:05<3:38:29,  1.74s/it] 58%|█████▊    | 10287/17834 [5:13:07<3:41:24,  1.76s/it] 58%|█████▊    | 10288/17834 [5:13:09<3:39:43,  1.75s/it] 58%|█████▊    | 10289/17834 [5:13:10<3:38:21,  1.74s/it] 58%|█████▊    | 10290/17834 [5:13:12<3:35:54,  1.72s/it] 58%|█████▊    | 10291/17834 [5:13:14<3:36:08,  1.72s/it] 58%|█████▊    | 10292/17834 [5:13:15<3:41:05,  1.76s/it] 58%|█████▊    | 10293/17834 [5:13:17<3:39:35,  1.75s/it] 58%|█████▊    | 10294/17834 [5:13:19<3:43:12,  1.78s/it] 58%|█████▊    | 10295/17834 [5:13:21<3:40:24,  1.75s/it] 58%|█████▊    | 10296/17834 [5:13:23<3:42:15,  1.77s/it] 58%|█████▊    | 10297/17834 [5:13:24<3:41:58,  1.77s/it] 58%|█████▊    | 10298/17834 [5:13:26<3:45:17,  1.79s/it] 58%|█████▊    | 10299/17834 [5:13:28<3:44:26,  1.79s/it]08/31/2024 00:27:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9740161895751953, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026217781007289886, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.165959358215332, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1661934852600098}
 58%|█████▊    | 10300/17834 [5:13:30<3:41:12,  1.76s/it] 58%|█████▊    | 10301/17834 [5:13:31<3:38:55,  1.74s/it] 58%|█████▊    | 10302/17834 [5:13:33<3:40:11,  1.75s/it] 58%|█████▊    | 10303/17834 [5:13:35<3:43:43,  1.78s/it] 58%|█████▊    | 10304/17834 [5:13:37<3:41:36,  1.77s/it] 58%|█████▊    | 10305/17834 [5:13:38<3:39:47,  1.75s/it] 58%|█████▊    | 10306/17834 [5:13:40<3:38:21,  1.74s/it] 58%|█████▊    | 10307/17834 [5:13:42<3:40:00,  1.75s/it] 58%|█████▊    | 10308/17834 [5:13:44<3:38:30,  1.74s/it] 58%|█████▊    | 10309/17834 [5:13:45<3:40:15,  1.76s/it] 58%|█████▊    | 10310/17834 [5:13:47<3:41:02,  1.76s/it] 58%|█████▊    | 10311/17834 [5:13:49<3:40:43,  1.76s/it] 58%|█████▊    | 10312/17834 [5:13:51<3:39:14,  1.75s/it] 58%|█████▊    | 10313/17834 [5:13:53<3:42:20,  1.77s/it] 58%|█████▊    | 10314/17834 [5:13:54<3:40:42,  1.76s/it] 58%|█████▊    | 10315/17834 [5:13:56<3:40:37,  1.76s/it] 58%|█████▊    | 10316/17834 [5:13:58<3:42:25,  1.78s/it] 58%|█████▊    | 10317/17834 [5:14:00<3:40:18,  1.76s/it] 58%|█████▊    | 10318/17834 [5:14:01<3:39:00,  1.75s/it] 58%|█████▊    | 10319/17834 [5:14:03<3:41:31,  1.77s/it] 58%|█████▊    | 10320/17834 [5:14:05<3:40:38,  1.76s/it] 58%|█████▊    | 10321/17834 [5:14:07<3:39:44,  1.75s/it] 58%|█████▊    | 10322/17834 [5:14:08<3:41:21,  1.77s/it] 58%|█████▊    | 10323/17834 [5:14:10<3:43:41,  1.79s/it] 58%|█████▊    | 10324/17834 [5:14:12<3:39:27,  1.75s/it] 58%|█████▊    | 10325/17834 [5:14:14<3:39:57,  1.76s/it] 58%|█████▊    | 10326/17834 [5:14:15<3:41:42,  1.77s/it] 58%|█████▊    | 10327/17834 [5:14:17<3:40:57,  1.77s/it] 58%|█████▊    | 10328/17834 [5:14:19<3:39:05,  1.75s/it] 58%|█████▊    | 10329/17834 [5:14:21<3:41:17,  1.77s/it] 58%|█████▊    | 10330/17834 [5:14:23<3:43:00,  1.78s/it] 58%|█████▊    | 10331/17834 [5:14:24<3:39:56,  1.76s/it] 58%|█████▊    | 10332/17834 [5:14:26<3:38:05,  1.74s/it] 58%|█████▊    | 10333/17834 [5:14:28<3:38:13,  1.75s/it] 58%|█████▊    | 10334/17834 [5:14:29<3:37:52,  1.74s/it] 58%|█████▊    | 10335/17834 [5:14:31<3:35:36,  1.73s/it] 58%|█████▊    | 10336/17834 [5:14:33<3:35:21,  1.72s/it] 58%|█████▊    | 10337/17834 [5:14:35<3:38:17,  1.75s/it] 58%|█████▊    | 10338/17834 [5:14:36<3:39:13,  1.75s/it] 58%|█████▊    | 10339/17834 [5:14:38<3:39:11,  1.75s/it] 58%|█████▊    | 10340/17834 [5:14:40<3:37:07,  1.74s/it] 58%|█████▊    | 10341/17834 [5:14:42<3:39:17,  1.76s/it] 58%|█████▊    | 10342/17834 [5:14:43<3:40:52,  1.77s/it] 58%|█████▊    | 10343/17834 [5:14:45<3:42:22,  1.78s/it] 58%|█████▊    | 10344/17834 [5:14:47<3:39:07,  1.76s/it] 58%|█████▊    | 10345/17834 [5:14:49<3:38:22,  1.75s/it] 58%|█████▊    | 10346/17834 [5:14:50<3:37:21,  1.74s/it] 58%|█████▊    | 10347/17834 [5:14:52<3:36:41,  1.74s/it] 58%|█████▊    | 10348/17834 [5:14:54<3:36:39,  1.74s/it] 58%|█████▊    | 10349/17834 [5:14:56<3:38:29,  1.75s/it]08/31/2024 00:29:15 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0415399074554443, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038304634392261505, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.179567575454712, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2594122886657715}
 58%|█████▊    | 10350/17834 [5:14:58<3:43:28,  1.79s/it] 58%|█████▊    | 10351/17834 [5:14:59<3:40:52,  1.77s/it] 58%|█████▊    | 10352/17834 [5:15:01<3:37:38,  1.75s/it] 58%|█████▊    | 10353/17834 [5:15:03<3:39:10,  1.76s/it] 58%|█████▊    | 10354/17834 [5:15:04<3:38:59,  1.76s/it] 58%|█████▊    | 10355/17834 [5:15:06<3:38:51,  1.76s/it] 58%|█████▊    | 10356/17834 [5:15:08<3:38:06,  1.75s/it] 58%|█████▊    | 10357/17834 [5:15:10<3:40:24,  1.77s/it] 58%|█████▊    | 10358/17834 [5:15:11<3:37:24,  1.74s/it] 58%|█████▊    | 10359/17834 [5:15:13<3:37:30,  1.75s/it] 58%|█████▊    | 10360/17834 [5:15:15<3:36:45,  1.74s/it] 58%|█████▊    | 10361/17834 [5:15:17<3:38:13,  1.75s/it] 58%|█████▊    | 10362/17834 [5:15:19<3:39:00,  1.76s/it] 58%|█████▊    | 10363/17834 [5:15:20<3:39:22,  1.76s/it] 58%|█████▊    | 10364/17834 [5:15:22<3:35:38,  1.73s/it] 58%|█████▊    | 10365/17834 [5:15:24<3:34:32,  1.72s/it] 58%|█████▊    | 10366/17834 [5:15:25<3:35:40,  1.73s/it] 58%|█████▊    | 10367/17834 [5:15:27<3:37:20,  1.75s/it] 58%|█████▊    | 10368/17834 [5:15:29<3:36:41,  1.74s/it] 58%|█████▊    | 10369/17834 [5:15:31<3:35:31,  1.73s/it] 58%|█████▊    | 10370/17834 [5:15:32<3:36:15,  1.74s/it] 58%|█████▊    | 10371/17834 [5:15:34<3:33:48,  1.72s/it] 58%|█████▊    | 10372/17834 [5:15:36<3:34:27,  1.72s/it] 58%|█████▊    | 10373/17834 [5:15:38<3:34:59,  1.73s/it] 58%|█████▊    | 10374/17834 [5:15:39<3:35:06,  1.73s/it] 58%|█████▊    | 10375/17834 [5:15:41<3:37:01,  1.75s/it] 58%|█████▊    | 10376/17834 [5:15:43<3:34:38,  1.73s/it] 58%|█████▊    | 10377/17834 [5:15:44<3:34:55,  1.73s/it] 58%|█████▊    | 10378/17834 [5:15:46<3:35:36,  1.73s/it] 58%|█████▊    | 10379/17834 [5:15:48<3:40:04,  1.77s/it] 58%|█████▊    | 10380/17834 [5:15:50<3:37:49,  1.75s/it] 58%|█████▊    | 10381/17834 [5:15:52<3:37:22,  1.75s/it] 58%|█████▊    | 10382/17834 [5:15:53<3:38:05,  1.76s/it] 58%|█████▊    | 10383/17834 [5:15:55<3:36:37,  1.74s/it] 58%|█████▊    | 10384/17834 [5:15:57<3:37:34,  1.75s/it] 58%|█████▊    | 10385/17834 [5:15:59<3:39:15,  1.77s/it] 58%|█████▊    | 10386/17834 [5:16:00<3:39:04,  1.76s/it] 58%|█████▊    | 10387/17834 [5:16:02<3:39:18,  1.77s/it] 58%|█████▊    | 10388/17834 [5:16:04<3:39:46,  1.77s/it] 58%|█████▊    | 10389/17834 [5:16:06<3:39:50,  1.77s/it] 58%|█████▊    | 10390/17834 [5:16:07<3:39:09,  1.77s/it] 58%|█████▊    | 10391/17834 [5:16:09<3:37:28,  1.75s/it] 58%|█████▊    | 10392/17834 [5:16:11<3:35:55,  1.74s/it] 58%|█████▊    | 10393/17834 [5:16:12<3:32:15,  1.71s/it] 58%|█████▊    | 10394/17834 [5:16:14<3:34:10,  1.73s/it] 58%|█████▊    | 10395/17834 [5:16:16<3:34:58,  1.73s/it] 58%|█████▊    | 10396/17834 [5:16:18<3:37:37,  1.76s/it] 58%|█████▊    | 10397/17834 [5:16:20<3:36:25,  1.75s/it] 58%|█████▊    | 10398/17834 [5:16:21<3:37:58,  1.76s/it] 58%|█████▊    | 10399/17834 [5:16:23<3:37:11,  1.75s/it]08/31/2024 00:30:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8562471866607666, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.021237676963210106, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.101907730102539, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.9793925285339355}
 58%|█████▊    | 10400/17834 [5:16:25<3:34:54,  1.73s/it] 58%|█████▊    | 10401/17834 [5:16:27<3:36:23,  1.75s/it] 58%|█████▊    | 10402/17834 [5:16:28<3:36:52,  1.75s/it] 58%|█████▊    | 10403/17834 [5:16:30<3:35:43,  1.74s/it] 58%|█████▊    | 10404/17834 [5:16:32<3:35:40,  1.74s/it] 58%|█████▊    | 10405/17834 [5:16:33<3:34:16,  1.73s/it] 58%|█████▊    | 10406/17834 [5:16:35<3:33:04,  1.72s/it] 58%|█████▊    | 10407/17834 [5:16:37<3:33:03,  1.72s/it] 58%|█████▊    | 10408/17834 [5:16:39<3:32:39,  1.72s/it] 58%|█████▊    | 10409/17834 [5:16:40<3:39:26,  1.77s/it] 58%|█████▊    | 10410/17834 [5:16:42<3:36:04,  1.75s/it] 58%|█████▊    | 10411/17834 [5:16:44<3:37:42,  1.76s/it] 58%|█████▊    | 10412/17834 [5:16:46<3:34:23,  1.73s/it] 58%|█████▊    | 10413/17834 [5:16:47<3:33:02,  1.72s/it] 58%|█████▊    | 10414/17834 [5:16:49<3:33:01,  1.72s/it] 58%|█████▊    | 10415/17834 [5:16:51<3:33:57,  1.73s/it] 58%|█████▊    | 10416/17834 [5:16:53<3:35:09,  1.74s/it] 58%|█████▊    | 10417/17834 [5:16:54<3:34:05,  1.73s/it] 58%|█████▊    | 10418/17834 [5:16:56<3:34:26,  1.74s/it] 58%|█████▊    | 10419/17834 [5:16:58<3:35:17,  1.74s/it] 58%|█████▊    | 10420/17834 [5:17:00<3:35:14,  1.74s/it] 58%|█████▊    | 10421/17834 [5:17:01<3:33:45,  1.73s/it] 58%|█████▊    | 10422/17834 [5:17:03<3:35:16,  1.74s/it] 58%|█████▊    | 10423/17834 [5:17:05<3:34:00,  1.73s/it] 58%|█████▊    | 10424/17834 [5:17:07<3:37:02,  1.76s/it] 58%|█████▊    | 10425/17834 [5:17:08<3:34:59,  1.74s/it] 58%|█████▊    | 10426/17834 [5:17:10<3:33:51,  1.73s/it] 58%|█████▊    | 10427/17834 [5:17:12<3:32:34,  1.72s/it] 58%|█████▊    | 10428/17834 [5:17:13<3:32:58,  1.73s/it] 58%|█████▊    | 10429/17834 [5:17:15<3:36:23,  1.75s/it] 58%|█████▊    | 10430/17834 [5:17:17<3:34:28,  1.74s/it] 58%|█████▊    | 10431/17834 [5:17:19<3:32:47,  1.72s/it] 58%|█████▊    | 10432/17834 [5:17:20<3:32:09,  1.72s/it] 59%|█████▊    | 10433/17834 [5:17:22<3:32:29,  1.72s/it] 59%|█████▊    | 10434/17834 [5:17:24<3:32:42,  1.72s/it] 59%|█████▊    | 10435/17834 [5:17:26<3:34:28,  1.74s/it] 59%|█████▊    | 10436/17834 [5:17:27<3:37:10,  1.76s/it] 59%|█████▊    | 10437/17834 [5:17:29<3:34:58,  1.74s/it] 59%|█████▊    | 10438/17834 [5:17:31<3:35:13,  1.75s/it] 59%|█████▊    | 10439/17834 [5:17:33<3:36:44,  1.76s/it] 59%|█████▊    | 10440/17834 [5:17:34<3:35:07,  1.75s/it] 59%|█████▊    | 10441/17834 [5:17:36<3:37:52,  1.77s/it] 59%|█████▊    | 10442/17834 [5:17:38<3:34:38,  1.74s/it] 59%|█████▊    | 10443/17834 [5:17:40<3:36:48,  1.76s/it] 59%|█████▊    | 10444/17834 [5:17:41<3:36:37,  1.76s/it] 59%|█████▊    | 10445/17834 [5:17:43<3:35:16,  1.75s/it] 59%|█████▊    | 10446/17834 [5:17:45<3:36:35,  1.76s/it] 59%|█████▊    | 10447/17834 [5:17:47<3:36:04,  1.76s/it] 59%|█████▊    | 10448/17834 [5:17:48<3:37:18,  1.77s/it] 59%|█████▊    | 10449/17834 [5:17:50<3:34:00,  1.74s/it]08/31/2024 00:32:09 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2837786674499512, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030919738113880157, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1903390884399414, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.505037546157837}
 59%|█████▊    | 10450/17834 [5:17:52<3:34:50,  1.75s/it] 59%|█████▊    | 10451/17834 [5:17:54<3:34:57,  1.75s/it] 59%|█████▊    | 10452/17834 [5:17:55<3:37:38,  1.77s/it] 59%|█████▊    | 10453/17834 [5:17:57<3:34:01,  1.74s/it] 59%|█████▊    | 10454/17834 [5:17:59<3:33:10,  1.73s/it] 59%|█████▊    | 10455/17834 [5:18:01<3:36:05,  1.76s/it] 59%|█████▊    | 10456/17834 [5:18:02<3:37:58,  1.77s/it] 59%|█████▊    | 10457/17834 [5:18:04<3:39:02,  1.78s/it] 59%|█████▊    | 10458/17834 [5:18:06<3:38:01,  1.77s/it] 59%|█████▊    | 10459/17834 [5:18:08<3:34:44,  1.75s/it] 59%|█████▊    | 10460/17834 [5:18:09<3:34:41,  1.75s/it] 59%|█████▊    | 10461/17834 [5:18:11<3:34:31,  1.75s/it] 59%|█████▊    | 10462/17834 [5:18:13<3:34:47,  1.75s/it] 59%|█████▊    | 10463/17834 [5:18:15<3:36:58,  1.77s/it] 59%|█████▊    | 10464/17834 [5:18:17<3:37:40,  1.77s/it] 59%|█████▊    | 10465/17834 [5:18:18<3:34:27,  1.75s/it] 59%|█████▊    | 10466/17834 [5:18:20<3:34:32,  1.75s/it] 59%|█████▊    | 10467/17834 [5:18:22<3:36:04,  1.76s/it] 59%|█████▊    | 10468/17834 [5:18:24<3:36:33,  1.76s/it] 59%|█████▊    | 10469/17834 [5:18:25<3:36:58,  1.77s/it] 59%|█████▊    | 10470/17834 [5:18:27<3:37:01,  1.77s/it] 59%|█████▊    | 10471/17834 [5:18:29<3:35:31,  1.76s/it] 59%|█████▊    | 10472/17834 [5:18:30<3:32:59,  1.74s/it] 59%|█████▊    | 10473/17834 [5:18:32<3:35:16,  1.75s/it] 59%|█████▊    | 10474/17834 [5:18:34<3:34:25,  1.75s/it] 59%|█████▊    | 10475/17834 [5:18:36<3:33:33,  1.74s/it] 59%|█████▊    | 10476/17834 [5:18:38<3:35:18,  1.76s/it] 59%|█████▊    | 10477/17834 [5:18:39<3:33:06,  1.74s/it] 59%|█████▉    | 10478/17834 [5:18:41<3:33:44,  1.74s/it] 59%|█████▉    | 10479/17834 [5:18:43<3:33:09,  1.74s/it] 59%|█████▉    | 10480/17834 [5:18:44<3:32:30,  1.73s/it] 59%|█████▉    | 10481/17834 [5:18:46<3:32:46,  1.74s/it] 59%|█████▉    | 10482/17834 [5:18:48<3:31:34,  1.73s/it] 59%|█████▉    | 10483/17834 [5:18:50<3:36:04,  1.76s/it] 59%|█████▉    | 10484/17834 [5:18:51<3:33:22,  1.74s/it] 59%|█████▉    | 10485/17834 [5:18:53<3:32:50,  1.74s/it] 59%|█████▉    | 10486/17834 [5:18:55<3:32:44,  1.74s/it] 59%|█████▉    | 10487/17834 [5:18:57<3:34:15,  1.75s/it] 59%|█████▉    | 10488/17834 [5:18:58<3:35:23,  1.76s/it] 59%|█████▉    | 10489/17834 [5:19:00<3:37:17,  1.78s/it] 59%|█████▉    | 10490/17834 [5:19:02<3:35:13,  1.76s/it] 59%|█████▉    | 10491/17834 [5:19:04<3:33:40,  1.75s/it] 59%|█████▉    | 10492/17834 [5:19:05<3:33:58,  1.75s/it] 59%|█████▉    | 10493/17834 [5:19:07<3:32:43,  1.74s/it] 59%|█████▉    | 10494/17834 [5:19:09<3:33:11,  1.74s/it] 59%|█████▉    | 10495/17834 [5:19:11<3:32:47,  1.74s/it] 59%|█████▉    | 10496/17834 [5:19:12<3:30:51,  1.72s/it] 59%|█████▉    | 10497/17834 [5:19:14<3:30:29,  1.72s/it] 59%|█████▉    | 10498/17834 [5:19:16<3:30:56,  1.73s/it] 59%|█████▉    | 10499/17834 [5:19:17<3:30:20,  1.72s/it]08/31/2024 00:33:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.90945303440094, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05598271265625954, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.4601781368255615, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.425613880157471}
 59%|█████▉    | 10500/17834 [5:19:19<3:28:43,  1.71s/it] 59%|█████▉    | 10501/17834 [5:19:21<3:31:42,  1.73s/it] 59%|█████▉    | 10502/17834 [5:19:23<3:34:34,  1.76s/it] 59%|█████▉    | 10503/17834 [5:19:24<3:30:42,  1.72s/it] 59%|█████▉    | 10504/17834 [5:19:26<3:31:04,  1.73s/it] 59%|█████▉    | 10505/17834 [5:19:28<3:33:44,  1.75s/it] 59%|█████▉    | 10506/17834 [5:19:30<3:33:14,  1.75s/it] 59%|█████▉    | 10507/17834 [5:19:31<3:31:44,  1.73s/it] 59%|█████▉    | 10508/17834 [5:19:33<3:31:52,  1.74s/it] 59%|█████▉    | 10509/17834 [5:19:35<3:31:36,  1.73s/it] 59%|█████▉    | 10510/17834 [5:19:37<3:30:17,  1.72s/it] 59%|█████▉    | 10511/17834 [5:19:38<3:31:35,  1.73s/it] 59%|█████▉    | 10512/17834 [5:19:40<3:34:30,  1.76s/it] 59%|█████▉    | 10513/17834 [5:19:42<3:34:57,  1.76s/it] 59%|█████▉    | 10514/17834 [5:19:44<3:35:33,  1.77s/it] 59%|█████▉    | 10515/17834 [5:19:45<3:35:11,  1.76s/it] 59%|█████▉    | 10516/17834 [5:19:47<3:32:58,  1.75s/it] 59%|█████▉    | 10517/17834 [5:19:49<3:32:14,  1.74s/it] 59%|█████▉    | 10518/17834 [5:19:51<3:31:44,  1.74s/it] 59%|█████▉    | 10519/17834 [5:19:52<3:30:16,  1.72s/it] 59%|█████▉    | 10520/17834 [5:19:54<3:31:49,  1.74s/it] 59%|█████▉    | 10521/17834 [5:19:56<3:32:30,  1.74s/it] 59%|█████▉    | 10522/17834 [5:19:58<3:32:04,  1.74s/it] 59%|█████▉    | 10523/17834 [5:19:59<3:31:59,  1.74s/it] 59%|█████▉    | 10524/17834 [5:20:01<3:31:33,  1.74s/it] 59%|█████▉    | 10525/17834 [5:20:03<3:31:22,  1.74s/it] 59%|█████▉    | 10526/17834 [5:20:05<3:32:45,  1.75s/it] 59%|█████▉    | 10527/17834 [5:20:06<3:32:05,  1.74s/it] 59%|█████▉    | 10528/17834 [5:20:08<3:33:50,  1.76s/it] 59%|█████▉    | 10529/17834 [5:20:10<3:32:00,  1.74s/it] 59%|█████▉    | 10530/17834 [5:20:11<3:32:11,  1.74s/it] 59%|█████▉    | 10531/17834 [5:20:13<3:30:38,  1.73s/it] 59%|█████▉    | 10532/17834 [5:20:15<3:29:03,  1.72s/it] 59%|█████▉    | 10533/17834 [5:20:17<3:30:50,  1.73s/it] 59%|█████▉    | 10534/17834 [5:20:18<3:31:05,  1.73s/it] 59%|█████▉    | 10535/17834 [5:20:20<3:32:40,  1.75s/it] 59%|█████▉    | 10536/17834 [5:20:22<3:33:02,  1.75s/it] 59%|█████▉    | 10537/17834 [5:20:24<3:32:43,  1.75s/it] 59%|█████▉    | 10538/17834 [5:20:25<3:32:15,  1.75s/it] 59%|█████▉    | 10539/17834 [5:20:27<3:29:53,  1.73s/it] 59%|█████▉    | 10540/17834 [5:20:29<3:27:25,  1.71s/it] 59%|█████▉    | 10541/17834 [5:20:31<3:33:14,  1.75s/it] 59%|█████▉    | 10542/17834 [5:20:32<3:30:02,  1.73s/it] 59%|█████▉    | 10543/17834 [5:20:34<3:27:32,  1.71s/it] 59%|█████▉    | 10544/17834 [5:20:36<3:29:04,  1.72s/it] 59%|█████▉    | 10545/17834 [5:20:37<3:29:22,  1.72s/it] 59%|█████▉    | 10546/17834 [5:20:39<3:30:42,  1.73s/it] 59%|█████▉    | 10547/17834 [5:20:41<3:33:55,  1.76s/it] 59%|█████▉    | 10548/17834 [5:20:43<3:33:51,  1.76s/it] 59%|█████▉    | 10549/17834 [5:20:45<3:34:26,  1.77s/it]08/31/2024 00:35:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.329634666442871, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05571269989013672, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2485547065734863, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.633902072906494}
 59%|█████▉    | 10550/17834 [5:20:46<3:37:28,  1.79s/it] 59%|█████▉    | 10551/17834 [5:20:48<3:35:02,  1.77s/it] 59%|█████▉    | 10552/17834 [5:20:50<3:35:52,  1.78s/it] 59%|█████▉    | 10553/17834 [5:20:52<3:33:58,  1.76s/it] 59%|█████▉    | 10554/17834 [5:20:53<3:32:48,  1.75s/it] 59%|█████▉    | 10555/17834 [5:20:55<3:35:35,  1.78s/it] 59%|█████▉    | 10556/17834 [5:20:57<3:31:12,  1.74s/it] 59%|█████▉    | 10557/17834 [5:20:59<3:31:59,  1.75s/it] 59%|█████▉    | 10558/17834 [5:21:00<3:30:06,  1.73s/it] 59%|█████▉    | 10559/17834 [5:21:02<3:30:51,  1.74s/it] 59%|█████▉    | 10560/17834 [5:21:04<3:30:40,  1.74s/it] 59%|█████▉    | 10561/17834 [5:21:06<3:31:09,  1.74s/it] 59%|█████▉    | 10562/17834 [5:21:07<3:29:52,  1.73s/it] 59%|█████▉    | 10563/17834 [5:21:09<3:32:09,  1.75s/it] 59%|█████▉    | 10564/17834 [5:21:11<3:31:08,  1.74s/it] 59%|█████▉    | 10565/17834 [5:21:13<3:32:07,  1.75s/it] 59%|█████▉    | 10566/17834 [5:21:14<3:32:00,  1.75s/it] 59%|█████▉    | 10567/17834 [5:21:16<3:30:32,  1.74s/it] 59%|█████▉    | 10568/17834 [5:21:18<3:30:44,  1.74s/it] 59%|█████▉    | 10569/17834 [5:21:20<3:32:33,  1.76s/it] 59%|█████▉    | 10570/17834 [5:21:21<3:33:09,  1.76s/it] 59%|█████▉    | 10571/17834 [5:21:23<3:34:34,  1.77s/it] 59%|█████▉    | 10572/17834 [5:21:25<3:33:51,  1.77s/it] 59%|█████▉    | 10573/17834 [5:21:27<3:31:49,  1.75s/it] 59%|█████▉    | 10574/17834 [5:21:28<3:31:58,  1.75s/it] 59%|█████▉    | 10575/17834 [5:21:30<3:30:41,  1.74s/it] 59%|█████▉    | 10576/17834 [5:21:32<3:32:04,  1.75s/it] 59%|█████▉    | 10577/17834 [5:21:34<3:32:18,  1.76s/it] 59%|█████▉    | 10578/17834 [5:21:35<3:30:44,  1.74s/it] 59%|█████▉    | 10579/17834 [5:21:37<3:29:06,  1.73s/it] 59%|█████▉    | 10580/17834 [5:21:39<3:28:10,  1.72s/it] 59%|█████▉    | 10581/17834 [5:21:40<3:29:17,  1.73s/it] 59%|█████▉    | 10582/17834 [5:21:42<3:32:06,  1.75s/it] 59%|█████▉    | 10583/17834 [5:21:44<3:29:29,  1.73s/it] 59%|█████▉    | 10584/17834 [5:21:46<3:28:51,  1.73s/it] 59%|█████▉    | 10585/17834 [5:21:47<3:30:17,  1.74s/it] 59%|█████▉    | 10586/17834 [5:21:49<3:29:49,  1.74s/it] 59%|█████▉    | 10587/17834 [5:21:51<3:28:19,  1.72s/it] 59%|█████▉    | 10588/17834 [5:21:53<3:29:35,  1.74s/it] 59%|█████▉    | 10589/17834 [5:21:54<3:29:26,  1.73s/it] 59%|█████▉    | 10590/17834 [5:21:56<3:28:43,  1.73s/it] 59%|█████▉    | 10591/17834 [5:21:58<3:28:25,  1.73s/it] 59%|█████▉    | 10592/17834 [5:22:00<3:28:32,  1.73s/it] 59%|█████▉    | 10593/17834 [5:22:01<3:30:18,  1.74s/it] 59%|█████▉    | 10594/17834 [5:22:03<3:28:40,  1.73s/it] 59%|█████▉    | 10595/17834 [5:22:05<3:29:28,  1.74s/it] 59%|█████▉    | 10596/17834 [5:22:06<3:27:06,  1.72s/it] 59%|█████▉    | 10597/17834 [5:22:08<3:27:46,  1.72s/it] 59%|█████▉    | 10598/17834 [5:22:10<3:30:23,  1.74s/it] 59%|█████▉    | 10599/17834 [5:22:12<3:28:48,  1.73s/it]08/31/2024 00:36:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0901644229888916, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0426977202296257, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1725504398345947, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.305412530899048}
 59%|█████▉    | 10600/17834 [5:22:13<3:31:31,  1.75s/it] 59%|█████▉    | 10601/17834 [5:22:15<3:29:14,  1.74s/it] 59%|█████▉    | 10602/17834 [5:22:17<3:29:28,  1.74s/it] 59%|█████▉    | 10603/17834 [5:22:19<3:29:40,  1.74s/it] 59%|█████▉    | 10604/17834 [5:22:20<3:32:40,  1.76s/it] 59%|█████▉    | 10605/17834 [5:22:22<3:32:32,  1.76s/it] 59%|█████▉    | 10606/17834 [5:22:24<3:32:37,  1.77s/it] 59%|█████▉    | 10607/17834 [5:22:26<3:33:17,  1.77s/it] 59%|█████▉    | 10608/17834 [5:22:28<3:33:42,  1.77s/it] 59%|█████▉    | 10609/17834 [5:22:29<3:32:44,  1.77s/it] 59%|█████▉    | 10610/17834 [5:22:31<3:30:48,  1.75s/it] 59%|█████▉    | 10611/17834 [5:22:33<3:30:13,  1.75s/it] 60%|█████▉    | 10612/17834 [5:22:34<3:29:15,  1.74s/it] 60%|█████▉    | 10613/17834 [5:22:36<3:28:27,  1.73s/it] 60%|█████▉    | 10614/17834 [5:22:38<3:27:35,  1.73s/it] 60%|█████▉    | 10615/17834 [5:22:40<3:26:10,  1.71s/it] 60%|█████▉    | 10616/17834 [5:22:41<3:24:48,  1.70s/it] 60%|█████▉    | 10617/17834 [5:22:43<3:25:02,  1.70s/it] 60%|█████▉    | 10618/17834 [5:22:45<3:25:57,  1.71s/it] 60%|█████▉    | 10619/17834 [5:22:46<3:27:21,  1.72s/it] 60%|█████▉    | 10620/17834 [5:22:48<3:28:18,  1.73s/it] 60%|█████▉    | 10621/17834 [5:22:50<3:32:15,  1.77s/it] 60%|█████▉    | 10622/17834 [5:22:52<3:31:41,  1.76s/it] 60%|█████▉    | 10623/17834 [5:22:53<3:28:22,  1.73s/it] 60%|█████▉    | 10624/17834 [5:22:55<3:31:12,  1.76s/it] 60%|█████▉    | 10625/17834 [5:22:57<3:30:33,  1.75s/it] 60%|█████▉    | 10626/17834 [5:22:59<3:27:16,  1.73s/it] 60%|█████▉    | 10627/17834 [5:23:00<3:27:54,  1.73s/it] 60%|█████▉    | 10628/17834 [5:23:02<3:26:48,  1.72s/it] 60%|█████▉    | 10629/17834 [5:23:04<3:28:23,  1.74s/it] 60%|█████▉    | 10630/17834 [5:23:06<3:30:09,  1.75s/it] 60%|█████▉    | 10631/17834 [5:23:07<3:28:06,  1.73s/it] 60%|█████▉    | 10632/17834 [5:23:09<3:30:44,  1.76s/it] 60%|█████▉    | 10633/17834 [5:23:11<3:29:06,  1.74s/it] 60%|█████▉    | 10634/17834 [5:23:13<3:32:20,  1.77s/it] 60%|█████▉    | 10635/17834 [5:23:15<3:35:50,  1.80s/it] 60%|█████▉    | 10636/17834 [5:23:16<3:32:54,  1.77s/it] 60%|█████▉    | 10637/17834 [5:23:18<3:33:56,  1.78s/it] 60%|█████▉    | 10638/17834 [5:23:20<3:31:28,  1.76s/it] 60%|█████▉    | 10639/17834 [5:23:22<3:31:50,  1.77s/it] 60%|█████▉    | 10640/17834 [5:23:23<3:33:00,  1.78s/it] 60%|█████▉    | 10641/17834 [5:23:25<3:33:53,  1.78s/it] 60%|█████▉    | 10642/17834 [5:23:27<3:33:20,  1.78s/it] 60%|█████▉    | 10643/17834 [5:23:29<3:34:59,  1.79s/it] 60%|█████▉    | 10644/17834 [5:23:31<3:32:41,  1.77s/it] 60%|█████▉    | 10645/17834 [5:23:32<3:32:55,  1.78s/it] 60%|█████▉    | 10646/17834 [5:23:34<3:29:18,  1.75s/it] 60%|█████▉    | 10647/17834 [5:23:36<3:31:06,  1.76s/it] 60%|█████▉    | 10648/17834 [5:23:38<3:31:31,  1.77s/it] 60%|█████▉    | 10649/17834 [5:23:39<3:29:20,  1.75s/it]08/31/2024 00:37:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.082758903503418, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.036437440663576126, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2206594944000244, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.339855670928955}
 60%|█████▉    | 10650/17834 [5:23:41<3:31:25,  1.77s/it] 60%|█████▉    | 10651/17834 [5:23:43<3:33:51,  1.79s/it] 60%|█████▉    | 10652/17834 [5:23:45<3:33:08,  1.78s/it] 60%|█████▉    | 10653/17834 [5:23:46<3:32:12,  1.77s/it] 60%|█████▉    | 10654/17834 [5:23:48<3:30:27,  1.76s/it] 60%|█████▉    | 10655/17834 [5:23:50<3:31:07,  1.76s/it] 60%|█████▉    | 10656/17834 [5:23:52<3:30:11,  1.76s/it] 60%|█████▉    | 10657/17834 [5:23:53<3:28:16,  1.74s/it] 60%|█████▉    | 10658/17834 [5:23:55<3:30:15,  1.76s/it] 60%|█████▉    | 10659/17834 [5:23:57<3:32:24,  1.78s/it] 60%|█████▉    | 10660/17834 [5:23:59<3:33:34,  1.79s/it] 60%|█████▉    | 10661/17834 [5:24:01<3:32:30,  1.78s/it] 60%|█████▉    | 10662/17834 [5:24:02<3:33:42,  1.79s/it] 60%|█████▉    | 10663/17834 [5:24:04<3:29:45,  1.76s/it] 60%|█████▉    | 10664/17834 [5:24:06<3:30:54,  1.76s/it] 60%|█████▉    | 10665/17834 [5:24:08<3:29:06,  1.75s/it] 60%|█████▉    | 10666/17834 [5:24:09<3:27:41,  1.74s/it] 60%|█████▉    | 10667/17834 [5:24:11<3:26:14,  1.73s/it] 60%|█████▉    | 10668/17834 [5:24:13<3:29:13,  1.75s/it] 60%|█████▉    | 10669/17834 [5:24:15<3:29:39,  1.76s/it] 60%|█████▉    | 10670/17834 [5:24:16<3:28:06,  1.74s/it] 60%|█████▉    | 10671/17834 [5:24:18<3:27:29,  1.74s/it] 60%|█████▉    | 10672/17834 [5:24:20<3:26:47,  1.73s/it] 60%|█████▉    | 10673/17834 [5:24:21<3:26:53,  1.73s/it] 60%|█████▉    | 10674/17834 [5:24:23<3:27:58,  1.74s/it] 60%|█████▉    | 10675/17834 [5:24:25<3:27:11,  1.74s/it] 60%|█████▉    | 10676/17834 [5:24:27<3:29:42,  1.76s/it] 60%|█████▉    | 10677/17834 [5:24:28<3:27:56,  1.74s/it] 60%|█████▉    | 10678/17834 [5:24:30<3:28:07,  1.75s/it] 60%|█████▉    | 10679/17834 [5:24:32<3:28:28,  1.75s/it] 60%|█████▉    | 10680/17834 [5:24:34<3:28:29,  1.75s/it] 60%|█████▉    | 10681/17834 [5:24:35<3:26:35,  1.73s/it] 60%|█████▉    | 10682/17834 [5:24:37<3:27:21,  1.74s/it] 60%|█████▉    | 10683/17834 [5:24:39<3:30:22,  1.77s/it] 60%|█████▉    | 10684/17834 [5:24:41<3:27:49,  1.74s/it] 60%|█████▉    | 10685/17834 [5:24:42<3:26:17,  1.73s/it] 60%|█████▉    | 10686/17834 [5:24:44<3:27:54,  1.75s/it] 60%|█████▉    | 10687/17834 [5:24:46<3:28:34,  1.75s/it] 60%|█████▉    | 10688/17834 [5:24:48<3:28:00,  1.75s/it] 60%|█████▉    | 10689/17834 [5:24:49<3:26:19,  1.73s/it] 60%|█████▉    | 10690/17834 [5:24:51<3:24:44,  1.72s/it] 60%|█████▉    | 10691/17834 [5:24:53<3:25:51,  1.73s/it]08/31/2024 00:39:12 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/31/2024 00:39:12 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:50,  4.34it/s][A
  1%|▏         | 3/221 [00:00<00:58,  3.72it/s][A
  2%|▏         | 4/221 [00:00<00:49,  4.36it/s][A
  2%|▏         | 5/221 [00:01<00:53,  4.02it/s][A
  3%|▎         | 6/221 [00:01<00:55,  3.89it/s][A
  3%|▎         | 7/221 [00:01<00:58,  3.67it/s][A
  4%|▎         | 8/221 [00:02<00:57,  3.73it/s][A
  4%|▍         | 9/221 [00:02<01:06,  3.17it/s][A
  5%|▍         | 10/221 [00:02<01:15,  2.78it/s][A
  5%|▍         | 11/221 [00:03<01:01,  3.42it/s][A
  5%|▌         | 12/221 [00:03<00:53,  3.90it/s][A
  6%|▌         | 13/221 [00:03<00:48,  4.29it/s][A
  6%|▋         | 14/221 [00:03<00:53,  3.89it/s][A
  7%|▋         | 15/221 [00:03<00:48,  4.25it/s][A
  7%|▋         | 16/221 [00:04<00:49,  4.12it/s][A
  8%|▊         | 17/221 [00:04<00:49,  4.15it/s][A
  8%|▊         | 18/221 [00:04<00:45,  4.43it/s][A
  9%|▊         | 19/221 [00:04<00:47,  4.25it/s][A
  9%|▉         | 20/221 [00:05<00:43,  4.58it/s][A
 10%|▉         | 21/221 [00:05<00:42,  4.67it/s][A
 10%|▉         | 22/221 [00:05<00:48,  4.10it/s][A
 10%|█         | 23/221 [00:05<00:47,  4.15it/s][A
 11%|█▏        | 25/221 [00:06<00:39,  4.94it/s][A
 12%|█▏        | 26/221 [00:06<00:39,  4.96it/s][A
 12%|█▏        | 27/221 [00:06<00:53,  3.61it/s][A
 13%|█▎        | 28/221 [00:07<01:00,  3.16it/s][A
 13%|█▎        | 29/221 [00:07<01:09,  2.75it/s][A
 14%|█▎        | 30/221 [00:08<01:11,  2.66it/s][A
 14%|█▍        | 31/221 [00:08<01:11,  2.65it/s][A
 14%|█▍        | 32/221 [00:08<01:06,  2.83it/s][A
 15%|█▍        | 33/221 [00:08<00:55,  3.41it/s][A
 15%|█▌        | 34/221 [00:09<00:54,  3.45it/s][A
 16%|█▌        | 35/221 [00:09<00:57,  3.21it/s][A
 16%|█▋        | 36/221 [00:09<00:57,  3.21it/s][A
 17%|█▋        | 37/221 [00:10<00:52,  3.52it/s][A
 17%|█▋        | 38/221 [00:10<00:51,  3.54it/s][A
 18%|█▊        | 39/221 [00:10<00:43,  4.23it/s][A
 18%|█▊        | 40/221 [00:10<00:51,  3.51it/s][A
 19%|█▊        | 41/221 [00:11<00:55,  3.23it/s][A
 19%|█▉        | 43/221 [00:11<00:42,  4.16it/s][A
 20%|█▉        | 44/221 [00:11<00:47,  3.73it/s][A
 20%|██        | 45/221 [00:12<00:53,  3.30it/s][A
 21%|██        | 46/221 [00:12<00:54,  3.19it/s][A
 21%|██▏       | 47/221 [00:12<00:51,  3.37it/s][A
 22%|██▏       | 48/221 [00:13<00:45,  3.82it/s][A
 22%|██▏       | 49/221 [00:13<00:43,  3.92it/s][A
 23%|██▎       | 50/221 [00:13<00:48,  3.55it/s][A
 23%|██▎       | 51/221 [00:14<00:50,  3.39it/s][A
 24%|██▎       | 52/221 [00:14<00:45,  3.71it/s][A
 24%|██▍       | 53/221 [00:14<00:40,  4.20it/s][A
 24%|██▍       | 54/221 [00:14<00:39,  4.20it/s][A
 25%|██▍       | 55/221 [00:15<00:45,  3.67it/s][A
 25%|██▌       | 56/221 [00:15<00:46,  3.51it/s][A
 26%|██▌       | 57/221 [00:15<00:57,  2.85it/s][A
 26%|██▌       | 58/221 [00:16<00:49,  3.27it/s][A
 27%|██▋       | 59/221 [00:16<00:41,  3.90it/s][A
 27%|██▋       | 60/221 [00:16<00:40,  4.00it/s][A
 28%|██▊       | 61/221 [00:16<00:44,  3.62it/s][A
 28%|██▊       | 62/221 [00:16<00:41,  3.84it/s][A
 29%|██▊       | 63/221 [00:17<00:43,  3.62it/s][A
 29%|██▉       | 64/221 [00:17<00:48,  3.25it/s][A
 29%|██▉       | 65/221 [00:18<00:51,  3.04it/s][A
 30%|██▉       | 66/221 [00:18<00:52,  2.95it/s][A
 30%|███       | 67/221 [00:18<00:51,  2.98it/s][A
 31%|███       | 68/221 [00:18<00:42,  3.62it/s][A
 31%|███       | 69/221 [00:19<00:53,  2.86it/s][A
 32%|███▏      | 70/221 [00:19<00:59,  2.55it/s][A
 32%|███▏      | 71/221 [00:20<00:49,  3.02it/s][A
 33%|███▎      | 72/221 [00:20<00:58,  2.57it/s][A
 33%|███▎      | 73/221 [00:20<00:54,  2.71it/s][A
 34%|███▍      | 75/221 [00:21<00:40,  3.58it/s][A
 34%|███▍      | 76/221 [00:21<00:52,  2.78it/s][A
 35%|███▍      | 77/221 [00:22<00:57,  2.51it/s][A
 35%|███▌      | 78/221 [00:22<00:53,  2.65it/s][A
 36%|███▌      | 79/221 [00:23<00:56,  2.50it/s][A
 36%|███▌      | 80/221 [00:23<00:54,  2.60it/s][A
 37%|███▋      | 81/221 [00:23<00:42,  3.26it/s][A
 37%|███▋      | 82/221 [00:23<00:40,  3.40it/s][A
 38%|███▊      | 83/221 [00:24<00:42,  3.28it/s][A
 38%|███▊      | 84/221 [00:24<00:38,  3.56it/s][A
 38%|███▊      | 85/221 [00:24<00:35,  3.85it/s][A
 39%|███▉      | 86/221 [00:25<00:48,  2.76it/s][A
 39%|███▉      | 87/221 [00:25<00:53,  2.49it/s][A
 40%|███▉      | 88/221 [00:26<00:58,  2.28it/s][A
 40%|████      | 89/221 [00:26<00:53,  2.49it/s][A
 41%|████      | 90/221 [00:26<00:47,  2.74it/s][A
 41%|████      | 91/221 [00:27<00:40,  3.19it/s][A
 42%|████▏     | 92/221 [00:27<00:40,  3.16it/s][A
 42%|████▏     | 93/221 [00:28<00:52,  2.42it/s][A
 43%|████▎     | 94/221 [00:28<00:47,  2.69it/s][A
 43%|████▎     | 95/221 [00:28<00:37,  3.32it/s][A
 43%|████▎     | 96/221 [00:28<00:40,  3.12it/s][A
 44%|████▍     | 97/221 [00:29<00:39,  3.11it/s][A
 44%|████▍     | 98/221 [00:29<00:45,  2.71it/s][A
 45%|████▍     | 99/221 [00:29<00:42,  2.86it/s][A
 45%|████▌     | 100/221 [00:30<00:43,  2.81it/s][A
 46%|████▌     | 101/221 [00:30<00:38,  3.11it/s][A
 46%|████▌     | 102/221 [00:30<00:33,  3.57it/s][A
 47%|████▋     | 103/221 [00:31<00:34,  3.42it/s][A
 47%|████▋     | 104/221 [00:31<00:44,  2.66it/s][A
 48%|████▊     | 105/221 [00:32<00:45,  2.55it/s][A
 48%|████▊     | 106/221 [00:32<00:39,  2.89it/s][A
 48%|████▊     | 107/221 [00:32<00:36,  3.14it/s][A
 49%|████▉     | 108/221 [00:32<00:33,  3.33it/s][A
 49%|████▉     | 109/221 [00:32<00:28,  3.97it/s][A
 50%|████▉     | 110/221 [00:33<00:28,  3.87it/s][A
 50%|█████     | 111/221 [00:33<00:29,  3.79it/s][A
 51%|█████     | 112/221 [00:33<00:28,  3.81it/s][A
 51%|█████     | 113/221 [00:33<00:28,  3.83it/s][A
 52%|█████▏    | 114/221 [00:34<00:25,  4.17it/s][A
 52%|█████▏    | 115/221 [00:34<00:32,  3.27it/s][A
 52%|█████▏    | 116/221 [00:34<00:30,  3.49it/s][A
 53%|█████▎    | 117/221 [00:35<00:26,  3.86it/s][A
 53%|█████▎    | 118/221 [00:35<00:25,  4.00it/s][A
 54%|█████▍    | 119/221 [00:35<00:26,  3.83it/s][A
 54%|█████▍    | 120/221 [00:36<00:32,  3.11it/s][A
 55%|█████▍    | 121/221 [00:36<00:30,  3.30it/s][A
 55%|█████▌    | 122/221 [00:36<00:31,  3.19it/s][A
 56%|█████▌    | 123/221 [00:36<00:27,  3.58it/s][A
 56%|█████▌    | 124/221 [00:37<00:28,  3.45it/s][A
 57%|█████▋    | 125/221 [00:37<00:31,  3.05it/s][A
 57%|█████▋    | 126/221 [00:37<00:25,  3.73it/s][A
 57%|█████▋    | 127/221 [00:38<00:27,  3.41it/s][A
 58%|█████▊    | 128/221 [00:38<00:26,  3.53it/s][A
 58%|█████▊    | 129/221 [00:38<00:22,  4.15it/s][A
 59%|█████▉    | 130/221 [00:38<00:22,  3.97it/s][A
 59%|█████▉    | 131/221 [00:39<00:25,  3.49it/s][A
 60%|█████▉    | 132/221 [00:39<00:28,  3.11it/s][A
 60%|██████    | 133/221 [00:39<00:31,  2.83it/s][A
 61%|██████    | 134/221 [00:40<00:31,  2.76it/s][A
 61%|██████    | 135/221 [00:40<00:29,  2.88it/s][A
 62%|██████▏   | 136/221 [00:40<00:26,  3.22it/s][A
 62%|██████▏   | 137/221 [00:41<00:23,  3.65it/s][A
 62%|██████▏   | 138/221 [00:41<00:22,  3.75it/s][A
 63%|██████▎   | 139/221 [00:41<00:20,  3.94it/s][A
 63%|██████▎   | 140/221 [00:41<00:22,  3.61it/s][A
 64%|██████▍   | 141/221 [00:42<00:23,  3.44it/s][A
 64%|██████▍   | 142/221 [00:42<00:21,  3.59it/s][A
 65%|██████▍   | 143/221 [00:42<00:25,  3.07it/s][A
 65%|██████▌   | 144/221 [00:43<00:24,  3.08it/s][A
 66%|██████▌   | 145/221 [00:43<00:25,  3.03it/s][A
 66%|██████▌   | 146/221 [00:43<00:21,  3.53it/s][A
 67%|██████▋   | 147/221 [00:44<00:21,  3.38it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:44<00:25,  2.82it/s][A
 68%|██████▊   | 150/221 [00:44<00:21,  3.34it/s][A
 68%|██████▊   | 151/221 [00:45<00:23,  3.04it/s][A
 69%|██████▉   | 152/221 [00:45<00:22,  3.09it/s][A
 69%|██████▉   | 153/221 [00:45<00:18,  3.71it/s][A
 70%|██████▉   | 154/221 [00:46<00:21,  3.07it/s][A
 70%|███████   | 155/221 [00:46<00:21,  3.01it/s][A
 71%|███████   | 156/221 [00:46<00:20,  3.15it/s][A
 71%|███████   | 157/221 [00:47<00:20,  3.12it/s][A
 71%|███████▏  | 158/221 [00:47<00:25,  2.49it/s][A
 72%|███████▏  | 159/221 [00:48<00:22,  2.78it/s][A
 72%|███████▏  | 160/221 [00:48<00:21,  2.86it/s][A
 73%|███████▎  | 161/221 [00:48<00:22,  2.68it/s][A
 73%|███████▎  | 162/221 [00:49<00:18,  3.14it/s][A
 74%|███████▍  | 163/221 [00:49<00:19,  2.98it/s][A
 75%|███████▍  | 165/221 [00:49<00:13,  4.04it/s][A
 75%|███████▌  | 166/221 [00:49<00:12,  4.51it/s][A
 76%|███████▌  | 167/221 [00:50<00:15,  3.41it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.51it/s][A
 76%|███████▋  | 169/221 [00:50<00:15,  3.39it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.27it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.19it/s][A
 78%|███████▊  | 172/221 [00:51<00:15,  3.19it/s][A
 78%|███████▊  | 173/221 [00:52<00:17,  2.80it/s][A
 79%|███████▊  | 174/221 [00:52<00:15,  3.10it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.10it/s][A
 80%|███████▉  | 176/221 [00:53<00:11,  3.78it/s][A
 80%|████████  | 177/221 [00:53<00:10,  4.10it/s][A
 81%|████████  | 178/221 [00:53<00:10,  4.15it/s][A
 81%|████████  | 179/221 [00:53<00:09,  4.21it/s][A
 81%|████████▏ | 180/221 [00:53<00:08,  4.58it/s][A
 82%|████████▏ | 181/221 [00:54<00:10,  3.98it/s][A
 82%|████████▏ | 182/221 [00:54<00:09,  4.04it/s][A
 83%|████████▎ | 183/221 [00:54<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:55<00:12,  3.07it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.21it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.30it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.17it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.47it/s][A
 86%|████████▌ | 189/221 [00:56<00:08,  3.74it/s][A
 86%|████████▌ | 190/221 [00:56<00:08,  3.79it/s][A
 86%|████████▋ | 191/221 [00:57<00:10,  2.96it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.36it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.71it/s][A
 88%|████████▊ | 194/221 [00:58<00:06,  3.98it/s][A
 88%|████████▊ | 195/221 [00:58<00:06,  3.96it/s][A
 89%|████████▊ | 196/221 [00:58<00:07,  3.21it/s][A
 89%|████████▉ | 197/221 [00:58<00:06,  3.56it/s][A
 90%|████████▉ | 198/221 [00:59<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  3.95it/s][A
 90%|█████████ | 200/221 [00:59<00:04,  4.22it/s][A
 91%|█████████ | 201/221 [00:59<00:04,  4.26it/s][A
 91%|█████████▏| 202/221 [01:00<00:04,  4.26it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.29it/s][A
 92%|█████████▏| 204/221 [01:00<00:03,  4.61it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.73it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.78it/s][A
 94%|█████████▎| 207/221 [01:01<00:02,  4.88it/s][A
 94%|█████████▍| 208/221 [01:01<00:02,  5.49it/s][A
 95%|█████████▍| 209/221 [01:01<00:02,  5.80it/s][A
 95%|█████████▌| 210/221 [01:01<00:01,  5.82it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  4.14it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.39it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.53it/s][A
 97%|█████████▋| 214/221 [01:02<00:01,  3.55it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.90it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  4.02it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.43it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.13it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  3.07it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.38it/s][A
100%|██████████| 221/221 [01:04<00:00,  3.65it/s][A100%|██████████| 221/221 [01:04<00:00,  3.41it/s]
08/31/2024 00:41:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 10691--===========

08/31/2024 00:41:36 - INFO - __main__ -   {'area_r1': 5.0, 'area_recall': '5.0/13.5/19.9', 'area_ravg': 12.8}
08/31/2024 00:41:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 10691--===========

08/31/2024 00:41:36 - INFO - __main__ -   {'area_r1': 40.3, 'area_recall': '40.3/71.9/82.6', 'area_ravg': 64.9}
08/31/2024 00:41:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 10691--===========

08/31/2024 00:41:36 - INFO - __main__ -   {'video_r1': 35.3, 'video_recall': '35.3/67.0/77.5', 'video_ravg': 59.9}
08/31/2024 00:41:36 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/31/2024 00:41:36 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/31/2024 00:41:36 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 10691--===========

08/31/2024 00:41:36 - INFO - __main__ -   {'video_r1': 56.4, 'video_recall': '56.4/79.3/85.9', 'video_ravg': 73.9}
08/31/2024 00:41:36 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/31/2024 00:41:36 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
 60%|█████▉    | 10692/17834 [5:27:39<101:11:41, 51.01s/it] 60%|█████▉    | 10693/17834 [5:27:40<71:49:02, 36.21s/it]  60%|█████▉    | 10694/17834 [5:27:42<51:22:25, 25.90s/it] 60%|█████▉    | 10695/17834 [5:27:44<36:59:25, 18.65s/it] 60%|█████▉    | 10696/17834 [5:27:46<26:54:07, 13.57s/it] 60%|█████▉    | 10697/17834 [5:27:48<19:52:01, 10.02s/it] 60%|█████▉    | 10698/17834 [5:27:49<14:56:10,  7.54s/it] 60%|█████▉    | 10699/17834 [5:27:51<11:28:56,  5.79s/it]08/31/2024 00:42:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.256221890449524, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03640032559633255, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.276738166809082, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5693602561950684}
 60%|█████▉    | 10700/17834 [5:27:53<9:05:32,  4.59s/it]  60%|██████    | 10701/17834 [5:27:55<7:24:28,  3.74s/it] 60%|██████    | 10702/17834 [5:27:56<6:14:25,  3.15s/it] 60%|██████    | 10703/17834 [5:27:58<5:26:52,  2.75s/it] 60%|██████    | 10704/17834 [5:28:00<4:49:59,  2.44s/it] 60%|██████    | 10705/17834 [5:28:02<4:25:07,  2.23s/it] 60%|██████    | 10706/17834 [5:28:03<4:08:52,  2.09s/it] 60%|██████    | 10707/17834 [5:28:05<3:54:47,  1.98s/it] 60%|██████    | 10708/17834 [5:28:07<3:49:31,  1.93s/it] 60%|██████    | 10709/17834 [5:28:09<3:45:40,  1.90s/it] 60%|██████    | 10710/17834 [5:28:10<3:36:31,  1.82s/it] 60%|██████    | 10711/17834 [5:28:12<3:35:16,  1.81s/it] 60%|██████    | 10712/17834 [5:28:14<3:35:14,  1.81s/it] 60%|██████    | 10713/17834 [5:28:16<3:36:21,  1.82s/it] 60%|██████    | 10714/17834 [5:28:18<3:33:18,  1.80s/it] 60%|██████    | 10715/17834 [5:28:19<3:31:28,  1.78s/it] 60%|██████    | 10716/17834 [5:28:21<3:28:59,  1.76s/it] 60%|██████    | 10717/17834 [5:28:23<3:30:21,  1.77s/it] 60%|██████    | 10718/17834 [5:28:25<3:28:49,  1.76s/it] 60%|██████    | 10719/17834 [5:28:26<3:29:11,  1.76s/it] 60%|██████    | 10720/17834 [5:28:28<3:27:16,  1.75s/it] 60%|██████    | 10721/17834 [5:28:30<3:30:10,  1.77s/it] 60%|██████    | 10722/17834 [5:28:32<3:25:57,  1.74s/it] 60%|██████    | 10723/17834 [5:28:33<3:25:13,  1.73s/it] 60%|██████    | 10724/17834 [5:28:35<3:24:32,  1.73s/it] 60%|██████    | 10725/17834 [5:28:37<3:27:07,  1.75s/it] 60%|██████    | 10726/17834 [5:28:38<3:26:49,  1.75s/it] 60%|██████    | 10727/17834 [5:28:40<3:26:07,  1.74s/it] 60%|██████    | 10728/17834 [5:28:42<3:25:41,  1.74s/it] 60%|██████    | 10729/17834 [5:28:44<3:26:28,  1.74s/it] 60%|██████    | 10730/17834 [5:28:46<3:29:52,  1.77s/it] 60%|██████    | 10731/17834 [5:28:47<3:27:22,  1.75s/it] 60%|██████    | 10732/17834 [5:28:49<3:27:48,  1.76s/it] 60%|██████    | 10733/17834 [5:28:51<3:27:43,  1.76s/it] 60%|██████    | 10734/17834 [5:28:52<3:27:22,  1.75s/it] 60%|██████    | 10735/17834 [5:28:54<3:26:24,  1.74s/it] 60%|██████    | 10736/17834 [5:28:56<3:26:05,  1.74s/it] 60%|██████    | 10737/17834 [5:28:58<3:26:37,  1.75s/it] 60%|██████    | 10738/17834 [5:28:59<3:24:51,  1.73s/it] 60%|██████    | 10739/17834 [5:29:01<3:26:57,  1.75s/it] 60%|██████    | 10740/17834 [5:29:03<3:27:19,  1.75s/it] 60%|██████    | 10741/17834 [5:29:05<3:27:12,  1.75s/it] 60%|██████    | 10742/17834 [5:29:06<3:25:56,  1.74s/it] 60%|██████    | 10743/17834 [5:29:08<3:27:46,  1.76s/it] 60%|██████    | 10744/17834 [5:29:10<3:30:17,  1.78s/it] 60%|██████    | 10745/17834 [5:29:12<3:30:18,  1.78s/it] 60%|██████    | 10746/17834 [5:29:14<3:29:26,  1.77s/it] 60%|██████    | 10747/17834 [5:29:15<3:28:05,  1.76s/it] 60%|██████    | 10748/17834 [5:29:17<3:27:50,  1.76s/it] 60%|██████    | 10749/17834 [5:29:19<3:27:57,  1.76s/it]08/31/2024 00:43:38 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2272475957870483, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023704828694462776, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1905832290649414, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.441535711288452}
 60%|██████    | 10750/17834 [5:29:21<3:26:09,  1.75s/it] 60%|██████    | 10751/17834 [5:29:22<3:26:19,  1.75s/it] 60%|██████    | 10752/17834 [5:29:24<3:27:33,  1.76s/it] 60%|██████    | 10753/17834 [5:29:26<3:27:42,  1.76s/it] 60%|██████    | 10754/17834 [5:29:28<3:29:08,  1.77s/it] 60%|██████    | 10755/17834 [5:29:29<3:27:57,  1.76s/it] 60%|██████    | 10756/17834 [5:29:31<3:25:54,  1.75s/it] 60%|██████    | 10757/17834 [5:29:33<3:26:43,  1.75s/it] 60%|██████    | 10758/17834 [5:29:35<3:26:46,  1.75s/it] 60%|██████    | 10759/17834 [5:29:36<3:24:10,  1.73s/it] 60%|██████    | 10760/17834 [5:29:38<3:23:29,  1.73s/it] 60%|██████    | 10761/17834 [5:29:40<3:24:07,  1.73s/it] 60%|██████    | 10762/17834 [5:29:42<3:24:21,  1.73s/it] 60%|██████    | 10763/17834 [5:29:43<3:24:40,  1.74s/it] 60%|██████    | 10764/17834 [5:29:45<3:22:39,  1.72s/it] 60%|██████    | 10765/17834 [5:29:47<3:23:00,  1.72s/it] 60%|██████    | 10766/17834 [5:29:48<3:24:36,  1.74s/it] 60%|██████    | 10767/17834 [5:29:50<3:24:49,  1.74s/it] 60%|██████    | 10768/17834 [5:29:52<3:22:53,  1.72s/it] 60%|██████    | 10769/17834 [5:29:54<3:27:11,  1.76s/it] 60%|██████    | 10770/17834 [5:29:55<3:26:49,  1.76s/it] 60%|██████    | 10771/17834 [5:29:57<3:26:16,  1.75s/it] 60%|██████    | 10772/17834 [5:29:59<3:25:04,  1.74s/it] 60%|██████    | 10773/17834 [5:30:01<3:25:03,  1.74s/it] 60%|██████    | 10774/17834 [5:30:02<3:28:00,  1.77s/it] 60%|██████    | 10775/17834 [5:30:04<3:28:45,  1.77s/it] 60%|██████    | 10776/17834 [5:30:06<3:27:25,  1.76s/it] 60%|██████    | 10777/17834 [5:30:08<3:27:49,  1.77s/it] 60%|██████    | 10778/17834 [5:30:10<3:29:04,  1.78s/it] 60%|██████    | 10779/17834 [5:30:11<3:27:37,  1.77s/it] 60%|██████    | 10780/17834 [5:30:13<3:25:23,  1.75s/it] 60%|██████    | 10781/17834 [5:30:15<3:26:37,  1.76s/it] 60%|██████    | 10782/17834 [5:30:17<3:27:41,  1.77s/it] 60%|██████    | 10783/17834 [5:30:18<3:25:55,  1.75s/it] 60%|██████    | 10784/17834 [5:30:20<3:24:58,  1.74s/it] 60%|██████    | 10785/17834 [5:30:22<3:25:35,  1.75s/it] 60%|██████    | 10786/17834 [5:30:23<3:23:07,  1.73s/it] 60%|██████    | 10787/17834 [5:30:25<3:21:59,  1.72s/it] 60%|██████    | 10788/17834 [5:30:27<3:24:22,  1.74s/it] 60%|██████    | 10789/17834 [5:30:29<3:25:34,  1.75s/it] 61%|██████    | 10790/17834 [5:30:31<3:26:18,  1.76s/it] 61%|██████    | 10791/17834 [5:30:32<3:24:41,  1.74s/it] 61%|██████    | 10792/17834 [5:30:34<3:23:31,  1.73s/it] 61%|██████    | 10793/17834 [5:30:36<3:24:58,  1.75s/it] 61%|██████    | 10794/17834 [5:30:37<3:24:43,  1.74s/it] 61%|██████    | 10795/17834 [5:30:39<3:23:26,  1.73s/it] 61%|██████    | 10796/17834 [5:30:41<3:22:10,  1.72s/it] 61%|██████    | 10797/17834 [5:30:43<3:24:16,  1.74s/it] 61%|██████    | 10798/17834 [5:30:44<3:24:40,  1.75s/it] 61%|██████    | 10799/17834 [5:30:46<3:27:48,  1.77s/it]08/31/2024 00:45:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1324002742767334, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04615116864442825, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2191359996795654, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3976874351501465}
 61%|██████    | 10800/17834 [5:30:48<3:27:43,  1.77s/it] 61%|██████    | 10801/17834 [5:30:50<3:27:55,  1.77s/it] 61%|██████    | 10802/17834 [5:30:51<3:24:26,  1.74s/it] 61%|██████    | 10803/17834 [5:30:53<3:25:56,  1.76s/it] 61%|██████    | 10804/17834 [5:30:55<3:24:52,  1.75s/it] 61%|██████    | 10805/17834 [5:30:57<3:25:01,  1.75s/it] 61%|██████    | 10806/17834 [5:30:58<3:24:28,  1.75s/it] 61%|██████    | 10807/17834 [5:31:00<3:24:49,  1.75s/it] 61%|██████    | 10808/17834 [5:31:02<3:22:32,  1.73s/it] 61%|██████    | 10809/17834 [5:31:04<3:22:38,  1.73s/it] 61%|██████    | 10810/17834 [5:31:05<3:24:48,  1.75s/it] 61%|██████    | 10811/17834 [5:31:07<3:24:55,  1.75s/it] 61%|██████    | 10812/17834 [5:31:09<3:23:22,  1.74s/it] 61%|██████    | 10813/17834 [5:31:11<3:22:52,  1.73s/it] 61%|██████    | 10814/17834 [5:31:12<3:24:39,  1.75s/it] 61%|██████    | 10815/17834 [5:31:14<3:24:58,  1.75s/it] 61%|██████    | 10816/17834 [5:31:16<3:25:44,  1.76s/it] 61%|██████    | 10817/17834 [5:31:18<3:26:02,  1.76s/it] 61%|██████    | 10818/17834 [5:31:19<3:23:08,  1.74s/it] 61%|██████    | 10819/17834 [5:31:21<3:27:10,  1.77s/it] 61%|██████    | 10820/17834 [5:31:23<3:24:34,  1.75s/it] 61%|██████    | 10821/17834 [5:31:25<3:25:19,  1.76s/it] 61%|██████    | 10822/17834 [5:31:27<3:27:01,  1.77s/it] 61%|██████    | 10823/17834 [5:31:28<3:24:59,  1.75s/it] 61%|██████    | 10824/17834 [5:31:30<3:24:12,  1.75s/it] 61%|██████    | 10825/17834 [5:31:32<3:22:34,  1.73s/it] 61%|██████    | 10826/17834 [5:31:33<3:23:14,  1.74s/it] 61%|██████    | 10827/17834 [5:31:35<3:22:59,  1.74s/it] 61%|██████    | 10828/17834 [5:31:37<3:23:36,  1.74s/it] 61%|██████    | 10829/17834 [5:31:39<3:25:06,  1.76s/it] 61%|██████    | 10830/17834 [5:31:40<3:23:27,  1.74s/it] 61%|██████    | 10831/17834 [5:31:42<3:23:40,  1.75s/it] 61%|██████    | 10832/17834 [5:31:44<3:23:46,  1.75s/it] 61%|██████    | 10833/17834 [5:31:46<3:22:58,  1.74s/it] 61%|██████    | 10834/17834 [5:31:47<3:22:36,  1.74s/it] 61%|██████    | 10835/17834 [5:31:49<3:22:16,  1.73s/it] 61%|██████    | 10836/17834 [5:31:51<3:23:08,  1.74s/it] 61%|██████    | 10837/17834 [5:31:53<3:25:06,  1.76s/it] 61%|██████    | 10838/17834 [5:31:54<3:24:20,  1.75s/it] 61%|██████    | 10839/17834 [5:31:56<3:22:02,  1.73s/it] 61%|██████    | 10840/17834 [5:31:58<3:22:57,  1.74s/it] 61%|██████    | 10841/17834 [5:32:00<3:26:01,  1.77s/it] 61%|██████    | 10842/17834 [5:32:01<3:26:05,  1.77s/it] 61%|██████    | 10843/17834 [5:32:03<3:24:33,  1.76s/it] 61%|██████    | 10844/17834 [5:32:05<3:23:47,  1.75s/it] 61%|██████    | 10845/17834 [5:32:07<3:22:44,  1.74s/it] 61%|██████    | 10846/17834 [5:32:08<3:22:56,  1.74s/it] 61%|██████    | 10847/17834 [5:32:10<3:20:33,  1.72s/it] 61%|██████    | 10848/17834 [5:32:12<3:21:10,  1.73s/it] 61%|██████    | 10849/17834 [5:32:14<3:26:02,  1.77s/it]08/31/2024 00:46:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1124142408370972, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0334879532456398, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2308783531188965, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3767805099487305}
 61%|██████    | 10850/17834 [5:32:15<3:24:00,  1.75s/it] 61%|██████    | 10851/17834 [5:32:17<3:22:15,  1.74s/it] 61%|██████    | 10852/17834 [5:32:19<3:20:08,  1.72s/it] 61%|██████    | 10853/17834 [5:32:21<3:21:20,  1.73s/it] 61%|██████    | 10854/17834 [5:32:22<3:20:29,  1.72s/it] 61%|██████    | 10855/17834 [5:32:24<3:23:45,  1.75s/it] 61%|██████    | 10856/17834 [5:32:26<3:21:29,  1.73s/it] 61%|██████    | 10857/17834 [5:32:27<3:22:52,  1.74s/it] 61%|██████    | 10858/17834 [5:32:29<3:21:04,  1.73s/it] 61%|██████    | 10859/17834 [5:32:31<3:21:00,  1.73s/it] 61%|██████    | 10860/17834 [5:32:33<3:23:23,  1.75s/it] 61%|██████    | 10861/17834 [5:32:34<3:23:29,  1.75s/it] 61%|██████    | 10862/17834 [5:32:36<3:21:10,  1.73s/it] 61%|██████    | 10863/17834 [5:32:38<3:25:40,  1.77s/it] 61%|██████    | 10864/17834 [5:32:40<3:25:05,  1.77s/it] 61%|██████    | 10865/17834 [5:32:41<3:23:19,  1.75s/it] 61%|██████    | 10866/17834 [5:32:43<3:27:25,  1.79s/it] 61%|██████    | 10867/17834 [5:32:45<3:26:05,  1.77s/it] 61%|██████    | 10868/17834 [5:32:47<3:25:18,  1.77s/it] 61%|██████    | 10869/17834 [5:32:49<3:22:53,  1.75s/it] 61%|██████    | 10870/17834 [5:32:50<3:23:50,  1.76s/it] 61%|██████    | 10871/17834 [5:32:52<3:22:47,  1.75s/it] 61%|██████    | 10872/17834 [5:32:54<3:21:30,  1.74s/it] 61%|██████    | 10873/17834 [5:32:56<3:24:44,  1.76s/it] 61%|██████    | 10874/17834 [5:32:57<3:25:31,  1.77s/it] 61%|██████    | 10875/17834 [5:32:59<3:24:54,  1.77s/it] 61%|██████    | 10876/17834 [5:33:01<3:22:16,  1.74s/it] 61%|██████    | 10877/17834 [5:33:03<3:24:38,  1.76s/it] 61%|██████    | 10878/17834 [5:33:04<3:21:36,  1.74s/it] 61%|██████    | 10879/17834 [5:33:06<3:20:52,  1.73s/it] 61%|██████    | 10880/17834 [5:33:08<3:22:16,  1.75s/it] 61%|██████    | 10881/17834 [5:33:10<3:22:38,  1.75s/it] 61%|██████    | 10882/17834 [5:33:11<3:24:11,  1.76s/it] 61%|██████    | 10883/17834 [5:33:13<3:25:35,  1.77s/it] 61%|██████    | 10884/17834 [5:33:15<3:21:30,  1.74s/it] 61%|██████    | 10885/17834 [5:33:17<3:22:29,  1.75s/it] 61%|██████    | 10886/17834 [5:33:18<3:21:19,  1.74s/it] 61%|██████    | 10887/17834 [5:33:20<3:22:11,  1.75s/it] 61%|██████    | 10888/17834 [5:33:22<3:21:43,  1.74s/it] 61%|██████    | 10889/17834 [5:33:24<3:22:33,  1.75s/it] 61%|██████    | 10890/17834 [5:33:25<3:21:00,  1.74s/it] 61%|██████    | 10891/17834 [5:33:27<3:22:02,  1.75s/it] 61%|██████    | 10892/17834 [5:33:29<3:24:59,  1.77s/it] 61%|██████    | 10893/17834 [5:33:31<3:23:04,  1.76s/it] 61%|██████    | 10894/17834 [5:33:32<3:22:23,  1.75s/it] 61%|██████    | 10895/17834 [5:33:34<3:22:13,  1.75s/it] 61%|██████    | 10896/17834 [5:33:36<3:23:29,  1.76s/it] 61%|██████    | 10897/17834 [5:33:38<3:23:40,  1.76s/it] 61%|██████    | 10898/17834 [5:33:39<3:21:39,  1.74s/it] 61%|██████    | 10899/17834 [5:33:41<3:21:25,  1.74s/it]08/31/2024 00:48:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1392114162445068, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030410487204790115, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.244886875152588, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.414508819580078}
 61%|██████    | 10900/17834 [5:33:43<3:21:26,  1.74s/it] 61%|██████    | 10901/17834 [5:33:45<3:23:04,  1.76s/it] 61%|██████    | 10902/17834 [5:33:46<3:22:21,  1.75s/it] 61%|██████    | 10903/17834 [5:33:48<3:24:25,  1.77s/it] 61%|██████    | 10904/17834 [5:33:50<3:23:33,  1.76s/it] 61%|██████    | 10905/17834 [5:33:52<3:22:24,  1.75s/it] 61%|██████    | 10906/17834 [5:33:53<3:21:39,  1.75s/it] 61%|██████    | 10907/17834 [5:33:55<3:22:50,  1.76s/it] 61%|██████    | 10908/17834 [5:33:57<3:21:46,  1.75s/it] 61%|██████    | 10909/17834 [5:33:59<3:22:03,  1.75s/it] 61%|██████    | 10910/17834 [5:34:00<3:20:55,  1.74s/it] 61%|██████    | 10911/17834 [5:34:02<3:23:17,  1.76s/it] 61%|██████    | 10912/17834 [5:34:04<3:20:53,  1.74s/it] 61%|██████    | 10913/17834 [5:34:06<3:18:59,  1.73s/it] 61%|██████    | 10914/17834 [5:34:07<3:18:30,  1.72s/it] 61%|██████    | 10915/17834 [5:34:09<3:19:42,  1.73s/it] 61%|██████    | 10916/17834 [5:34:11<3:23:00,  1.76s/it] 61%|██████    | 10917/17834 [5:34:13<3:25:06,  1.78s/it] 61%|██████    | 10918/17834 [5:34:14<3:23:56,  1.77s/it] 61%|██████    | 10919/17834 [5:34:16<3:23:39,  1.77s/it] 61%|██████    | 10920/17834 [5:34:18<3:23:17,  1.76s/it] 61%|██████    | 10921/17834 [5:34:20<3:23:11,  1.76s/it] 61%|██████    | 10922/17834 [5:34:21<3:23:45,  1.77s/it] 61%|██████    | 10923/17834 [5:34:23<3:23:47,  1.77s/it] 61%|██████▏   | 10924/17834 [5:34:25<3:23:43,  1.77s/it] 61%|██████▏   | 10925/17834 [5:34:27<3:25:44,  1.79s/it] 61%|██████▏   | 10926/17834 [5:34:29<3:27:25,  1.80s/it] 61%|██████▏   | 10927/17834 [5:34:30<3:25:57,  1.79s/it] 61%|██████▏   | 10928/17834 [5:34:32<3:23:17,  1.77s/it] 61%|██████▏   | 10929/17834 [5:34:34<3:21:36,  1.75s/it] 61%|██████▏   | 10930/17834 [5:34:36<3:21:29,  1.75s/it] 61%|██████▏   | 10931/17834 [5:34:37<3:20:04,  1.74s/it] 61%|██████▏   | 10932/17834 [5:34:39<3:20:12,  1.74s/it] 61%|██████▏   | 10933/17834 [5:34:41<3:21:55,  1.76s/it] 61%|██████▏   | 10934/17834 [5:34:43<3:22:13,  1.76s/it] 61%|██████▏   | 10935/17834 [5:34:44<3:21:10,  1.75s/it] 61%|██████▏   | 10936/17834 [5:34:46<3:20:48,  1.75s/it] 61%|██████▏   | 10937/17834 [5:34:48<3:20:21,  1.74s/it] 61%|██████▏   | 10938/17834 [5:34:50<3:21:40,  1.75s/it] 61%|██████▏   | 10939/17834 [5:34:51<3:22:44,  1.76s/it] 61%|██████▏   | 10940/17834 [5:34:53<3:22:24,  1.76s/it] 61%|██████▏   | 10941/17834 [5:34:55<3:22:37,  1.76s/it] 61%|██████▏   | 10942/17834 [5:34:57<3:23:06,  1.77s/it] 61%|██████▏   | 10943/17834 [5:34:58<3:23:13,  1.77s/it] 61%|██████▏   | 10944/17834 [5:35:00<3:23:29,  1.77s/it] 61%|██████▏   | 10945/17834 [5:35:02<3:21:26,  1.75s/it] 61%|██████▏   | 10946/17834 [5:35:04<3:26:05,  1.80s/it] 61%|██████▏   | 10947/17834 [5:35:06<3:22:50,  1.77s/it] 61%|██████▏   | 10948/17834 [5:35:07<3:21:23,  1.75s/it] 61%|██████▏   | 10949/17834 [5:35:09<3:20:38,  1.75s/it]08/31/2024 00:49:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0203531980514526, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025040563195943832, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.165299892425537, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.210693597793579}
 61%|██████▏   | 10950/17834 [5:35:11<3:22:10,  1.76s/it] 61%|██████▏   | 10951/17834 [5:35:13<3:22:18,  1.76s/it] 61%|██████▏   | 10952/17834 [5:35:14<3:21:54,  1.76s/it] 61%|██████▏   | 10953/17834 [5:35:16<3:22:45,  1.77s/it] 61%|██████▏   | 10954/17834 [5:35:18<3:23:36,  1.78s/it] 61%|██████▏   | 10955/17834 [5:35:20<3:24:20,  1.78s/it] 61%|██████▏   | 10956/17834 [5:35:21<3:23:52,  1.78s/it] 61%|██████▏   | 10957/17834 [5:35:23<3:24:19,  1.78s/it] 61%|██████▏   | 10958/17834 [5:35:25<3:26:16,  1.80s/it] 61%|██████▏   | 10959/17834 [5:35:27<3:22:54,  1.77s/it] 61%|██████▏   | 10960/17834 [5:35:29<3:23:51,  1.78s/it] 61%|██████▏   | 10961/17834 [5:35:30<3:23:02,  1.77s/it] 61%|██████▏   | 10962/17834 [5:35:32<3:24:41,  1.79s/it] 61%|██████▏   | 10963/17834 [5:35:34<3:21:39,  1.76s/it] 61%|██████▏   | 10964/17834 [5:35:36<3:22:25,  1.77s/it] 61%|██████▏   | 10965/17834 [5:35:37<3:19:18,  1.74s/it] 61%|██████▏   | 10966/17834 [5:35:39<3:18:59,  1.74s/it] 61%|██████▏   | 10967/17834 [5:35:41<3:21:59,  1.76s/it] 62%|██████▏   | 10968/17834 [5:35:43<3:23:55,  1.78s/it] 62%|██████▏   | 10969/17834 [5:35:44<3:22:32,  1.77s/it] 62%|██████▏   | 10970/17834 [5:35:46<3:23:47,  1.78s/it] 62%|██████▏   | 10971/17834 [5:35:48<3:22:59,  1.77s/it] 62%|██████▏   | 10972/17834 [5:35:50<3:20:49,  1.76s/it] 62%|██████▏   | 10973/17834 [5:35:51<3:20:05,  1.75s/it] 62%|██████▏   | 10974/17834 [5:35:53<3:21:26,  1.76s/it] 62%|██████▏   | 10975/17834 [5:35:55<3:21:14,  1.76s/it] 62%|██████▏   | 10976/17834 [5:35:57<3:19:51,  1.75s/it] 62%|██████▏   | 10977/17834 [5:35:58<3:17:32,  1.73s/it] 62%|██████▏   | 10978/17834 [5:36:00<3:17:09,  1.73s/it] 62%|██████▏   | 10979/17834 [5:36:02<3:20:30,  1.76s/it] 62%|██████▏   | 10980/17834 [5:36:04<3:21:46,  1.77s/it] 62%|██████▏   | 10981/17834 [5:36:06<3:22:23,  1.77s/it] 62%|██████▏   | 10982/17834 [5:36:07<3:20:38,  1.76s/it] 62%|██████▏   | 10983/17834 [5:36:09<3:20:20,  1.75s/it] 62%|██████▏   | 10984/17834 [5:36:11<3:20:40,  1.76s/it] 62%|██████▏   | 10985/17834 [5:36:13<3:18:50,  1.74s/it] 62%|██████▏   | 10986/17834 [5:36:14<3:16:38,  1.72s/it] 62%|██████▏   | 10987/17834 [5:36:16<3:22:32,  1.77s/it] 62%|██████▏   | 10988/17834 [5:36:18<3:20:02,  1.75s/it] 62%|██████▏   | 10989/17834 [5:36:19<3:18:28,  1.74s/it] 62%|██████▏   | 10990/17834 [5:36:21<3:18:11,  1.74s/it] 62%|██████▏   | 10991/17834 [5:36:23<3:21:23,  1.77s/it] 62%|██████▏   | 10992/17834 [5:36:25<3:19:34,  1.75s/it] 62%|██████▏   | 10993/17834 [5:36:27<3:20:32,  1.76s/it] 62%|██████▏   | 10994/17834 [5:36:28<3:19:10,  1.75s/it] 62%|██████▏   | 10995/17834 [5:36:30<3:17:51,  1.74s/it] 62%|██████▏   | 10996/17834 [5:36:32<3:16:43,  1.73s/it] 62%|██████▏   | 10997/17834 [5:36:33<3:18:18,  1.74s/it] 62%|██████▏   | 10998/17834 [5:36:35<3:16:07,  1.72s/it] 62%|██████▏   | 10999/17834 [5:36:37<3:16:29,  1.72s/it]08/31/2024 00:50:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1116955280303955, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038622401654720306, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2772560119628906, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4275739192962646}
 62%|██████▏   | 11000/17834 [5:36:39<3:17:10,  1.73s/it] 62%|██████▏   | 11001/17834 [5:36:40<3:16:58,  1.73s/it] 62%|██████▏   | 11002/17834 [5:36:42<3:17:00,  1.73s/it] 62%|██████▏   | 11003/17834 [5:36:44<3:18:21,  1.74s/it] 62%|██████▏   | 11004/17834 [5:36:46<3:19:37,  1.75s/it] 62%|██████▏   | 11005/17834 [5:36:47<3:22:45,  1.78s/it] 62%|██████▏   | 11006/17834 [5:36:49<3:22:27,  1.78s/it] 62%|██████▏   | 11007/17834 [5:36:51<3:19:12,  1.75s/it] 62%|██████▏   | 11008/17834 [5:36:53<3:22:49,  1.78s/it] 62%|██████▏   | 11009/17834 [5:36:54<3:19:09,  1.75s/it] 62%|██████▏   | 11010/17834 [5:36:56<3:20:28,  1.76s/it] 62%|██████▏   | 11011/17834 [5:36:58<3:20:35,  1.76s/it] 62%|██████▏   | 11012/17834 [5:37:00<3:18:27,  1.75s/it] 62%|██████▏   | 11013/17834 [5:37:02<3:24:23,  1.80s/it] 62%|██████▏   | 11014/17834 [5:37:03<3:19:21,  1.75s/it] 62%|██████▏   | 11015/17834 [5:37:05<3:17:22,  1.74s/it] 62%|██████▏   | 11016/17834 [5:37:07<3:18:11,  1.74s/it] 62%|██████▏   | 11017/17834 [5:37:08<3:15:55,  1.72s/it] 62%|██████▏   | 11018/17834 [5:37:10<3:16:59,  1.73s/it] 62%|██████▏   | 11019/17834 [5:37:12<3:20:16,  1.76s/it] 62%|██████▏   | 11020/17834 [5:37:14<3:17:29,  1.74s/it] 62%|██████▏   | 11021/17834 [5:37:15<3:16:14,  1.73s/it] 62%|██████▏   | 11022/17834 [5:37:17<3:17:21,  1.74s/it] 62%|██████▏   | 11023/17834 [5:37:19<3:18:59,  1.75s/it] 62%|██████▏   | 11024/17834 [5:37:21<3:19:44,  1.76s/it] 62%|██████▏   | 11025/17834 [5:37:22<3:18:31,  1.75s/it] 62%|██████▏   | 11026/17834 [5:37:24<3:19:04,  1.75s/it] 62%|██████▏   | 11027/17834 [5:37:26<3:22:21,  1.78s/it] 62%|██████▏   | 11028/17834 [5:37:28<3:20:01,  1.76s/it] 62%|██████▏   | 11029/17834 [5:37:30<3:24:01,  1.80s/it] 62%|██████▏   | 11030/17834 [5:37:31<3:19:39,  1.76s/it] 62%|██████▏   | 11031/17834 [5:37:33<3:15:42,  1.73s/it] 62%|██████▏   | 11032/17834 [5:37:35<3:19:10,  1.76s/it] 62%|██████▏   | 11033/17834 [5:37:37<3:21:10,  1.77s/it] 62%|██████▏   | 11034/17834 [5:37:38<3:20:18,  1.77s/it] 62%|██████▏   | 11035/17834 [5:37:40<3:19:44,  1.76s/it] 62%|██████▏   | 11036/17834 [5:37:42<3:19:36,  1.76s/it] 62%|██████▏   | 11037/17834 [5:37:44<3:17:44,  1.75s/it] 62%|██████▏   | 11038/17834 [5:37:45<3:18:20,  1.75s/it] 62%|██████▏   | 11039/17834 [5:37:47<3:17:15,  1.74s/it] 62%|██████▏   | 11040/17834 [5:37:49<3:19:00,  1.76s/it] 62%|██████▏   | 11041/17834 [5:37:51<3:18:32,  1.75s/it] 62%|██████▏   | 11042/17834 [5:37:52<3:18:38,  1.75s/it] 62%|██████▏   | 11043/17834 [5:37:54<3:18:34,  1.75s/it] 62%|██████▏   | 11044/17834 [5:37:56<3:16:10,  1.73s/it] 62%|██████▏   | 11045/17834 [5:37:58<3:17:49,  1.75s/it] 62%|██████▏   | 11046/17834 [5:37:59<3:19:24,  1.76s/it] 62%|██████▏   | 11047/17834 [5:38:01<3:18:28,  1.75s/it] 62%|██████▏   | 11048/17834 [5:38:03<3:19:44,  1.77s/it] 62%|██████▏   | 11049/17834 [5:38:05<3:18:41,  1.76s/it]08/31/2024 00:52:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9845262169837952, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030469542369246483, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.169586181640625, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.184581995010376}
 62%|██████▏   | 11050/17834 [5:38:06<3:18:13,  1.75s/it] 62%|██████▏   | 11051/17834 [5:38:08<3:16:42,  1.74s/it] 62%|██████▏   | 11052/17834 [5:38:10<3:17:30,  1.75s/it] 62%|██████▏   | 11053/17834 [5:38:12<3:18:55,  1.76s/it] 62%|██████▏   | 11054/17834 [5:38:13<3:17:46,  1.75s/it] 62%|██████▏   | 11055/17834 [5:38:15<3:16:09,  1.74s/it] 62%|██████▏   | 11056/17834 [5:38:17<3:17:24,  1.75s/it] 62%|██████▏   | 11057/17834 [5:38:19<3:18:23,  1.76s/it] 62%|██████▏   | 11058/17834 [5:38:20<3:17:45,  1.75s/it] 62%|██████▏   | 11059/17834 [5:38:22<3:18:42,  1.76s/it] 62%|██████▏   | 11060/17834 [5:38:24<3:15:36,  1.73s/it] 62%|██████▏   | 11061/17834 [5:38:26<3:18:10,  1.76s/it] 62%|██████▏   | 11062/17834 [5:38:27<3:18:24,  1.76s/it] 62%|██████▏   | 11063/17834 [5:38:29<3:22:16,  1.79s/it] 62%|██████▏   | 11064/17834 [5:38:31<3:20:49,  1.78s/it] 62%|██████▏   | 11065/17834 [5:38:33<3:18:43,  1.76s/it] 62%|██████▏   | 11066/17834 [5:38:34<3:17:22,  1.75s/it] 62%|██████▏   | 11067/17834 [5:38:36<3:22:04,  1.79s/it] 62%|██████▏   | 11068/17834 [5:38:38<3:18:59,  1.76s/it] 62%|██████▏   | 11069/17834 [5:38:40<3:16:28,  1.74s/it] 62%|██████▏   | 11070/17834 [5:38:42<3:17:09,  1.75s/it] 62%|██████▏   | 11071/17834 [5:38:43<3:18:27,  1.76s/it] 62%|██████▏   | 11072/17834 [5:38:45<3:17:54,  1.76s/it] 62%|██████▏   | 11073/17834 [5:38:47<3:17:49,  1.76s/it] 62%|██████▏   | 11074/17834 [5:38:49<3:19:14,  1.77s/it] 62%|██████▏   | 11075/17834 [5:38:50<3:18:06,  1.76s/it] 62%|██████▏   | 11076/17834 [5:38:52<3:17:02,  1.75s/it] 62%|██████▏   | 11077/17834 [5:38:54<3:17:58,  1.76s/it] 62%|██████▏   | 11078/17834 [5:38:56<3:16:35,  1.75s/it] 62%|██████▏   | 11079/17834 [5:38:57<3:17:10,  1.75s/it] 62%|██████▏   | 11080/17834 [5:38:59<3:20:15,  1.78s/it] 62%|██████▏   | 11081/17834 [5:39:01<3:17:05,  1.75s/it] 62%|██████▏   | 11082/17834 [5:39:03<3:20:08,  1.78s/it] 62%|██████▏   | 11083/17834 [5:39:04<3:19:43,  1.78s/it] 62%|██████▏   | 11084/17834 [5:39:06<3:20:58,  1.79s/it] 62%|██████▏   | 11085/17834 [5:39:08<3:22:10,  1.80s/it] 62%|██████▏   | 11086/17834 [5:39:10<3:19:41,  1.78s/it] 62%|██████▏   | 11087/17834 [5:39:12<3:21:53,  1.80s/it] 62%|██████▏   | 11088/17834 [5:39:13<3:17:02,  1.75s/it] 62%|██████▏   | 11089/17834 [5:39:15<3:17:26,  1.76s/it] 62%|██████▏   | 11090/17834 [5:39:17<3:17:02,  1.75s/it] 62%|██████▏   | 11091/17834 [5:39:19<3:15:41,  1.74s/it] 62%|██████▏   | 11092/17834 [5:39:20<3:19:21,  1.77s/it] 62%|██████▏   | 11093/17834 [5:39:22<3:18:03,  1.76s/it] 62%|██████▏   | 11094/17834 [5:39:24<3:18:42,  1.77s/it] 62%|██████▏   | 11095/17834 [5:39:26<3:18:50,  1.77s/it] 62%|██████▏   | 11096/17834 [5:39:27<3:17:03,  1.75s/it] 62%|██████▏   | 11097/17834 [5:39:29<3:19:24,  1.78s/it] 62%|██████▏   | 11098/17834 [5:39:31<3:15:40,  1.74s/it] 62%|██████▏   | 11099/17834 [5:39:33<3:17:11,  1.76s/it]08/31/2024 00:53:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.6264071464538574, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05863106995820999, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.6198010444641113, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 4.304839134216309}
 62%|██████▏   | 11100/17834 [5:39:34<3:16:01,  1.75s/it] 62%|██████▏   | 11101/17834 [5:39:36<3:16:15,  1.75s/it] 62%|██████▏   | 11102/17834 [5:39:38<3:19:45,  1.78s/it] 62%|██████▏   | 11103/17834 [5:39:40<3:20:39,  1.79s/it] 62%|██████▏   | 11104/17834 [5:39:42<3:17:12,  1.76s/it] 62%|██████▏   | 11105/17834 [5:39:43<3:16:02,  1.75s/it] 62%|██████▏   | 11106/17834 [5:39:45<3:18:29,  1.77s/it] 62%|██████▏   | 11107/17834 [5:39:47<3:17:02,  1.76s/it] 62%|██████▏   | 11108/17834 [5:39:49<3:17:25,  1.76s/it] 62%|██████▏   | 11109/17834 [5:39:50<3:16:13,  1.75s/it] 62%|██████▏   | 11110/17834 [5:39:52<3:15:31,  1.74s/it] 62%|██████▏   | 11111/17834 [5:39:54<3:19:23,  1.78s/it] 62%|██████▏   | 11112/17834 [5:39:56<3:19:46,  1.78s/it] 62%|██████▏   | 11113/17834 [5:39:57<3:16:44,  1.76s/it] 62%|██████▏   | 11114/17834 [5:39:59<3:14:58,  1.74s/it] 62%|██████▏   | 11115/17834 [5:40:01<3:14:35,  1.74s/it] 62%|██████▏   | 11116/17834 [5:40:03<3:16:23,  1.75s/it] 62%|██████▏   | 11117/17834 [5:40:04<3:15:26,  1.75s/it] 62%|██████▏   | 11118/17834 [5:40:06<3:17:58,  1.77s/it] 62%|██████▏   | 11119/17834 [5:40:08<3:15:57,  1.75s/it] 62%|██████▏   | 11120/17834 [5:40:10<3:15:03,  1.74s/it] 62%|██████▏   | 11121/17834 [5:40:11<3:14:13,  1.74s/it] 62%|██████▏   | 11122/17834 [5:40:13<3:13:43,  1.73s/it] 62%|██████▏   | 11123/17834 [5:40:15<3:15:16,  1.75s/it] 62%|██████▏   | 11124/17834 [5:40:16<3:13:25,  1.73s/it] 62%|██████▏   | 11125/17834 [5:40:18<3:11:13,  1.71s/it] 62%|██████▏   | 11126/17834 [5:40:20<3:12:27,  1.72s/it] 62%|██████▏   | 11127/17834 [5:40:22<3:14:06,  1.74s/it] 62%|██████▏   | 11128/17834 [5:40:23<3:13:21,  1.73s/it] 62%|██████▏   | 11129/17834 [5:40:25<3:13:05,  1.73s/it] 62%|██████▏   | 11130/17834 [5:40:27<3:14:29,  1.74s/it] 62%|██████▏   | 11131/17834 [5:40:29<3:15:39,  1.75s/it] 62%|██████▏   | 11132/17834 [5:40:30<3:14:37,  1.74s/it] 62%|██████▏   | 11133/17834 [5:40:32<3:14:00,  1.74s/it] 62%|██████▏   | 11134/17834 [5:40:34<3:13:36,  1.73s/it] 62%|██████▏   | 11135/17834 [5:40:36<3:15:04,  1.75s/it] 62%|██████▏   | 11136/17834 [5:40:37<3:16:53,  1.76s/it] 62%|██████▏   | 11137/17834 [5:40:39<3:19:17,  1.79s/it] 62%|██████▏   | 11138/17834 [5:40:41<3:18:11,  1.78s/it] 62%|██████▏   | 11139/17834 [5:40:43<3:17:39,  1.77s/it] 62%|██████▏   | 11140/17834 [5:40:44<3:16:19,  1.76s/it] 62%|██████▏   | 11141/17834 [5:40:46<3:13:38,  1.74s/it] 62%|██████▏   | 11142/17834 [5:40:48<3:12:49,  1.73s/it] 62%|██████▏   | 11143/17834 [5:40:50<3:15:07,  1.75s/it] 62%|██████▏   | 11144/17834 [5:40:51<3:13:59,  1.74s/it] 62%|██████▏   | 11145/17834 [5:40:53<3:15:22,  1.75s/it] 62%|██████▏   | 11146/17834 [5:40:55<3:13:21,  1.73s/it] 63%|██████▎   | 11147/17834 [5:40:57<3:12:49,  1.73s/it] 63%|██████▎   | 11148/17834 [5:40:58<3:11:54,  1.72s/it] 63%|██████▎   | 11149/17834 [5:41:00<3:13:23,  1.74s/it]08/31/2024 00:55:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2351739406585693, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032096993178129196, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.289393901824951, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5566649436950684}
 63%|██████▎   | 11150/17834 [5:41:02<3:15:12,  1.75s/it] 63%|██████▎   | 11151/17834 [5:41:04<3:16:08,  1.76s/it] 63%|██████▎   | 11152/17834 [5:41:05<3:16:07,  1.76s/it] 63%|██████▎   | 11153/17834 [5:41:07<3:16:10,  1.76s/it] 63%|██████▎   | 11154/17834 [5:41:09<3:14:24,  1.75s/it] 63%|██████▎   | 11155/17834 [5:41:11<3:14:12,  1.74s/it] 63%|██████▎   | 11156/17834 [5:41:12<3:15:13,  1.75s/it] 63%|██████▎   | 11157/17834 [5:41:14<3:13:47,  1.74s/it] 63%|██████▎   | 11158/17834 [5:41:16<3:12:18,  1.73s/it] 63%|██████▎   | 11159/17834 [5:41:18<3:12:09,  1.73s/it] 63%|██████▎   | 11160/17834 [5:41:19<3:13:39,  1.74s/it] 63%|██████▎   | 11161/17834 [5:41:21<3:14:24,  1.75s/it] 63%|██████▎   | 11162/17834 [5:41:23<3:15:41,  1.76s/it] 63%|██████▎   | 11163/17834 [5:41:25<3:15:22,  1.76s/it] 63%|██████▎   | 11164/17834 [5:41:26<3:16:01,  1.76s/it] 63%|██████▎   | 11165/17834 [5:41:28<3:15:50,  1.76s/it] 63%|██████▎   | 11166/17834 [5:41:30<3:15:12,  1.76s/it] 63%|██████▎   | 11167/17834 [5:41:32<3:14:41,  1.75s/it] 63%|██████▎   | 11168/17834 [5:41:33<3:14:50,  1.75s/it] 63%|██████▎   | 11169/17834 [5:41:35<3:17:44,  1.78s/it] 63%|██████▎   | 11170/17834 [5:41:37<3:15:43,  1.76s/it] 63%|██████▎   | 11171/17834 [5:41:39<3:15:30,  1.76s/it] 63%|██████▎   | 11172/17834 [5:41:40<3:13:58,  1.75s/it] 63%|██████▎   | 11173/17834 [5:41:42<3:15:06,  1.76s/it] 63%|██████▎   | 11174/17834 [5:41:44<3:16:22,  1.77s/it] 63%|██████▎   | 11175/17834 [5:41:46<3:16:07,  1.77s/it] 63%|██████▎   | 11176/17834 [5:41:47<3:14:39,  1.75s/it] 63%|██████▎   | 11177/17834 [5:41:49<3:11:46,  1.73s/it] 63%|██████▎   | 11178/17834 [5:41:51<3:14:40,  1.75s/it] 63%|██████▎   | 11179/17834 [5:41:53<3:14:54,  1.76s/it] 63%|██████▎   | 11180/17834 [5:41:55<3:16:09,  1.77s/it] 63%|██████▎   | 11181/17834 [5:41:56<3:15:14,  1.76s/it] 63%|██████▎   | 11182/17834 [5:41:58<3:14:35,  1.76s/it] 63%|██████▎   | 11183/17834 [5:42:00<3:13:28,  1.75s/it] 63%|██████▎   | 11184/17834 [5:42:02<3:14:59,  1.76s/it] 63%|██████▎   | 11185/17834 [5:42:03<3:13:42,  1.75s/it] 63%|██████▎   | 11186/17834 [5:42:05<3:16:47,  1.78s/it] 63%|██████▎   | 11187/17834 [5:42:07<3:16:19,  1.77s/it] 63%|██████▎   | 11188/17834 [5:42:09<3:16:44,  1.78s/it] 63%|██████▎   | 11189/17834 [5:42:10<3:14:01,  1.75s/it] 63%|██████▎   | 11190/17834 [5:42:12<3:13:19,  1.75s/it] 63%|██████▎   | 11191/17834 [5:42:14<3:12:48,  1.74s/it] 63%|██████▎   | 11192/17834 [5:42:16<3:13:47,  1.75s/it] 63%|██████▎   | 11193/17834 [5:42:17<3:14:23,  1.76s/it] 63%|██████▎   | 11194/17834 [5:42:19<3:14:02,  1.75s/it] 63%|██████▎   | 11195/17834 [5:42:21<3:12:19,  1.74s/it] 63%|██████▎   | 11196/17834 [5:42:23<3:14:01,  1.75s/it] 63%|██████▎   | 11197/17834 [5:42:24<3:13:41,  1.75s/it] 63%|██████▎   | 11198/17834 [5:42:26<3:11:13,  1.73s/it] 63%|██████▎   | 11199/17834 [5:42:28<3:12:31,  1.74s/it]08/31/2024 00:56:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8699136972427368, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020587148144841194, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.125722646713257, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.016223430633545}
 63%|██████▎   | 11200/17834 [5:42:30<3:15:43,  1.77s/it] 63%|██████▎   | 11201/17834 [5:42:31<3:13:25,  1.75s/it] 63%|██████▎   | 11202/17834 [5:42:33<3:12:59,  1.75s/it] 63%|██████▎   | 11203/17834 [5:42:35<3:15:53,  1.77s/it] 63%|██████▎   | 11204/17834 [5:42:37<3:16:53,  1.78s/it] 63%|██████▎   | 11205/17834 [5:42:38<3:14:32,  1.76s/it] 63%|██████▎   | 11206/17834 [5:42:40<3:15:00,  1.77s/it] 63%|██████▎   | 11207/17834 [5:42:42<3:14:14,  1.76s/it] 63%|██████▎   | 11208/17834 [5:42:44<3:17:45,  1.79s/it] 63%|██████▎   | 11209/17834 [5:42:45<3:14:41,  1.76s/it] 63%|██████▎   | 11210/17834 [5:42:47<3:13:45,  1.76s/it] 63%|██████▎   | 11211/17834 [5:42:49<3:12:25,  1.74s/it] 63%|██████▎   | 11212/17834 [5:42:51<3:11:52,  1.74s/it] 63%|██████▎   | 11213/17834 [5:42:53<3:15:28,  1.77s/it] 63%|██████▎   | 11214/17834 [5:42:54<3:13:43,  1.76s/it] 63%|██████▎   | 11215/17834 [5:42:56<3:13:27,  1.75s/it] 63%|██████▎   | 11216/17834 [5:42:58<3:10:19,  1.73s/it] 63%|██████▎   | 11217/17834 [5:42:59<3:12:53,  1.75s/it] 63%|██████▎   | 11218/17834 [5:43:01<3:13:08,  1.75s/it] 63%|██████▎   | 11219/17834 [5:43:03<3:12:13,  1.74s/it] 63%|██████▎   | 11220/17834 [5:43:05<3:12:19,  1.74s/it] 63%|██████▎   | 11221/17834 [5:43:06<3:13:15,  1.75s/it] 63%|██████▎   | 11222/17834 [5:43:08<3:12:29,  1.75s/it] 63%|██████▎   | 11223/17834 [5:43:10<3:12:28,  1.75s/it] 63%|██████▎   | 11224/17834 [5:43:12<3:11:37,  1.74s/it] 63%|██████▎   | 11225/17834 [5:43:13<3:13:28,  1.76s/it] 63%|██████▎   | 11226/17834 [5:43:15<3:11:54,  1.74s/it] 63%|██████▎   | 11227/17834 [5:43:17<3:13:19,  1.76s/it] 63%|██████▎   | 11228/17834 [5:43:19<3:11:49,  1.74s/it] 63%|██████▎   | 11229/17834 [5:43:20<3:12:05,  1.74s/it] 63%|██████▎   | 11230/17834 [5:43:22<3:09:14,  1.72s/it] 63%|██████▎   | 11231/17834 [5:43:24<3:12:55,  1.75s/it] 63%|██████▎   | 11232/17834 [5:43:26<3:11:09,  1.74s/it] 63%|██████▎   | 11233/17834 [5:43:27<3:09:53,  1.73s/it] 63%|██████▎   | 11234/17834 [5:43:29<3:08:46,  1.72s/it] 63%|██████▎   | 11235/17834 [5:43:31<3:09:51,  1.73s/it] 63%|██████▎   | 11236/17834 [5:43:32<3:10:44,  1.73s/it] 63%|██████▎   | 11237/17834 [5:43:34<3:11:46,  1.74s/it] 63%|██████▎   | 11238/17834 [5:43:36<3:11:49,  1.74s/it] 63%|██████▎   | 11239/17834 [5:43:38<3:11:12,  1.74s/it] 63%|██████▎   | 11240/17834 [5:43:39<3:11:38,  1.74s/it] 63%|██████▎   | 11241/17834 [5:43:41<3:12:44,  1.75s/it] 63%|██████▎   | 11242/17834 [5:43:43<3:10:35,  1.73s/it] 63%|██████▎   | 11243/17834 [5:43:45<3:12:05,  1.75s/it] 63%|██████▎   | 11244/17834 [5:43:46<3:12:37,  1.75s/it] 63%|██████▎   | 11245/17834 [5:43:48<3:15:43,  1.78s/it] 63%|██████▎   | 11246/17834 [5:43:50<3:15:24,  1.78s/it] 63%|██████▎   | 11247/17834 [5:43:52<3:13:38,  1.76s/it] 63%|██████▎   | 11248/17834 [5:43:54<3:14:32,  1.77s/it] 63%|██████▎   | 11249/17834 [5:43:55<3:13:20,  1.76s/it]08/31/2024 00:58:15 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1788709163665771, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.034712038934230804, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2107291221618652, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.424312114715576}
 63%|██████▎   | 11250/17834 [5:43:57<3:13:46,  1.77s/it] 63%|██████▎   | 11251/17834 [5:43:59<3:14:28,  1.77s/it] 63%|██████▎   | 11252/17834 [5:44:01<3:12:36,  1.76s/it] 63%|██████▎   | 11253/17834 [5:44:02<3:10:56,  1.74s/it] 63%|██████▎   | 11254/17834 [5:44:04<3:11:30,  1.75s/it] 63%|██████▎   | 11255/17834 [5:44:06<3:11:06,  1.74s/it] 63%|██████▎   | 11256/17834 [5:44:08<3:12:09,  1.75s/it] 63%|██████▎   | 11257/17834 [5:44:09<3:15:38,  1.78s/it] 63%|██████▎   | 11258/17834 [5:44:11<3:16:14,  1.79s/it] 63%|██████▎   | 11259/17834 [5:44:13<3:15:36,  1.79s/it] 63%|██████▎   | 11260/17834 [5:44:15<3:18:12,  1.81s/it] 63%|██████▎   | 11261/17834 [5:44:17<3:13:35,  1.77s/it] 63%|██████▎   | 11262/17834 [5:44:18<3:12:31,  1.76s/it] 63%|██████▎   | 11263/17834 [5:44:20<3:13:46,  1.77s/it] 63%|██████▎   | 11264/17834 [5:44:22<3:13:17,  1.77s/it] 63%|██████▎   | 11265/17834 [5:44:24<3:11:40,  1.75s/it] 63%|██████▎   | 11266/17834 [5:44:25<3:10:34,  1.74s/it] 63%|██████▎   | 11267/17834 [5:44:27<3:11:48,  1.75s/it] 63%|██████▎   | 11268/17834 [5:44:29<3:09:50,  1.73s/it] 63%|██████▎   | 11269/17834 [5:44:31<3:10:26,  1.74s/it] 63%|██████▎   | 11270/17834 [5:44:32<3:10:33,  1.74s/it] 63%|██████▎   | 11271/17834 [5:44:34<3:10:17,  1.74s/it] 63%|██████▎   | 11272/17834 [5:44:36<3:11:37,  1.75s/it] 63%|██████▎   | 11273/17834 [5:44:38<3:11:15,  1.75s/it] 63%|██████▎   | 11274/17834 [5:44:39<3:10:59,  1.75s/it] 63%|██████▎   | 11275/17834 [5:44:41<3:12:24,  1.76s/it] 63%|██████▎   | 11276/17834 [5:44:43<3:08:51,  1.73s/it] 63%|██████▎   | 11277/17834 [5:44:44<3:09:10,  1.73s/it] 63%|██████▎   | 11278/17834 [5:44:46<3:09:12,  1.73s/it] 63%|██████▎   | 11279/17834 [5:44:48<3:12:19,  1.76s/it] 63%|██████▎   | 11280/17834 [5:44:50<3:10:46,  1.75s/it] 63%|██████▎   | 11281/17834 [5:44:51<3:09:26,  1.73s/it] 63%|██████▎   | 11282/17834 [5:44:53<3:10:04,  1.74s/it] 63%|██████▎   | 11283/17834 [5:44:55<3:09:25,  1.73s/it] 63%|██████▎   | 11284/17834 [5:44:57<3:12:11,  1.76s/it] 63%|██████▎   | 11285/17834 [5:44:58<3:11:17,  1.75s/it] 63%|██████▎   | 11286/17834 [5:45:00<3:16:10,  1.80s/it] 63%|██████▎   | 11287/17834 [5:45:02<3:14:21,  1.78s/it] 63%|██████▎   | 11288/17834 [5:45:04<3:14:16,  1.78s/it] 63%|██████▎   | 11289/17834 [5:45:06<3:13:16,  1.77s/it] 63%|██████▎   | 11290/17834 [5:45:07<3:12:06,  1.76s/it] 63%|██████▎   | 11291/17834 [5:45:09<3:11:11,  1.75s/it] 63%|██████▎   | 11292/17834 [5:45:11<3:11:23,  1.76s/it] 63%|██████▎   | 11293/17834 [5:45:13<3:10:28,  1.75s/it] 63%|██████▎   | 11294/17834 [5:45:14<3:11:34,  1.76s/it] 63%|██████▎   | 11295/17834 [5:45:16<3:10:32,  1.75s/it] 63%|██████▎   | 11296/17834 [5:45:18<3:09:58,  1.74s/it] 63%|██████▎   | 11297/17834 [5:45:20<3:13:36,  1.78s/it] 63%|██████▎   | 11298/17834 [5:45:21<3:11:21,  1.76s/it] 63%|██████▎   | 11299/17834 [5:45:23<3:09:49,  1.74s/it]08/31/2024 00:59:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9453315734863281, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02116556465625763, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1557087898254395, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.122205972671509}
 63%|██████▎   | 11300/17834 [5:45:25<3:12:16,  1.77s/it] 63%|██████▎   | 11301/17834 [5:45:27<3:10:42,  1.75s/it] 63%|██████▎   | 11302/17834 [5:45:28<3:09:45,  1.74s/it] 63%|██████▎   | 11303/17834 [5:45:30<3:12:25,  1.77s/it] 63%|██████▎   | 11304/17834 [5:45:32<3:10:16,  1.75s/it] 63%|██████▎   | 11305/17834 [5:45:34<3:09:21,  1.74s/it] 63%|██████▎   | 11306/17834 [5:45:35<3:11:05,  1.76s/it] 63%|██████▎   | 11307/17834 [5:45:37<3:09:23,  1.74s/it] 63%|██████▎   | 11308/17834 [5:45:39<3:10:29,  1.75s/it] 63%|██████▎   | 11309/17834 [5:45:41<3:07:47,  1.73s/it] 63%|██████▎   | 11310/17834 [5:45:42<3:08:09,  1.73s/it] 63%|██████▎   | 11311/17834 [5:45:44<3:12:05,  1.77s/it] 63%|██████▎   | 11312/17834 [5:45:46<3:12:25,  1.77s/it] 63%|██████▎   | 11313/17834 [5:45:48<3:10:58,  1.76s/it] 63%|██████▎   | 11314/17834 [5:45:49<3:10:14,  1.75s/it] 63%|██████▎   | 11315/17834 [5:45:51<3:09:16,  1.74s/it] 63%|██████▎   | 11316/17834 [5:45:53<3:07:46,  1.73s/it] 63%|██████▎   | 11317/17834 [5:45:55<3:07:27,  1.73s/it] 63%|██████▎   | 11318/17834 [5:45:56<3:11:07,  1.76s/it] 63%|██████▎   | 11319/17834 [5:45:58<3:13:06,  1.78s/it] 63%|██████▎   | 11320/17834 [5:46:00<3:13:23,  1.78s/it] 63%|██████▎   | 11321/17834 [5:46:02<3:12:52,  1.78s/it] 63%|██████▎   | 11322/17834 [5:46:04<3:13:01,  1.78s/it] 63%|██████▎   | 11323/17834 [5:46:05<3:13:38,  1.78s/it] 63%|██████▎   | 11324/17834 [5:46:07<3:11:31,  1.77s/it] 64%|██████▎   | 11325/17834 [5:46:09<3:10:41,  1.76s/it] 64%|██████▎   | 11326/17834 [5:46:11<3:09:42,  1.75s/it] 64%|██████▎   | 11327/17834 [5:46:12<3:11:29,  1.77s/it] 64%|██████▎   | 11328/17834 [5:46:14<3:12:43,  1.78s/it] 64%|██████▎   | 11329/17834 [5:46:16<3:10:22,  1.76s/it] 64%|██████▎   | 11330/17834 [5:46:18<3:09:14,  1.75s/it] 64%|██████▎   | 11331/17834 [5:46:19<3:08:38,  1.74s/it] 64%|██████▎   | 11332/17834 [5:46:21<3:09:20,  1.75s/it] 64%|██████▎   | 11333/17834 [5:46:23<3:09:49,  1.75s/it] 64%|██████▎   | 11334/17834 [5:46:25<3:08:33,  1.74s/it] 64%|██████▎   | 11335/17834 [5:46:26<3:06:31,  1.72s/it] 64%|██████▎   | 11336/17834 [5:46:28<3:04:58,  1.71s/it] 64%|██████▎   | 11337/17834 [5:46:30<3:05:43,  1.72s/it] 64%|██████▎   | 11338/17834 [5:46:31<3:08:35,  1.74s/it] 64%|██████▎   | 11339/17834 [5:46:33<3:07:55,  1.74s/it] 64%|██████▎   | 11340/17834 [5:46:35<3:08:05,  1.74s/it] 64%|██████▎   | 11341/17834 [5:46:37<3:09:37,  1.75s/it] 64%|██████▎   | 11342/17834 [5:46:38<3:09:09,  1.75s/it] 64%|██████▎   | 11343/17834 [5:46:40<3:09:33,  1.75s/it] 64%|██████▎   | 11344/17834 [5:46:42<3:11:10,  1.77s/it] 64%|██████▎   | 11345/17834 [5:46:44<3:09:49,  1.76s/it] 64%|██████▎   | 11346/17834 [5:46:45<3:09:56,  1.76s/it] 64%|██████▎   | 11347/17834 [5:46:47<3:10:28,  1.76s/it] 64%|██████▎   | 11348/17834 [5:46:49<3:09:08,  1.75s/it] 64%|██████▎   | 11349/17834 [5:46:51<3:10:25,  1.76s/it]08/31/2024 01:01:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0975353717803955, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026962371543049812, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.152801513671875, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.277299404144287}
 64%|██████▎   | 11350/17834 [5:46:53<3:11:38,  1.77s/it] 64%|██████▎   | 11351/17834 [5:46:54<3:10:44,  1.77s/it] 64%|██████▎   | 11352/17834 [5:46:56<3:11:40,  1.77s/it] 64%|██████▎   | 11353/17834 [5:46:58<3:13:45,  1.79s/it] 64%|██████▎   | 11354/17834 [5:47:00<3:11:17,  1.77s/it] 64%|██████▎   | 11355/17834 [5:47:01<3:11:29,  1.77s/it] 64%|██████▎   | 11356/17834 [5:47:03<3:13:31,  1.79s/it] 64%|██████▎   | 11357/17834 [5:47:05<3:10:44,  1.77s/it] 64%|██████▎   | 11358/17834 [5:47:07<3:07:47,  1.74s/it] 64%|██████▎   | 11359/17834 [5:47:08<3:06:51,  1.73s/it] 64%|██████▎   | 11360/17834 [5:47:10<3:11:02,  1.77s/it] 64%|██████▎   | 11361/17834 [5:47:12<3:07:33,  1.74s/it] 64%|██████▎   | 11362/17834 [5:47:14<3:06:47,  1.73s/it] 64%|██████▎   | 11363/17834 [5:47:15<3:08:47,  1.75s/it] 64%|██████▎   | 11364/17834 [5:47:17<3:09:20,  1.76s/it] 64%|██████▎   | 11365/17834 [5:47:19<3:08:14,  1.75s/it] 64%|██████▎   | 11366/17834 [5:47:21<3:08:34,  1.75s/it] 64%|██████▎   | 11367/17834 [5:47:22<3:07:04,  1.74s/it] 64%|██████▎   | 11368/17834 [5:47:24<3:07:33,  1.74s/it] 64%|██████▎   | 11369/17834 [5:47:26<3:08:23,  1.75s/it] 64%|██████▍   | 11370/17834 [5:47:28<3:07:27,  1.74s/it] 64%|██████▍   | 11371/17834 [5:47:29<3:05:45,  1.72s/it] 64%|██████▍   | 11372/17834 [5:47:31<3:07:30,  1.74s/it] 64%|██████▍   | 11373/17834 [5:47:33<3:07:02,  1.74s/it] 64%|██████▍   | 11374/17834 [5:47:35<3:08:53,  1.75s/it] 64%|██████▍   | 11375/17834 [5:47:36<3:07:42,  1.74s/it] 64%|██████▍   | 11376/17834 [5:47:38<3:07:15,  1.74s/it] 64%|██████▍   | 11377/17834 [5:47:40<3:07:44,  1.74s/it] 64%|██████▍   | 11378/17834 [5:47:41<3:06:20,  1.73s/it] 64%|██████▍   | 11379/17834 [5:47:43<3:05:39,  1.73s/it] 64%|██████▍   | 11380/17834 [5:47:45<3:06:39,  1.74s/it] 64%|██████▍   | 11381/17834 [5:47:47<3:06:16,  1.73s/it] 64%|██████▍   | 11382/17834 [5:47:48<3:06:46,  1.74s/it] 64%|██████▍   | 11383/17834 [5:47:50<3:06:35,  1.74s/it] 64%|██████▍   | 11384/17834 [5:47:52<3:07:51,  1.75s/it] 64%|██████▍   | 11385/17834 [5:47:54<3:08:01,  1.75s/it] 64%|██████▍   | 11386/17834 [5:47:55<3:07:14,  1.74s/it] 64%|██████▍   | 11387/17834 [5:47:57<3:07:16,  1.74s/it] 64%|██████▍   | 11388/17834 [5:47:59<3:08:53,  1.76s/it] 64%|██████▍   | 11389/17834 [5:48:01<3:12:17,  1.79s/it] 64%|██████▍   | 11390/17834 [5:48:03<3:08:59,  1.76s/it] 64%|██████▍   | 11391/17834 [5:48:04<3:10:47,  1.78s/it] 64%|██████▍   | 11392/17834 [5:48:06<3:09:17,  1.76s/it] 64%|██████▍   | 11393/17834 [5:48:08<3:08:51,  1.76s/it] 64%|██████▍   | 11394/17834 [5:48:10<3:08:31,  1.76s/it] 64%|██████▍   | 11395/17834 [5:48:11<3:09:46,  1.77s/it] 64%|██████▍   | 11396/17834 [5:48:13<3:07:34,  1.75s/it] 64%|██████▍   | 11397/17834 [5:48:15<3:05:28,  1.73s/it] 64%|██████▍   | 11398/17834 [5:48:16<3:06:05,  1.73s/it] 64%|██████▍   | 11399/17834 [5:48:18<3:05:58,  1.73s/it]08/31/2024 01:02:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2301280498504639, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030763451009988785, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.313251495361328, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5741429328918457}
 64%|██████▍   | 11400/17834 [5:48:20<3:04:25,  1.72s/it] 64%|██████▍   | 11401/17834 [5:48:22<3:06:57,  1.74s/it] 64%|██████▍   | 11402/17834 [5:48:23<3:06:14,  1.74s/it] 64%|██████▍   | 11403/17834 [5:48:25<3:12:21,  1.79s/it] 64%|██████▍   | 11404/17834 [5:48:27<3:08:04,  1.76s/it] 64%|██████▍   | 11405/17834 [5:48:29<3:09:35,  1.77s/it] 64%|██████▍   | 11406/17834 [5:48:31<3:09:41,  1.77s/it] 64%|██████▍   | 11407/17834 [5:48:32<3:09:35,  1.77s/it] 64%|██████▍   | 11408/17834 [5:48:34<3:09:24,  1.77s/it] 64%|██████▍   | 11409/17834 [5:48:36<3:07:44,  1.75s/it] 64%|██████▍   | 11410/17834 [5:48:38<3:06:04,  1.74s/it] 64%|██████▍   | 11411/17834 [5:48:39<3:07:47,  1.75s/it] 64%|██████▍   | 11412/17834 [5:48:41<3:10:02,  1.78s/it] 64%|██████▍   | 11413/17834 [5:48:43<3:08:44,  1.76s/it] 64%|██████▍   | 11414/17834 [5:48:45<3:06:33,  1.74s/it] 64%|██████▍   | 11415/17834 [5:48:46<3:04:30,  1.72s/it] 64%|██████▍   | 11416/17834 [5:48:48<3:05:36,  1.74s/it] 64%|██████▍   | 11417/17834 [5:48:50<3:05:23,  1.73s/it] 64%|██████▍   | 11418/17834 [5:48:52<3:08:19,  1.76s/it] 64%|██████▍   | 11419/17834 [5:48:53<3:06:42,  1.75s/it] 64%|██████▍   | 11420/17834 [5:48:55<3:06:30,  1.74s/it] 64%|██████▍   | 11421/17834 [5:48:57<3:09:24,  1.77s/it] 64%|██████▍   | 11422/17834 [5:48:59<3:07:40,  1.76s/it] 64%|██████▍   | 11423/17834 [5:49:00<3:07:14,  1.75s/it] 64%|██████▍   | 11424/17834 [5:49:02<3:06:54,  1.75s/it] 64%|██████▍   | 11425/17834 [5:49:04<3:07:27,  1.75s/it] 64%|██████▍   | 11426/17834 [5:49:06<3:08:50,  1.77s/it] 64%|██████▍   | 11427/17834 [5:49:07<3:09:46,  1.78s/it] 64%|██████▍   | 11428/17834 [5:49:09<3:09:43,  1.78s/it] 64%|██████▍   | 11429/17834 [5:49:11<3:05:49,  1.74s/it] 64%|██████▍   | 11430/17834 [5:49:13<3:04:25,  1.73s/it] 64%|██████▍   | 11431/17834 [5:49:14<3:02:22,  1.71s/it] 64%|██████▍   | 11432/17834 [5:49:16<3:02:02,  1.71s/it] 64%|██████▍   | 11433/17834 [5:49:18<3:03:53,  1.72s/it] 64%|██████▍   | 11434/17834 [5:49:19<3:05:11,  1.74s/it] 64%|██████▍   | 11435/17834 [5:49:21<3:07:14,  1.76s/it] 64%|██████▍   | 11436/17834 [5:49:23<3:04:56,  1.73s/it] 64%|██████▍   | 11437/17834 [5:49:25<3:07:14,  1.76s/it] 64%|██████▍   | 11438/17834 [5:49:27<3:06:58,  1.75s/it] 64%|██████▍   | 11439/17834 [5:49:28<3:05:38,  1.74s/it] 64%|██████▍   | 11440/17834 [5:49:30<3:07:12,  1.76s/it] 64%|██████▍   | 11441/17834 [5:49:32<3:07:58,  1.76s/it] 64%|██████▍   | 11442/17834 [5:49:34<3:07:28,  1.76s/it] 64%|██████▍   | 11443/17834 [5:49:35<3:05:24,  1.74s/it] 64%|██████▍   | 11444/17834 [5:49:37<3:06:21,  1.75s/it] 64%|██████▍   | 11445/17834 [5:49:39<3:07:34,  1.76s/it] 64%|██████▍   | 11446/17834 [5:49:41<3:07:34,  1.76s/it] 64%|██████▍   | 11447/17834 [5:49:42<3:08:42,  1.77s/it] 64%|██████▍   | 11448/17834 [5:49:44<3:06:36,  1.75s/it] 64%|██████▍   | 11449/17834 [5:49:46<3:07:38,  1.76s/it]08/31/2024 01:04:05 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.363100290298462, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03085947036743164, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2913687229156494, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.685328483581543}
 64%|██████▍   | 11450/17834 [5:49:48<3:06:07,  1.75s/it] 64%|██████▍   | 11451/17834 [5:49:49<3:07:03,  1.76s/it] 64%|██████▍   | 11452/17834 [5:49:51<3:05:52,  1.75s/it] 64%|██████▍   | 11453/17834 [5:49:53<3:05:57,  1.75s/it] 64%|██████▍   | 11454/17834 [5:49:55<3:04:55,  1.74s/it] 64%|██████▍   | 11455/17834 [5:49:56<3:05:01,  1.74s/it] 64%|██████▍   | 11456/17834 [5:49:58<3:09:12,  1.78s/it] 64%|██████▍   | 11457/17834 [5:50:00<3:08:12,  1.77s/it] 64%|██████▍   | 11458/17834 [5:50:02<3:06:55,  1.76s/it] 64%|██████▍   | 11459/17834 [5:50:03<3:05:49,  1.75s/it] 64%|██████▍   | 11460/17834 [5:50:05<3:07:06,  1.76s/it] 64%|██████▍   | 11461/17834 [5:50:07<3:04:56,  1.74s/it] 64%|██████▍   | 11462/17834 [5:50:09<3:04:48,  1.74s/it] 64%|██████▍   | 11463/17834 [5:50:10<3:04:34,  1.74s/it] 64%|██████▍   | 11464/17834 [5:50:12<3:06:19,  1.76s/it] 64%|██████▍   | 11465/17834 [5:50:14<3:04:24,  1.74s/it] 64%|██████▍   | 11466/17834 [5:50:16<3:05:20,  1.75s/it] 64%|██████▍   | 11467/17834 [5:50:17<3:04:26,  1.74s/it] 64%|██████▍   | 11468/17834 [5:50:19<3:06:26,  1.76s/it] 64%|██████▍   | 11469/17834 [5:50:21<3:05:25,  1.75s/it] 64%|██████▍   | 11470/17834 [5:50:23<3:04:14,  1.74s/it] 64%|██████▍   | 11471/17834 [5:50:24<3:05:16,  1.75s/it] 64%|██████▍   | 11472/17834 [5:50:26<3:03:04,  1.73s/it] 64%|██████▍   | 11473/17834 [5:50:28<3:05:33,  1.75s/it] 64%|██████▍   | 11474/17834 [5:50:30<3:06:57,  1.76s/it] 64%|██████▍   | 11475/17834 [5:50:31<3:06:34,  1.76s/it] 64%|██████▍   | 11476/17834 [5:50:33<3:03:13,  1.73s/it] 64%|██████▍   | 11477/17834 [5:50:35<3:05:44,  1.75s/it] 64%|██████▍   | 11478/17834 [5:50:37<3:05:15,  1.75s/it] 64%|██████▍   | 11479/17834 [5:50:38<3:05:22,  1.75s/it] 64%|██████▍   | 11480/17834 [5:50:40<3:06:32,  1.76s/it] 64%|██████▍   | 11481/17834 [5:50:42<3:05:34,  1.75s/it] 64%|██████▍   | 11482/17834 [5:50:44<3:05:33,  1.75s/it] 64%|██████▍   | 11483/17834 [5:50:45<3:05:24,  1.75s/it] 64%|██████▍   | 11484/17834 [5:50:47<3:05:54,  1.76s/it] 64%|██████▍   | 11485/17834 [5:50:49<3:03:43,  1.74s/it] 64%|██████▍   | 11486/17834 [5:50:51<3:06:08,  1.76s/it] 64%|██████▍   | 11487/17834 [5:50:52<3:06:41,  1.76s/it] 64%|██████▍   | 11488/17834 [5:50:54<3:06:59,  1.77s/it] 64%|██████▍   | 11489/17834 [5:50:56<3:07:25,  1.77s/it] 64%|██████▍   | 11490/17834 [5:50:58<3:05:35,  1.76s/it] 64%|██████▍   | 11491/17834 [5:50:59<3:06:20,  1.76s/it] 64%|██████▍   | 11492/17834 [5:51:01<3:07:12,  1.77s/it] 64%|██████▍   | 11493/17834 [5:51:03<3:05:30,  1.76s/it] 64%|██████▍   | 11494/17834 [5:51:05<3:03:24,  1.74s/it] 64%|██████▍   | 11495/17834 [5:51:06<3:02:45,  1.73s/it] 64%|██████▍   | 11496/17834 [5:51:08<3:04:14,  1.74s/it] 64%|██████▍   | 11497/17834 [5:51:10<3:03:49,  1.74s/it] 64%|██████▍   | 11498/17834 [5:51:12<3:04:34,  1.75s/it] 64%|██████▍   | 11499/17834 [5:51:13<3:03:13,  1.74s/it]08/31/2024 01:05:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2550870180130005, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03981740027666092, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2092370986938477, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5041415691375732}
 64%|██████▍   | 11500/17834 [5:51:15<3:06:16,  1.76s/it] 64%|██████▍   | 11501/17834 [5:51:17<3:06:03,  1.76s/it] 64%|██████▍   | 11502/17834 [5:51:19<3:04:38,  1.75s/it] 65%|██████▍   | 11503/17834 [5:51:20<3:03:32,  1.74s/it] 65%|██████▍   | 11504/17834 [5:51:22<3:04:43,  1.75s/it] 65%|██████▍   | 11505/17834 [5:51:24<3:03:13,  1.74s/it] 65%|██████▍   | 11506/17834 [5:51:26<3:01:51,  1.72s/it] 65%|██████▍   | 11507/17834 [5:51:27<3:03:32,  1.74s/it] 65%|██████▍   | 11508/17834 [5:51:29<3:03:56,  1.74s/it] 65%|██████▍   | 11509/17834 [5:51:31<3:04:25,  1.75s/it] 65%|██████▍   | 11510/17834 [5:51:33<3:04:51,  1.75s/it] 65%|██████▍   | 11511/17834 [5:51:34<3:04:46,  1.75s/it] 65%|██████▍   | 11512/17834 [5:51:36<3:04:01,  1.75s/it] 65%|██████▍   | 11513/17834 [5:51:38<3:02:07,  1.73s/it] 65%|██████▍   | 11514/17834 [5:51:40<3:02:38,  1.73s/it] 65%|██████▍   | 11515/17834 [5:51:41<3:04:40,  1.75s/it] 65%|██████▍   | 11516/17834 [5:51:43<3:03:56,  1.75s/it] 65%|██████▍   | 11517/17834 [5:51:45<3:03:04,  1.74s/it] 65%|██████▍   | 11518/17834 [5:51:47<3:03:52,  1.75s/it] 65%|██████▍   | 11519/17834 [5:51:48<3:05:42,  1.76s/it] 65%|██████▍   | 11520/17834 [5:51:50<3:09:53,  1.80s/it] 65%|██████▍   | 11521/17834 [5:51:52<3:05:16,  1.76s/it] 65%|██████▍   | 11522/17834 [5:51:54<3:04:16,  1.75s/it] 65%|██████▍   | 11523/17834 [5:51:55<3:06:07,  1.77s/it] 65%|██████▍   | 11524/17834 [5:51:57<3:04:27,  1.75s/it] 65%|██████▍   | 11525/17834 [5:51:59<3:03:48,  1.75s/it] 65%|██████▍   | 11526/17834 [5:52:01<3:05:41,  1.77s/it] 65%|██████▍   | 11527/17834 [5:52:02<3:04:32,  1.76s/it] 65%|██████▍   | 11528/17834 [5:52:04<3:04:08,  1.75s/it] 65%|██████▍   | 11529/17834 [5:52:06<3:02:45,  1.74s/it] 65%|██████▍   | 11530/17834 [5:52:08<3:03:08,  1.74s/it] 65%|██████▍   | 11531/17834 [5:52:09<3:03:10,  1.74s/it] 65%|██████▍   | 11532/17834 [5:52:11<3:06:28,  1.78s/it] 65%|██████▍   | 11533/17834 [5:52:13<3:02:32,  1.74s/it] 65%|██████▍   | 11534/17834 [5:52:15<3:05:12,  1.76s/it] 65%|██████▍   | 11535/17834 [5:52:16<3:02:27,  1.74s/it] 65%|██████▍   | 11536/17834 [5:52:18<3:04:06,  1.75s/it] 65%|██████▍   | 11537/17834 [5:52:20<3:04:09,  1.75s/it] 65%|██████▍   | 11538/17834 [5:52:22<3:07:08,  1.78s/it] 65%|██████▍   | 11539/17834 [5:52:23<3:04:40,  1.76s/it] 65%|██████▍   | 11540/17834 [5:52:25<3:04:43,  1.76s/it] 65%|██████▍   | 11541/17834 [5:52:27<3:02:16,  1.74s/it] 65%|██████▍   | 11542/17834 [5:52:29<3:02:19,  1.74s/it] 65%|██████▍   | 11543/17834 [5:52:30<3:00:52,  1.73s/it] 65%|██████▍   | 11544/17834 [5:52:32<3:02:29,  1.74s/it] 65%|██████▍   | 11545/17834 [5:52:34<3:01:47,  1.73s/it] 65%|██████▍   | 11546/17834 [5:52:36<3:04:40,  1.76s/it] 65%|██████▍   | 11547/17834 [5:52:37<3:03:44,  1.75s/it] 65%|██████▍   | 11548/17834 [5:52:39<3:03:11,  1.75s/it] 65%|██████▍   | 11549/17834 [5:52:41<3:02:46,  1.74s/it]08/31/2024 01:07:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4906079769134521, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0554235577583313, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2959213256835938, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.8419528007507324}
 65%|██████▍   | 11550/17834 [5:52:43<3:04:25,  1.76s/it] 65%|██████▍   | 11551/17834 [5:52:44<3:04:50,  1.77s/it] 65%|██████▍   | 11552/17834 [5:52:46<3:03:22,  1.75s/it] 65%|██████▍   | 11553/17834 [5:52:48<3:05:43,  1.77s/it] 65%|██████▍   | 11554/17834 [5:52:50<3:03:20,  1.75s/it] 65%|██████▍   | 11555/17834 [5:52:51<3:02:36,  1.74s/it] 65%|██████▍   | 11556/17834 [5:52:53<3:01:40,  1.74s/it] 65%|██████▍   | 11557/17834 [5:52:55<3:03:36,  1.76s/it] 65%|██████▍   | 11558/17834 [5:52:57<3:02:46,  1.75s/it] 65%|██████▍   | 11559/17834 [5:52:58<3:03:16,  1.75s/it] 65%|██████▍   | 11560/17834 [5:53:00<3:01:28,  1.74s/it] 65%|██████▍   | 11561/17834 [5:53:02<3:01:24,  1.74s/it] 65%|██████▍   | 11562/17834 [5:53:04<2:59:34,  1.72s/it] 65%|██████▍   | 11563/17834 [5:53:05<3:02:55,  1.75s/it] 65%|██████▍   | 11564/17834 [5:53:07<3:02:46,  1.75s/it] 65%|██████▍   | 11565/17834 [5:53:09<3:01:16,  1.74s/it] 65%|██████▍   | 11566/17834 [5:53:11<3:01:33,  1.74s/it] 65%|██████▍   | 11567/17834 [5:53:12<3:01:09,  1.73s/it] 65%|██████▍   | 11568/17834 [5:53:14<3:00:34,  1.73s/it] 65%|██████▍   | 11569/17834 [5:53:16<3:03:28,  1.76s/it] 65%|██████▍   | 11570/17834 [5:53:18<3:00:37,  1.73s/it] 65%|██████▍   | 11571/17834 [5:53:19<3:01:27,  1.74s/it] 65%|██████▍   | 11572/17834 [5:53:21<3:03:20,  1.76s/it] 65%|██████▍   | 11573/17834 [5:53:23<3:03:08,  1.76s/it] 65%|██████▍   | 11574/17834 [5:53:25<3:02:44,  1.75s/it] 65%|██████▍   | 11575/17834 [5:53:26<3:02:27,  1.75s/it] 65%|██████▍   | 11576/17834 [5:53:28<3:03:19,  1.76s/it] 65%|██████▍   | 11577/17834 [5:53:30<3:05:13,  1.78s/it] 65%|██████▍   | 11578/17834 [5:53:32<3:02:49,  1.75s/it] 65%|██████▍   | 11579/17834 [5:53:33<3:01:37,  1.74s/it] 65%|██████▍   | 11580/17834 [5:53:35<3:00:54,  1.74s/it] 65%|██████▍   | 11581/17834 [5:53:37<3:00:19,  1.73s/it] 65%|██████▍   | 11582/17834 [5:53:39<3:01:36,  1.74s/it] 65%|██████▍   | 11583/17834 [5:53:40<3:02:56,  1.76s/it] 65%|██████▍   | 11584/17834 [5:53:42<3:04:52,  1.77s/it] 65%|██████▍   | 11585/17834 [5:53:44<3:06:02,  1.79s/it] 65%|██████▍   | 11586/17834 [5:53:46<3:03:22,  1.76s/it] 65%|██████▍   | 11587/17834 [5:53:47<3:00:59,  1.74s/it] 65%|██████▍   | 11588/17834 [5:53:49<2:58:56,  1.72s/it] 65%|██████▍   | 11589/17834 [5:53:51<2:59:24,  1.72s/it] 65%|██████▍   | 11590/17834 [5:53:53<3:00:42,  1.74s/it] 65%|██████▍   | 11591/17834 [5:53:54<3:01:43,  1.75s/it] 65%|██████▍   | 11592/17834 [5:53:56<3:01:02,  1.74s/it] 65%|██████▌   | 11593/17834 [5:53:58<3:02:19,  1.75s/it] 65%|██████▌   | 11594/17834 [5:53:59<3:00:45,  1.74s/it] 65%|██████▌   | 11595/17834 [5:54:01<3:01:26,  1.74s/it] 65%|██████▌   | 11596/17834 [5:54:03<3:01:37,  1.75s/it] 65%|██████▌   | 11597/17834 [5:54:05<3:01:53,  1.75s/it] 65%|██████▌   | 11598/17834 [5:54:06<3:00:06,  1.73s/it] 65%|██████▌   | 11599/17834 [5:54:08<3:03:31,  1.77s/it]08/31/2024 01:08:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.408407211303711, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026189491152763367, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3875350952148438, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.822131633758545}
 65%|██████▌   | 11600/17834 [5:54:10<3:01:45,  1.75s/it] 65%|██████▌   | 11601/17834 [5:54:12<3:01:27,  1.75s/it] 65%|██████▌   | 11602/17834 [5:54:13<3:00:42,  1.74s/it] 65%|██████▌   | 11603/17834 [5:54:15<3:01:41,  1.75s/it] 65%|██████▌   | 11604/17834 [5:54:17<3:02:44,  1.76s/it] 65%|██████▌   | 11605/17834 [5:54:19<2:59:24,  1.73s/it] 65%|██████▌   | 11606/17834 [5:54:20<3:01:12,  1.75s/it] 65%|██████▌   | 11607/17834 [5:54:22<3:03:08,  1.76s/it] 65%|██████▌   | 11608/17834 [5:54:24<3:00:28,  1.74s/it] 65%|██████▌   | 11609/17834 [5:54:26<3:01:23,  1.75s/it] 65%|██████▌   | 11610/17834 [5:54:27<2:59:38,  1.73s/it] 65%|██████▌   | 11611/17834 [5:54:29<2:59:26,  1.73s/it] 65%|██████▌   | 11612/17834 [5:54:31<2:59:29,  1.73s/it] 65%|██████▌   | 11613/17834 [5:54:33<3:00:13,  1.74s/it] 65%|██████▌   | 11614/17834 [5:54:34<3:00:48,  1.74s/it] 65%|██████▌   | 11615/17834 [5:54:36<2:59:19,  1.73s/it] 65%|██████▌   | 11616/17834 [5:54:38<3:00:30,  1.74s/it] 65%|██████▌   | 11617/17834 [5:54:40<3:01:55,  1.76s/it] 65%|██████▌   | 11618/17834 [5:54:41<3:00:04,  1.74s/it] 65%|██████▌   | 11619/17834 [5:54:43<2:59:55,  1.74s/it] 65%|██████▌   | 11620/17834 [5:54:45<3:00:01,  1.74s/it] 65%|██████▌   | 11621/17834 [5:54:47<2:59:13,  1.73s/it] 65%|██████▌   | 11622/17834 [5:54:48<3:00:55,  1.75s/it] 65%|██████▌   | 11623/17834 [5:54:50<3:00:45,  1.75s/it] 65%|██████▌   | 11624/17834 [5:54:52<3:00:45,  1.75s/it] 65%|██████▌   | 11625/17834 [5:54:54<3:00:43,  1.75s/it] 65%|██████▌   | 11626/17834 [5:54:55<3:00:45,  1.75s/it] 65%|██████▌   | 11627/17834 [5:54:57<3:00:28,  1.74s/it] 65%|██████▌   | 11628/17834 [5:54:59<3:01:07,  1.75s/it] 65%|██████▌   | 11629/17834 [5:55:01<3:02:18,  1.76s/it] 65%|██████▌   | 11630/17834 [5:55:02<2:59:59,  1.74s/it] 65%|██████▌   | 11631/17834 [5:55:04<3:03:02,  1.77s/it] 65%|██████▌   | 11632/17834 [5:55:06<3:02:45,  1.77s/it] 65%|██████▌   | 11633/17834 [5:55:08<3:00:36,  1.75s/it] 65%|██████▌   | 11634/17834 [5:55:09<2:59:58,  1.74s/it] 65%|██████▌   | 11635/17834 [5:55:11<3:01:50,  1.76s/it] 65%|██████▌   | 11636/17834 [5:55:13<3:01:05,  1.75s/it] 65%|██████▌   | 11637/17834 [5:55:15<3:00:58,  1.75s/it] 65%|██████▌   | 11638/17834 [5:55:16<3:01:59,  1.76s/it] 65%|██████▌   | 11639/17834 [5:55:18<3:01:45,  1.76s/it] 65%|██████▌   | 11640/17834 [5:55:20<3:02:34,  1.77s/it] 65%|██████▌   | 11641/17834 [5:55:22<3:02:55,  1.77s/it] 65%|██████▌   | 11642/17834 [5:55:23<3:00:38,  1.75s/it] 65%|██████▌   | 11643/17834 [5:55:25<3:00:53,  1.75s/it] 65%|██████▌   | 11644/17834 [5:55:27<3:01:31,  1.76s/it] 65%|██████▌   | 11645/17834 [5:55:29<3:05:20,  1.80s/it] 65%|██████▌   | 11646/17834 [5:55:30<3:01:06,  1.76s/it] 65%|██████▌   | 11647/17834 [5:55:32<2:58:49,  1.73s/it] 65%|██████▌   | 11648/17834 [5:55:34<2:58:52,  1.73s/it] 65%|██████▌   | 11649/17834 [5:55:36<2:57:26,  1.72s/it]08/31/2024 01:09:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0294082164764404, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03736152499914169, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.188800811767578, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.255570411682129}
 65%|██████▌   | 11650/17834 [5:55:37<3:00:07,  1.75s/it] 65%|██████▌   | 11651/17834 [5:55:39<2:59:43,  1.74s/it] 65%|██████▌   | 11652/17834 [5:55:41<3:02:02,  1.77s/it] 65%|██████▌   | 11653/17834 [5:55:43<3:04:21,  1.79s/it] 65%|██████▌   | 11654/17834 [5:55:44<3:00:58,  1.76s/it] 65%|██████▌   | 11655/17834 [5:55:46<2:59:41,  1.74s/it] 65%|██████▌   | 11656/17834 [5:55:48<3:01:26,  1.76s/it] 65%|██████▌   | 11657/17834 [5:55:50<3:00:06,  1.75s/it] 65%|██████▌   | 11658/17834 [5:55:51<2:59:17,  1.74s/it] 65%|██████▌   | 11659/17834 [5:55:53<3:03:34,  1.78s/it] 65%|██████▌   | 11660/17834 [5:55:55<3:00:20,  1.75s/it] 65%|██████▌   | 11661/17834 [5:55:57<2:58:12,  1.73s/it] 65%|██████▌   | 11662/17834 [5:55:58<2:57:10,  1.72s/it] 65%|██████▌   | 11663/17834 [5:56:00<2:57:17,  1.72s/it] 65%|██████▌   | 11664/17834 [5:56:02<2:58:08,  1.73s/it] 65%|██████▌   | 11665/17834 [5:56:04<2:58:31,  1.74s/it] 65%|██████▌   | 11666/17834 [5:56:05<2:56:43,  1.72s/it] 65%|██████▌   | 11667/17834 [5:56:07<3:00:32,  1.76s/it] 65%|██████▌   | 11668/17834 [5:56:09<3:01:31,  1.77s/it] 65%|██████▌   | 11669/17834 [5:56:11<3:00:50,  1.76s/it] 65%|██████▌   | 11670/17834 [5:56:12<2:59:17,  1.75s/it] 65%|██████▌   | 11671/17834 [5:56:14<2:58:34,  1.74s/it] 65%|██████▌   | 11672/17834 [5:56:16<3:00:52,  1.76s/it] 65%|██████▌   | 11673/17834 [5:56:18<3:03:41,  1.79s/it] 65%|██████▌   | 11674/17834 [5:56:20<3:02:01,  1.77s/it] 65%|██████▌   | 11675/17834 [5:56:21<3:03:25,  1.79s/it] 65%|██████▌   | 11676/17834 [5:56:23<3:03:59,  1.79s/it] 65%|██████▌   | 11677/17834 [5:56:25<3:03:42,  1.79s/it] 65%|██████▌   | 11678/17834 [5:56:27<3:01:54,  1.77s/it] 65%|██████▌   | 11679/17834 [5:56:28<3:01:30,  1.77s/it] 65%|██████▌   | 11680/17834 [5:56:30<3:02:03,  1.77s/it] 65%|██████▌   | 11681/17834 [5:56:32<3:00:32,  1.76s/it] 66%|██████▌   | 11682/17834 [5:56:34<2:59:11,  1.75s/it] 66%|██████▌   | 11683/17834 [5:56:35<2:58:47,  1.74s/it] 66%|██████▌   | 11684/17834 [5:56:37<2:57:48,  1.73s/it] 66%|██████▌   | 11685/17834 [5:56:39<2:57:55,  1.74s/it] 66%|██████▌   | 11686/17834 [5:56:41<2:58:55,  1.75s/it] 66%|██████▌   | 11687/17834 [5:56:42<2:55:51,  1.72s/it] 66%|██████▌   | 11688/17834 [5:56:44<2:55:52,  1.72s/it] 66%|██████▌   | 11689/17834 [5:56:46<2:58:16,  1.74s/it] 66%|██████▌   | 11690/17834 [5:56:47<2:56:37,  1.72s/it] 66%|██████▌   | 11691/17834 [5:56:49<2:56:50,  1.73s/it] 66%|██████▌   | 11692/17834 [5:56:51<2:56:46,  1.73s/it] 66%|██████▌   | 11693/17834 [5:56:53<2:58:19,  1.74s/it] 66%|██████▌   | 11694/17834 [5:56:54<2:57:34,  1.74s/it] 66%|██████▌   | 11695/17834 [5:56:56<2:58:14,  1.74s/it] 66%|██████▌   | 11696/17834 [5:56:58<2:56:59,  1.73s/it] 66%|██████▌   | 11697/17834 [5:57:00<2:56:16,  1.72s/it] 66%|██████▌   | 11698/17834 [5:57:01<2:57:59,  1.74s/it] 66%|██████▌   | 11699/17834 [5:57:03<2:56:01,  1.72s/it]08/31/2024 01:11:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2520053386688232, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.037565115839242935, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2189300060272217, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.508500576019287}
 66%|██████▌   | 11700/17834 [5:57:05<2:55:49,  1.72s/it] 66%|██████▌   | 11701/17834 [5:57:07<2:56:31,  1.73s/it] 66%|██████▌   | 11702/17834 [5:57:08<3:00:23,  1.77s/it] 66%|██████▌   | 11703/17834 [5:57:10<3:03:57,  1.80s/it] 66%|██████▌   | 11704/17834 [5:57:12<2:59:25,  1.76s/it] 66%|██████▌   | 11705/17834 [5:57:14<3:01:11,  1.77s/it] 66%|██████▌   | 11706/17834 [5:57:15<3:00:31,  1.77s/it] 66%|██████▌   | 11707/17834 [5:57:17<2:58:23,  1.75s/it] 66%|██████▌   | 11708/17834 [5:57:19<2:59:16,  1.76s/it] 66%|██████▌   | 11709/17834 [5:57:21<2:59:33,  1.76s/it] 66%|██████▌   | 11710/17834 [5:57:22<3:00:15,  1.77s/it] 66%|██████▌   | 11711/17834 [5:57:24<2:59:04,  1.75s/it] 66%|██████▌   | 11712/17834 [5:57:26<2:58:31,  1.75s/it] 66%|██████▌   | 11713/17834 [5:57:28<3:00:05,  1.77s/it] 66%|██████▌   | 11714/17834 [5:57:30<3:02:06,  1.79s/it] 66%|██████▌   | 11715/17834 [5:57:31<2:59:15,  1.76s/it] 66%|██████▌   | 11716/17834 [5:57:33<2:59:44,  1.76s/it] 66%|██████▌   | 11717/17834 [5:57:35<2:58:58,  1.76s/it] 66%|██████▌   | 11718/17834 [5:57:37<2:57:36,  1.74s/it] 66%|██████▌   | 11719/17834 [5:57:38<2:58:49,  1.75s/it] 66%|██████▌   | 11720/17834 [5:57:40<2:57:40,  1.74s/it] 66%|██████▌   | 11721/17834 [5:57:42<2:59:03,  1.76s/it] 66%|██████▌   | 11722/17834 [5:57:44<2:58:25,  1.75s/it] 66%|██████▌   | 11723/17834 [5:57:45<2:56:44,  1.74s/it] 66%|██████▌   | 11724/17834 [5:57:47<2:58:53,  1.76s/it] 66%|██████▌   | 11725/17834 [5:57:49<2:58:23,  1.75s/it] 66%|██████▌   | 11726/17834 [5:57:51<2:58:50,  1.76s/it] 66%|██████▌   | 11727/17834 [5:57:52<2:59:57,  1.77s/it] 66%|██████▌   | 11728/17834 [5:57:54<2:57:59,  1.75s/it] 66%|██████▌   | 11729/17834 [5:57:56<2:56:27,  1.73s/it] 66%|██████▌   | 11730/17834 [5:57:58<2:59:31,  1.76s/it] 66%|██████▌   | 11731/17834 [5:57:59<2:58:32,  1.76s/it] 66%|██████▌   | 11732/17834 [5:58:01<2:59:21,  1.76s/it] 66%|██████▌   | 11733/17834 [5:58:03<3:02:07,  1.79s/it] 66%|██████▌   | 11734/17834 [5:58:05<3:00:51,  1.78s/it] 66%|██████▌   | 11735/17834 [5:58:06<2:59:43,  1.77s/it] 66%|██████▌   | 11736/17834 [5:58:08<3:00:53,  1.78s/it] 66%|██████▌   | 11737/17834 [5:58:10<2:59:05,  1.76s/it] 66%|██████▌   | 11738/17834 [5:58:12<2:59:25,  1.77s/it] 66%|██████▌   | 11739/17834 [5:58:13<2:58:02,  1.75s/it] 66%|██████▌   | 11740/17834 [5:58:15<2:58:32,  1.76s/it] 66%|██████▌   | 11741/17834 [5:58:17<3:00:14,  1.77s/it] 66%|██████▌   | 11742/17834 [5:58:19<2:59:43,  1.77s/it] 66%|██████▌   | 11743/17834 [5:58:21<3:00:20,  1.78s/it] 66%|██████▌   | 11744/17834 [5:58:22<3:02:13,  1.80s/it] 66%|██████▌   | 11745/17834 [5:58:24<3:00:20,  1.78s/it] 66%|██████▌   | 11746/17834 [5:58:26<2:59:25,  1.77s/it] 66%|██████▌   | 11747/17834 [5:58:28<2:57:39,  1.75s/it] 66%|██████▌   | 11748/17834 [5:58:29<2:57:18,  1.75s/it] 66%|██████▌   | 11749/17834 [5:58:31<2:56:32,  1.74s/it]08/31/2024 01:12:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0678333044052124, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04304623231291771, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1524486541748047, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2633280754089355}
 66%|██████▌   | 11750/17834 [5:58:33<2:55:17,  1.73s/it] 66%|██████▌   | 11751/17834 [5:58:35<2:55:39,  1.73s/it] 66%|██████▌   | 11752/17834 [5:58:36<2:56:50,  1.74s/it] 66%|██████▌   | 11753/17834 [5:58:38<2:56:54,  1.75s/it] 66%|██████▌   | 11754/17834 [5:58:40<2:55:50,  1.74s/it] 66%|██████▌   | 11755/17834 [5:58:42<2:56:25,  1.74s/it] 66%|██████▌   | 11756/17834 [5:58:43<2:58:02,  1.76s/it] 66%|██████▌   | 11757/17834 [5:58:45<2:57:20,  1.75s/it] 66%|██████▌   | 11758/17834 [5:58:47<2:55:36,  1.73s/it] 66%|██████▌   | 11759/17834 [5:58:49<2:56:32,  1.74s/it] 66%|██████▌   | 11760/17834 [5:58:50<2:57:39,  1.75s/it] 66%|██████▌   | 11761/17834 [5:58:52<2:59:24,  1.77s/it] 66%|██████▌   | 11762/17834 [5:58:54<2:56:39,  1.75s/it] 66%|██████▌   | 11763/17834 [5:58:56<2:57:26,  1.75s/it] 66%|██████▌   | 11764/17834 [5:58:57<2:59:14,  1.77s/it] 66%|██████▌   | 11765/17834 [5:58:59<2:58:47,  1.77s/it] 66%|██████▌   | 11766/17834 [5:59:01<2:59:07,  1.77s/it] 66%|██████▌   | 11767/17834 [5:59:03<2:58:22,  1.76s/it] 66%|██████▌   | 11768/17834 [5:59:04<2:58:13,  1.76s/it] 66%|██████▌   | 11769/17834 [5:59:06<2:58:11,  1.76s/it] 66%|██████▌   | 11770/17834 [5:59:08<2:56:42,  1.75s/it] 66%|██████▌   | 11771/17834 [5:59:10<2:59:24,  1.78s/it] 66%|██████▌   | 11772/17834 [5:59:11<2:57:39,  1.76s/it] 66%|██████▌   | 11773/17834 [5:59:13<2:56:53,  1.75s/it] 66%|██████▌   | 11774/17834 [5:59:15<2:57:50,  1.76s/it] 66%|██████▌   | 11775/17834 [5:59:17<2:56:30,  1.75s/it] 66%|██████▌   | 11776/17834 [5:59:18<2:57:49,  1.76s/it] 66%|██████▌   | 11777/17834 [5:59:20<2:57:07,  1.75s/it] 66%|██████▌   | 11778/17834 [5:59:22<3:00:08,  1.78s/it] 66%|██████▌   | 11779/17834 [5:59:24<2:59:19,  1.78s/it] 66%|██████▌   | 11780/17834 [5:59:26<2:59:03,  1.77s/it] 66%|██████▌   | 11781/17834 [5:59:27<2:55:49,  1.74s/it] 66%|██████▌   | 11782/17834 [5:59:29<2:53:22,  1.72s/it] 66%|██████▌   | 11783/17834 [5:59:31<2:54:49,  1.73s/it] 66%|██████▌   | 11784/17834 [5:59:32<2:54:38,  1.73s/it] 66%|██████▌   | 11785/17834 [5:59:34<2:54:49,  1.73s/it] 66%|██████▌   | 11786/17834 [5:59:36<2:54:53,  1.74s/it] 66%|██████▌   | 11787/17834 [5:59:38<2:54:46,  1.73s/it] 66%|██████▌   | 11788/17834 [5:59:39<2:53:36,  1.72s/it] 66%|██████▌   | 11789/17834 [5:59:41<2:53:36,  1.72s/it] 66%|██████▌   | 11790/17834 [5:59:43<2:54:02,  1.73s/it] 66%|██████▌   | 11791/17834 [5:59:45<2:56:46,  1.76s/it] 66%|██████▌   | 11792/17834 [5:59:46<2:57:26,  1.76s/it] 66%|██████▌   | 11793/17834 [5:59:48<2:55:34,  1.74s/it] 66%|██████▌   | 11794/17834 [5:59:50<2:56:33,  1.75s/it] 66%|██████▌   | 11795/17834 [5:59:52<2:56:14,  1.75s/it] 66%|██████▌   | 11796/17834 [5:59:53<2:54:52,  1.74s/it] 66%|██████▌   | 11797/17834 [5:59:55<2:54:37,  1.74s/it] 66%|██████▌   | 11798/17834 [5:59:57<2:56:23,  1.75s/it] 66%|██████▌   | 11799/17834 [5:59:59<2:55:34,  1.75s/it]08/31/2024 01:14:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.278554916381836, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.050507619976997375, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.245115041732788, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5741777420043945}
 66%|██████▌   | 11800/17834 [6:00:00<2:55:57,  1.75s/it] 66%|██████▌   | 11801/17834 [6:00:02<2:55:35,  1.75s/it] 66%|██████▌   | 11802/17834 [6:00:04<2:55:30,  1.75s/it] 66%|██████▌   | 11803/17834 [6:00:06<2:53:36,  1.73s/it] 66%|██████▌   | 11804/17834 [6:00:07<2:54:41,  1.74s/it] 66%|██████▌   | 11805/17834 [6:00:09<2:55:15,  1.74s/it] 66%|██████▌   | 11806/17834 [6:00:11<2:57:06,  1.76s/it] 66%|██████▌   | 11807/17834 [6:00:13<2:55:10,  1.74s/it] 66%|██████▌   | 11808/17834 [6:00:14<2:54:47,  1.74s/it] 66%|██████▌   | 11809/17834 [6:00:16<2:52:56,  1.72s/it] 66%|██████▌   | 11810/17834 [6:00:18<2:54:12,  1.74s/it] 66%|██████▌   | 11811/17834 [6:00:19<2:54:47,  1.74s/it] 66%|██████▌   | 11812/17834 [6:00:21<2:55:55,  1.75s/it] 66%|██████▌   | 11813/17834 [6:00:23<2:56:08,  1.76s/it] 66%|██████▌   | 11814/17834 [6:00:25<2:56:55,  1.76s/it] 66%|██████▌   | 11815/17834 [6:00:27<2:55:44,  1.75s/it] 66%|██████▋   | 11816/17834 [6:00:28<2:55:56,  1.75s/it] 66%|██████▋   | 11817/17834 [6:00:30<2:54:45,  1.74s/it] 66%|██████▋   | 11818/17834 [6:00:32<2:55:10,  1.75s/it] 66%|██████▋   | 11819/17834 [6:00:34<2:55:38,  1.75s/it] 66%|██████▋   | 11820/17834 [6:00:35<2:55:02,  1.75s/it] 66%|██████▋   | 11821/17834 [6:00:37<2:54:06,  1.74s/it] 66%|██████▋   | 11822/17834 [6:00:39<2:56:28,  1.76s/it] 66%|██████▋   | 11823/17834 [6:00:40<2:53:29,  1.73s/it] 66%|██████▋   | 11824/17834 [6:00:42<2:53:52,  1.74s/it] 66%|██████▋   | 11825/17834 [6:00:44<2:55:15,  1.75s/it] 66%|██████▋   | 11826/17834 [6:00:46<2:55:11,  1.75s/it] 66%|██████▋   | 11827/17834 [6:00:47<2:55:38,  1.75s/it] 66%|██████▋   | 11828/17834 [6:00:49<2:55:02,  1.75s/it] 66%|██████▋   | 11829/17834 [6:00:51<2:55:33,  1.75s/it] 66%|██████▋   | 11830/17834 [6:00:53<2:55:32,  1.75s/it] 66%|██████▋   | 11831/17834 [6:00:55<2:56:44,  1.77s/it] 66%|██████▋   | 11832/17834 [6:00:56<2:55:59,  1.76s/it] 66%|██████▋   | 11833/17834 [6:00:58<2:54:34,  1.75s/it] 66%|██████▋   | 11834/17834 [6:01:00<2:53:08,  1.73s/it] 66%|██████▋   | 11835/17834 [6:01:01<2:52:01,  1.72s/it] 66%|██████▋   | 11836/17834 [6:01:03<2:54:33,  1.75s/it] 66%|██████▋   | 11837/17834 [6:01:05<2:52:45,  1.73s/it] 66%|██████▋   | 11838/17834 [6:01:07<2:53:04,  1.73s/it] 66%|██████▋   | 11839/17834 [6:01:08<2:54:25,  1.75s/it] 66%|██████▋   | 11840/17834 [6:01:10<2:52:45,  1.73s/it] 66%|██████▋   | 11841/17834 [6:01:12<2:56:10,  1.76s/it] 66%|██████▋   | 11842/17834 [6:01:14<2:55:42,  1.76s/it] 66%|██████▋   | 11843/17834 [6:01:15<2:54:52,  1.75s/it] 66%|██████▋   | 11844/17834 [6:01:17<2:55:22,  1.76s/it] 66%|██████▋   | 11845/17834 [6:01:19<2:52:57,  1.73s/it] 66%|██████▋   | 11846/17834 [6:01:21<2:51:27,  1.72s/it] 66%|██████▋   | 11847/17834 [6:01:22<2:52:38,  1.73s/it] 66%|██████▋   | 11848/17834 [6:01:24<2:54:55,  1.75s/it] 66%|██████▋   | 11849/17834 [6:01:26<2:54:02,  1.74s/it]08/31/2024 01:15:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0981345176696777, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032469023019075394, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2891390323638916, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4197425842285156}
 66%|██████▋   | 11850/17834 [6:01:28<2:53:45,  1.74s/it] 66%|██████▋   | 11851/17834 [6:01:29<2:52:44,  1.73s/it] 66%|██████▋   | 11852/17834 [6:01:31<2:55:25,  1.76s/it] 66%|██████▋   | 11853/17834 [6:01:33<2:55:37,  1.76s/it] 66%|██████▋   | 11854/17834 [6:01:35<2:56:48,  1.77s/it] 66%|██████▋   | 11855/17834 [6:01:36<2:55:06,  1.76s/it] 66%|██████▋   | 11856/17834 [6:01:38<2:55:24,  1.76s/it] 66%|██████▋   | 11857/17834 [6:01:40<2:55:58,  1.77s/it] 66%|██████▋   | 11858/17834 [6:01:42<2:56:16,  1.77s/it] 66%|██████▋   | 11859/17834 [6:01:43<2:54:56,  1.76s/it] 67%|██████▋   | 11860/17834 [6:01:45<2:54:56,  1.76s/it] 67%|██████▋   | 11861/17834 [6:01:47<2:54:33,  1.75s/it] 67%|██████▋   | 11862/17834 [6:01:49<3:00:39,  1.82s/it] 67%|██████▋   | 11863/17834 [6:01:51<2:56:36,  1.77s/it] 67%|██████▋   | 11864/17834 [6:01:52<2:55:55,  1.77s/it] 67%|██████▋   | 11865/17834 [6:01:54<2:54:52,  1.76s/it] 67%|██████▋   | 11866/17834 [6:01:56<2:53:24,  1.74s/it] 67%|██████▋   | 11867/17834 [6:01:58<2:53:02,  1.74s/it] 67%|██████▋   | 11868/17834 [6:01:59<2:52:33,  1.74s/it] 67%|██████▋   | 11869/17834 [6:02:01<2:53:06,  1.74s/it] 67%|██████▋   | 11870/17834 [6:02:03<2:52:39,  1.74s/it] 67%|██████▋   | 11871/17834 [6:02:04<2:53:01,  1.74s/it] 67%|██████▋   | 11872/17834 [6:02:06<2:53:28,  1.75s/it] 67%|██████▋   | 11873/17834 [6:02:08<2:56:40,  1.78s/it] 67%|██████▋   | 11874/17834 [6:02:10<2:56:02,  1.77s/it] 67%|██████▋   | 11875/17834 [6:02:12<2:54:07,  1.75s/it] 67%|██████▋   | 11876/17834 [6:02:13<2:54:18,  1.76s/it] 67%|██████▋   | 11877/17834 [6:02:15<2:53:21,  1.75s/it] 67%|██████▋   | 11878/17834 [6:02:17<2:51:38,  1.73s/it] 67%|██████▋   | 11879/17834 [6:02:18<2:51:58,  1.73s/it] 67%|██████▋   | 11880/17834 [6:02:20<2:51:23,  1.73s/it] 67%|██████▋   | 11881/17834 [6:02:22<2:53:00,  1.74s/it] 67%|██████▋   | 11882/17834 [6:02:24<2:54:15,  1.76s/it] 67%|██████▋   | 11883/17834 [6:02:25<2:51:53,  1.73s/it] 67%|██████▋   | 11884/17834 [6:02:27<2:53:06,  1.75s/it] 67%|██████▋   | 11885/17834 [6:02:29<2:51:54,  1.73s/it] 67%|██████▋   | 11886/17834 [6:02:31<2:54:37,  1.76s/it] 67%|██████▋   | 11887/17834 [6:02:33<2:56:46,  1.78s/it] 67%|██████▋   | 11888/17834 [6:02:34<2:55:33,  1.77s/it] 67%|██████▋   | 11889/17834 [6:02:36<2:54:41,  1.76s/it] 67%|██████▋   | 11890/17834 [6:02:38<2:52:23,  1.74s/it] 67%|██████▋   | 11891/17834 [6:02:40<2:53:23,  1.75s/it] 67%|██████▋   | 11892/17834 [6:02:41<2:54:04,  1.76s/it] 67%|██████▋   | 11893/17834 [6:02:43<2:54:55,  1.77s/it] 67%|██████▋   | 11894/17834 [6:02:45<2:51:37,  1.73s/it] 67%|██████▋   | 11895/17834 [6:02:47<2:52:42,  1.74s/it] 67%|██████▋   | 11896/17834 [6:02:48<2:55:53,  1.78s/it] 67%|██████▋   | 11897/17834 [6:02:50<2:56:52,  1.79s/it] 67%|██████▋   | 11898/17834 [6:02:52<2:53:15,  1.75s/it] 67%|██████▋   | 11899/17834 [6:02:54<2:55:50,  1.78s/it]08/31/2024 01:17:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0648248195648193, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030791860073804855, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.13356351852417, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.229180335998535}
 67%|██████▋   | 11900/17834 [6:02:55<2:53:50,  1.76s/it] 67%|██████▋   | 11901/17834 [6:02:57<2:52:14,  1.74s/it] 67%|██████▋   | 11902/17834 [6:02:59<2:53:22,  1.75s/it] 67%|██████▋   | 11903/17834 [6:03:01<2:52:35,  1.75s/it] 67%|██████▋   | 11904/17834 [6:03:02<2:53:07,  1.75s/it] 67%|██████▋   | 11905/17834 [6:03:04<2:51:12,  1.73s/it] 67%|██████▋   | 11906/17834 [6:03:06<2:50:42,  1.73s/it] 67%|██████▋   | 11907/17834 [6:03:08<2:51:40,  1.74s/it] 67%|██████▋   | 11908/17834 [6:03:09<2:51:58,  1.74s/it] 67%|██████▋   | 11909/17834 [6:03:11<2:52:48,  1.75s/it] 67%|██████▋   | 11910/17834 [6:03:13<2:53:01,  1.75s/it] 67%|██████▋   | 11911/17834 [6:03:15<2:53:54,  1.76s/it] 67%|██████▋   | 11912/17834 [6:03:16<2:52:38,  1.75s/it] 67%|██████▋   | 11913/17834 [6:03:18<2:51:32,  1.74s/it] 67%|██████▋   | 11914/17834 [6:03:20<2:53:52,  1.76s/it] 67%|██████▋   | 11915/17834 [6:03:22<2:52:49,  1.75s/it] 67%|██████▋   | 11916/17834 [6:03:23<2:54:28,  1.77s/it] 67%|██████▋   | 11917/17834 [6:03:25<2:53:13,  1.76s/it] 67%|██████▋   | 11918/17834 [6:03:27<2:53:07,  1.76s/it] 67%|██████▋   | 11919/17834 [6:03:29<2:51:36,  1.74s/it] 67%|██████▋   | 11920/17834 [6:03:30<2:52:57,  1.75s/it] 67%|██████▋   | 11921/17834 [6:03:32<2:51:31,  1.74s/it] 67%|██████▋   | 11922/17834 [6:03:34<2:53:37,  1.76s/it] 67%|██████▋   | 11923/17834 [6:03:36<2:54:38,  1.77s/it] 67%|██████▋   | 11924/17834 [6:03:37<2:53:17,  1.76s/it] 67%|██████▋   | 11925/17834 [6:03:39<2:52:21,  1.75s/it] 67%|██████▋   | 11926/17834 [6:03:41<2:53:26,  1.76s/it] 67%|██████▋   | 11927/17834 [6:03:43<2:53:47,  1.77s/it] 67%|██████▋   | 11928/17834 [6:03:45<2:54:46,  1.78s/it] 67%|██████▋   | 11929/17834 [6:03:46<2:52:07,  1.75s/it] 67%|██████▋   | 11930/17834 [6:03:48<2:51:12,  1.74s/it] 67%|██████▋   | 11931/17834 [6:03:50<2:51:17,  1.74s/it] 67%|██████▋   | 11932/17834 [6:03:51<2:49:47,  1.73s/it] 67%|██████▋   | 11933/17834 [6:03:53<2:52:06,  1.75s/it] 67%|██████▋   | 11934/17834 [6:03:55<2:51:56,  1.75s/it] 67%|██████▋   | 11935/17834 [6:03:57<2:52:45,  1.76s/it] 67%|██████▋   | 11936/17834 [6:03:58<2:52:14,  1.75s/it] 67%|██████▋   | 11937/17834 [6:04:00<2:52:21,  1.75s/it] 67%|██████▋   | 11938/17834 [6:04:02<2:52:56,  1.76s/it] 67%|██████▋   | 11939/17834 [6:04:04<2:53:18,  1.76s/it] 67%|██████▋   | 11940/17834 [6:04:05<2:53:24,  1.77s/it] 67%|██████▋   | 11941/17834 [6:04:07<2:52:56,  1.76s/it] 67%|██████▋   | 11942/17834 [6:04:09<2:53:18,  1.76s/it] 67%|██████▋   | 11943/17834 [6:04:11<2:52:52,  1.76s/it] 67%|██████▋   | 11944/17834 [6:04:12<2:51:26,  1.75s/it] 67%|██████▋   | 11945/17834 [6:04:14<2:52:27,  1.76s/it] 67%|██████▋   | 11946/17834 [6:04:16<2:51:10,  1.74s/it] 67%|██████▋   | 11947/17834 [6:04:18<2:49:18,  1.73s/it] 67%|██████▋   | 11948/17834 [6:04:20<2:55:08,  1.79s/it] 67%|██████▋   | 11949/17834 [6:04:21<2:55:00,  1.78s/it]08/31/2024 01:18:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9726017713546753, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025896120816469193, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.182783842086792, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.181281805038452}
 67%|██████▋   | 11950/17834 [6:04:23<2:53:54,  1.77s/it] 67%|██████▋   | 11951/17834 [6:04:25<2:53:04,  1.77s/it] 67%|██████▋   | 11952/17834 [6:04:27<2:54:29,  1.78s/it] 67%|██████▋   | 11953/17834 [6:04:28<2:51:56,  1.75s/it] 67%|██████▋   | 11954/17834 [6:04:30<2:51:03,  1.75s/it] 67%|██████▋   | 11955/17834 [6:04:32<2:53:35,  1.77s/it] 67%|██████▋   | 11956/17834 [6:04:34<2:53:04,  1.77s/it] 67%|██████▋   | 11957/17834 [6:04:35<2:52:44,  1.76s/it] 67%|██████▋   | 11958/17834 [6:04:37<2:52:39,  1.76s/it] 67%|██████▋   | 11959/17834 [6:04:39<2:50:58,  1.75s/it] 67%|██████▋   | 11960/17834 [6:04:41<2:51:04,  1.75s/it] 67%|██████▋   | 11961/17834 [6:04:42<2:52:17,  1.76s/it] 67%|██████▋   | 11962/17834 [6:04:44<2:51:49,  1.76s/it] 67%|██████▋   | 11963/17834 [6:04:46<2:51:00,  1.75s/it] 67%|██████▋   | 11964/17834 [6:04:48<2:50:38,  1.74s/it] 67%|██████▋   | 11965/17834 [6:04:49<2:51:26,  1.75s/it] 67%|██████▋   | 11966/17834 [6:04:51<2:52:01,  1.76s/it] 67%|██████▋   | 11967/17834 [6:04:53<2:53:37,  1.78s/it] 67%|██████▋   | 11968/17834 [6:04:55<2:55:19,  1.79s/it] 67%|██████▋   | 11969/17834 [6:04:57<2:53:37,  1.78s/it] 67%|██████▋   | 11970/17834 [6:04:58<2:53:19,  1.77s/it] 67%|██████▋   | 11971/17834 [6:05:00<2:51:02,  1.75s/it] 67%|██████▋   | 11972/17834 [6:05:02<2:50:17,  1.74s/it] 67%|██████▋   | 11973/17834 [6:05:04<2:50:38,  1.75s/it] 67%|██████▋   | 11974/17834 [6:05:05<2:49:44,  1.74s/it] 67%|██████▋   | 11975/17834 [6:05:07<2:49:34,  1.74s/it] 67%|██████▋   | 11976/17834 [6:05:09<2:48:10,  1.72s/it] 67%|██████▋   | 11977/17834 [6:05:10<2:49:01,  1.73s/it] 67%|██████▋   | 11978/17834 [6:05:12<2:47:59,  1.72s/it] 67%|██████▋   | 11979/17834 [6:05:14<2:47:43,  1.72s/it] 67%|██████▋   | 11980/17834 [6:05:16<2:48:16,  1.72s/it] 67%|██████▋   | 11981/17834 [6:05:17<2:50:00,  1.74s/it] 67%|██████▋   | 11982/17834 [6:05:19<2:49:25,  1.74s/it] 67%|██████▋   | 11983/17834 [6:05:21<2:47:29,  1.72s/it] 67%|██████▋   | 11984/17834 [6:05:23<2:51:14,  1.76s/it] 67%|██████▋   | 11985/17834 [6:05:24<2:49:22,  1.74s/it] 67%|██████▋   | 11986/17834 [6:05:26<2:53:35,  1.78s/it] 67%|██████▋   | 11987/17834 [6:05:28<2:51:13,  1.76s/it] 67%|██████▋   | 11988/17834 [6:05:30<2:50:47,  1.75s/it] 67%|██████▋   | 11989/17834 [6:05:31<2:50:16,  1.75s/it] 67%|██████▋   | 11990/17834 [6:05:33<2:51:26,  1.76s/it] 67%|██████▋   | 11991/17834 [6:05:35<2:49:39,  1.74s/it] 67%|██████▋   | 11992/17834 [6:05:37<2:49:10,  1.74s/it] 67%|██████▋   | 11993/17834 [6:05:38<2:48:48,  1.73s/it] 67%|██████▋   | 11994/17834 [6:05:40<2:46:55,  1.71s/it] 67%|██████▋   | 11995/17834 [6:05:42<2:47:00,  1.72s/it] 67%|██████▋   | 11996/17834 [6:05:43<2:47:34,  1.72s/it] 67%|██████▋   | 11997/17834 [6:05:45<2:47:58,  1.73s/it] 67%|██████▋   | 11998/17834 [6:05:47<2:51:39,  1.76s/it] 67%|██████▋   | 11999/17834 [6:05:49<2:52:53,  1.78s/it]08/31/2024 01:20:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1476116180419922, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04425661265850067, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2217698097229004, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.413638114929199}
 67%|██████▋   | 12000/17834 [6:05:51<2:50:25,  1.75s/it] 67%|██████▋   | 12001/17834 [6:05:52<2:50:11,  1.75s/it] 67%|██████▋   | 12002/17834 [6:05:54<2:49:16,  1.74s/it] 67%|██████▋   | 12003/17834 [6:05:56<2:50:22,  1.75s/it] 67%|██████▋   | 12004/17834 [6:05:58<2:50:27,  1.75s/it] 67%|██████▋   | 12005/17834 [6:05:59<2:49:53,  1.75s/it] 67%|██████▋   | 12006/17834 [6:06:01<2:48:38,  1.74s/it] 67%|██████▋   | 12007/17834 [6:06:03<2:47:57,  1.73s/it] 67%|██████▋   | 12008/17834 [6:06:05<2:51:07,  1.76s/it] 67%|██████▋   | 12009/17834 [6:06:06<2:51:18,  1.76s/it] 67%|██████▋   | 12010/17834 [6:06:08<2:49:06,  1.74s/it] 67%|██████▋   | 12011/17834 [6:06:10<2:49:14,  1.74s/it] 67%|██████▋   | 12012/17834 [6:06:11<2:49:49,  1.75s/it] 67%|██████▋   | 12013/17834 [6:06:13<2:49:33,  1.75s/it] 67%|██████▋   | 12014/17834 [6:06:15<2:47:46,  1.73s/it] 67%|██████▋   | 12015/17834 [6:06:17<2:47:27,  1.73s/it] 67%|██████▋   | 12016/17834 [6:06:18<2:49:04,  1.74s/it] 67%|██████▋   | 12017/17834 [6:06:20<2:48:54,  1.74s/it] 67%|██████▋   | 12018/17834 [6:06:22<2:48:36,  1.74s/it] 67%|██████▋   | 12019/17834 [6:06:24<2:48:49,  1.74s/it] 67%|██████▋   | 12020/17834 [6:06:25<2:47:54,  1.73s/it] 67%|██████▋   | 12021/17834 [6:06:27<2:50:39,  1.76s/it] 67%|██████▋   | 12022/17834 [6:06:29<2:48:41,  1.74s/it] 67%|██████▋   | 12023/17834 [6:06:31<2:49:49,  1.75s/it] 67%|██████▋   | 12024/17834 [6:06:32<2:49:51,  1.75s/it] 67%|██████▋   | 12025/17834 [6:06:34<2:48:39,  1.74s/it] 67%|██████▋   | 12026/17834 [6:06:36<2:48:37,  1.74s/it] 67%|██████▋   | 12027/17834 [6:06:38<2:48:29,  1.74s/it] 67%|██████▋   | 12028/17834 [6:06:39<2:49:44,  1.75s/it] 67%|██████▋   | 12029/17834 [6:06:41<2:49:03,  1.75s/it] 67%|██████▋   | 12030/17834 [6:06:43<2:48:59,  1.75s/it] 67%|██████▋   | 12031/17834 [6:06:45<2:48:10,  1.74s/it] 67%|██████▋   | 12032/17834 [6:06:46<2:47:09,  1.73s/it] 67%|██████▋   | 12033/17834 [6:06:48<2:48:17,  1.74s/it] 67%|██████▋   | 12034/17834 [6:06:50<2:47:27,  1.73s/it] 67%|██████▋   | 12035/17834 [6:06:51<2:45:29,  1.71s/it] 67%|██████▋   | 12036/17834 [6:06:53<2:46:42,  1.73s/it] 67%|██████▋   | 12037/17834 [6:06:55<2:48:22,  1.74s/it] 68%|██████▊   | 12038/17834 [6:06:57<2:47:39,  1.74s/it] 68%|██████▊   | 12039/17834 [6:06:58<2:47:45,  1.74s/it] 68%|██████▊   | 12040/17834 [6:07:00<2:49:09,  1.75s/it] 68%|██████▊   | 12041/17834 [6:07:02<2:50:01,  1.76s/it] 68%|██████▊   | 12042/17834 [6:07:04<2:49:18,  1.75s/it] 68%|██████▊   | 12043/17834 [6:07:06<2:51:04,  1.77s/it] 68%|██████▊   | 12044/17834 [6:07:07<2:50:39,  1.77s/it] 68%|██████▊   | 12045/17834 [6:07:09<2:50:33,  1.77s/it] 68%|██████▊   | 12046/17834 [6:07:11<2:48:52,  1.75s/it] 68%|██████▊   | 12047/17834 [6:07:13<2:50:30,  1.77s/it] 68%|██████▊   | 12048/17834 [6:07:14<2:48:44,  1.75s/it] 68%|██████▊   | 12049/17834 [6:07:16<2:48:02,  1.74s/it]08/31/2024 01:21:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2116049528121948, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.047026317566633224, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.238560199737549, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4971914291381836}
 68%|██████▊   | 12050/17834 [6:07:18<2:48:35,  1.75s/it] 68%|██████▊   | 12051/17834 [6:07:20<2:48:44,  1.75s/it] 68%|██████▊   | 12052/17834 [6:07:21<2:50:28,  1.77s/it] 68%|██████▊   | 12053/17834 [6:07:23<2:51:55,  1.78s/it] 68%|██████▊   | 12054/17834 [6:07:25<2:51:03,  1.78s/it] 68%|██████▊   | 12055/17834 [6:07:27<2:51:51,  1.78s/it] 68%|██████▊   | 12056/17834 [6:07:29<2:51:32,  1.78s/it] 68%|██████▊   | 12057/17834 [6:07:30<2:50:30,  1.77s/it] 68%|██████▊   | 12058/17834 [6:07:32<2:48:35,  1.75s/it] 68%|██████▊   | 12059/17834 [6:07:34<2:49:16,  1.76s/it] 68%|██████▊   | 12060/17834 [6:07:35<2:48:47,  1.75s/it] 68%|██████▊   | 12061/17834 [6:07:37<2:48:49,  1.75s/it] 68%|██████▊   | 12062/17834 [6:07:39<2:50:51,  1.78s/it] 68%|██████▊   | 12063/17834 [6:07:41<2:51:59,  1.79s/it] 68%|██████▊   | 12064/17834 [6:07:43<2:49:39,  1.76s/it] 68%|██████▊   | 12065/17834 [6:07:44<2:48:55,  1.76s/it] 68%|██████▊   | 12066/17834 [6:07:46<2:49:15,  1.76s/it] 68%|██████▊   | 12067/17834 [6:07:48<2:50:16,  1.77s/it] 68%|██████▊   | 12068/17834 [6:07:50<2:49:55,  1.77s/it] 68%|██████▊   | 12069/17834 [6:07:51<2:48:35,  1.75s/it] 68%|██████▊   | 12070/17834 [6:07:53<2:52:16,  1.79s/it] 68%|██████▊   | 12071/17834 [6:07:55<2:49:14,  1.76s/it] 68%|██████▊   | 12072/17834 [6:07:57<2:48:12,  1.75s/it] 68%|██████▊   | 12073/17834 [6:07:58<2:47:13,  1.74s/it] 68%|██████▊   | 12074/17834 [6:08:00<2:49:01,  1.76s/it] 68%|██████▊   | 12075/17834 [6:08:02<2:48:45,  1.76s/it] 68%|██████▊   | 12076/17834 [6:08:04<2:47:32,  1.75s/it] 68%|██████▊   | 12077/17834 [6:08:05<2:48:25,  1.76s/it] 68%|██████▊   | 12078/17834 [6:08:07<2:48:34,  1.76s/it] 68%|██████▊   | 12079/17834 [6:08:09<2:48:44,  1.76s/it] 68%|██████▊   | 12080/17834 [6:08:11<2:48:25,  1.76s/it] 68%|██████▊   | 12081/17834 [6:08:12<2:48:04,  1.75s/it] 68%|██████▊   | 12082/17834 [6:08:14<2:47:05,  1.74s/it] 68%|██████▊   | 12083/17834 [6:08:16<2:48:11,  1.75s/it] 68%|██████▊   | 12084/17834 [6:08:18<2:47:58,  1.75s/it] 68%|██████▊   | 12085/17834 [6:08:19<2:47:59,  1.75s/it] 68%|██████▊   | 12086/17834 [6:08:21<2:50:26,  1.78s/it] 68%|██████▊   | 12087/17834 [6:08:23<2:50:27,  1.78s/it] 68%|██████▊   | 12088/17834 [6:08:25<2:48:30,  1.76s/it] 68%|██████▊   | 12089/17834 [6:08:27<2:49:57,  1.77s/it] 68%|██████▊   | 12090/17834 [6:08:28<2:50:37,  1.78s/it] 68%|██████▊   | 12091/17834 [6:08:30<2:48:20,  1.76s/it] 68%|██████▊   | 12092/17834 [6:08:32<2:48:22,  1.76s/it] 68%|██████▊   | 12093/17834 [6:08:34<2:46:02,  1.74s/it] 68%|██████▊   | 12094/17834 [6:08:35<2:45:26,  1.73s/it] 68%|██████▊   | 12095/17834 [6:08:37<2:45:39,  1.73s/it] 68%|██████▊   | 12096/17834 [6:08:39<2:48:03,  1.76s/it] 68%|██████▊   | 12097/17834 [6:08:41<2:46:16,  1.74s/it] 68%|██████▊   | 12098/17834 [6:08:42<2:46:13,  1.74s/it] 68%|██████▊   | 12099/17834 [6:08:44<2:44:54,  1.73s/it]08/31/2024 01:23:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9821786880493164, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03808920830488205, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.175665855407715, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1959338188171387}
 68%|██████▊   | 12100/17834 [6:08:46<2:45:52,  1.74s/it] 68%|██████▊   | 12101/17834 [6:08:47<2:45:55,  1.74s/it] 68%|██████▊   | 12102/17834 [6:08:49<2:47:52,  1.76s/it] 68%|██████▊   | 12103/17834 [6:08:51<2:49:07,  1.77s/it] 68%|██████▊   | 12104/17834 [6:08:53<2:48:49,  1.77s/it] 68%|██████▊   | 12105/17834 [6:08:55<2:46:00,  1.74s/it] 68%|██████▊   | 12106/17834 [6:08:56<2:46:06,  1.74s/it] 68%|██████▊   | 12107/17834 [6:08:58<2:47:48,  1.76s/it] 68%|██████▊   | 12108/17834 [6:09:00<2:46:24,  1.74s/it] 68%|██████▊   | 12109/17834 [6:09:01<2:45:27,  1.73s/it] 68%|██████▊   | 12110/17834 [6:09:03<2:46:54,  1.75s/it] 68%|██████▊   | 12111/17834 [6:09:05<2:47:32,  1.76s/it] 68%|██████▊   | 12112/17834 [6:09:07<2:45:40,  1.74s/it] 68%|██████▊   | 12113/17834 [6:09:08<2:46:08,  1.74s/it] 68%|██████▊   | 12114/17834 [6:09:10<2:44:53,  1.73s/it] 68%|██████▊   | 12115/17834 [6:09:12<2:50:03,  1.78s/it] 68%|██████▊   | 12116/17834 [6:09:14<2:49:49,  1.78s/it] 68%|██████▊   | 12117/17834 [6:09:16<2:47:15,  1.76s/it] 68%|██████▊   | 12118/17834 [6:09:17<2:46:29,  1.75s/it] 68%|██████▊   | 12119/17834 [6:09:19<2:47:49,  1.76s/it] 68%|██████▊   | 12120/17834 [6:09:21<2:45:47,  1.74s/it] 68%|██████▊   | 12121/17834 [6:09:23<2:45:38,  1.74s/it] 68%|██████▊   | 12122/17834 [6:09:24<2:45:19,  1.74s/it] 68%|██████▊   | 12123/17834 [6:09:26<2:44:46,  1.73s/it] 68%|██████▊   | 12124/17834 [6:09:28<2:48:37,  1.77s/it] 68%|██████▊   | 12125/17834 [6:09:30<2:46:00,  1.74s/it] 68%|██████▊   | 12126/17834 [6:09:31<2:46:09,  1.75s/it] 68%|██████▊   | 12127/17834 [6:09:33<2:46:45,  1.75s/it] 68%|██████▊   | 12128/17834 [6:09:35<2:46:51,  1.75s/it] 68%|██████▊   | 12129/17834 [6:09:37<2:50:14,  1.79s/it] 68%|██████▊   | 12130/17834 [6:09:38<2:50:04,  1.79s/it] 68%|██████▊   | 12131/17834 [6:09:40<2:51:43,  1.81s/it] 68%|██████▊   | 12132/17834 [6:09:42<2:49:55,  1.79s/it] 68%|██████▊   | 12133/17834 [6:09:44<2:51:49,  1.81s/it] 68%|██████▊   | 12134/17834 [6:09:46<2:50:00,  1.79s/it] 68%|██████▊   | 12135/17834 [6:09:47<2:46:56,  1.76s/it] 68%|██████▊   | 12136/17834 [6:09:49<2:45:58,  1.75s/it] 68%|██████▊   | 12137/17834 [6:09:51<2:48:28,  1.77s/it] 68%|██████▊   | 12138/17834 [6:09:53<2:50:52,  1.80s/it] 68%|██████▊   | 12139/17834 [6:09:54<2:47:53,  1.77s/it] 68%|██████▊   | 12140/17834 [6:09:56<2:46:36,  1.76s/it] 68%|██████▊   | 12141/17834 [6:09:58<2:46:44,  1.76s/it] 68%|██████▊   | 12142/17834 [6:10:00<2:46:48,  1.76s/it] 68%|██████▊   | 12143/17834 [6:10:01<2:48:22,  1.78s/it] 68%|██████▊   | 12144/17834 [6:10:03<2:47:20,  1.76s/it] 68%|██████▊   | 12145/17834 [6:10:05<2:47:39,  1.77s/it] 68%|██████▊   | 12146/17834 [6:10:07<2:49:16,  1.79s/it] 68%|██████▊   | 12147/17834 [6:10:09<2:46:46,  1.76s/it] 68%|██████▊   | 12148/17834 [6:10:10<2:44:28,  1.74s/it] 68%|██████▊   | 12149/17834 [6:10:12<2:44:56,  1.74s/it]08/31/2024 01:24:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3953757286071777, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04086187481880188, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3470311164855957, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.783268690109253}
 68%|██████▊   | 12150/17834 [6:10:14<2:46:30,  1.76s/it] 68%|██████▊   | 12151/17834 [6:10:16<2:46:14,  1.76s/it] 68%|██████▊   | 12152/17834 [6:10:17<2:45:35,  1.75s/it] 68%|██████▊   | 12153/17834 [6:10:19<2:44:51,  1.74s/it] 68%|██████▊   | 12154/17834 [6:10:21<2:47:29,  1.77s/it] 68%|██████▊   | 12155/17834 [6:10:23<2:47:26,  1.77s/it] 68%|██████▊   | 12156/17834 [6:10:24<2:45:44,  1.75s/it] 68%|██████▊   | 12157/17834 [6:10:26<2:43:28,  1.73s/it] 68%|██████▊   | 12158/17834 [6:10:28<2:43:55,  1.73s/it] 68%|██████▊   | 12159/17834 [6:10:29<2:45:24,  1.75s/it] 68%|██████▊   | 12160/17834 [6:10:31<2:46:16,  1.76s/it] 68%|██████▊   | 12161/17834 [6:10:33<2:43:43,  1.73s/it] 68%|██████▊   | 12162/17834 [6:10:35<2:41:49,  1.71s/it] 68%|██████▊   | 12163/17834 [6:10:36<2:42:17,  1.72s/it] 68%|██████▊   | 12164/17834 [6:10:38<2:41:45,  1.71s/it] 68%|██████▊   | 12165/17834 [6:10:40<2:42:53,  1.72s/it] 68%|██████▊   | 12166/17834 [6:10:41<2:42:10,  1.72s/it] 68%|██████▊   | 12167/17834 [6:10:43<2:43:55,  1.74s/it] 68%|██████▊   | 12168/17834 [6:10:45<2:43:53,  1.74s/it] 68%|██████▊   | 12169/17834 [6:10:47<2:43:03,  1.73s/it] 68%|██████▊   | 12170/17834 [6:10:48<2:42:44,  1.72s/it] 68%|██████▊   | 12171/17834 [6:10:50<2:42:21,  1.72s/it] 68%|██████▊   | 12172/17834 [6:10:52<2:42:39,  1.72s/it] 68%|██████▊   | 12173/17834 [6:10:54<2:44:34,  1.74s/it] 68%|██████▊   | 12174/17834 [6:10:55<2:45:00,  1.75s/it] 68%|██████▊   | 12175/17834 [6:10:57<2:45:39,  1.76s/it] 68%|██████▊   | 12176/17834 [6:10:59<2:47:36,  1.78s/it] 68%|██████▊   | 12177/17834 [6:11:01<2:45:54,  1.76s/it] 68%|██████▊   | 12178/17834 [6:11:03<2:46:34,  1.77s/it] 68%|██████▊   | 12179/17834 [6:11:04<2:47:12,  1.77s/it] 68%|██████▊   | 12180/17834 [6:11:06<2:46:28,  1.77s/it] 68%|██████▊   | 12181/17834 [6:11:08<2:45:34,  1.76s/it] 68%|██████▊   | 12182/17834 [6:11:10<2:44:53,  1.75s/it] 68%|██████▊   | 12183/17834 [6:11:11<2:44:41,  1.75s/it] 68%|██████▊   | 12184/17834 [6:11:13<2:44:06,  1.74s/it] 68%|██████▊   | 12185/17834 [6:11:15<2:44:58,  1.75s/it] 68%|██████▊   | 12186/17834 [6:11:17<2:46:45,  1.77s/it] 68%|██████▊   | 12187/17834 [6:11:18<2:45:38,  1.76s/it] 68%|██████▊   | 12188/17834 [6:11:20<2:45:01,  1.75s/it] 68%|██████▊   | 12189/17834 [6:11:22<2:44:18,  1.75s/it] 68%|██████▊   | 12190/17834 [6:11:24<2:45:04,  1.75s/it] 68%|██████▊   | 12191/17834 [6:11:25<2:44:34,  1.75s/it] 68%|██████▊   | 12192/17834 [6:11:27<2:43:18,  1.74s/it] 68%|██████▊   | 12193/17834 [6:11:29<2:45:12,  1.76s/it] 68%|██████▊   | 12194/17834 [6:11:31<2:44:54,  1.75s/it] 68%|██████▊   | 12195/17834 [6:11:32<2:43:59,  1.74s/it] 68%|██████▊   | 12196/17834 [6:11:34<2:47:17,  1.78s/it] 68%|██████▊   | 12197/17834 [6:11:36<2:45:13,  1.76s/it] 68%|██████▊   | 12198/17834 [6:11:38<2:45:19,  1.76s/it] 68%|██████▊   | 12199/17834 [6:11:39<2:44:06,  1.75s/it]08/31/2024 01:25:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3940885066986084, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.034152284264564514, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3685996532440186, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7968404293060303}
 68%|██████▊   | 12200/17834 [6:11:41<2:43:32,  1.74s/it] 68%|██████▊   | 12201/17834 [6:11:43<2:47:00,  1.78s/it] 68%|██████▊   | 12202/17834 [6:11:45<2:45:12,  1.76s/it] 68%|██████▊   | 12203/17834 [6:11:46<2:42:54,  1.74s/it] 68%|██████▊   | 12204/17834 [6:11:48<2:44:04,  1.75s/it] 68%|██████▊   | 12205/17834 [6:11:50<2:43:29,  1.74s/it] 68%|██████▊   | 12206/17834 [6:11:52<2:43:42,  1.75s/it] 68%|██████▊   | 12207/17834 [6:11:53<2:41:48,  1.73s/it] 68%|██████▊   | 12208/17834 [6:11:55<2:41:45,  1.73s/it] 68%|██████▊   | 12209/17834 [6:11:57<2:40:54,  1.72s/it] 68%|██████▊   | 12210/17834 [6:11:58<2:42:58,  1.74s/it] 68%|██████▊   | 12211/17834 [6:12:00<2:41:13,  1.72s/it] 68%|██████▊   | 12212/17834 [6:12:02<2:42:02,  1.73s/it] 68%|██████▊   | 12213/17834 [6:12:04<2:43:33,  1.75s/it] 68%|██████▊   | 12214/17834 [6:12:06<2:45:44,  1.77s/it] 68%|██████▊   | 12215/17834 [6:12:07<2:45:30,  1.77s/it] 68%|██████▊   | 12216/17834 [6:12:09<2:43:53,  1.75s/it] 69%|██████▊   | 12217/17834 [6:12:11<2:42:14,  1.73s/it] 69%|██████▊   | 12218/17834 [6:12:12<2:42:57,  1.74s/it] 69%|██████▊   | 12219/17834 [6:12:14<2:43:57,  1.75s/it] 69%|██████▊   | 12220/17834 [6:12:16<2:43:16,  1.74s/it] 69%|██████▊   | 12221/17834 [6:12:18<2:42:47,  1.74s/it] 69%|██████▊   | 12222/17834 [6:12:19<2:44:34,  1.76s/it] 69%|██████▊   | 12223/17834 [6:12:21<2:43:39,  1.75s/it] 69%|██████▊   | 12224/17834 [6:12:23<2:42:56,  1.74s/it] 69%|██████▊   | 12225/17834 [6:12:25<2:45:49,  1.77s/it] 69%|██████▊   | 12226/17834 [6:12:27<2:47:18,  1.79s/it] 69%|██████▊   | 12227/17834 [6:12:28<2:47:01,  1.79s/it] 69%|██████▊   | 12228/17834 [6:12:30<2:44:24,  1.76s/it] 69%|██████▊   | 12229/17834 [6:12:32<2:45:47,  1.77s/it] 69%|██████▊   | 12230/17834 [6:12:34<2:43:30,  1.75s/it] 69%|██████▊   | 12231/17834 [6:12:35<2:42:45,  1.74s/it] 69%|██████▊   | 12232/17834 [6:12:37<2:43:25,  1.75s/it] 69%|██████▊   | 12233/17834 [6:12:39<2:45:39,  1.77s/it] 69%|██████▊   | 12234/17834 [6:12:41<2:42:16,  1.74s/it] 69%|██████▊   | 12235/17834 [6:12:42<2:43:13,  1.75s/it] 69%|██████▊   | 12236/17834 [6:12:44<2:40:21,  1.72s/it] 69%|██████▊   | 12237/17834 [6:12:46<2:41:42,  1.73s/it] 69%|██████▊   | 12238/17834 [6:12:48<2:43:21,  1.75s/it] 69%|██████▊   | 12239/17834 [6:12:49<2:44:54,  1.77s/it] 69%|██████▊   | 12240/17834 [6:12:51<2:43:42,  1.76s/it] 69%|██████▊   | 12241/17834 [6:12:53<2:42:07,  1.74s/it] 69%|██████▊   | 12242/17834 [6:12:54<2:40:58,  1.73s/it] 69%|██████▊   | 12243/17834 [6:12:56<2:41:32,  1.73s/it] 69%|██████▊   | 12244/17834 [6:12:58<2:41:48,  1.74s/it] 69%|██████▊   | 12245/17834 [6:13:00<2:43:38,  1.76s/it] 69%|██████▊   | 12246/17834 [6:13:02<2:43:45,  1.76s/it] 69%|██████▊   | 12247/17834 [6:13:03<2:43:39,  1.76s/it] 69%|██████▊   | 12248/17834 [6:13:05<2:42:36,  1.75s/it] 69%|██████▊   | 12249/17834 [6:13:07<2:43:13,  1.75s/it]08/31/2024 01:27:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9063642621040344, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025640588253736496, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1507139205932617, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.082718849182129}
 69%|██████▊   | 12250/17834 [6:13:09<2:43:17,  1.75s/it] 69%|██████▊   | 12251/17834 [6:13:10<2:43:23,  1.76s/it] 69%|██████▊   | 12252/17834 [6:13:12<2:43:38,  1.76s/it] 69%|██████▊   | 12253/17834 [6:13:14<2:43:00,  1.75s/it] 69%|██████▊   | 12254/17834 [6:13:16<2:44:07,  1.76s/it] 69%|██████▊   | 12255/17834 [6:13:17<2:44:06,  1.76s/it] 69%|██████▊   | 12256/17834 [6:13:19<2:44:43,  1.77s/it] 69%|██████▊   | 12257/17834 [6:13:21<2:42:56,  1.75s/it] 69%|██████▊   | 12258/17834 [6:13:23<2:44:02,  1.77s/it] 69%|██████▊   | 12259/17834 [6:13:24<2:42:17,  1.75s/it] 69%|██████▊   | 12260/17834 [6:13:26<2:41:35,  1.74s/it] 69%|██████▉   | 12261/17834 [6:13:28<2:42:17,  1.75s/it] 69%|██████▉   | 12262/17834 [6:13:30<2:42:23,  1.75s/it] 69%|██████▉   | 12263/17834 [6:13:31<2:41:02,  1.73s/it] 69%|██████▉   | 12264/17834 [6:13:33<2:41:49,  1.74s/it] 69%|██████▉   | 12265/17834 [6:13:35<2:40:28,  1.73s/it] 69%|██████▉   | 12266/17834 [6:13:37<2:40:30,  1.73s/it] 69%|██████▉   | 12267/17834 [6:13:38<2:38:44,  1.71s/it] 69%|██████▉   | 12268/17834 [6:13:40<2:39:06,  1.72s/it] 69%|██████▉   | 12269/17834 [6:13:42<2:43:49,  1.77s/it] 69%|██████▉   | 12270/17834 [6:13:44<2:43:11,  1.76s/it] 69%|██████▉   | 12271/17834 [6:13:45<2:43:08,  1.76s/it] 69%|██████▉   | 12272/17834 [6:13:47<2:45:13,  1.78s/it] 69%|██████▉   | 12273/17834 [6:13:49<2:46:00,  1.79s/it] 69%|██████▉   | 12274/17834 [6:13:51<2:46:11,  1.79s/it] 69%|██████▉   | 12275/17834 [6:13:52<2:45:00,  1.78s/it] 69%|██████▉   | 12276/17834 [6:13:54<2:44:03,  1.77s/it] 69%|██████▉   | 12277/17834 [6:13:56<2:41:53,  1.75s/it] 69%|██████▉   | 12278/17834 [6:13:58<2:39:42,  1.72s/it] 69%|██████▉   | 12279/17834 [6:13:59<2:42:48,  1.76s/it] 69%|██████▉   | 12280/17834 [6:14:01<2:42:05,  1.75s/it] 69%|██████▉   | 12281/17834 [6:14:03<2:43:00,  1.76s/it] 69%|██████▉   | 12282/17834 [6:14:05<2:42:34,  1.76s/it] 69%|██████▉   | 12283/17834 [6:14:07<2:44:05,  1.77s/it] 69%|██████▉   | 12284/17834 [6:14:08<2:42:15,  1.75s/it] 69%|██████▉   | 12285/17834 [6:14:10<2:43:02,  1.76s/it] 69%|██████▉   | 12286/17834 [6:14:12<2:42:20,  1.76s/it] 69%|██████▉   | 12287/17834 [6:14:13<2:41:39,  1.75s/it] 69%|██████▉   | 12288/17834 [6:14:15<2:43:19,  1.77s/it] 69%|██████▉   | 12289/17834 [6:14:17<2:43:02,  1.76s/it] 69%|██████▉   | 12290/17834 [6:14:19<2:42:09,  1.76s/it] 69%|██████▉   | 12291/17834 [6:14:21<2:42:58,  1.76s/it] 69%|██████▉   | 12292/17834 [6:14:22<2:42:03,  1.75s/it] 69%|██████▉   | 12293/17834 [6:14:24<2:42:59,  1.76s/it] 69%|██████▉   | 12294/17834 [6:14:26<2:41:56,  1.75s/it] 69%|██████▉   | 12295/17834 [6:14:28<2:40:43,  1.74s/it] 69%|██████▉   | 12296/17834 [6:14:29<2:40:56,  1.74s/it] 69%|██████▉   | 12297/17834 [6:14:31<2:42:57,  1.77s/it] 69%|██████▉   | 12298/17834 [6:14:33<2:41:22,  1.75s/it] 69%|██████▉   | 12299/17834 [6:14:35<2:41:11,  1.75s/it]08/31/2024 01:28:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1752781867980957, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.040492717176675797, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.243699312210083, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.459470272064209}
 69%|██████▉   | 12300/17834 [6:14:36<2:39:13,  1.73s/it] 69%|██████▉   | 12301/17834 [6:14:38<2:38:17,  1.72s/it] 69%|██████▉   | 12302/17834 [6:14:40<2:39:46,  1.73s/it] 69%|██████▉   | 12303/17834 [6:14:41<2:39:15,  1.73s/it] 69%|██████▉   | 12304/17834 [6:14:43<2:43:39,  1.78s/it] 69%|██████▉   | 12305/17834 [6:14:45<2:43:04,  1.77s/it] 69%|██████▉   | 12306/17834 [6:14:47<2:41:27,  1.75s/it] 69%|██████▉   | 12307/17834 [6:14:48<2:39:00,  1.73s/it] 69%|██████▉   | 12308/17834 [6:14:50<2:40:59,  1.75s/it] 69%|██████▉   | 12309/17834 [6:14:52<2:40:19,  1.74s/it] 69%|██████▉   | 12310/17834 [6:14:54<2:40:14,  1.74s/it] 69%|██████▉   | 12311/17834 [6:14:55<2:39:31,  1.73s/it] 69%|██████▉   | 12312/17834 [6:14:57<2:38:47,  1.73s/it] 69%|██████▉   | 12313/17834 [6:14:59<2:38:19,  1.72s/it] 69%|██████▉   | 12314/17834 [6:15:01<2:38:12,  1.72s/it] 69%|██████▉   | 12315/17834 [6:15:02<2:39:48,  1.74s/it] 69%|██████▉   | 12316/17834 [6:15:04<2:40:53,  1.75s/it] 69%|██████▉   | 12317/17834 [6:15:06<2:40:52,  1.75s/it] 69%|██████▉   | 12318/17834 [6:15:08<2:41:53,  1.76s/it] 69%|██████▉   | 12319/17834 [6:15:09<2:41:39,  1.76s/it] 69%|██████▉   | 12320/17834 [6:15:11<2:39:25,  1.73s/it] 69%|██████▉   | 12321/17834 [6:15:13<2:40:29,  1.75s/it] 69%|██████▉   | 12322/17834 [6:15:15<2:39:55,  1.74s/it] 69%|██████▉   | 12323/17834 [6:15:16<2:40:54,  1.75s/it] 69%|██████▉   | 12324/17834 [6:15:18<2:41:15,  1.76s/it] 69%|██████▉   | 12325/17834 [6:15:20<2:40:59,  1.75s/it] 69%|██████▉   | 12326/17834 [6:15:22<2:41:16,  1.76s/it] 69%|██████▉   | 12327/17834 [6:15:23<2:40:14,  1.75s/it] 69%|██████▉   | 12328/17834 [6:15:25<2:42:59,  1.78s/it] 69%|██████▉   | 12329/17834 [6:15:27<2:42:24,  1.77s/it] 69%|██████▉   | 12330/17834 [6:15:29<2:41:14,  1.76s/it] 69%|██████▉   | 12331/17834 [6:15:30<2:40:15,  1.75s/it] 69%|██████▉   | 12332/17834 [6:15:32<2:37:56,  1.72s/it] 69%|██████▉   | 12333/17834 [6:15:34<2:40:26,  1.75s/it] 69%|██████▉   | 12334/17834 [6:15:36<2:39:30,  1.74s/it] 69%|██████▉   | 12335/17834 [6:15:37<2:39:04,  1.74s/it] 69%|██████▉   | 12336/17834 [6:15:39<2:38:40,  1.73s/it] 69%|██████▉   | 12337/17834 [6:15:41<2:39:55,  1.75s/it] 69%|██████▉   | 12338/17834 [6:15:43<2:44:28,  1.80s/it] 69%|██████▉   | 12339/17834 [6:15:44<2:41:55,  1.77s/it] 69%|██████▉   | 12340/17834 [6:15:46<2:39:31,  1.74s/it] 69%|██████▉   | 12341/17834 [6:15:48<2:40:31,  1.75s/it] 69%|██████▉   | 12342/17834 [6:15:50<2:40:06,  1.75s/it] 69%|██████▉   | 12343/17834 [6:15:51<2:41:41,  1.77s/it] 69%|██████▉   | 12344/17834 [6:15:53<2:39:14,  1.74s/it] 69%|██████▉   | 12345/17834 [6:15:55<2:41:43,  1.77s/it] 69%|██████▉   | 12346/17834 [6:15:57<2:43:14,  1.78s/it] 69%|██████▉   | 12347/17834 [6:15:59<2:41:49,  1.77s/it] 69%|██████▉   | 12348/17834 [6:16:00<2:41:05,  1.76s/it] 69%|██████▉   | 12349/17834 [6:16:02<2:40:05,  1.75s/it]08/31/2024 01:30:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2239511013031006, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.042948149144649506, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.216524600982666, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4834237098693848}
 69%|██████▉   | 12350/17834 [6:16:04<2:40:24,  1.75s/it] 69%|██████▉   | 12351/17834 [6:16:05<2:39:22,  1.74s/it] 69%|██████▉   | 12352/17834 [6:16:07<2:39:41,  1.75s/it] 69%|██████▉   | 12353/17834 [6:16:09<2:39:08,  1.74s/it] 69%|██████▉   | 12354/17834 [6:16:11<2:37:00,  1.72s/it] 69%|██████▉   | 12355/17834 [6:16:12<2:39:30,  1.75s/it] 69%|██████▉   | 12356/17834 [6:16:14<2:39:27,  1.75s/it] 69%|██████▉   | 12357/17834 [6:16:16<2:38:04,  1.73s/it] 69%|██████▉   | 12358/17834 [6:16:18<2:37:30,  1.73s/it] 69%|██████▉   | 12359/17834 [6:16:19<2:36:49,  1.72s/it] 69%|██████▉   | 12360/17834 [6:16:21<2:37:40,  1.73s/it] 69%|██████▉   | 12361/17834 [6:16:23<2:40:22,  1.76s/it] 69%|██████▉   | 12362/17834 [6:16:25<2:40:12,  1.76s/it] 69%|██████▉   | 12363/17834 [6:16:26<2:39:57,  1.75s/it] 69%|██████▉   | 12364/17834 [6:16:28<2:38:16,  1.74s/it] 69%|██████▉   | 12365/17834 [6:16:30<2:37:16,  1.73s/it] 69%|██████▉   | 12366/17834 [6:16:32<2:38:15,  1.74s/it] 69%|██████▉   | 12367/17834 [6:16:33<2:40:35,  1.76s/it] 69%|██████▉   | 12368/17834 [6:16:35<2:41:01,  1.77s/it] 69%|██████▉   | 12369/17834 [6:16:37<2:40:01,  1.76s/it] 69%|██████▉   | 12370/17834 [6:16:39<2:40:15,  1.76s/it] 69%|██████▉   | 12371/17834 [6:16:40<2:39:46,  1.75s/it] 69%|██████▉   | 12372/17834 [6:16:42<2:40:17,  1.76s/it] 69%|██████▉   | 12373/17834 [6:16:44<2:39:33,  1.75s/it] 69%|██████▉   | 12374/17834 [6:16:46<2:38:39,  1.74s/it] 69%|██████▉   | 12375/17834 [6:16:47<2:38:13,  1.74s/it] 69%|██████▉   | 12376/17834 [6:16:49<2:37:03,  1.73s/it] 69%|██████▉   | 12377/17834 [6:16:51<2:38:01,  1.74s/it] 69%|██████▉   | 12378/17834 [6:16:53<2:38:15,  1.74s/it] 69%|██████▉   | 12379/17834 [6:16:54<2:42:24,  1.79s/it] 69%|██████▉   | 12380/17834 [6:16:56<2:40:48,  1.77s/it] 69%|██████▉   | 12381/17834 [6:16:58<2:40:37,  1.77s/it] 69%|██████▉   | 12382/17834 [6:17:00<2:38:13,  1.74s/it] 69%|██████▉   | 12383/17834 [6:17:01<2:40:00,  1.76s/it] 69%|██████▉   | 12384/17834 [6:17:03<2:40:07,  1.76s/it] 69%|██████▉   | 12385/17834 [6:17:05<2:38:34,  1.75s/it] 69%|██████▉   | 12386/17834 [6:17:07<2:39:48,  1.76s/it] 69%|██████▉   | 12387/17834 [6:17:08<2:39:18,  1.75s/it] 69%|██████▉   | 12388/17834 [6:17:10<2:38:07,  1.74s/it] 69%|██████▉   | 12389/17834 [6:17:12<2:36:51,  1.73s/it] 69%|██████▉   | 12390/17834 [6:17:14<2:39:52,  1.76s/it] 69%|██████▉   | 12391/17834 [6:17:15<2:38:07,  1.74s/it] 69%|██████▉   | 12392/17834 [6:17:17<2:36:45,  1.73s/it] 69%|██████▉   | 12393/17834 [6:17:19<2:36:09,  1.72s/it] 69%|██████▉   | 12394/17834 [6:17:20<2:36:29,  1.73s/it] 70%|██████▉   | 12395/17834 [6:17:22<2:35:37,  1.72s/it] 70%|██████▉   | 12396/17834 [6:17:24<2:36:23,  1.73s/it] 70%|██████▉   | 12397/17834 [6:17:26<2:35:55,  1.72s/it] 70%|██████▉   | 12398/17834 [6:17:27<2:36:47,  1.73s/it] 70%|██████▉   | 12399/17834 [6:17:29<2:36:42,  1.73s/it]08/31/2024 01:31:48 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1447627544403076, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0357617512345314, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1921584606170654, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.372683048248291}
 70%|██████▉   | 12400/17834 [6:17:31<2:39:05,  1.76s/it] 70%|██████▉   | 12401/17834 [6:17:33<2:38:44,  1.75s/it] 70%|██████▉   | 12402/17834 [6:17:34<2:40:10,  1.77s/it] 70%|██████▉   | 12403/17834 [6:17:36<2:40:33,  1.77s/it] 70%|██████▉   | 12404/17834 [6:17:38<2:37:13,  1.74s/it] 70%|██████▉   | 12405/17834 [6:17:40<2:37:21,  1.74s/it] 70%|██████▉   | 12406/17834 [6:17:42<2:40:00,  1.77s/it] 70%|██████▉   | 12407/17834 [6:17:43<2:38:31,  1.75s/it] 70%|██████▉   | 12408/17834 [6:17:45<2:37:19,  1.74s/it] 70%|██████▉   | 12409/17834 [6:17:47<2:38:10,  1.75s/it] 70%|██████▉   | 12410/17834 [6:17:48<2:37:59,  1.75s/it] 70%|██████▉   | 12411/17834 [6:17:50<2:37:20,  1.74s/it] 70%|██████▉   | 12412/17834 [6:17:52<2:39:28,  1.76s/it] 70%|██████▉   | 12413/17834 [6:17:54<2:38:47,  1.76s/it] 70%|██████▉   | 12414/17834 [6:17:55<2:36:56,  1.74s/it] 70%|██████▉   | 12415/17834 [6:17:57<2:36:12,  1.73s/it] 70%|██████▉   | 12416/17834 [6:17:59<2:36:17,  1.73s/it] 70%|██████▉   | 12417/17834 [6:18:01<2:38:08,  1.75s/it] 70%|██████▉   | 12418/17834 [6:18:02<2:37:33,  1.75s/it] 70%|██████▉   | 12419/17834 [6:18:04<2:37:56,  1.75s/it] 70%|██████▉   | 12420/17834 [6:18:06<2:37:04,  1.74s/it] 70%|██████▉   | 12421/17834 [6:18:08<2:37:28,  1.75s/it] 70%|██████▉   | 12422/17834 [6:18:09<2:38:32,  1.76s/it] 70%|██████▉   | 12423/17834 [6:18:11<2:38:35,  1.76s/it] 70%|██████▉   | 12424/17834 [6:18:13<2:37:54,  1.75s/it] 70%|██████▉   | 12425/17834 [6:18:15<2:38:07,  1.75s/it] 70%|██████▉   | 12426/17834 [6:18:16<2:38:33,  1.76s/it] 70%|██████▉   | 12427/17834 [6:18:18<2:38:36,  1.76s/it] 70%|██████▉   | 12428/17834 [6:18:20<2:38:07,  1.76s/it] 70%|██████▉   | 12429/17834 [6:18:22<2:37:12,  1.75s/it] 70%|██████▉   | 12430/17834 [6:18:23<2:36:46,  1.74s/it] 70%|██████▉   | 12431/17834 [6:18:25<2:39:11,  1.77s/it] 70%|██████▉   | 12432/17834 [6:18:27<2:38:47,  1.76s/it] 70%|██████▉   | 12433/17834 [6:18:29<2:37:52,  1.75s/it] 70%|██████▉   | 12434/17834 [6:18:30<2:36:53,  1.74s/it] 70%|██████▉   | 12435/17834 [6:18:32<2:36:05,  1.73s/it] 70%|██████▉   | 12436/17834 [6:18:34<2:35:23,  1.73s/it] 70%|██████▉   | 12437/17834 [6:18:36<2:35:48,  1.73s/it] 70%|██████▉   | 12438/17834 [6:18:37<2:38:03,  1.76s/it] 70%|██████▉   | 12439/17834 [6:18:39<2:37:21,  1.75s/it] 70%|██████▉   | 12440/17834 [6:18:41<2:36:11,  1.74s/it] 70%|██████▉   | 12441/17834 [6:18:43<2:35:13,  1.73s/it] 70%|██████▉   | 12442/17834 [6:18:44<2:36:51,  1.75s/it] 70%|██████▉   | 12443/17834 [6:18:46<2:37:30,  1.75s/it] 70%|██████▉   | 12444/17834 [6:18:48<2:37:07,  1.75s/it] 70%|██████▉   | 12445/17834 [6:18:50<2:39:58,  1.78s/it] 70%|██████▉   | 12446/17834 [6:18:51<2:38:03,  1.76s/it] 70%|██████▉   | 12447/17834 [6:18:53<2:36:10,  1.74s/it] 70%|██████▉   | 12448/17834 [6:18:55<2:38:24,  1.76s/it] 70%|██████▉   | 12449/17834 [6:18:57<2:37:43,  1.76s/it]08/31/2024 01:33:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.032634973526001, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03288549184799194, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.230719566345215, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2962400913238525}
 70%|██████▉   | 12450/17834 [6:18:58<2:36:07,  1.74s/it] 70%|██████▉   | 12451/17834 [6:19:00<2:35:49,  1.74s/it] 70%|██████▉   | 12452/17834 [6:19:02<2:38:50,  1.77s/it] 70%|██████▉   | 12453/17834 [6:19:04<2:37:44,  1.76s/it] 70%|██████▉   | 12454/17834 [6:19:05<2:37:22,  1.76s/it] 70%|██████▉   | 12455/17834 [6:19:07<2:37:33,  1.76s/it] 70%|██████▉   | 12456/17834 [6:19:09<2:38:44,  1.77s/it] 70%|██████▉   | 12457/17834 [6:19:11<2:37:57,  1.76s/it] 70%|██████▉   | 12458/17834 [6:19:13<2:38:18,  1.77s/it] 70%|██████▉   | 12459/17834 [6:19:14<2:36:20,  1.75s/it] 70%|██████▉   | 12460/17834 [6:19:16<2:36:58,  1.75s/it] 70%|██████▉   | 12461/17834 [6:19:18<2:35:57,  1.74s/it] 70%|██████▉   | 12462/17834 [6:19:20<2:37:19,  1.76s/it] 70%|██████▉   | 12463/17834 [6:19:21<2:37:18,  1.76s/it] 70%|██████▉   | 12464/17834 [6:19:23<2:34:41,  1.73s/it] 70%|██████▉   | 12465/17834 [6:19:25<2:34:34,  1.73s/it] 70%|██████▉   | 12466/17834 [6:19:26<2:36:48,  1.75s/it] 70%|██████▉   | 12467/17834 [6:19:28<2:36:55,  1.75s/it] 70%|██████▉   | 12468/17834 [6:19:30<2:37:26,  1.76s/it] 70%|██████▉   | 12469/17834 [6:19:32<2:38:51,  1.78s/it] 70%|██████▉   | 12470/17834 [6:19:34<2:38:03,  1.77s/it] 70%|██████▉   | 12471/17834 [6:19:35<2:36:30,  1.75s/it] 70%|██████▉   | 12472/17834 [6:19:37<2:35:52,  1.74s/it] 70%|██████▉   | 12473/17834 [6:19:39<2:35:10,  1.74s/it]08/31/2024 01:33:58 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/31/2024 01:33:58 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:51,  4.26it/s][A
  1%|▏         | 3/221 [00:00<00:55,  3.95it/s][A
  2%|▏         | 4/221 [00:00<00:45,  4.75it/s][A
  2%|▏         | 5/221 [00:01<00:54,  3.98it/s][A
  3%|▎         | 6/221 [00:01<00:46,  4.67it/s][A
  3%|▎         | 7/221 [00:01<00:51,  4.18it/s][A
  4%|▎         | 8/221 [00:01<00:53,  4.00it/s][A
  4%|▍         | 9/221 [00:02<01:07,  3.14it/s][A
  5%|▍         | 10/221 [00:02<01:10,  2.97it/s][A
  5%|▍         | 11/221 [00:02<01:01,  3.40it/s][A
  5%|▌         | 12/221 [00:03<00:54,  3.83it/s][A
  6%|▌         | 13/221 [00:03<00:49,  4.23it/s][A
  6%|▋         | 14/221 [00:03<00:48,  4.25it/s][A
  7%|▋         | 15/221 [00:03<00:45,  4.55it/s][A
  7%|▋         | 16/221 [00:04<00:47,  4.30it/s][A
  8%|▊         | 17/221 [00:04<00:47,  4.31it/s][A
  8%|▊         | 18/221 [00:04<00:42,  4.73it/s][A
  9%|▊         | 19/221 [00:04<00:44,  4.49it/s][A
  9%|▉         | 20/221 [00:04<00:41,  4.83it/s][A
 10%|▉         | 21/221 [00:05<00:40,  4.92it/s][A
 10%|▉         | 22/221 [00:05<00:43,  4.52it/s][A
 10%|█         | 23/221 [00:05<00:44,  4.47it/s][A
 11%|█         | 24/221 [00:05<00:38,  5.11it/s][A
 11%|█▏        | 25/221 [00:05<00:41,  4.78it/s][A
 12%|█▏        | 26/221 [00:06<00:39,  4.92it/s][A
 12%|█▏        | 27/221 [00:06<00:54,  3.57it/s][A
 13%|█▎        | 28/221 [00:06<01:02,  3.09it/s][A
 13%|█▎        | 29/221 [00:07<01:06,  2.89it/s][A
 14%|█▎        | 30/221 [00:07<01:11,  2.65it/s][A
 14%|█▍        | 31/221 [00:08<01:11,  2.65it/s][A
 14%|█▍        | 32/221 [00:08<01:10,  2.68it/s][A
 15%|█▍        | 33/221 [00:08<00:59,  3.15it/s][A
 15%|█▌        | 34/221 [00:08<00:55,  3.39it/s][A
 16%|█▌        | 35/221 [00:09<01:01,  3.01it/s][A
 16%|█▋        | 36/221 [00:09<01:01,  3.01it/s][A
 17%|█▋        | 37/221 [00:09<00:53,  3.42it/s][A
 17%|█▋        | 38/221 [00:10<00:52,  3.49it/s][A
 18%|█▊        | 39/221 [00:10<00:44,  4.11it/s][A
 18%|█▊        | 40/221 [00:10<00:49,  3.62it/s][A
 19%|█▊        | 41/221 [00:11<00:55,  3.24it/s][A
 19%|█▉        | 43/221 [00:11<00:43,  4.14it/s][A
 20%|█▉        | 44/221 [00:11<00:48,  3.65it/s][A
 20%|██        | 45/221 [00:12<00:52,  3.32it/s][A
 21%|██        | 46/221 [00:12<00:56,  3.11it/s][A
 21%|██▏       | 47/221 [00:12<00:52,  3.32it/s][A
 22%|██▏       | 48/221 [00:12<00:46,  3.69it/s][A
 22%|██▏       | 49/221 [00:13<00:45,  3.81it/s][A
 23%|██▎       | 50/221 [00:13<00:48,  3.50it/s][A
 23%|██▎       | 51/221 [00:13<00:46,  3.64it/s][A
 24%|██▎       | 52/221 [00:13<00:42,  3.98it/s][A
 24%|██▍       | 53/221 [00:14<00:39,  4.24it/s][A
 24%|██▍       | 54/221 [00:14<00:40,  4.10it/s][A
 25%|██▍       | 55/221 [00:14<00:44,  3.76it/s][A
 25%|██▌       | 56/221 [00:15<00:44,  3.69it/s][A
 26%|██▌       | 57/221 [00:15<00:51,  3.15it/s][A
 26%|██▌       | 58/221 [00:15<00:44,  3.66it/s][A
 27%|██▋       | 59/221 [00:15<00:37,  4.27it/s][A
 27%|██▋       | 60/221 [00:16<00:39,  4.12it/s][A
 28%|██▊       | 61/221 [00:16<00:45,  3.52it/s][A
 28%|██▊       | 62/221 [00:16<00:40,  3.89it/s][A
 29%|██▊       | 63/221 [00:16<00:42,  3.76it/s][A
 29%|██▉       | 64/221 [00:17<00:53,  2.95it/s][A
 29%|██▉       | 65/221 [00:17<00:55,  2.84it/s][A
 30%|██▉       | 66/221 [00:18<00:52,  2.93it/s][A
 30%|███       | 67/221 [00:18<00:55,  2.79it/s][A
 31%|███       | 68/221 [00:18<00:47,  3.21it/s][A
 31%|███       | 69/221 [00:19<00:54,  2.80it/s][A
 32%|███▏      | 70/221 [00:19<00:53,  2.82it/s][A
 32%|███▏      | 71/221 [00:19<00:47,  3.19it/s][A
 33%|███▎      | 72/221 [00:20<00:52,  2.85it/s][A
 33%|███▎      | 73/221 [00:20<00:50,  2.91it/s][A
 34%|███▍      | 75/221 [00:20<00:38,  3.77it/s][A
 34%|███▍      | 76/221 [00:21<00:49,  2.95it/s][A
 35%|███▍      | 77/221 [00:22<00:58,  2.46it/s][A
 35%|███▌      | 78/221 [00:22<00:53,  2.69it/s][A
 36%|███▌      | 79/221 [00:22<00:58,  2.41it/s][A
 36%|███▌      | 80/221 [00:23<00:55,  2.53it/s][A
 37%|███▋      | 81/221 [00:23<00:45,  3.05it/s][A
 37%|███▋      | 82/221 [00:23<00:42,  3.25it/s][A
 38%|███▊      | 83/221 [00:23<00:44,  3.07it/s][A
 38%|███▊      | 84/221 [00:24<00:40,  3.37it/s][A
 38%|███▊      | 85/221 [00:24<00:39,  3.40it/s][A
 39%|███▉      | 86/221 [00:24<00:47,  2.86it/s][A
 39%|███▉      | 87/221 [00:25<00:49,  2.73it/s][A
 40%|███▉      | 88/221 [00:25<00:52,  2.53it/s][A
 40%|████      | 89/221 [00:26<00:49,  2.68it/s][A
 41%|████      | 90/221 [00:26<00:43,  3.01it/s][A
 41%|████      | 91/221 [00:26<00:37,  3.46it/s][A
 42%|████▏     | 92/221 [00:26<00:38,  3.33it/s][A
 42%|████▏     | 93/221 [00:27<00:46,  2.74it/s][A
 43%|████▎     | 94/221 [00:27<00:42,  3.01it/s][A
 43%|████▎     | 95/221 [00:27<00:36,  3.50it/s][A
 43%|████▎     | 96/221 [00:28<00:37,  3.34it/s][A
 44%|████▍     | 97/221 [00:28<00:36,  3.43it/s][A
 44%|████▍     | 98/221 [00:29<00:46,  2.65it/s][A
 45%|████▍     | 99/221 [00:29<00:44,  2.77it/s][A
 45%|████▌     | 100/221 [00:29<00:43,  2.76it/s][A
 46%|████▌     | 101/221 [00:29<00:38,  3.16it/s][A
 46%|████▌     | 102/221 [00:30<00:34,  3.43it/s][A
 47%|████▋     | 103/221 [00:30<00:35,  3.35it/s][A
 47%|████▋     | 104/221 [00:30<00:41,  2.82it/s][A
 48%|████▊     | 105/221 [00:31<00:43,  2.67it/s][A
 48%|████▊     | 106/221 [00:31<00:39,  2.91it/s][A
 48%|████▊     | 107/221 [00:31<00:35,  3.23it/s][A
 49%|████▉     | 108/221 [00:32<00:35,  3.17it/s][A
 49%|████▉     | 109/221 [00:32<00:31,  3.53it/s][A
 50%|████▉     | 110/221 [00:32<00:31,  3.52it/s][A
 50%|█████     | 111/221 [00:33<00:31,  3.51it/s][A
 51%|█████     | 112/221 [00:33<00:30,  3.61it/s][A
 51%|█████     | 113/221 [00:33<00:28,  3.76it/s][A
 52%|█████▏    | 114/221 [00:33<00:25,  4.17it/s][A
 52%|█████▏    | 115/221 [00:34<00:28,  3.74it/s][A
 52%|█████▏    | 116/221 [00:34<00:27,  3.76it/s][A
 53%|█████▎    | 117/221 [00:34<00:25,  4.09it/s][A
 53%|█████▎    | 118/221 [00:34<00:24,  4.12it/s][A
 54%|█████▍    | 119/221 [00:34<00:25,  4.04it/s][A
 54%|█████▍    | 120/221 [00:35<00:31,  3.21it/s][A
 55%|█████▍    | 121/221 [00:35<00:29,  3.34it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:36<00:28,  3.45it/s][A
 56%|█████▌    | 124/221 [00:36<00:28,  3.39it/s][A
 57%|█████▋    | 125/221 [00:36<00:30,  3.17it/s][A
 57%|█████▋    | 126/221 [00:37<00:24,  3.81it/s][A
 57%|█████▋    | 127/221 [00:37<00:26,  3.60it/s][A
 58%|█████▊    | 128/221 [00:37<00:26,  3.57it/s][A
 58%|█████▊    | 129/221 [00:37<00:21,  4.20it/s][A
 59%|█████▉    | 130/221 [00:38<00:22,  4.03it/s][A
 59%|█████▉    | 131/221 [00:38<00:25,  3.54it/s][A
 60%|█████▉    | 132/221 [00:38<00:26,  3.30it/s][A
 60%|██████    | 133/221 [00:39<00:30,  2.92it/s][A
 61%|██████    | 134/221 [00:39<00:30,  2.82it/s][A
 61%|██████    | 135/221 [00:39<00:30,  2.86it/s][A
 62%|██████▏   | 136/221 [00:40<00:26,  3.18it/s][A
 62%|██████▏   | 137/221 [00:40<00:22,  3.77it/s][A
 62%|██████▏   | 138/221 [00:40<00:21,  3.82it/s][A
 63%|██████▎   | 139/221 [00:40<00:19,  4.12it/s][A
 63%|██████▎   | 140/221 [00:41<00:20,  4.04it/s][A
 64%|██████▍   | 141/221 [00:41<00:21,  3.68it/s][A
 64%|██████▍   | 142/221 [00:41<00:20,  3.82it/s][A
 65%|██████▍   | 143/221 [00:42<00:29,  2.63it/s][A
 65%|██████▌   | 144/221 [00:42<00:31,  2.44it/s][A
 66%|██████▌   | 145/221 [00:43<00:29,  2.60it/s][A
 67%|██████▋   | 147/221 [00:43<00:22,  3.23it/s][A
 67%|██████▋   | 148/221 [00:43<00:22,  3.30it/s][A
 67%|██████▋   | 149/221 [00:44<00:24,  2.91it/s][A
 68%|██████▊   | 150/221 [00:44<00:21,  3.32it/s][A
 68%|██████▊   | 151/221 [00:44<00:21,  3.26it/s][A
 69%|██████▉   | 152/221 [00:45<00:20,  3.40it/s][A
 69%|██████▉   | 153/221 [00:45<00:16,  4.12it/s][A
 70%|██████▉   | 154/221 [00:45<00:20,  3.34it/s][A
 70%|███████   | 155/221 [00:45<00:19,  3.43it/s][A
 71%|███████   | 156/221 [00:46<00:19,  3.38it/s][A
 71%|███████   | 157/221 [00:46<00:19,  3.23it/s][A
 71%|███████▏  | 158/221 [00:47<00:24,  2.59it/s][A
 72%|███████▏  | 159/221 [00:47<00:22,  2.81it/s][A
 72%|███████▏  | 160/221 [00:47<00:22,  2.75it/s][A
 73%|███████▎  | 161/221 [00:48<00:25,  2.34it/s][A
 73%|███████▎  | 162/221 [00:48<00:22,  2.58it/s][A
 74%|███████▍  | 163/221 [00:48<00:21,  2.69it/s][A
 74%|███████▍  | 164/221 [00:49<00:16,  3.37it/s][A
 75%|███████▍  | 165/221 [00:49<00:15,  3.72it/s][A
 75%|███████▌  | 166/221 [00:49<00:12,  4.29it/s][A
 76%|███████▌  | 167/221 [00:49<00:16,  3.27it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.49it/s][A
 76%|███████▋  | 169/221 [00:50<00:14,  3.51it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.63it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:51<00:15,  3.11it/s][A
 78%|███████▊  | 173/221 [00:51<00:17,  2.69it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.20it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.26it/s][A
 80%|███████▉  | 176/221 [00:52<00:12,  3.71it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.33it/s][A
 81%|████████  | 178/221 [00:52<00:09,  4.49it/s][A
 81%|████████  | 179/221 [00:53<00:09,  4.34it/s][A
 81%|████████▏ | 180/221 [00:53<00:08,  4.85it/s][A
 82%|████████▏ | 181/221 [00:53<00:08,  4.66it/s][A
 82%|████████▏ | 182/221 [00:53<00:09,  4.26it/s][A
 83%|████████▎ | 183/221 [00:54<00:10,  3.62it/s][A
 83%|████████▎ | 184/221 [00:54<00:12,  3.02it/s][A
 84%|████████▎ | 185/221 [00:54<00:11,  3.26it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.37it/s][A
 85%|████████▍ | 187/221 [00:55<00:10,  3.24it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.42it/s][A
 86%|████████▌ | 189/221 [00:55<00:08,  3.76it/s][A
 86%|████████▌ | 190/221 [00:56<00:07,  3.88it/s][A
 86%|████████▋ | 191/221 [00:56<00:09,  3.15it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.56it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.56it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.79it/s][A
 88%|████████▊ | 195/221 [00:57<00:07,  3.61it/s][A
 89%|████████▊ | 196/221 [00:58<00:08,  2.96it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [00:58<00:07,  3.21it/s][A
 90%|█████████ | 199/221 [00:58<00:05,  3.85it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.75it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.93it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.79it/s][A
 92%|█████████▏| 203/221 [00:59<00:04,  3.89it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.15it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.40it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.98it/s][A
 94%|█████████▎| 207/221 [01:00<00:02,  4.70it/s][A
 94%|█████████▍| 208/221 [01:00<00:02,  5.16it/s][A
 95%|█████████▍| 209/221 [01:00<00:02,  5.19it/s][A
 95%|█████████▌| 210/221 [01:01<00:02,  5.44it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  3.89it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.25it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.43it/s][A
 97%|█████████▋| 214/221 [01:02<00:02,  3.29it/s][A
 97%|█████████▋| 215/221 [01:02<00:01,  4.09it/s][A
 98%|█████████▊| 216/221 [01:02<00:01,  3.95it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.63it/s][A
 99%|█████████▊| 218/221 [01:03<00:00,  3.15it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  2.92it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.18it/s][A
100%|██████████| 221/221 [01:04<00:00,  3.37it/s][A100%|██████████| 221/221 [01:04<00:00,  3.42it/s]
08/31/2024 01:36:22 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 12473--===========

08/31/2024 01:36:22 - INFO - __main__ -   {'area_r1': 5.5, 'area_recall': '5.5/12.9/19.8', 'area_ravg': 12.7}
08/31/2024 01:36:22 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 12473--===========

08/31/2024 01:36:22 - INFO - __main__ -   {'area_r1': 38.8, 'area_recall': '38.8/71.7/83.5', 'area_ravg': 64.7}
08/31/2024 01:36:22 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 12473--===========

08/31/2024 01:36:22 - INFO - __main__ -   {'video_r1': 34.3, 'video_recall': '34.3/66.9/76.8', 'video_ravg': 59.3}
08/31/2024 01:36:22 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/31/2024 01:36:22 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/31/2024 01:36:22 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 12473--===========

08/31/2024 01:36:22 - INFO - __main__ -   {'video_r1': 56.9, 'video_recall': '56.9/79.5/85.2', 'video_ravg': 73.9}
08/31/2024 01:36:22 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/31/2024 01:36:22 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
 70%|██████▉   | 12474/17834 [6:22:25<75:55:48, 51.00s/it] 70%|██████▉   | 12475/17834 [6:22:26<53:55:19, 36.22s/it] 70%|██████▉   | 12476/17834 [6:22:28<38:29:54, 25.87s/it] 70%|██████▉   | 12477/17834 [6:22:30<27:45:03, 18.65s/it] 70%|██████▉   | 12478/17834 [6:22:32<20:11:28, 13.57s/it] 70%|██████▉   | 12479/17834 [6:22:33<14:53:13, 10.01s/it] 70%|██████▉   | 12480/17834 [6:22:35<11:12:24,  7.54s/it] 70%|██████▉   | 12481/17834 [6:22:37<8:39:54,  5.83s/it]  70%|██████▉   | 12482/17834 [6:22:39<6:52:36,  4.63s/it] 70%|██████▉   | 12483/17834 [6:22:41<5:36:00,  3.77s/it] 70%|███████   | 12484/17834 [6:22:42<4:43:56,  3.18s/it] 70%|███████   | 12485/17834 [6:22:44<4:04:38,  2.74s/it] 70%|███████   | 12486/17834 [6:22:46<3:38:54,  2.46s/it] 70%|███████   | 12487/17834 [6:22:48<3:21:56,  2.27s/it] 70%|███████   | 12488/17834 [6:22:49<3:07:11,  2.10s/it] 70%|███████   | 12489/17834 [6:22:51<2:58:17,  2.00s/it] 70%|███████   | 12490/17834 [6:22:53<2:52:53,  1.94s/it] 70%|███████   | 12491/17834 [6:22:55<2:49:27,  1.90s/it] 70%|███████   | 12492/17834 [6:22:57<2:45:06,  1.85s/it] 70%|███████   | 12493/17834 [6:22:58<2:41:33,  1.81s/it] 70%|███████   | 12494/17834 [6:23:00<2:40:37,  1.80s/it] 70%|███████   | 12495/17834 [6:23:02<2:40:21,  1.80s/it] 70%|███████   | 12496/17834 [6:23:04<2:38:59,  1.79s/it] 70%|███████   | 12497/17834 [6:23:05<2:37:11,  1.77s/it] 70%|███████   | 12498/17834 [6:23:07<2:37:22,  1.77s/it] 70%|███████   | 12499/17834 [6:23:09<2:38:15,  1.78s/it]08/31/2024 01:37:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3245551586151123, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04701944440603256, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.217663049697876, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.589237689971924}
 70%|███████   | 12500/17834 [6:23:11<2:35:47,  1.75s/it] 70%|███████   | 12501/17834 [6:23:12<2:35:51,  1.75s/it] 70%|███████   | 12502/17834 [6:23:14<2:36:35,  1.76s/it] 70%|███████   | 12503/17834 [6:23:16<2:34:41,  1.74s/it] 70%|███████   | 12504/17834 [6:23:18<2:35:17,  1.75s/it] 70%|███████   | 12505/17834 [6:23:19<2:34:18,  1.74s/it] 70%|███████   | 12506/17834 [6:23:21<2:34:35,  1.74s/it] 70%|███████   | 12507/17834 [6:23:23<2:34:42,  1.74s/it] 70%|███████   | 12508/17834 [6:23:25<2:34:43,  1.74s/it] 70%|███████   | 12509/17834 [6:23:26<2:34:13,  1.74s/it] 70%|███████   | 12510/17834 [6:23:28<2:34:04,  1.74s/it] 70%|███████   | 12511/17834 [6:23:30<2:34:45,  1.74s/it] 70%|███████   | 12512/17834 [6:23:31<2:34:46,  1.74s/it] 70%|███████   | 12513/17834 [6:23:33<2:33:36,  1.73s/it] 70%|███████   | 12514/17834 [6:23:35<2:35:16,  1.75s/it] 70%|███████   | 12515/17834 [6:23:37<2:35:33,  1.75s/it] 70%|███████   | 12516/17834 [6:23:39<2:36:23,  1.76s/it] 70%|███████   | 12517/17834 [6:23:40<2:38:10,  1.78s/it] 70%|███████   | 12518/17834 [6:23:42<2:37:49,  1.78s/it] 70%|███████   | 12519/17834 [6:23:44<2:35:07,  1.75s/it] 70%|███████   | 12520/17834 [6:23:46<2:35:57,  1.76s/it] 70%|███████   | 12521/17834 [6:23:47<2:36:17,  1.77s/it] 70%|███████   | 12522/17834 [6:23:49<2:37:24,  1.78s/it] 70%|███████   | 12523/17834 [6:23:51<2:35:11,  1.75s/it] 70%|███████   | 12524/17834 [6:23:53<2:36:13,  1.77s/it] 70%|███████   | 12525/17834 [6:23:54<2:37:05,  1.78s/it] 70%|███████   | 12526/17834 [6:23:56<2:36:51,  1.77s/it] 70%|███████   | 12527/17834 [6:23:58<2:36:48,  1.77s/it] 70%|███████   | 12528/17834 [6:24:00<2:39:16,  1.80s/it] 70%|███████   | 12529/17834 [6:24:02<2:37:14,  1.78s/it] 70%|███████   | 12530/17834 [6:24:03<2:34:31,  1.75s/it] 70%|███████   | 12531/17834 [6:24:05<2:34:18,  1.75s/it] 70%|███████   | 12532/17834 [6:24:07<2:35:20,  1.76s/it] 70%|███████   | 12533/17834 [6:24:08<2:33:09,  1.73s/it] 70%|███████   | 12534/17834 [6:24:10<2:34:04,  1.74s/it] 70%|███████   | 12535/17834 [6:24:12<2:33:04,  1.73s/it] 70%|███████   | 12536/17834 [6:24:14<2:32:37,  1.73s/it] 70%|███████   | 12537/17834 [6:24:15<2:33:39,  1.74s/it] 70%|███████   | 12538/17834 [6:24:17<2:34:41,  1.75s/it] 70%|███████   | 12539/17834 [6:24:19<2:36:13,  1.77s/it] 70%|███████   | 12540/17834 [6:24:21<2:36:04,  1.77s/it] 70%|███████   | 12541/17834 [6:24:23<2:35:42,  1.77s/it] 70%|███████   | 12542/17834 [6:24:24<2:34:53,  1.76s/it] 70%|███████   | 12543/17834 [6:24:26<2:34:42,  1.75s/it] 70%|███████   | 12544/17834 [6:24:28<2:32:33,  1.73s/it] 70%|███████   | 12545/17834 [6:24:29<2:33:09,  1.74s/it] 70%|███████   | 12546/17834 [6:24:31<2:32:37,  1.73s/it] 70%|███████   | 12547/17834 [6:24:33<2:34:55,  1.76s/it] 70%|███████   | 12548/17834 [6:24:35<2:32:19,  1.73s/it] 70%|███████   | 12549/17834 [6:24:36<2:33:14,  1.74s/it]08/31/2024 01:38:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9130553007125854, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020142577588558197, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1505367755889893, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0837345123291016}
 70%|███████   | 12550/17834 [6:24:38<2:35:05,  1.76s/it] 70%|███████   | 12551/17834 [6:24:40<2:34:42,  1.76s/it] 70%|███████   | 12552/17834 [6:24:42<2:35:04,  1.76s/it] 70%|███████   | 12553/17834 [6:24:44<2:37:47,  1.79s/it] 70%|███████   | 12554/17834 [6:24:45<2:33:52,  1.75s/it] 70%|███████   | 12555/17834 [6:24:47<2:35:41,  1.77s/it] 70%|███████   | 12556/17834 [6:24:49<2:35:04,  1.76s/it] 70%|███████   | 12557/17834 [6:24:51<2:36:39,  1.78s/it] 70%|███████   | 12558/17834 [6:24:52<2:36:44,  1.78s/it] 70%|███████   | 12559/17834 [6:24:54<2:34:32,  1.76s/it] 70%|███████   | 12560/17834 [6:24:56<2:33:38,  1.75s/it] 70%|███████   | 12561/17834 [6:24:58<2:33:19,  1.74s/it] 70%|███████   | 12562/17834 [6:24:59<2:32:47,  1.74s/it] 70%|███████   | 12563/17834 [6:25:01<2:31:32,  1.73s/it] 70%|███████   | 12564/17834 [6:25:03<2:33:28,  1.75s/it] 70%|███████   | 12565/17834 [6:25:05<2:34:37,  1.76s/it] 70%|███████   | 12566/17834 [6:25:06<2:33:53,  1.75s/it] 70%|███████   | 12567/17834 [6:25:08<2:33:33,  1.75s/it] 70%|███████   | 12568/17834 [6:25:10<2:32:53,  1.74s/it] 70%|███████   | 12569/17834 [6:25:12<2:32:52,  1.74s/it] 70%|███████   | 12570/17834 [6:25:13<2:32:29,  1.74s/it] 70%|███████   | 12571/17834 [6:25:15<2:32:04,  1.73s/it] 70%|███████   | 12572/17834 [6:25:17<2:32:36,  1.74s/it] 71%|███████   | 12573/17834 [6:25:19<2:33:53,  1.76s/it] 71%|███████   | 12574/17834 [6:25:20<2:34:24,  1.76s/it] 71%|███████   | 12575/17834 [6:25:22<2:33:57,  1.76s/it] 71%|███████   | 12576/17834 [6:25:24<2:32:35,  1.74s/it] 71%|███████   | 12577/17834 [6:25:26<2:32:53,  1.74s/it] 71%|███████   | 12578/17834 [6:25:27<2:34:10,  1.76s/it] 71%|███████   | 12579/17834 [6:25:29<2:34:13,  1.76s/it] 71%|███████   | 12580/17834 [6:25:31<2:35:31,  1.78s/it] 71%|███████   | 12581/17834 [6:25:33<2:33:10,  1.75s/it] 71%|███████   | 12582/17834 [6:25:34<2:33:13,  1.75s/it] 71%|███████   | 12583/17834 [6:25:36<2:32:21,  1.74s/it] 71%|███████   | 12584/17834 [6:25:38<2:33:12,  1.75s/it] 71%|███████   | 12585/17834 [6:25:40<2:33:56,  1.76s/it] 71%|███████   | 12586/17834 [6:25:41<2:32:35,  1.74s/it] 71%|███████   | 12587/17834 [6:25:43<2:30:45,  1.72s/it] 71%|███████   | 12588/17834 [6:25:45<2:32:58,  1.75s/it] 71%|███████   | 12589/17834 [6:25:47<2:31:12,  1.73s/it] 71%|███████   | 12590/17834 [6:25:48<2:33:28,  1.76s/it] 71%|███████   | 12591/17834 [6:25:50<2:35:23,  1.78s/it] 71%|███████   | 12592/17834 [6:25:52<2:33:47,  1.76s/it] 71%|███████   | 12593/17834 [6:25:54<2:35:08,  1.78s/it] 71%|███████   | 12594/17834 [6:25:55<2:33:42,  1.76s/it] 71%|███████   | 12595/17834 [6:25:57<2:32:57,  1.75s/it] 71%|███████   | 12596/17834 [6:25:59<2:32:43,  1.75s/it] 71%|███████   | 12597/17834 [6:26:01<2:31:28,  1.74s/it] 71%|███████   | 12598/17834 [6:26:02<2:31:32,  1.74s/it] 71%|███████   | 12599/17834 [6:26:04<2:33:03,  1.75s/it]08/31/2024 01:40:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9455134868621826, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.031180672347545624, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1272733211517334, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1039674282073975}
 71%|███████   | 12600/17834 [6:26:06<2:33:31,  1.76s/it] 71%|███████   | 12601/17834 [6:26:08<2:31:30,  1.74s/it] 71%|███████   | 12602/17834 [6:26:09<2:30:42,  1.73s/it] 71%|███████   | 12603/17834 [6:26:11<2:31:45,  1.74s/it] 71%|███████   | 12604/17834 [6:26:13<2:30:32,  1.73s/it] 71%|███████   | 12605/17834 [6:26:15<2:31:19,  1.74s/it] 71%|███████   | 12606/17834 [6:26:16<2:32:08,  1.75s/it] 71%|███████   | 12607/17834 [6:26:18<2:31:49,  1.74s/it] 71%|███████   | 12608/17834 [6:26:20<2:32:17,  1.75s/it] 71%|███████   | 12609/17834 [6:26:22<2:32:31,  1.75s/it] 71%|███████   | 12610/17834 [6:26:23<2:31:49,  1.74s/it] 71%|███████   | 12611/17834 [6:26:25<2:31:16,  1.74s/it] 71%|███████   | 12612/17834 [6:26:27<2:32:25,  1.75s/it] 71%|███████   | 12613/17834 [6:26:28<2:30:09,  1.73s/it] 71%|███████   | 12614/17834 [6:26:30<2:34:52,  1.78s/it] 71%|███████   | 12615/17834 [6:26:32<2:33:22,  1.76s/it] 71%|███████   | 12616/17834 [6:26:34<2:33:08,  1.76s/it] 71%|███████   | 12617/17834 [6:26:36<2:32:17,  1.75s/it] 71%|███████   | 12618/17834 [6:26:37<2:30:34,  1.73s/it] 71%|███████   | 12619/17834 [6:26:39<2:30:22,  1.73s/it] 71%|███████   | 12620/17834 [6:26:41<2:33:21,  1.76s/it] 71%|███████   | 12621/17834 [6:26:43<2:32:06,  1.75s/it] 71%|███████   | 12622/17834 [6:26:44<2:31:52,  1.75s/it] 71%|███████   | 12623/17834 [6:26:46<2:32:29,  1.76s/it] 71%|███████   | 12624/17834 [6:26:48<2:30:58,  1.74s/it] 71%|███████   | 12625/17834 [6:26:50<2:33:03,  1.76s/it] 71%|███████   | 12626/17834 [6:26:51<2:31:26,  1.74s/it] 71%|███████   | 12627/17834 [6:26:53<2:33:48,  1.77s/it] 71%|███████   | 12628/17834 [6:26:55<2:33:51,  1.77s/it] 71%|███████   | 12629/17834 [6:26:57<2:35:26,  1.79s/it] 71%|███████   | 12630/17834 [6:26:58<2:32:47,  1.76s/it] 71%|███████   | 12631/17834 [6:27:00<2:32:05,  1.75s/it] 71%|███████   | 12632/17834 [6:27:02<2:31:50,  1.75s/it] 71%|███████   | 12633/17834 [6:27:04<2:32:06,  1.75s/it] 71%|███████   | 12634/17834 [6:27:05<2:31:27,  1.75s/it] 71%|███████   | 12635/17834 [6:27:07<2:33:35,  1.77s/it] 71%|███████   | 12636/17834 [6:27:09<2:33:57,  1.78s/it] 71%|███████   | 12637/17834 [6:27:11<2:33:43,  1.77s/it] 71%|███████   | 12638/17834 [6:27:13<2:35:13,  1.79s/it] 71%|███████   | 12639/17834 [6:27:14<2:34:45,  1.79s/it] 71%|███████   | 12640/17834 [6:27:16<2:34:22,  1.78s/it] 71%|███████   | 12641/17834 [6:27:18<2:33:35,  1.77s/it] 71%|███████   | 12642/17834 [6:27:20<2:32:36,  1.76s/it] 71%|███████   | 12643/17834 [6:27:21<2:31:54,  1.76s/it] 71%|███████   | 12644/17834 [6:27:23<2:33:09,  1.77s/it] 71%|███████   | 12645/17834 [6:27:25<2:34:00,  1.78s/it] 71%|███████   | 12646/17834 [6:27:27<2:33:40,  1.78s/it] 71%|███████   | 12647/17834 [6:27:28<2:31:45,  1.76s/it] 71%|███████   | 12648/17834 [6:27:30<2:31:29,  1.75s/it] 71%|███████   | 12649/17834 [6:27:32<2:33:51,  1.78s/it]08/31/2024 01:41:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.785151481628418, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02125147171318531, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.115798234939575, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.922201156616211}
 71%|███████   | 12650/17834 [6:27:34<2:31:20,  1.75s/it] 71%|███████   | 12651/17834 [6:27:36<2:32:21,  1.76s/it] 71%|███████   | 12652/17834 [6:27:37<2:31:47,  1.76s/it] 71%|███████   | 12653/17834 [6:27:39<2:31:23,  1.75s/it] 71%|███████   | 12654/17834 [6:27:41<2:30:07,  1.74s/it] 71%|███████   | 12655/17834 [6:27:43<2:31:22,  1.75s/it] 71%|███████   | 12656/17834 [6:27:44<2:32:02,  1.76s/it] 71%|███████   | 12657/17834 [6:27:46<2:34:12,  1.79s/it] 71%|███████   | 12658/17834 [6:27:48<2:31:34,  1.76s/it] 71%|███████   | 12659/17834 [6:27:50<2:31:02,  1.75s/it] 71%|███████   | 12660/17834 [6:27:51<2:32:34,  1.77s/it] 71%|███████   | 12661/17834 [6:27:53<2:34:14,  1.79s/it] 71%|███████   | 12662/17834 [6:27:55<2:33:47,  1.78s/it] 71%|███████   | 12663/17834 [6:27:57<2:32:03,  1.76s/it] 71%|███████   | 12664/17834 [6:27:58<2:31:15,  1.76s/it] 71%|███████   | 12665/17834 [6:28:00<2:30:00,  1.74s/it] 71%|███████   | 12666/17834 [6:28:02<2:30:09,  1.74s/it] 71%|███████   | 12667/17834 [6:28:04<2:30:50,  1.75s/it] 71%|███████   | 12668/17834 [6:28:05<2:31:02,  1.75s/it] 71%|███████   | 12669/17834 [6:28:07<2:31:27,  1.76s/it] 71%|███████   | 12670/17834 [6:28:09<2:31:34,  1.76s/it] 71%|███████   | 12671/17834 [6:28:11<2:31:10,  1.76s/it] 71%|███████   | 12672/17834 [6:28:12<2:32:07,  1.77s/it] 71%|███████   | 12673/17834 [6:28:14<2:31:31,  1.76s/it] 71%|███████   | 12674/17834 [6:28:16<2:30:21,  1.75s/it] 71%|███████   | 12675/17834 [6:28:18<2:30:52,  1.75s/it] 71%|███████   | 12676/17834 [6:28:19<2:30:57,  1.76s/it] 71%|███████   | 12677/17834 [6:28:21<2:29:51,  1.74s/it] 71%|███████   | 12678/17834 [6:28:23<2:29:59,  1.75s/it] 71%|███████   | 12679/17834 [6:28:25<2:30:34,  1.75s/it] 71%|███████   | 12680/17834 [6:28:26<2:30:09,  1.75s/it] 71%|███████   | 12681/17834 [6:28:28<2:29:45,  1.74s/it] 71%|███████   | 12682/17834 [6:28:30<2:28:43,  1.73s/it] 71%|███████   | 12683/17834 [6:28:32<2:28:49,  1.73s/it] 71%|███████   | 12684/17834 [6:28:33<2:29:19,  1.74s/it] 71%|███████   | 12685/17834 [6:28:35<2:28:03,  1.73s/it] 71%|███████   | 12686/17834 [6:28:37<2:28:35,  1.73s/it] 71%|███████   | 12687/17834 [6:28:39<2:30:39,  1.76s/it] 71%|███████   | 12688/17834 [6:28:40<2:29:58,  1.75s/it] 71%|███████   | 12689/17834 [6:28:42<2:29:21,  1.74s/it] 71%|███████   | 12690/17834 [6:28:44<2:28:53,  1.74s/it] 71%|███████   | 12691/17834 [6:28:46<2:31:56,  1.77s/it] 71%|███████   | 12692/17834 [6:28:47<2:31:04,  1.76s/it] 71%|███████   | 12693/17834 [6:28:49<2:29:43,  1.75s/it] 71%|███████   | 12694/17834 [6:28:51<2:29:01,  1.74s/it] 71%|███████   | 12695/17834 [6:28:53<2:27:32,  1.72s/it] 71%|███████   | 12696/17834 [6:28:54<2:28:16,  1.73s/it] 71%|███████   | 12697/17834 [6:28:56<2:28:30,  1.73s/it] 71%|███████   | 12698/17834 [6:28:58<2:28:20,  1.73s/it] 71%|███████   | 12699/17834 [6:28:59<2:28:21,  1.73s/it]08/31/2024 01:43:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4784502983093262, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.047760069370269775, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.341498613357544, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.867708921432495}
 71%|███████   | 12700/17834 [6:29:01<2:29:30,  1.75s/it] 71%|███████   | 12701/17834 [6:29:03<2:28:50,  1.74s/it] 71%|███████   | 12702/17834 [6:29:05<2:31:20,  1.77s/it] 71%|███████   | 12703/17834 [6:29:07<2:30:31,  1.76s/it] 71%|███████   | 12704/17834 [6:29:08<2:31:46,  1.78s/it] 71%|███████   | 12705/17834 [6:29:10<2:29:47,  1.75s/it] 71%|███████   | 12706/17834 [6:29:12<2:29:55,  1.75s/it] 71%|███████▏  | 12707/17834 [6:29:14<2:30:10,  1.76s/it] 71%|███████▏  | 12708/17834 [6:29:15<2:31:19,  1.77s/it] 71%|███████▏  | 12709/17834 [6:29:17<2:28:54,  1.74s/it] 71%|███████▏  | 12710/17834 [6:29:19<2:27:20,  1.73s/it] 71%|███████▏  | 12711/17834 [6:29:21<2:28:05,  1.73s/it] 71%|███████▏  | 12712/17834 [6:29:22<2:27:46,  1.73s/it] 71%|███████▏  | 12713/17834 [6:29:24<2:26:37,  1.72s/it] 71%|███████▏  | 12714/17834 [6:29:26<2:26:30,  1.72s/it] 71%|███████▏  | 12715/17834 [6:29:27<2:27:11,  1.73s/it] 71%|███████▏  | 12716/17834 [6:29:29<2:27:47,  1.73s/it] 71%|███████▏  | 12717/17834 [6:29:31<2:26:21,  1.72s/it] 71%|███████▏  | 12718/17834 [6:29:33<2:27:54,  1.73s/it] 71%|███████▏  | 12719/17834 [6:29:34<2:28:18,  1.74s/it] 71%|███████▏  | 12720/17834 [6:29:36<2:28:19,  1.74s/it] 71%|███████▏  | 12721/17834 [6:29:38<2:29:39,  1.76s/it] 71%|███████▏  | 12722/17834 [6:29:40<2:28:24,  1.74s/it] 71%|███████▏  | 12723/17834 [6:29:41<2:29:00,  1.75s/it] 71%|███████▏  | 12724/17834 [6:29:43<2:29:08,  1.75s/it] 71%|███████▏  | 12725/17834 [6:29:45<2:30:36,  1.77s/it] 71%|███████▏  | 12726/17834 [6:29:47<2:28:56,  1.75s/it] 71%|███████▏  | 12727/17834 [6:29:48<2:28:53,  1.75s/it] 71%|███████▏  | 12728/17834 [6:29:50<2:26:59,  1.73s/it] 71%|███████▏  | 12729/17834 [6:29:52<2:27:16,  1.73s/it] 71%|███████▏  | 12730/17834 [6:29:54<2:26:49,  1.73s/it] 71%|███████▏  | 12731/17834 [6:29:55<2:27:40,  1.74s/it] 71%|███████▏  | 12732/17834 [6:29:57<2:28:25,  1.75s/it] 71%|███████▏  | 12733/17834 [6:29:59<2:26:58,  1.73s/it] 71%|███████▏  | 12734/17834 [6:30:00<2:26:41,  1.73s/it] 71%|███████▏  | 12735/17834 [6:30:02<2:27:13,  1.73s/it] 71%|███████▏  | 12736/17834 [6:30:04<2:27:07,  1.73s/it] 71%|███████▏  | 12737/17834 [6:30:06<2:29:00,  1.75s/it] 71%|███████▏  | 12738/17834 [6:30:07<2:28:47,  1.75s/it] 71%|███████▏  | 12739/17834 [6:30:09<2:28:35,  1.75s/it] 71%|███████▏  | 12740/17834 [6:30:11<2:27:23,  1.74s/it] 71%|███████▏  | 12741/17834 [6:30:13<2:27:00,  1.73s/it] 71%|███████▏  | 12742/17834 [6:30:14<2:28:16,  1.75s/it] 71%|███████▏  | 12743/17834 [6:30:16<2:29:32,  1.76s/it] 71%|███████▏  | 12744/17834 [6:30:18<2:28:00,  1.74s/it] 71%|███████▏  | 12745/17834 [6:30:20<2:27:06,  1.73s/it] 71%|███████▏  | 12746/17834 [6:30:21<2:26:45,  1.73s/it] 71%|███████▏  | 12747/17834 [6:30:23<2:27:31,  1.74s/it] 71%|███████▏  | 12748/17834 [6:30:25<2:28:23,  1.75s/it] 71%|███████▏  | 12749/17834 [6:30:27<2:29:06,  1.76s/it]08/31/2024 01:44:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1207001209259033, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.034584030508995056, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2200028896331787, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3752870559692383}
 71%|███████▏  | 12750/17834 [6:30:28<2:27:07,  1.74s/it] 71%|███████▏  | 12751/17834 [6:30:30<2:26:25,  1.73s/it] 72%|███████▏  | 12752/17834 [6:30:32<2:26:55,  1.73s/it] 72%|███████▏  | 12753/17834 [6:30:34<2:27:26,  1.74s/it] 72%|███████▏  | 12754/17834 [6:30:35<2:27:27,  1.74s/it] 72%|███████▏  | 12755/17834 [6:30:37<2:26:18,  1.73s/it] 72%|███████▏  | 12756/17834 [6:30:39<2:26:31,  1.73s/it] 72%|███████▏  | 12757/17834 [6:30:40<2:26:53,  1.74s/it] 72%|███████▏  | 12758/17834 [6:30:42<2:27:03,  1.74s/it] 72%|███████▏  | 12759/17834 [6:30:44<2:26:29,  1.73s/it] 72%|███████▏  | 12760/17834 [6:30:46<2:27:50,  1.75s/it] 72%|███████▏  | 12761/17834 [6:30:48<2:29:52,  1.77s/it] 72%|███████▏  | 12762/17834 [6:30:49<2:28:50,  1.76s/it] 72%|███████▏  | 12763/17834 [6:30:51<2:27:23,  1.74s/it] 72%|███████▏  | 12764/17834 [6:30:53<2:27:19,  1.74s/it] 72%|███████▏  | 12765/17834 [6:30:54<2:27:03,  1.74s/it] 72%|███████▏  | 12766/17834 [6:30:56<2:27:14,  1.74s/it] 72%|███████▏  | 12767/17834 [6:30:58<2:26:35,  1.74s/it] 72%|███████▏  | 12768/17834 [6:31:00<2:27:31,  1.75s/it] 72%|███████▏  | 12769/17834 [6:31:01<2:27:40,  1.75s/it] 72%|███████▏  | 12770/17834 [6:31:03<2:27:55,  1.75s/it] 72%|███████▏  | 12771/17834 [6:31:05<2:28:36,  1.76s/it] 72%|███████▏  | 12772/17834 [6:31:07<2:27:41,  1.75s/it] 72%|███████▏  | 12773/17834 [6:31:08<2:27:00,  1.74s/it] 72%|███████▏  | 12774/17834 [6:31:10<2:26:33,  1.74s/it] 72%|███████▏  | 12775/17834 [6:31:12<2:26:18,  1.74s/it] 72%|███████▏  | 12776/17834 [6:31:14<2:26:33,  1.74s/it] 72%|███████▏  | 12777/17834 [6:31:15<2:26:08,  1.73s/it] 72%|███████▏  | 12778/17834 [6:31:17<2:28:47,  1.77s/it] 72%|███████▏  | 12779/17834 [6:31:19<2:29:31,  1.77s/it] 72%|███████▏  | 12780/17834 [6:31:21<2:28:27,  1.76s/it] 72%|███████▏  | 12781/17834 [6:31:23<2:27:42,  1.75s/it] 72%|███████▏  | 12782/17834 [6:31:24<2:26:55,  1.74s/it] 72%|███████▏  | 12783/17834 [6:31:26<2:28:44,  1.77s/it] 72%|███████▏  | 12784/17834 [6:31:28<2:26:32,  1.74s/it] 72%|███████▏  | 12785/17834 [6:31:29<2:26:06,  1.74s/it] 72%|███████▏  | 12786/17834 [6:31:31<2:27:34,  1.75s/it] 72%|███████▏  | 12787/17834 [6:31:33<2:25:15,  1.73s/it] 72%|███████▏  | 12788/17834 [6:31:35<2:24:43,  1.72s/it] 72%|███████▏  | 12789/17834 [6:31:36<2:25:47,  1.73s/it] 72%|███████▏  | 12790/17834 [6:31:38<2:27:13,  1.75s/it] 72%|███████▏  | 12791/17834 [6:31:40<2:27:03,  1.75s/it] 72%|███████▏  | 12792/17834 [6:31:42<2:28:19,  1.77s/it] 72%|███████▏  | 12793/17834 [6:31:43<2:25:08,  1.73s/it] 72%|███████▏  | 12794/17834 [6:31:45<2:30:55,  1.80s/it] 72%|███████▏  | 12795/17834 [6:31:47<2:27:38,  1.76s/it] 72%|███████▏  | 12796/17834 [6:31:49<2:27:15,  1.75s/it] 72%|███████▏  | 12797/17834 [6:31:51<2:29:03,  1.78s/it] 72%|███████▏  | 12798/17834 [6:31:52<2:26:49,  1.75s/it] 72%|███████▏  | 12799/17834 [6:31:54<2:26:14,  1.74s/it]08/31/2024 01:46:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0264794826507568, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.019619666039943695, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1134696006774902, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1595687866210938}
 72%|███████▏  | 12800/17834 [6:31:56<2:26:19,  1.74s/it] 72%|███████▏  | 12801/17834 [6:31:57<2:25:27,  1.73s/it] 72%|███████▏  | 12802/17834 [6:31:59<2:26:53,  1.75s/it] 72%|███████▏  | 12803/17834 [6:32:01<2:25:44,  1.74s/it] 72%|███████▏  | 12804/17834 [6:32:03<2:28:04,  1.77s/it] 72%|███████▏  | 12805/17834 [6:32:05<2:28:28,  1.77s/it] 72%|███████▏  | 12806/17834 [6:32:06<2:27:56,  1.77s/it] 72%|███████▏  | 12807/17834 [6:32:08<2:26:38,  1.75s/it] 72%|███████▏  | 12808/17834 [6:32:10<2:27:57,  1.77s/it] 72%|███████▏  | 12809/17834 [6:32:12<2:27:29,  1.76s/it] 72%|███████▏  | 12810/17834 [6:32:13<2:25:57,  1.74s/it] 72%|███████▏  | 12811/17834 [6:32:15<2:25:47,  1.74s/it] 72%|███████▏  | 12812/17834 [6:32:17<2:26:09,  1.75s/it] 72%|███████▏  | 12813/17834 [6:32:19<2:26:16,  1.75s/it] 72%|███████▏  | 12814/17834 [6:32:20<2:29:16,  1.78s/it] 72%|███████▏  | 12815/17834 [6:32:22<2:26:33,  1.75s/it] 72%|███████▏  | 12816/17834 [6:32:24<2:27:09,  1.76s/it] 72%|███████▏  | 12817/17834 [6:32:26<2:25:34,  1.74s/it] 72%|███████▏  | 12818/17834 [6:32:27<2:24:28,  1.73s/it] 72%|███████▏  | 12819/17834 [6:32:29<2:25:15,  1.74s/it] 72%|███████▏  | 12820/17834 [6:32:31<2:26:51,  1.76s/it] 72%|███████▏  | 12821/17834 [6:32:33<2:27:20,  1.76s/it] 72%|███████▏  | 12822/17834 [6:32:34<2:26:27,  1.75s/it] 72%|███████▏  | 12823/17834 [6:32:36<2:25:16,  1.74s/it] 72%|███████▏  | 12824/17834 [6:32:38<2:24:48,  1.73s/it] 72%|███████▏  | 12825/17834 [6:32:39<2:24:44,  1.73s/it] 72%|███████▏  | 12826/17834 [6:32:41<2:24:14,  1.73s/it] 72%|███████▏  | 12827/17834 [6:32:43<2:23:32,  1.72s/it] 72%|███████▏  | 12828/17834 [6:32:45<2:26:29,  1.76s/it] 72%|███████▏  | 12829/17834 [6:32:47<2:27:25,  1.77s/it] 72%|███████▏  | 12830/17834 [6:32:48<2:26:14,  1.75s/it] 72%|███████▏  | 12831/17834 [6:32:50<2:25:06,  1.74s/it] 72%|███████▏  | 12832/17834 [6:32:52<2:25:28,  1.75s/it] 72%|███████▏  | 12833/17834 [6:32:53<2:25:18,  1.74s/it] 72%|███████▏  | 12834/17834 [6:32:55<2:25:22,  1.74s/it] 72%|███████▏  | 12835/17834 [6:32:57<2:24:22,  1.73s/it] 72%|███████▏  | 12836/17834 [6:32:59<2:24:26,  1.73s/it] 72%|███████▏  | 12837/17834 [6:33:00<2:25:13,  1.74s/it] 72%|███████▏  | 12838/17834 [6:33:02<2:27:32,  1.77s/it] 72%|███████▏  | 12839/17834 [6:33:04<2:26:27,  1.76s/it] 72%|███████▏  | 12840/17834 [6:33:06<2:25:24,  1.75s/it] 72%|███████▏  | 12841/17834 [6:33:07<2:25:38,  1.75s/it] 72%|███████▏  | 12842/17834 [6:33:09<2:26:47,  1.76s/it] 72%|███████▏  | 12843/17834 [6:33:11<2:26:09,  1.76s/it] 72%|███████▏  | 12844/17834 [6:33:13<2:25:26,  1.75s/it] 72%|███████▏  | 12845/17834 [6:33:14<2:25:14,  1.75s/it] 72%|███████▏  | 12846/17834 [6:33:16<2:23:45,  1.73s/it] 72%|███████▏  | 12847/17834 [6:33:18<2:23:10,  1.72s/it] 72%|███████▏  | 12848/17834 [6:33:20<2:22:33,  1.72s/it] 72%|███████▏  | 12849/17834 [6:33:21<2:23:26,  1.73s/it]08/31/2024 01:47:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0083956718444824, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029388049617409706, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1793487071990967, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.217132568359375}
 72%|███████▏  | 12850/17834 [6:33:23<2:24:23,  1.74s/it] 72%|███████▏  | 12851/17834 [6:33:25<2:27:05,  1.77s/it] 72%|███████▏  | 12852/17834 [6:33:27<2:26:02,  1.76s/it] 72%|███████▏  | 12853/17834 [6:33:28<2:26:46,  1.77s/it] 72%|███████▏  | 12854/17834 [6:33:30<2:27:16,  1.77s/it] 72%|███████▏  | 12855/17834 [6:33:32<2:26:01,  1.76s/it] 72%|███████▏  | 12856/17834 [6:33:34<2:23:38,  1.73s/it] 72%|███████▏  | 12857/17834 [6:33:35<2:22:48,  1.72s/it] 72%|███████▏  | 12858/17834 [6:33:37<2:23:26,  1.73s/it] 72%|███████▏  | 12859/17834 [6:33:39<2:23:58,  1.74s/it] 72%|███████▏  | 12860/17834 [6:33:41<2:23:28,  1.73s/it] 72%|███████▏  | 12861/17834 [6:33:42<2:24:54,  1.75s/it] 72%|███████▏  | 12862/17834 [6:33:44<2:25:55,  1.76s/it] 72%|███████▏  | 12863/17834 [6:33:46<2:25:09,  1.75s/it] 72%|███████▏  | 12864/17834 [6:33:48<2:23:28,  1.73s/it] 72%|███████▏  | 12865/17834 [6:33:49<2:24:06,  1.74s/it] 72%|███████▏  | 12866/17834 [6:33:51<2:27:21,  1.78s/it] 72%|███████▏  | 12867/17834 [6:33:53<2:26:19,  1.77s/it] 72%|███████▏  | 12868/17834 [6:33:55<2:25:35,  1.76s/it] 72%|███████▏  | 12869/17834 [6:33:56<2:25:33,  1.76s/it] 72%|███████▏  | 12870/17834 [6:33:58<2:24:30,  1.75s/it] 72%|███████▏  | 12871/17834 [6:34:00<2:24:21,  1.75s/it] 72%|███████▏  | 12872/17834 [6:34:02<2:22:40,  1.73s/it] 72%|███████▏  | 12873/17834 [6:34:03<2:22:25,  1.72s/it] 72%|███████▏  | 12874/17834 [6:34:05<2:23:07,  1.73s/it] 72%|███████▏  | 12875/17834 [6:34:07<2:24:35,  1.75s/it] 72%|███████▏  | 12876/17834 [6:34:09<2:23:35,  1.74s/it] 72%|███████▏  | 12877/17834 [6:34:10<2:23:07,  1.73s/it] 72%|███████▏  | 12878/17834 [6:34:12<2:24:17,  1.75s/it] 72%|███████▏  | 12879/17834 [6:34:14<2:24:06,  1.75s/it] 72%|███████▏  | 12880/17834 [6:34:15<2:23:28,  1.74s/it] 72%|███████▏  | 12881/17834 [6:34:17<2:23:46,  1.74s/it] 72%|███████▏  | 12882/17834 [6:34:19<2:24:04,  1.75s/it] 72%|███████▏  | 12883/17834 [6:34:21<2:24:38,  1.75s/it] 72%|███████▏  | 12884/17834 [6:34:22<2:23:15,  1.74s/it] 72%|███████▏  | 12885/17834 [6:34:24<2:24:35,  1.75s/it] 72%|███████▏  | 12886/17834 [6:34:26<2:25:49,  1.77s/it] 72%|███████▏  | 12887/17834 [6:34:28<2:26:51,  1.78s/it] 72%|███████▏  | 12888/17834 [6:34:30<2:24:22,  1.75s/it] 72%|███████▏  | 12889/17834 [6:34:31<2:24:04,  1.75s/it] 72%|███████▏  | 12890/17834 [6:34:33<2:26:09,  1.77s/it] 72%|███████▏  | 12891/17834 [6:34:35<2:24:47,  1.76s/it] 72%|███████▏  | 12892/17834 [6:34:37<2:23:39,  1.74s/it] 72%|███████▏  | 12893/17834 [6:34:38<2:24:42,  1.76s/it] 72%|███████▏  | 12894/17834 [6:34:40<2:24:00,  1.75s/it] 72%|███████▏  | 12895/17834 [6:34:42<2:24:06,  1.75s/it] 72%|███████▏  | 12896/17834 [6:34:44<2:26:00,  1.77s/it] 72%|███████▏  | 12897/17834 [6:34:45<2:24:03,  1.75s/it] 72%|███████▏  | 12898/17834 [6:34:47<2:25:05,  1.76s/it] 72%|███████▏  | 12899/17834 [6:34:49<2:23:56,  1.75s/it]08/31/2024 01:49:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9554836750030518, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03271694853901863, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.117389678955078, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.105590343475342}
 72%|███████▏  | 12900/17834 [6:34:51<2:21:52,  1.73s/it] 72%|███████▏  | 12901/17834 [6:34:52<2:22:23,  1.73s/it] 72%|███████▏  | 12902/17834 [6:34:54<2:22:45,  1.74s/it] 72%|███████▏  | 12903/17834 [6:34:56<2:25:20,  1.77s/it] 72%|███████▏  | 12904/17834 [6:34:58<2:24:25,  1.76s/it] 72%|███████▏  | 12905/17834 [6:34:59<2:23:16,  1.74s/it] 72%|███████▏  | 12906/17834 [6:35:01<2:24:08,  1.76s/it] 72%|███████▏  | 12907/17834 [6:35:03<2:23:24,  1.75s/it] 72%|███████▏  | 12908/17834 [6:35:05<2:22:28,  1.74s/it] 72%|███████▏  | 12909/17834 [6:35:06<2:27:27,  1.80s/it] 72%|███████▏  | 12910/17834 [6:35:08<2:25:27,  1.77s/it] 72%|███████▏  | 12911/17834 [6:35:10<2:26:33,  1.79s/it] 72%|███████▏  | 12912/17834 [6:35:12<2:25:08,  1.77s/it] 72%|███████▏  | 12913/17834 [6:35:14<2:25:45,  1.78s/it] 72%|███████▏  | 12914/17834 [6:35:15<2:26:29,  1.79s/it] 72%|███████▏  | 12915/17834 [6:35:17<2:24:03,  1.76s/it] 72%|███████▏  | 12916/17834 [6:35:19<2:25:45,  1.78s/it] 72%|███████▏  | 12917/17834 [6:35:21<2:24:24,  1.76s/it] 72%|███████▏  | 12918/17834 [6:35:22<2:23:44,  1.75s/it] 72%|███████▏  | 12919/17834 [6:35:24<2:23:19,  1.75s/it] 72%|███████▏  | 12920/17834 [6:35:26<2:23:05,  1.75s/it] 72%|███████▏  | 12921/17834 [6:35:28<2:22:50,  1.74s/it] 72%|███████▏  | 12922/17834 [6:35:29<2:23:53,  1.76s/it] 72%|███████▏  | 12923/17834 [6:35:31<2:22:10,  1.74s/it] 72%|███████▏  | 12924/17834 [6:35:33<2:22:21,  1.74s/it] 72%|███████▏  | 12925/17834 [6:35:34<2:21:59,  1.74s/it] 72%|███████▏  | 12926/17834 [6:35:36<2:22:49,  1.75s/it] 72%|███████▏  | 12927/17834 [6:35:38<2:22:32,  1.74s/it] 72%|███████▏  | 12928/17834 [6:35:40<2:23:36,  1.76s/it] 72%|███████▏  | 12929/17834 [6:35:42<2:24:42,  1.77s/it] 73%|███████▎  | 12930/17834 [6:35:43<2:23:17,  1.75s/it] 73%|███████▎  | 12931/17834 [6:35:45<2:23:06,  1.75s/it] 73%|███████▎  | 12932/17834 [6:35:47<2:23:33,  1.76s/it] 73%|███████▎  | 12933/17834 [6:35:48<2:21:56,  1.74s/it] 73%|███████▎  | 12934/17834 [6:35:50<2:22:11,  1.74s/it] 73%|███████▎  | 12935/17834 [6:35:52<2:21:03,  1.73s/it] 73%|███████▎  | 12936/17834 [6:35:54<2:23:45,  1.76s/it] 73%|███████▎  | 12937/17834 [6:35:55<2:22:12,  1.74s/it] 73%|███████▎  | 12938/17834 [6:35:57<2:24:07,  1.77s/it] 73%|███████▎  | 12939/17834 [6:35:59<2:24:51,  1.78s/it] 73%|███████▎  | 12940/17834 [6:36:01<2:23:32,  1.76s/it] 73%|███████▎  | 12941/17834 [6:36:03<2:23:48,  1.76s/it] 73%|███████▎  | 12942/17834 [6:36:04<2:23:00,  1.75s/it] 73%|███████▎  | 12943/17834 [6:36:06<2:22:56,  1.75s/it] 73%|███████▎  | 12944/17834 [6:36:08<2:21:28,  1.74s/it] 73%|███████▎  | 12945/17834 [6:36:10<2:22:53,  1.75s/it] 73%|███████▎  | 12946/17834 [6:36:11<2:21:41,  1.74s/it] 73%|███████▎  | 12947/17834 [6:36:13<2:22:37,  1.75s/it] 73%|███████▎  | 12948/17834 [6:36:15<2:20:08,  1.72s/it] 73%|███████▎  | 12949/17834 [6:36:16<2:20:17,  1.72s/it]08/31/2024 01:50:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9826152324676514, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03129392862319946, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2157912254333496, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2297003269195557}
 73%|███████▎  | 12950/17834 [6:36:18<2:21:53,  1.74s/it] 73%|███████▎  | 12951/17834 [6:36:20<2:24:11,  1.77s/it] 73%|███████▎  | 12952/17834 [6:36:22<2:23:36,  1.77s/it] 73%|███████▎  | 12953/17834 [6:36:24<2:23:02,  1.76s/it] 73%|███████▎  | 12954/17834 [6:36:25<2:22:47,  1.76s/it] 73%|███████▎  | 12955/17834 [6:36:27<2:21:50,  1.74s/it] 73%|███████▎  | 12956/17834 [6:36:29<2:22:52,  1.76s/it] 73%|███████▎  | 12957/17834 [6:36:31<2:23:11,  1.76s/it] 73%|███████▎  | 12958/17834 [6:36:32<2:21:44,  1.74s/it] 73%|███████▎  | 12959/17834 [6:36:34<2:22:42,  1.76s/it] 73%|███████▎  | 12960/17834 [6:36:36<2:24:00,  1.77s/it] 73%|███████▎  | 12961/17834 [6:36:38<2:24:10,  1.78s/it] 73%|███████▎  | 12962/17834 [6:36:39<2:23:19,  1.77s/it] 73%|███████▎  | 12963/17834 [6:36:41<2:24:07,  1.78s/it] 73%|███████▎  | 12964/17834 [6:36:43<2:22:35,  1.76s/it] 73%|███████▎  | 12965/17834 [6:36:45<2:23:48,  1.77s/it] 73%|███████▎  | 12966/17834 [6:36:46<2:22:18,  1.75s/it] 73%|███████▎  | 12967/17834 [6:36:48<2:22:23,  1.76s/it] 73%|███████▎  | 12968/17834 [6:36:50<2:22:13,  1.75s/it] 73%|███████▎  | 12969/17834 [6:36:52<2:21:17,  1.74s/it] 73%|███████▎  | 12970/17834 [6:36:53<2:20:58,  1.74s/it] 73%|███████▎  | 12971/17834 [6:36:55<2:22:04,  1.75s/it] 73%|███████▎  | 12972/17834 [6:36:57<2:19:52,  1.73s/it] 73%|███████▎  | 12973/17834 [6:36:59<2:20:48,  1.74s/it] 73%|███████▎  | 12974/17834 [6:37:00<2:22:17,  1.76s/it] 73%|███████▎  | 12975/17834 [6:37:02<2:20:16,  1.73s/it] 73%|███████▎  | 12976/17834 [6:37:04<2:19:51,  1.73s/it] 73%|███████▎  | 12977/17834 [6:37:06<2:20:28,  1.74s/it] 73%|███████▎  | 12978/17834 [6:37:07<2:19:29,  1.72s/it] 73%|███████▎  | 12979/17834 [6:37:09<2:19:33,  1.72s/it] 73%|███████▎  | 12980/17834 [6:37:11<2:20:01,  1.73s/it] 73%|███████▎  | 12981/17834 [6:37:12<2:20:44,  1.74s/it] 73%|███████▎  | 12982/17834 [6:37:14<2:22:38,  1.76s/it] 73%|███████▎  | 12983/17834 [6:37:16<2:23:50,  1.78s/it] 73%|███████▎  | 12984/17834 [6:37:18<2:22:36,  1.76s/it] 73%|███████▎  | 12985/17834 [6:37:20<2:22:05,  1.76s/it] 73%|███████▎  | 12986/17834 [6:37:21<2:21:13,  1.75s/it] 73%|███████▎  | 12987/17834 [6:37:23<2:20:06,  1.73s/it] 73%|███████▎  | 12988/17834 [6:37:25<2:21:38,  1.75s/it] 73%|███████▎  | 12989/17834 [6:37:27<2:21:22,  1.75s/it] 73%|███████▎  | 12990/17834 [6:37:28<2:21:04,  1.75s/it] 73%|███████▎  | 12991/17834 [6:37:30<2:20:26,  1.74s/it] 73%|███████▎  | 12992/17834 [6:37:32<2:18:17,  1.71s/it] 73%|███████▎  | 12993/17834 [6:37:34<2:22:58,  1.77s/it] 73%|███████▎  | 12994/17834 [6:37:35<2:21:27,  1.75s/it] 73%|███████▎  | 12995/17834 [6:37:37<2:19:59,  1.74s/it] 73%|███████▎  | 12996/17834 [6:37:39<2:20:04,  1.74s/it] 73%|███████▎  | 12997/17834 [6:37:41<2:21:47,  1.76s/it] 73%|███████▎  | 12998/17834 [6:37:42<2:24:29,  1.79s/it] 73%|███████▎  | 12999/17834 [6:37:44<2:22:21,  1.77s/it]08/31/2024 01:52:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0371891260147095, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02445017546415329, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.246396541595459, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3080358505249023}
 73%|███████▎  | 13000/17834 [6:37:46<2:26:25,  1.82s/it] 73%|███████▎  | 13001/17834 [6:37:48<2:22:49,  1.77s/it] 73%|███████▎  | 13002/17834 [6:37:49<2:21:22,  1.76s/it] 73%|███████▎  | 13003/17834 [6:37:51<2:21:27,  1.76s/it] 73%|███████▎  | 13004/17834 [6:37:53<2:20:41,  1.75s/it] 73%|███████▎  | 13005/17834 [6:37:55<2:23:01,  1.78s/it] 73%|███████▎  | 13006/17834 [6:37:56<2:21:06,  1.75s/it] 73%|███████▎  | 13007/17834 [6:37:58<2:20:51,  1.75s/it] 73%|███████▎  | 13008/17834 [6:38:00<2:21:53,  1.76s/it] 73%|███████▎  | 13009/17834 [6:38:02<2:22:55,  1.78s/it] 73%|███████▎  | 13010/17834 [6:38:04<2:22:35,  1.77s/it] 73%|███████▎  | 13011/17834 [6:38:05<2:23:34,  1.79s/it] 73%|███████▎  | 13012/17834 [6:38:07<2:22:14,  1.77s/it] 73%|███████▎  | 13013/17834 [6:38:09<2:22:24,  1.77s/it] 73%|███████▎  | 13014/17834 [6:38:11<2:22:43,  1.78s/it] 73%|███████▎  | 13015/17834 [6:38:12<2:22:54,  1.78s/it] 73%|███████▎  | 13016/17834 [6:38:14<2:22:15,  1.77s/it] 73%|███████▎  | 13017/17834 [6:38:16<2:22:02,  1.77s/it] 73%|███████▎  | 13018/17834 [6:38:18<2:21:16,  1.76s/it] 73%|███████▎  | 13019/17834 [6:38:19<2:21:16,  1.76s/it] 73%|███████▎  | 13020/17834 [6:38:21<2:22:20,  1.77s/it] 73%|███████▎  | 13021/17834 [6:38:23<2:20:39,  1.75s/it] 73%|███████▎  | 13022/17834 [6:38:25<2:20:27,  1.75s/it] 73%|███████▎  | 13023/17834 [6:38:27<2:21:25,  1.76s/it] 73%|███████▎  | 13024/17834 [6:38:28<2:20:56,  1.76s/it] 73%|███████▎  | 13025/17834 [6:38:30<2:19:06,  1.74s/it] 73%|███████▎  | 13026/17834 [6:38:32<2:20:31,  1.75s/it] 73%|███████▎  | 13027/17834 [6:38:33<2:19:26,  1.74s/it] 73%|███████▎  | 13028/17834 [6:38:35<2:17:48,  1.72s/it] 73%|███████▎  | 13029/17834 [6:38:37<2:18:18,  1.73s/it] 73%|███████▎  | 13030/17834 [6:38:39<2:18:30,  1.73s/it] 73%|███████▎  | 13031/17834 [6:38:40<2:17:45,  1.72s/it] 73%|███████▎  | 13032/17834 [6:38:42<2:17:14,  1.71s/it] 73%|███████▎  | 13033/17834 [6:38:44<2:18:54,  1.74s/it] 73%|███████▎  | 13034/17834 [6:38:45<2:18:02,  1.73s/it] 73%|███████▎  | 13035/17834 [6:38:47<2:18:05,  1.73s/it] 73%|███████▎  | 13036/17834 [6:38:49<2:17:58,  1.73s/it] 73%|███████▎  | 13037/17834 [6:38:51<2:18:35,  1.73s/it] 73%|███████▎  | 13038/17834 [6:38:52<2:18:10,  1.73s/it] 73%|███████▎  | 13039/17834 [6:38:54<2:19:16,  1.74s/it] 73%|███████▎  | 13040/17834 [6:38:56<2:19:30,  1.75s/it] 73%|███████▎  | 13041/17834 [6:38:58<2:18:28,  1.73s/it] 73%|███████▎  | 13042/17834 [6:38:59<2:20:00,  1.75s/it] 73%|███████▎  | 13043/17834 [6:39:01<2:19:29,  1.75s/it] 73%|███████▎  | 13044/17834 [6:39:03<2:19:07,  1.74s/it] 73%|███████▎  | 13045/17834 [6:39:05<2:17:04,  1.72s/it] 73%|███████▎  | 13046/17834 [6:39:06<2:19:43,  1.75s/it] 73%|███████▎  | 13047/17834 [6:39:08<2:19:28,  1.75s/it] 73%|███████▎  | 13048/17834 [6:39:10<2:19:48,  1.75s/it] 73%|███████▎  | 13049/17834 [6:39:12<2:18:25,  1.74s/it]08/31/2024 01:53:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9890168905258179, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.036236755549907684, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2084922790527344, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.23374605178833}
 73%|███████▎  | 13050/17834 [6:39:13<2:17:14,  1.72s/it] 73%|███████▎  | 13051/17834 [6:39:15<2:19:09,  1.75s/it] 73%|███████▎  | 13052/17834 [6:39:17<2:19:14,  1.75s/it] 73%|███████▎  | 13053/17834 [6:39:19<2:20:28,  1.76s/it] 73%|███████▎  | 13054/17834 [6:39:20<2:19:37,  1.75s/it] 73%|███████▎  | 13055/17834 [6:39:22<2:20:21,  1.76s/it] 73%|███████▎  | 13056/17834 [6:39:24<2:20:31,  1.76s/it] 73%|███████▎  | 13057/17834 [6:39:26<2:18:40,  1.74s/it] 73%|███████▎  | 13058/17834 [6:39:27<2:20:20,  1.76s/it] 73%|███████▎  | 13059/17834 [6:39:29<2:19:07,  1.75s/it] 73%|███████▎  | 13060/17834 [6:39:31<2:18:42,  1.74s/it] 73%|███████▎  | 13061/17834 [6:39:33<2:19:06,  1.75s/it] 73%|███████▎  | 13062/17834 [6:39:34<2:21:40,  1.78s/it] 73%|███████▎  | 13063/17834 [6:39:36<2:20:19,  1.76s/it] 73%|███████▎  | 13064/17834 [6:39:38<2:21:06,  1.77s/it] 73%|███████▎  | 13065/17834 [6:39:40<2:21:53,  1.79s/it] 73%|███████▎  | 13066/17834 [6:39:41<2:18:52,  1.75s/it] 73%|███████▎  | 13067/17834 [6:39:43<2:20:07,  1.76s/it] 73%|███████▎  | 13068/17834 [6:39:45<2:18:47,  1.75s/it] 73%|███████▎  | 13069/17834 [6:39:47<2:18:56,  1.75s/it] 73%|███████▎  | 13070/17834 [6:39:48<2:18:51,  1.75s/it] 73%|███████▎  | 13071/17834 [6:39:50<2:17:23,  1.73s/it] 73%|███████▎  | 13072/17834 [6:39:52<2:17:18,  1.73s/it] 73%|███████▎  | 13073/17834 [6:39:54<2:16:39,  1.72s/it] 73%|███████▎  | 13074/17834 [6:39:55<2:16:24,  1.72s/it] 73%|███████▎  | 13075/17834 [6:39:57<2:16:10,  1.72s/it] 73%|███████▎  | 13076/17834 [6:39:59<2:16:46,  1.72s/it] 73%|███████▎  | 13077/17834 [6:40:01<2:16:37,  1.72s/it] 73%|███████▎  | 13078/17834 [6:40:02<2:16:35,  1.72s/it] 73%|███████▎  | 13079/17834 [6:40:04<2:18:27,  1.75s/it] 73%|███████▎  | 13080/17834 [6:40:06<2:18:44,  1.75s/it] 73%|███████▎  | 13081/17834 [6:40:08<2:18:11,  1.74s/it] 73%|███████▎  | 13082/17834 [6:40:09<2:18:19,  1.75s/it] 73%|███████▎  | 13083/17834 [6:40:11<2:17:07,  1.73s/it] 73%|███████▎  | 13084/17834 [6:40:13<2:16:33,  1.72s/it] 73%|███████▎  | 13085/17834 [6:40:14<2:16:30,  1.72s/it] 73%|███████▎  | 13086/17834 [6:40:16<2:17:55,  1.74s/it] 73%|███████▎  | 13087/17834 [6:40:18<2:21:39,  1.79s/it] 73%|███████▎  | 13088/17834 [6:40:20<2:20:42,  1.78s/it] 73%|███████▎  | 13089/17834 [6:40:22<2:21:28,  1.79s/it] 73%|███████▎  | 13090/17834 [6:40:23<2:20:06,  1.77s/it] 73%|███████▎  | 13091/17834 [6:40:25<2:18:42,  1.75s/it] 73%|███████▎  | 13092/17834 [6:40:27<2:19:38,  1.77s/it] 73%|███████▎  | 13093/17834 [6:40:29<2:18:38,  1.75s/it] 73%|███████▎  | 13094/17834 [6:40:30<2:20:29,  1.78s/it] 73%|███████▎  | 13095/17834 [6:40:32<2:20:52,  1.78s/it] 73%|███████▎  | 13096/17834 [6:40:34<2:19:48,  1.77s/it] 73%|███████▎  | 13097/17834 [6:40:36<2:19:36,  1.77s/it] 73%|███████▎  | 13098/17834 [6:40:38<2:19:44,  1.77s/it] 73%|███████▎  | 13099/17834 [6:40:39<2:18:58,  1.76s/it]08/31/2024 01:54:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9888513088226318, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01879940554499626, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1437058448791504, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1513566970825195}
 73%|███████▎  | 13100/17834 [6:40:41<2:19:04,  1.76s/it] 73%|███████▎  | 13101/17834 [6:40:43<2:17:42,  1.75s/it] 73%|███████▎  | 13102/17834 [6:40:45<2:18:37,  1.76s/it] 73%|███████▎  | 13103/17834 [6:40:46<2:18:35,  1.76s/it] 73%|███████▎  | 13104/17834 [6:40:48<2:19:02,  1.76s/it] 73%|███████▎  | 13105/17834 [6:40:50<2:19:26,  1.77s/it] 73%|███████▎  | 13106/17834 [6:40:52<2:19:20,  1.77s/it] 73%|███████▎  | 13107/17834 [6:40:53<2:18:14,  1.75s/it] 74%|███████▎  | 13108/17834 [6:40:55<2:19:13,  1.77s/it] 74%|███████▎  | 13109/17834 [6:40:57<2:19:11,  1.77s/it] 74%|███████▎  | 13110/17834 [6:40:59<2:18:55,  1.76s/it] 74%|███████▎  | 13111/17834 [6:41:00<2:18:50,  1.76s/it] 74%|███████▎  | 13112/17834 [6:41:02<2:18:33,  1.76s/it] 74%|███████▎  | 13113/17834 [6:41:04<2:18:46,  1.76s/it] 74%|███████▎  | 13114/17834 [6:41:06<2:18:02,  1.75s/it] 74%|███████▎  | 13115/17834 [6:41:08<2:19:46,  1.78s/it] 74%|███████▎  | 13116/17834 [6:41:09<2:20:06,  1.78s/it] 74%|███████▎  | 13117/17834 [6:41:11<2:19:38,  1.78s/it] 74%|███████▎  | 13118/17834 [6:41:13<2:20:04,  1.78s/it] 74%|███████▎  | 13119/17834 [6:41:15<2:18:06,  1.76s/it] 74%|███████▎  | 13120/17834 [6:41:16<2:17:45,  1.75s/it] 74%|███████▎  | 13121/17834 [6:41:18<2:18:02,  1.76s/it] 74%|███████▎  | 13122/17834 [6:41:20<2:18:05,  1.76s/it] 74%|███████▎  | 13123/17834 [6:41:21<2:15:56,  1.73s/it] 74%|███████▎  | 13124/17834 [6:41:23<2:16:06,  1.73s/it] 74%|███████▎  | 13125/17834 [6:41:25<2:16:10,  1.74s/it] 74%|███████▎  | 13126/17834 [6:41:27<2:16:11,  1.74s/it] 74%|███████▎  | 13127/17834 [6:41:28<2:16:23,  1.74s/it] 74%|███████▎  | 13128/17834 [6:41:30<2:16:28,  1.74s/it] 74%|███████▎  | 13129/17834 [6:41:32<2:17:21,  1.75s/it] 74%|███████▎  | 13130/17834 [6:41:34<2:17:25,  1.75s/it] 74%|███████▎  | 13131/17834 [6:41:35<2:16:58,  1.75s/it] 74%|███████▎  | 13132/17834 [6:41:37<2:15:56,  1.73s/it] 74%|███████▎  | 13133/17834 [6:41:39<2:16:47,  1.75s/it] 74%|███████▎  | 13134/17834 [6:41:41<2:16:29,  1.74s/it] 74%|███████▎  | 13135/17834 [6:41:42<2:16:53,  1.75s/it] 74%|███████▎  | 13136/17834 [6:41:44<2:14:17,  1.72s/it] 74%|███████▎  | 13137/17834 [6:41:46<2:13:40,  1.71s/it] 74%|███████▎  | 13138/17834 [6:41:48<2:16:23,  1.74s/it] 74%|███████▎  | 13139/17834 [6:41:49<2:17:44,  1.76s/it] 74%|███████▎  | 13140/17834 [6:41:51<2:13:21,  1.70s/it] 74%|███████▎  | 13141/17834 [6:41:53<2:16:49,  1.75s/it] 74%|███████▎  | 13142/17834 [6:41:55<2:15:12,  1.73s/it] 74%|███████▎  | 13143/17834 [6:41:56<2:16:59,  1.75s/it] 74%|███████▎  | 13144/17834 [6:41:58<2:16:13,  1.74s/it] 74%|███████▎  | 13145/17834 [6:42:00<2:16:58,  1.75s/it] 74%|███████▎  | 13146/17834 [6:42:02<2:16:31,  1.75s/it] 74%|███████▎  | 13147/17834 [6:42:03<2:16:16,  1.74s/it] 74%|███████▎  | 13148/17834 [6:42:05<2:15:46,  1.74s/it] 74%|███████▎  | 13149/17834 [6:42:07<2:14:55,  1.73s/it]08/31/2024 01:56:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2341959476470947, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02886866219341755, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2540488243103027, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.517113447189331}
 74%|███████▎  | 13150/17834 [6:42:08<2:15:27,  1.74s/it] 74%|███████▎  | 13151/17834 [6:42:10<2:16:57,  1.75s/it] 74%|███████▎  | 13152/17834 [6:42:12<2:17:38,  1.76s/it] 74%|███████▍  | 13153/17834 [6:42:14<2:17:31,  1.76s/it] 74%|███████▍  | 13154/17834 [6:42:16<2:17:48,  1.77s/it] 74%|███████▍  | 13155/17834 [6:42:17<2:17:41,  1.77s/it] 74%|███████▍  | 13156/17834 [6:42:19<2:16:03,  1.75s/it] 74%|███████▍  | 13157/17834 [6:42:21<2:14:48,  1.73s/it] 74%|███████▍  | 13158/17834 [6:42:23<2:15:37,  1.74s/it] 74%|███████▍  | 13159/17834 [6:42:24<2:16:37,  1.75s/it] 74%|███████▍  | 13160/17834 [6:42:26<2:15:51,  1.74s/it] 74%|███████▍  | 13161/17834 [6:42:28<2:16:47,  1.76s/it] 74%|███████▍  | 13162/17834 [6:42:30<2:17:18,  1.76s/it] 74%|███████▍  | 13163/17834 [6:42:31<2:15:27,  1.74s/it] 74%|███████▍  | 13164/17834 [6:42:33<2:17:10,  1.76s/it] 74%|███████▍  | 13165/17834 [6:42:35<2:16:47,  1.76s/it] 74%|███████▍  | 13166/17834 [6:42:37<2:16:17,  1.75s/it] 74%|███████▍  | 13167/17834 [6:42:38<2:15:50,  1.75s/it] 74%|███████▍  | 13168/17834 [6:42:40<2:15:19,  1.74s/it] 74%|███████▍  | 13169/17834 [6:42:42<2:14:58,  1.74s/it] 74%|███████▍  | 13170/17834 [6:42:43<2:15:14,  1.74s/it] 74%|███████▍  | 13171/17834 [6:42:45<2:17:21,  1.77s/it] 74%|███████▍  | 13172/17834 [6:42:47<2:16:21,  1.75s/it] 74%|███████▍  | 13173/17834 [6:42:49<2:16:31,  1.76s/it] 74%|███████▍  | 13174/17834 [6:42:51<2:16:41,  1.76s/it] 74%|███████▍  | 13175/17834 [6:42:52<2:17:36,  1.77s/it] 74%|███████▍  | 13176/17834 [6:42:54<2:18:02,  1.78s/it] 74%|███████▍  | 13177/17834 [6:42:56<2:16:37,  1.76s/it] 74%|███████▍  | 13178/17834 [6:42:58<2:17:29,  1.77s/it] 74%|███████▍  | 13179/17834 [6:42:59<2:16:17,  1.76s/it] 74%|███████▍  | 13180/17834 [6:43:01<2:17:11,  1.77s/it] 74%|███████▍  | 13181/17834 [6:43:03<2:15:19,  1.75s/it] 74%|███████▍  | 13182/17834 [6:43:05<2:17:42,  1.78s/it] 74%|███████▍  | 13183/17834 [6:43:06<2:15:36,  1.75s/it] 74%|███████▍  | 13184/17834 [6:43:08<2:16:30,  1.76s/it] 74%|███████▍  | 13185/17834 [6:43:10<2:14:48,  1.74s/it] 74%|███████▍  | 13186/17834 [6:43:12<2:14:47,  1.74s/it] 74%|███████▍  | 13187/17834 [6:43:13<2:14:53,  1.74s/it] 74%|███████▍  | 13188/17834 [6:43:15<2:15:28,  1.75s/it] 74%|███████▍  | 13189/17834 [6:43:17<2:15:08,  1.75s/it] 74%|███████▍  | 13190/17834 [6:43:19<2:14:37,  1.74s/it] 74%|███████▍  | 13191/17834 [6:43:20<2:14:18,  1.74s/it] 74%|███████▍  | 13192/17834 [6:43:22<2:15:22,  1.75s/it] 74%|███████▍  | 13193/17834 [6:43:24<2:15:17,  1.75s/it] 74%|███████▍  | 13194/17834 [6:43:26<2:15:22,  1.75s/it] 74%|███████▍  | 13195/17834 [6:43:27<2:15:44,  1.76s/it] 74%|███████▍  | 13196/17834 [6:43:29<2:15:00,  1.75s/it] 74%|███████▍  | 13197/17834 [6:43:31<2:15:26,  1.75s/it] 74%|███████▍  | 13198/17834 [6:43:33<2:15:57,  1.76s/it] 74%|███████▍  | 13199/17834 [6:43:34<2:16:27,  1.77s/it]08/31/2024 01:57:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9705595970153809, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024338528513908386, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.138702392578125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1336004734039307}
 74%|███████▍  | 13200/17834 [6:43:36<2:15:59,  1.76s/it] 74%|███████▍  | 13201/17834 [6:43:38<2:16:52,  1.77s/it] 74%|███████▍  | 13202/17834 [6:43:40<2:17:52,  1.79s/it] 74%|███████▍  | 13203/17834 [6:43:42<2:16:22,  1.77s/it] 74%|███████▍  | 13204/17834 [6:43:43<2:16:31,  1.77s/it] 74%|███████▍  | 13205/17834 [6:43:45<2:13:54,  1.74s/it] 74%|███████▍  | 13206/17834 [6:43:47<2:13:56,  1.74s/it] 74%|███████▍  | 13207/17834 [6:43:49<2:15:39,  1.76s/it] 74%|███████▍  | 13208/17834 [6:43:50<2:15:54,  1.76s/it] 74%|███████▍  | 13209/17834 [6:43:52<2:15:43,  1.76s/it] 74%|███████▍  | 13210/17834 [6:43:54<2:13:26,  1.73s/it] 74%|███████▍  | 13211/17834 [6:43:55<2:13:41,  1.74s/it] 74%|███████▍  | 13212/17834 [6:43:57<2:14:12,  1.74s/it] 74%|███████▍  | 13213/17834 [6:43:59<2:14:39,  1.75s/it] 74%|███████▍  | 13214/17834 [6:44:01<2:14:38,  1.75s/it] 74%|███████▍  | 13215/17834 [6:44:03<2:16:06,  1.77s/it] 74%|███████▍  | 13216/17834 [6:44:04<2:15:35,  1.76s/it] 74%|███████▍  | 13217/17834 [6:44:06<2:15:41,  1.76s/it] 74%|███████▍  | 13218/17834 [6:44:08<2:15:38,  1.76s/it] 74%|███████▍  | 13219/17834 [6:44:10<2:17:00,  1.78s/it] 74%|███████▍  | 13220/17834 [6:44:11<2:17:06,  1.78s/it] 74%|███████▍  | 13221/17834 [6:44:13<2:16:30,  1.78s/it] 74%|███████▍  | 13222/17834 [6:44:15<2:18:45,  1.81s/it] 74%|███████▍  | 13223/17834 [6:44:17<2:18:02,  1.80s/it] 74%|███████▍  | 13224/17834 [6:44:19<2:16:50,  1.78s/it] 74%|███████▍  | 13225/17834 [6:44:20<2:15:49,  1.77s/it] 74%|███████▍  | 13226/17834 [6:44:22<2:15:42,  1.77s/it] 74%|███████▍  | 13227/17834 [6:44:24<2:18:54,  1.81s/it] 74%|███████▍  | 13228/17834 [6:44:26<2:17:40,  1.79s/it] 74%|███████▍  | 13229/17834 [6:44:28<2:18:53,  1.81s/it] 74%|███████▍  | 13230/17834 [6:44:29<2:15:20,  1.76s/it] 74%|███████▍  | 13231/17834 [6:44:31<2:15:58,  1.77s/it] 74%|███████▍  | 13232/17834 [6:44:33<2:13:38,  1.74s/it] 74%|███████▍  | 13233/17834 [6:44:34<2:13:21,  1.74s/it] 74%|███████▍  | 13234/17834 [6:44:36<2:12:55,  1.73s/it] 74%|███████▍  | 13235/17834 [6:44:38<2:13:03,  1.74s/it] 74%|███████▍  | 13236/17834 [6:44:40<2:13:44,  1.75s/it] 74%|███████▍  | 13237/17834 [6:44:41<2:14:43,  1.76s/it] 74%|███████▍  | 13238/17834 [6:44:43<2:15:48,  1.77s/it] 74%|███████▍  | 13239/17834 [6:44:45<2:14:16,  1.75s/it] 74%|███████▍  | 13240/17834 [6:44:47<2:14:11,  1.75s/it] 74%|███████▍  | 13241/17834 [6:44:49<2:14:51,  1.76s/it] 74%|███████▍  | 13242/17834 [6:44:50<2:15:14,  1.77s/it] 74%|███████▍  | 13243/17834 [6:44:52<2:13:06,  1.74s/it] 74%|███████▍  | 13244/17834 [6:44:54<2:12:55,  1.74s/it] 74%|███████▍  | 13245/17834 [6:44:55<2:13:18,  1.74s/it] 74%|███████▍  | 13246/17834 [6:44:57<2:13:06,  1.74s/it] 74%|███████▍  | 13247/17834 [6:44:59<2:13:09,  1.74s/it] 74%|███████▍  | 13248/17834 [6:45:01<2:16:00,  1.78s/it] 74%|███████▍  | 13249/17834 [6:45:03<2:14:45,  1.76s/it]08/31/2024 01:59:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1229532957077026, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03989051282405853, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.34647274017334, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5093164443969727}
 74%|███████▍  | 13250/17834 [6:45:04<2:14:36,  1.76s/it] 74%|███████▍  | 13251/17834 [6:45:06<2:13:07,  1.74s/it] 74%|███████▍  | 13252/17834 [6:45:08<2:13:00,  1.74s/it] 74%|███████▍  | 13253/17834 [6:45:09<2:12:34,  1.74s/it] 74%|███████▍  | 13254/17834 [6:45:11<2:13:52,  1.75s/it] 74%|███████▍  | 13255/17834 [6:45:13<2:13:40,  1.75s/it] 74%|███████▍  | 13256/17834 [6:45:15<2:11:52,  1.73s/it] 74%|███████▍  | 13257/17834 [6:45:16<2:11:21,  1.72s/it] 74%|███████▍  | 13258/17834 [6:45:18<2:11:59,  1.73s/it] 74%|███████▍  | 13259/17834 [6:45:20<2:12:12,  1.73s/it] 74%|███████▍  | 13260/17834 [6:45:22<2:14:22,  1.76s/it] 74%|███████▍  | 13261/17834 [6:45:23<2:13:59,  1.76s/it] 74%|███████▍  | 13262/17834 [6:45:25<2:13:30,  1.75s/it] 74%|███████▍  | 13263/17834 [6:45:27<2:13:21,  1.75s/it] 74%|███████▍  | 13264/17834 [6:45:29<2:12:55,  1.75s/it] 74%|███████▍  | 13265/17834 [6:45:30<2:13:39,  1.76s/it] 74%|███████▍  | 13266/17834 [6:45:32<2:12:43,  1.74s/it] 74%|███████▍  | 13267/17834 [6:45:34<2:12:04,  1.74s/it] 74%|███████▍  | 13268/17834 [6:45:36<2:10:50,  1.72s/it] 74%|███████▍  | 13269/17834 [6:45:37<2:13:21,  1.75s/it] 74%|███████▍  | 13270/17834 [6:45:39<2:13:54,  1.76s/it] 74%|███████▍  | 13271/17834 [6:45:41<2:13:10,  1.75s/it] 74%|███████▍  | 13272/17834 [6:45:43<2:13:58,  1.76s/it] 74%|███████▍  | 13273/17834 [6:45:44<2:14:09,  1.76s/it] 74%|███████▍  | 13274/17834 [6:45:46<2:12:59,  1.75s/it] 74%|███████▍  | 13275/17834 [6:45:48<2:14:00,  1.76s/it] 74%|███████▍  | 13276/17834 [6:45:50<2:13:24,  1.76s/it] 74%|███████▍  | 13277/17834 [6:45:51<2:11:49,  1.74s/it] 74%|███████▍  | 13278/17834 [6:45:53<2:12:56,  1.75s/it] 74%|███████▍  | 13279/17834 [6:45:55<2:12:47,  1.75s/it] 74%|███████▍  | 13280/17834 [6:45:57<2:12:38,  1.75s/it] 74%|███████▍  | 13281/17834 [6:45:58<2:12:32,  1.75s/it] 74%|███████▍  | 13282/17834 [6:46:00<2:13:39,  1.76s/it] 74%|███████▍  | 13283/17834 [6:46:02<2:13:04,  1.75s/it] 74%|███████▍  | 13284/17834 [6:46:04<2:16:03,  1.79s/it] 74%|███████▍  | 13285/17834 [6:46:06<2:14:23,  1.77s/it] 74%|███████▍  | 13286/17834 [6:46:07<2:16:47,  1.80s/it] 75%|███████▍  | 13287/17834 [6:46:09<2:14:23,  1.77s/it] 75%|███████▍  | 13288/17834 [6:46:11<2:13:01,  1.76s/it] 75%|███████▍  | 13289/17834 [6:46:13<2:13:45,  1.77s/it] 75%|███████▍  | 13290/17834 [6:46:14<2:12:47,  1.75s/it] 75%|███████▍  | 13291/17834 [6:46:16<2:12:46,  1.75s/it] 75%|███████▍  | 13292/17834 [6:46:18<2:12:56,  1.76s/it] 75%|███████▍  | 13293/17834 [6:46:20<2:11:19,  1.74s/it] 75%|███████▍  | 13294/17834 [6:46:21<2:13:01,  1.76s/it] 75%|███████▍  | 13295/17834 [6:46:23<2:12:09,  1.75s/it] 75%|███████▍  | 13296/17834 [6:46:25<2:12:42,  1.75s/it] 75%|███████▍  | 13297/17834 [6:46:27<2:12:46,  1.76s/it] 75%|███████▍  | 13298/17834 [6:46:28<2:13:52,  1.77s/it] 75%|███████▍  | 13299/17834 [6:46:30<2:12:27,  1.75s/it]08/31/2024 02:00:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0045299530029297, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02757258154451847, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1708247661590576, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.202927350997925}
 75%|███████▍  | 13300/17834 [6:46:32<2:11:42,  1.74s/it] 75%|███████▍  | 13301/17834 [6:46:34<2:11:45,  1.74s/it] 75%|███████▍  | 13302/17834 [6:46:35<2:11:04,  1.74s/it] 75%|███████▍  | 13303/17834 [6:46:37<2:12:28,  1.75s/it] 75%|███████▍  | 13304/17834 [6:46:39<2:09:33,  1.72s/it] 75%|███████▍  | 13305/17834 [6:46:41<2:10:28,  1.73s/it] 75%|███████▍  | 13306/17834 [6:46:42<2:14:25,  1.78s/it] 75%|███████▍  | 13307/17834 [6:46:44<2:12:38,  1.76s/it] 75%|███████▍  | 13308/17834 [6:46:46<2:12:48,  1.76s/it] 75%|███████▍  | 13309/17834 [6:46:48<2:14:01,  1.78s/it] 75%|███████▍  | 13310/17834 [6:46:49<2:13:10,  1.77s/it] 75%|███████▍  | 13311/17834 [6:46:51<2:12:48,  1.76s/it] 75%|███████▍  | 13312/17834 [6:46:53<2:11:45,  1.75s/it] 75%|███████▍  | 13313/17834 [6:46:55<2:11:44,  1.75s/it] 75%|███████▍  | 13314/17834 [6:46:56<2:10:00,  1.73s/it] 75%|███████▍  | 13315/17834 [6:46:58<2:11:52,  1.75s/it] 75%|███████▍  | 13316/17834 [6:47:00<2:12:11,  1.76s/it] 75%|███████▍  | 13317/17834 [6:47:02<2:11:11,  1.74s/it] 75%|███████▍  | 13318/17834 [6:47:03<2:10:06,  1.73s/it] 75%|███████▍  | 13319/17834 [6:47:05<2:10:45,  1.74s/it] 75%|███████▍  | 13320/17834 [6:47:07<2:09:29,  1.72s/it] 75%|███████▍  | 13321/17834 [6:47:09<2:11:51,  1.75s/it] 75%|███████▍  | 13322/17834 [6:47:10<2:10:32,  1.74s/it] 75%|███████▍  | 13323/17834 [6:47:12<2:11:04,  1.74s/it] 75%|███████▍  | 13324/17834 [6:47:14<2:11:16,  1.75s/it] 75%|███████▍  | 13325/17834 [6:47:15<2:10:06,  1.73s/it] 75%|███████▍  | 13326/17834 [6:47:17<2:10:54,  1.74s/it] 75%|███████▍  | 13327/17834 [6:47:19<2:11:26,  1.75s/it] 75%|███████▍  | 13328/17834 [6:47:21<2:12:43,  1.77s/it] 75%|███████▍  | 13329/17834 [6:47:23<2:11:21,  1.75s/it] 75%|███████▍  | 13330/17834 [6:47:24<2:11:57,  1.76s/it] 75%|███████▍  | 13331/17834 [6:47:26<2:11:08,  1.75s/it] 75%|███████▍  | 13332/17834 [6:47:28<2:11:39,  1.75s/it] 75%|███████▍  | 13333/17834 [6:47:29<2:10:04,  1.73s/it] 75%|███████▍  | 13334/17834 [6:47:31<2:09:29,  1.73s/it] 75%|███████▍  | 13335/17834 [6:47:33<2:09:11,  1.72s/it] 75%|███████▍  | 13336/17834 [6:47:35<2:09:03,  1.72s/it] 75%|███████▍  | 13337/17834 [6:47:36<2:09:58,  1.73s/it] 75%|███████▍  | 13338/17834 [6:47:38<2:10:35,  1.74s/it] 75%|███████▍  | 13339/17834 [6:47:40<2:12:50,  1.77s/it] 75%|███████▍  | 13340/17834 [6:47:42<2:11:16,  1.75s/it] 75%|███████▍  | 13341/17834 [6:47:43<2:11:28,  1.76s/it] 75%|███████▍  | 13342/17834 [6:47:45<2:10:49,  1.75s/it] 75%|███████▍  | 13343/17834 [6:47:47<2:12:13,  1.77s/it] 75%|███████▍  | 13344/17834 [6:47:49<2:12:56,  1.78s/it] 75%|███████▍  | 13345/17834 [6:47:51<2:13:01,  1.78s/it] 75%|███████▍  | 13346/17834 [6:47:52<2:12:26,  1.77s/it] 75%|███████▍  | 13347/17834 [6:47:54<2:10:43,  1.75s/it] 75%|███████▍  | 13348/17834 [6:47:56<2:09:57,  1.74s/it] 75%|███████▍  | 13349/17834 [6:47:57<2:09:49,  1.74s/it]08/31/2024 02:02:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3192636966705322, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03350866213440895, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.287815570831299, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.64058780670166}
 75%|███████▍  | 13350/17834 [6:47:59<2:11:00,  1.75s/it] 75%|███████▍  | 13351/17834 [6:48:01<2:10:31,  1.75s/it] 75%|███████▍  | 13352/17834 [6:48:03<2:12:27,  1.77s/it] 75%|███████▍  | 13353/17834 [6:48:05<2:12:11,  1.77s/it] 75%|███████▍  | 13354/17834 [6:48:06<2:11:21,  1.76s/it] 75%|███████▍  | 13355/17834 [6:48:08<2:09:32,  1.74s/it] 75%|███████▍  | 13356/17834 [6:48:10<2:11:10,  1.76s/it] 75%|███████▍  | 13357/17834 [6:48:12<2:09:18,  1.73s/it] 75%|███████▍  | 13358/17834 [6:48:13<2:10:15,  1.75s/it] 75%|███████▍  | 13359/17834 [6:48:15<2:10:08,  1.74s/it] 75%|███████▍  | 13360/17834 [6:48:17<2:09:26,  1.74s/it] 75%|███████▍  | 13361/17834 [6:48:19<2:10:26,  1.75s/it] 75%|███████▍  | 13362/17834 [6:48:20<2:11:40,  1.77s/it] 75%|███████▍  | 13363/17834 [6:48:22<2:10:13,  1.75s/it] 75%|███████▍  | 13364/17834 [6:48:24<2:11:10,  1.76s/it] 75%|███████▍  | 13365/17834 [6:48:26<2:11:32,  1.77s/it] 75%|███████▍  | 13366/17834 [6:48:27<2:11:51,  1.77s/it] 75%|███████▍  | 13367/17834 [6:48:29<2:10:32,  1.75s/it] 75%|███████▍  | 13368/17834 [6:48:31<2:11:08,  1.76s/it] 75%|███████▍  | 13369/17834 [6:48:33<2:10:41,  1.76s/it] 75%|███████▍  | 13370/17834 [6:48:34<2:10:57,  1.76s/it] 75%|███████▍  | 13371/17834 [6:48:36<2:10:07,  1.75s/it] 75%|███████▍  | 13372/17834 [6:48:38<2:10:37,  1.76s/it] 75%|███████▍  | 13373/17834 [6:48:40<2:09:54,  1.75s/it] 75%|███████▍  | 13374/17834 [6:48:41<2:09:23,  1.74s/it] 75%|███████▍  | 13375/17834 [6:48:43<2:09:36,  1.74s/it] 75%|███████▌  | 13376/17834 [6:48:45<2:08:39,  1.73s/it] 75%|███████▌  | 13377/17834 [6:48:46<2:07:41,  1.72s/it] 75%|███████▌  | 13378/17834 [6:48:48<2:09:22,  1.74s/it] 75%|███████▌  | 13379/17834 [6:48:50<2:09:44,  1.75s/it] 75%|███████▌  | 13380/17834 [6:48:52<2:08:55,  1.74s/it] 75%|███████▌  | 13381/17834 [6:48:53<2:08:27,  1.73s/it] 75%|███████▌  | 13382/17834 [6:48:55<2:09:08,  1.74s/it] 75%|███████▌  | 13383/17834 [6:48:57<2:08:37,  1.73s/it] 75%|███████▌  | 13384/17834 [6:48:59<2:09:33,  1.75s/it] 75%|███████▌  | 13385/17834 [6:49:00<2:09:33,  1.75s/it] 75%|███████▌  | 13386/17834 [6:49:02<2:09:12,  1.74s/it] 75%|███████▌  | 13387/17834 [6:49:04<2:07:39,  1.72s/it] 75%|███████▌  | 13388/17834 [6:49:06<2:08:34,  1.74s/it] 75%|███████▌  | 13389/17834 [6:49:07<2:09:15,  1.74s/it] 75%|███████▌  | 13390/17834 [6:49:09<2:08:48,  1.74s/it] 75%|███████▌  | 13391/17834 [6:49:11<2:09:04,  1.74s/it] 75%|███████▌  | 13392/17834 [6:49:13<2:08:29,  1.74s/it] 75%|███████▌  | 13393/17834 [6:49:14<2:08:39,  1.74s/it] 75%|███████▌  | 13394/17834 [6:49:16<2:08:28,  1.74s/it] 75%|███████▌  | 13395/17834 [6:49:18<2:10:06,  1.76s/it] 75%|███████▌  | 13396/17834 [6:49:20<2:09:06,  1.75s/it] 75%|███████▌  | 13397/17834 [6:49:21<2:11:06,  1.77s/it] 75%|███████▌  | 13398/17834 [6:49:23<2:10:27,  1.76s/it] 75%|███████▌  | 13399/17834 [6:49:25<2:11:02,  1.77s/it]08/31/2024 02:03:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0970141887664795, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03136323764920235, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.171248197555542, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2996256351470947}
 75%|███████▌  | 13400/17834 [6:49:27<2:10:36,  1.77s/it] 75%|███████▌  | 13401/17834 [6:49:29<2:11:33,  1.78s/it] 75%|███████▌  | 13402/17834 [6:49:30<2:09:21,  1.75s/it] 75%|███████▌  | 13403/17834 [6:49:32<2:12:24,  1.79s/it] 75%|███████▌  | 13404/17834 [6:49:34<2:09:36,  1.76s/it] 75%|███████▌  | 13405/17834 [6:49:36<2:08:29,  1.74s/it] 75%|███████▌  | 13406/17834 [6:49:37<2:10:28,  1.77s/it] 75%|███████▌  | 13407/17834 [6:49:39<2:10:44,  1.77s/it] 75%|███████▌  | 13408/17834 [6:49:41<2:11:50,  1.79s/it] 75%|███████▌  | 13409/17834 [6:49:43<2:10:01,  1.76s/it] 75%|███████▌  | 13410/17834 [6:49:44<2:08:01,  1.74s/it] 75%|███████▌  | 13411/17834 [6:49:46<2:07:45,  1.73s/it] 75%|███████▌  | 13412/17834 [6:49:48<2:07:08,  1.73s/it] 75%|███████▌  | 13413/17834 [6:49:49<2:07:19,  1.73s/it] 75%|███████▌  | 13414/17834 [6:49:51<2:09:01,  1.75s/it] 75%|███████▌  | 13415/17834 [6:49:53<2:09:31,  1.76s/it] 75%|███████▌  | 13416/17834 [6:49:55<2:09:39,  1.76s/it] 75%|███████▌  | 13417/17834 [6:49:57<2:08:20,  1.74s/it] 75%|███████▌  | 13418/17834 [6:49:58<2:09:13,  1.76s/it] 75%|███████▌  | 13419/17834 [6:50:00<2:08:41,  1.75s/it] 75%|███████▌  | 13420/17834 [6:50:02<2:07:56,  1.74s/it] 75%|███████▌  | 13421/17834 [6:50:04<2:07:52,  1.74s/it] 75%|███████▌  | 13422/17834 [6:50:05<2:07:04,  1.73s/it] 75%|███████▌  | 13423/17834 [6:50:07<2:08:44,  1.75s/it] 75%|███████▌  | 13424/17834 [6:50:09<2:08:35,  1.75s/it] 75%|███████▌  | 13425/17834 [6:50:11<2:10:11,  1.77s/it] 75%|███████▌  | 13426/17834 [6:50:12<2:09:59,  1.77s/it] 75%|███████▌  | 13427/17834 [6:50:14<2:08:54,  1.76s/it] 75%|███████▌  | 13428/17834 [6:50:16<2:07:19,  1.73s/it] 75%|███████▌  | 13429/17834 [6:50:17<2:06:43,  1.73s/it] 75%|███████▌  | 13430/17834 [6:50:19<2:07:59,  1.74s/it] 75%|███████▌  | 13431/17834 [6:50:21<2:07:05,  1.73s/it] 75%|███████▌  | 13432/17834 [6:50:23<2:08:50,  1.76s/it] 75%|███████▌  | 13433/17834 [6:50:25<2:09:03,  1.76s/it] 75%|███████▌  | 13434/17834 [6:50:26<2:10:27,  1.78s/it] 75%|███████▌  | 13435/17834 [6:50:28<2:09:41,  1.77s/it] 75%|███████▌  | 13436/17834 [6:50:30<2:09:01,  1.76s/it] 75%|███████▌  | 13437/17834 [6:50:32<2:08:07,  1.75s/it] 75%|███████▌  | 13438/17834 [6:50:33<2:07:33,  1.74s/it] 75%|███████▌  | 13439/17834 [6:50:35<2:09:04,  1.76s/it] 75%|███████▌  | 13440/17834 [6:50:37<2:08:17,  1.75s/it] 75%|███████▌  | 13441/17834 [6:50:39<2:08:46,  1.76s/it] 75%|███████▌  | 13442/17834 [6:50:40<2:07:40,  1.74s/it] 75%|███████▌  | 13443/17834 [6:50:42<2:08:24,  1.75s/it] 75%|███████▌  | 13444/17834 [6:50:44<2:10:52,  1.79s/it] 75%|███████▌  | 13445/17834 [6:50:46<2:09:53,  1.78s/it] 75%|███████▌  | 13446/17834 [6:50:47<2:09:57,  1.78s/it] 75%|███████▌  | 13447/17834 [6:50:49<2:08:02,  1.75s/it] 75%|███████▌  | 13448/17834 [6:50:51<2:07:39,  1.75s/it] 75%|███████▌  | 13449/17834 [6:50:53<2:09:39,  1.77s/it]08/31/2024 02:05:12 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1016465425491333, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04025912284851074, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2803664207458496, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.422272205352783}
 75%|███████▌  | 13450/17834 [6:50:54<2:08:12,  1.75s/it] 75%|███████▌  | 13451/17834 [6:50:56<2:07:12,  1.74s/it] 75%|███████▌  | 13452/17834 [6:50:58<2:06:21,  1.73s/it] 75%|███████▌  | 13453/17834 [6:51:00<2:07:37,  1.75s/it] 75%|███████▌  | 13454/17834 [6:51:01<2:06:17,  1.73s/it] 75%|███████▌  | 13455/17834 [6:51:03<2:06:02,  1.73s/it] 75%|███████▌  | 13456/17834 [6:51:05<2:06:38,  1.74s/it] 75%|███████▌  | 13457/17834 [6:51:07<2:06:01,  1.73s/it] 75%|███████▌  | 13458/17834 [6:51:08<2:06:39,  1.74s/it] 75%|███████▌  | 13459/17834 [6:51:10<2:06:42,  1.74s/it] 75%|███████▌  | 13460/17834 [6:51:12<2:06:50,  1.74s/it] 75%|███████▌  | 13461/17834 [6:51:14<2:08:23,  1.76s/it] 75%|███████▌  | 13462/17834 [6:51:15<2:06:18,  1.73s/it] 75%|███████▌  | 13463/17834 [6:51:17<2:05:03,  1.72s/it] 75%|███████▌  | 13464/17834 [6:51:19<2:05:01,  1.72s/it] 76%|███████▌  | 13465/17834 [6:51:20<2:07:26,  1.75s/it] 76%|███████▌  | 13466/17834 [6:51:22<2:06:03,  1.73s/it] 76%|███████▌  | 13467/17834 [6:51:24<2:06:40,  1.74s/it] 76%|███████▌  | 13468/17834 [6:51:26<2:05:20,  1.72s/it] 76%|███████▌  | 13469/17834 [6:51:27<2:08:05,  1.76s/it] 76%|███████▌  | 13470/17834 [6:51:29<2:06:53,  1.74s/it] 76%|███████▌  | 13471/17834 [6:51:31<2:07:55,  1.76s/it] 76%|███████▌  | 13472/17834 [6:51:33<2:06:37,  1.74s/it] 76%|███████▌  | 13473/17834 [6:51:34<2:07:39,  1.76s/it] 76%|███████▌  | 13474/17834 [6:51:36<2:07:31,  1.75s/it] 76%|███████▌  | 13475/17834 [6:51:38<2:07:46,  1.76s/it] 76%|███████▌  | 13476/17834 [6:51:40<2:07:24,  1.75s/it] 76%|███████▌  | 13477/17834 [6:51:41<2:06:50,  1.75s/it] 76%|███████▌  | 13478/17834 [6:51:43<2:08:49,  1.77s/it] 76%|███████▌  | 13479/17834 [6:51:45<2:08:48,  1.77s/it] 76%|███████▌  | 13480/17834 [6:51:47<2:08:09,  1.77s/it] 76%|███████▌  | 13481/17834 [6:51:49<2:07:11,  1.75s/it] 76%|███████▌  | 13482/17834 [6:51:50<2:08:59,  1.78s/it] 76%|███████▌  | 13483/17834 [6:51:52<2:09:15,  1.78s/it] 76%|███████▌  | 13484/17834 [6:51:54<2:07:27,  1.76s/it] 76%|███████▌  | 13485/17834 [6:51:56<2:09:13,  1.78s/it] 76%|███████▌  | 13486/17834 [6:51:57<2:08:19,  1.77s/it] 76%|███████▌  | 13487/17834 [6:51:59<2:07:14,  1.76s/it] 76%|███████▌  | 13488/17834 [6:52:01<2:06:18,  1.74s/it] 76%|███████▌  | 13489/17834 [6:52:03<2:08:03,  1.77s/it] 76%|███████▌  | 13490/17834 [6:52:04<2:07:00,  1.75s/it] 76%|███████▌  | 13491/17834 [6:52:06<2:07:09,  1.76s/it] 76%|███████▌  | 13492/17834 [6:52:08<2:07:06,  1.76s/it] 76%|███████▌  | 13493/17834 [6:52:10<2:06:21,  1.75s/it] 76%|███████▌  | 13494/17834 [6:52:11<2:06:46,  1.75s/it] 76%|███████▌  | 13495/17834 [6:52:13<2:06:49,  1.75s/it] 76%|███████▌  | 13496/17834 [6:52:15<2:06:50,  1.75s/it] 76%|███████▌  | 13497/17834 [6:52:17<2:04:28,  1.72s/it] 76%|███████▌  | 13498/17834 [6:52:18<2:05:03,  1.73s/it] 76%|███████▌  | 13499/17834 [6:52:20<2:06:36,  1.75s/it]08/31/2024 02:06:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9585973024368286, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024333376437425613, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.173548698425293, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1564793586730957}
 76%|███████▌  | 13500/17834 [6:52:22<2:05:45,  1.74s/it] 76%|███████▌  | 13501/17834 [6:52:24<2:07:16,  1.76s/it] 76%|███████▌  | 13502/17834 [6:52:25<2:07:40,  1.77s/it] 76%|███████▌  | 13503/17834 [6:52:27<2:06:53,  1.76s/it] 76%|███████▌  | 13504/17834 [6:52:29<2:05:08,  1.73s/it] 76%|███████▌  | 13505/17834 [6:52:31<2:04:33,  1.73s/it] 76%|███████▌  | 13506/17834 [6:52:32<2:04:36,  1.73s/it] 76%|███████▌  | 13507/17834 [6:52:34<2:05:27,  1.74s/it] 76%|███████▌  | 13508/17834 [6:52:36<2:07:47,  1.77s/it] 76%|███████▌  | 13509/17834 [6:52:38<2:08:19,  1.78s/it] 76%|███████▌  | 13510/17834 [6:52:40<2:08:53,  1.79s/it] 76%|███████▌  | 13511/17834 [6:52:41<2:08:51,  1.79s/it] 76%|███████▌  | 13512/17834 [6:52:43<2:08:36,  1.79s/it] 76%|███████▌  | 13513/17834 [6:52:45<2:06:05,  1.75s/it] 76%|███████▌  | 13514/17834 [6:52:46<2:05:32,  1.74s/it] 76%|███████▌  | 13515/17834 [6:52:48<2:05:47,  1.75s/it] 76%|███████▌  | 13516/17834 [6:52:50<2:05:34,  1.74s/it] 76%|███████▌  | 13517/17834 [6:52:52<2:04:45,  1.73s/it] 76%|███████▌  | 13518/17834 [6:52:53<2:05:47,  1.75s/it] 76%|███████▌  | 13519/17834 [6:52:55<2:05:15,  1.74s/it] 76%|███████▌  | 13520/17834 [6:52:57<2:06:05,  1.75s/it] 76%|███████▌  | 13521/17834 [6:52:59<2:06:03,  1.75s/it] 76%|███████▌  | 13522/17834 [6:53:01<2:06:52,  1.77s/it] 76%|███████▌  | 13523/17834 [6:53:02<2:08:25,  1.79s/it] 76%|███████▌  | 13524/17834 [6:53:04<2:06:44,  1.76s/it] 76%|███████▌  | 13525/17834 [6:53:06<2:06:06,  1.76s/it] 76%|███████▌  | 13526/17834 [6:53:08<2:06:06,  1.76s/it] 76%|███████▌  | 13527/17834 [6:53:09<2:04:33,  1.74s/it] 76%|███████▌  | 13528/17834 [6:53:11<2:04:27,  1.73s/it] 76%|███████▌  | 13529/17834 [6:53:13<2:05:33,  1.75s/it] 76%|███████▌  | 13530/17834 [6:53:15<2:05:22,  1.75s/it] 76%|███████▌  | 13531/17834 [6:53:16<2:05:22,  1.75s/it] 76%|███████▌  | 13532/17834 [6:53:18<2:06:33,  1.77s/it] 76%|███████▌  | 13533/17834 [6:53:20<2:05:59,  1.76s/it] 76%|███████▌  | 13534/17834 [6:53:22<2:05:55,  1.76s/it] 76%|███████▌  | 13535/17834 [6:53:23<2:05:31,  1.75s/it] 76%|███████▌  | 13536/17834 [6:53:25<2:04:51,  1.74s/it] 76%|███████▌  | 13537/17834 [6:53:27<2:07:32,  1.78s/it] 76%|███████▌  | 13538/17834 [6:53:29<2:05:30,  1.75s/it] 76%|███████▌  | 13539/17834 [6:53:30<2:06:09,  1.76s/it] 76%|███████▌  | 13540/17834 [6:53:32<2:04:48,  1.74s/it] 76%|███████▌  | 13541/17834 [6:53:34<2:05:56,  1.76s/it] 76%|███████▌  | 13542/17834 [6:53:36<2:05:36,  1.76s/it] 76%|███████▌  | 13543/17834 [6:53:37<2:03:37,  1.73s/it] 76%|███████▌  | 13544/17834 [6:53:39<2:03:22,  1.73s/it] 76%|███████▌  | 13545/17834 [6:53:41<2:02:45,  1.72s/it] 76%|███████▌  | 13546/17834 [6:53:42<2:02:48,  1.72s/it] 76%|███████▌  | 13547/17834 [6:53:44<2:03:15,  1.73s/it] 76%|███████▌  | 13548/17834 [6:53:46<2:03:52,  1.73s/it] 76%|███████▌  | 13549/17834 [6:53:48<2:06:34,  1.77s/it]08/31/2024 02:08:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3941407203674316, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028168052434921265, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3049769401550293, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.727285861968994}
 76%|███████▌  | 13550/17834 [6:53:49<2:04:04,  1.74s/it] 76%|███████▌  | 13551/17834 [6:53:51<2:04:12,  1.74s/it] 76%|███████▌  | 13552/17834 [6:53:53<2:03:22,  1.73s/it] 76%|███████▌  | 13553/17834 [6:53:55<2:04:00,  1.74s/it] 76%|███████▌  | 13554/17834 [6:53:56<2:03:46,  1.74s/it] 76%|███████▌  | 13555/17834 [6:53:58<2:03:07,  1.73s/it] 76%|███████▌  | 13556/17834 [6:54:00<2:03:22,  1.73s/it] 76%|███████▌  | 13557/17834 [6:54:02<2:04:28,  1.75s/it] 76%|███████▌  | 13558/17834 [6:54:03<2:03:44,  1.74s/it] 76%|███████▌  | 13559/17834 [6:54:05<2:05:28,  1.76s/it] 76%|███████▌  | 13560/17834 [6:54:07<2:05:14,  1.76s/it] 76%|███████▌  | 13561/17834 [6:54:09<2:03:54,  1.74s/it] 76%|███████▌  | 13562/17834 [6:54:10<2:02:50,  1.73s/it] 76%|███████▌  | 13563/17834 [6:54:12<2:05:57,  1.77s/it] 76%|███████▌  | 13564/17834 [6:54:14<2:04:45,  1.75s/it] 76%|███████▌  | 13565/17834 [6:54:16<2:04:18,  1.75s/it] 76%|███████▌  | 13566/17834 [6:54:17<2:04:18,  1.75s/it] 76%|███████▌  | 13567/17834 [6:54:19<2:04:07,  1.75s/it] 76%|███████▌  | 13568/17834 [6:54:21<2:03:28,  1.74s/it] 76%|███████▌  | 13569/17834 [6:54:23<2:06:18,  1.78s/it] 76%|███████▌  | 13570/17834 [6:54:24<2:05:58,  1.77s/it] 76%|███████▌  | 13571/17834 [6:54:26<2:04:20,  1.75s/it] 76%|███████▌  | 13572/17834 [6:54:28<2:04:07,  1.75s/it] 76%|███████▌  | 13573/17834 [6:54:30<2:04:13,  1.75s/it] 76%|███████▌  | 13574/17834 [6:54:31<2:04:04,  1.75s/it] 76%|███████▌  | 13575/17834 [6:54:33<2:04:45,  1.76s/it] 76%|███████▌  | 13576/17834 [6:54:35<2:03:42,  1.74s/it] 76%|███████▌  | 13577/17834 [6:54:37<2:04:51,  1.76s/it] 76%|███████▌  | 13578/17834 [6:54:38<2:03:07,  1.74s/it] 76%|███████▌  | 13579/17834 [6:54:40<2:03:30,  1.74s/it] 76%|███████▌  | 13580/17834 [6:54:42<2:04:03,  1.75s/it] 76%|███████▌  | 13581/17834 [6:54:44<2:04:08,  1.75s/it] 76%|███████▌  | 13582/17834 [6:54:45<2:05:03,  1.76s/it] 76%|███████▌  | 13583/17834 [6:54:47<2:05:05,  1.77s/it] 76%|███████▌  | 13584/17834 [6:54:49<2:05:33,  1.77s/it] 76%|███████▌  | 13585/17834 [6:54:51<2:04:17,  1.76s/it] 76%|███████▌  | 13586/17834 [6:54:52<2:03:30,  1.74s/it] 76%|███████▌  | 13587/17834 [6:54:54<2:03:19,  1.74s/it] 76%|███████▌  | 13588/17834 [6:54:56<2:03:01,  1.74s/it] 76%|███████▌  | 13589/17834 [6:54:58<2:04:18,  1.76s/it] 76%|███████▌  | 13590/17834 [6:55:00<2:05:52,  1.78s/it] 76%|███████▌  | 13591/17834 [6:55:01<2:05:13,  1.77s/it] 76%|███████▌  | 13592/17834 [6:55:03<2:04:57,  1.77s/it] 76%|███████▌  | 13593/17834 [6:55:05<2:03:47,  1.75s/it] 76%|███████▌  | 13594/17834 [6:55:06<2:04:02,  1.76s/it] 76%|███████▌  | 13595/17834 [6:55:08<2:05:22,  1.77s/it] 76%|███████▌  | 13596/17834 [6:55:10<2:05:03,  1.77s/it] 76%|███████▌  | 13597/17834 [6:55:12<2:05:51,  1.78s/it] 76%|███████▌  | 13598/17834 [6:55:14<2:05:03,  1.77s/it] 76%|███████▋  | 13599/17834 [6:55:15<2:06:51,  1.80s/it]08/31/2024 02:09:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9745469093322754, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.027102872729301453, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1268465518951416, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1284961700439453}
 76%|███████▋  | 13600/17834 [6:55:17<2:06:18,  1.79s/it] 76%|███████▋  | 13601/17834 [6:55:19<2:08:06,  1.82s/it] 76%|███████▋  | 13602/17834 [6:55:21<2:06:35,  1.79s/it] 76%|███████▋  | 13603/17834 [6:55:23<2:04:20,  1.76s/it] 76%|███████▋  | 13604/17834 [6:55:24<2:03:18,  1.75s/it] 76%|███████▋  | 13605/17834 [6:55:26<2:03:23,  1.75s/it] 76%|███████▋  | 13606/17834 [6:55:28<2:02:46,  1.74s/it] 76%|███████▋  | 13607/17834 [6:55:30<2:02:55,  1.74s/it] 76%|███████▋  | 13608/17834 [6:55:31<2:02:44,  1.74s/it] 76%|███████▋  | 13609/17834 [6:55:33<2:01:18,  1.72s/it] 76%|███████▋  | 13610/17834 [6:55:35<2:01:37,  1.73s/it] 76%|███████▋  | 13611/17834 [6:55:36<2:03:25,  1.75s/it] 76%|███████▋  | 13612/17834 [6:55:38<2:03:36,  1.76s/it] 76%|███████▋  | 13613/17834 [6:55:40<2:03:16,  1.75s/it] 76%|███████▋  | 13614/17834 [6:55:42<2:02:05,  1.74s/it] 76%|███████▋  | 13615/17834 [6:55:43<2:01:27,  1.73s/it] 76%|███████▋  | 13616/17834 [6:55:45<2:03:40,  1.76s/it] 76%|███████▋  | 13617/17834 [6:55:47<2:02:45,  1.75s/it] 76%|███████▋  | 13618/17834 [6:55:49<2:01:26,  1.73s/it] 76%|███████▋  | 13619/17834 [6:55:50<2:01:21,  1.73s/it] 76%|███████▋  | 13620/17834 [6:55:52<2:02:30,  1.74s/it] 76%|███████▋  | 13621/17834 [6:55:54<2:01:34,  1.73s/it] 76%|███████▋  | 13622/17834 [6:55:56<2:02:55,  1.75s/it] 76%|███████▋  | 13623/17834 [6:55:57<2:03:39,  1.76s/it] 76%|███████▋  | 13624/17834 [6:55:59<2:02:19,  1.74s/it] 76%|███████▋  | 13625/17834 [6:56:01<2:05:52,  1.79s/it] 76%|███████▋  | 13626/17834 [6:56:03<2:03:32,  1.76s/it] 76%|███████▋  | 13627/17834 [6:56:04<2:03:06,  1.76s/it] 76%|███████▋  | 13628/17834 [6:56:06<2:03:42,  1.76s/it] 76%|███████▋  | 13629/17834 [6:56:08<2:04:06,  1.77s/it] 76%|███████▋  | 13630/17834 [6:56:10<2:03:19,  1.76s/it] 76%|███████▋  | 13631/17834 [6:56:12<2:02:58,  1.76s/it] 76%|███████▋  | 13632/17834 [6:56:13<2:02:45,  1.75s/it] 76%|███████▋  | 13633/17834 [6:56:15<2:02:37,  1.75s/it] 76%|███████▋  | 13634/17834 [6:56:17<2:01:21,  1.73s/it] 76%|███████▋  | 13635/17834 [6:56:19<2:02:38,  1.75s/it] 76%|███████▋  | 13636/17834 [6:56:20<2:01:45,  1.74s/it] 76%|███████▋  | 13637/17834 [6:56:22<2:03:06,  1.76s/it] 76%|███████▋  | 13638/17834 [6:56:24<2:01:05,  1.73s/it] 76%|███████▋  | 13639/17834 [6:56:26<2:03:09,  1.76s/it] 76%|███████▋  | 13640/17834 [6:56:27<2:04:08,  1.78s/it] 76%|███████▋  | 13641/17834 [6:56:29<2:02:09,  1.75s/it] 76%|███████▋  | 13642/17834 [6:56:31<2:02:00,  1.75s/it] 76%|███████▋  | 13643/17834 [6:56:32<2:01:09,  1.73s/it] 77%|███████▋  | 13644/17834 [6:56:34<2:00:25,  1.72s/it] 77%|███████▋  | 13645/17834 [6:56:36<2:04:35,  1.78s/it] 77%|███████▋  | 13646/17834 [6:56:38<2:01:58,  1.75s/it] 77%|███████▋  | 13647/17834 [6:56:40<2:03:08,  1.76s/it] 77%|███████▋  | 13648/17834 [6:56:41<2:01:28,  1.74s/it] 77%|███████▋  | 13649/17834 [6:56:43<2:05:19,  1.80s/it]08/31/2024 02:11:02 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9011709094047546, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01739172823727131, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1339271068573, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.052489757537842}
 77%|███████▋  | 13650/17834 [6:56:45<2:03:37,  1.77s/it] 77%|███████▋  | 13651/17834 [6:56:47<2:03:08,  1.77s/it] 77%|███████▋  | 13652/17834 [6:56:48<2:03:35,  1.77s/it] 77%|███████▋  | 13653/17834 [6:56:50<2:02:48,  1.76s/it] 77%|███████▋  | 13654/17834 [6:56:52<2:04:04,  1.78s/it] 77%|███████▋  | 13655/17834 [6:56:54<2:03:52,  1.78s/it] 77%|███████▋  | 13656/17834 [6:56:56<2:04:41,  1.79s/it] 77%|███████▋  | 13657/17834 [6:56:57<2:04:41,  1.79s/it] 77%|███████▋  | 13658/17834 [6:56:59<2:03:36,  1.78s/it] 77%|███████▋  | 13659/17834 [6:57:01<2:03:02,  1.77s/it] 77%|███████▋  | 13660/17834 [6:57:03<2:04:18,  1.79s/it] 77%|███████▋  | 13661/17834 [6:57:04<2:03:24,  1.77s/it] 77%|███████▋  | 13662/17834 [6:57:06<2:01:51,  1.75s/it] 77%|███████▋  | 13663/17834 [6:57:08<2:01:57,  1.75s/it] 77%|███████▋  | 13664/17834 [6:57:10<2:01:01,  1.74s/it] 77%|███████▋  | 13665/17834 [6:57:11<2:01:47,  1.75s/it] 77%|███████▋  | 13666/17834 [6:57:13<2:02:45,  1.77s/it] 77%|███████▋  | 13667/17834 [6:57:15<2:01:14,  1.75s/it] 77%|███████▋  | 13668/17834 [6:57:17<2:00:32,  1.74s/it] 77%|███████▋  | 13669/17834 [6:57:18<1:59:05,  1.72s/it] 77%|███████▋  | 13670/17834 [6:57:20<1:59:11,  1.72s/it] 77%|███████▋  | 13671/17834 [6:57:22<1:59:46,  1.73s/it] 77%|███████▋  | 13672/17834 [6:57:23<2:00:33,  1.74s/it] 77%|███████▋  | 13673/17834 [6:57:25<2:00:13,  1.73s/it] 77%|███████▋  | 13674/17834 [6:57:27<2:01:07,  1.75s/it] 77%|███████▋  | 13675/17834 [6:57:29<2:01:38,  1.75s/it] 77%|███████▋  | 13676/17834 [6:57:30<2:00:40,  1.74s/it] 77%|███████▋  | 13677/17834 [6:57:32<1:59:40,  1.73s/it] 77%|███████▋  | 13678/17834 [6:57:34<2:02:27,  1.77s/it] 77%|███████▋  | 13679/17834 [6:57:36<2:01:55,  1.76s/it] 77%|███████▋  | 13680/17834 [6:57:38<2:02:07,  1.76s/it] 77%|███████▋  | 13681/17834 [6:57:39<2:03:17,  1.78s/it] 77%|███████▋  | 13682/17834 [6:57:41<2:02:22,  1.77s/it] 77%|███████▋  | 13683/17834 [6:57:43<2:02:29,  1.77s/it] 77%|███████▋  | 13684/17834 [6:57:45<2:02:23,  1.77s/it] 77%|███████▋  | 13685/17834 [6:57:46<2:01:03,  1.75s/it] 77%|███████▋  | 13686/17834 [6:57:48<2:00:01,  1.74s/it] 77%|███████▋  | 13687/17834 [6:57:50<1:59:43,  1.73s/it] 77%|███████▋  | 13688/17834 [6:57:52<2:01:06,  1.75s/it] 77%|███████▋  | 13689/17834 [6:57:53<2:00:44,  1.75s/it] 77%|███████▋  | 13690/17834 [6:57:55<2:00:22,  1.74s/it] 77%|███████▋  | 13691/17834 [6:57:57<2:00:36,  1.75s/it] 77%|███████▋  | 13692/17834 [6:57:59<2:01:44,  1.76s/it] 77%|███████▋  | 13693/17834 [6:58:00<2:02:41,  1.78s/it] 77%|███████▋  | 13694/17834 [6:58:02<2:00:20,  1.74s/it] 77%|███████▋  | 13695/17834 [6:58:04<1:59:23,  1.73s/it] 77%|███████▋  | 13696/17834 [6:58:06<1:59:17,  1.73s/it] 77%|███████▋  | 13697/17834 [6:58:07<2:00:05,  1.74s/it] 77%|███████▋  | 13698/17834 [6:58:09<2:02:10,  1.77s/it] 77%|███████▋  | 13699/17834 [6:58:11<2:01:28,  1.76s/it]08/31/2024 02:12:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.324091911315918, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05256829410791397, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.319491386413574, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6961517333984375}
 77%|███████▋  | 13700/17834 [6:58:13<1:59:39,  1.74s/it] 77%|███████▋  | 13701/17834 [6:58:14<1:58:53,  1.73s/it] 77%|███████▋  | 13702/17834 [6:58:16<2:00:57,  1.76s/it] 77%|███████▋  | 13703/17834 [6:58:18<2:00:18,  1.75s/it] 77%|███████▋  | 13704/17834 [6:58:19<1:58:56,  1.73s/it] 77%|███████▋  | 13705/17834 [6:58:21<1:59:09,  1.73s/it] 77%|███████▋  | 13706/17834 [6:58:23<1:59:18,  1.73s/it] 77%|███████▋  | 13707/17834 [6:58:25<1:59:09,  1.73s/it] 77%|███████▋  | 13708/17834 [6:58:26<1:59:42,  1.74s/it] 77%|███████▋  | 13709/17834 [6:58:28<2:00:36,  1.75s/it] 77%|███████▋  | 13710/17834 [6:58:30<2:01:20,  1.77s/it] 77%|███████▋  | 13711/17834 [6:58:32<2:00:09,  1.75s/it] 77%|███████▋  | 13712/17834 [6:58:33<1:59:40,  1.74s/it] 77%|███████▋  | 13713/17834 [6:58:35<1:59:23,  1.74s/it] 77%|███████▋  | 13714/17834 [6:58:37<1:59:48,  1.74s/it] 77%|███████▋  | 13715/17834 [6:58:39<1:59:01,  1.73s/it] 77%|███████▋  | 13716/17834 [6:58:40<1:58:43,  1.73s/it] 77%|███████▋  | 13717/17834 [6:58:42<1:58:32,  1.73s/it] 77%|███████▋  | 13718/17834 [6:58:44<1:58:08,  1.72s/it] 77%|███████▋  | 13719/17834 [6:58:46<1:59:45,  1.75s/it] 77%|███████▋  | 13720/17834 [6:58:47<2:00:56,  1.76s/it] 77%|███████▋  | 13721/17834 [6:58:49<2:00:58,  1.76s/it] 77%|███████▋  | 13722/17834 [6:58:51<2:00:55,  1.76s/it] 77%|███████▋  | 13723/17834 [6:58:53<2:01:15,  1.77s/it] 77%|███████▋  | 13724/17834 [6:58:54<1:58:37,  1.73s/it] 77%|███████▋  | 13725/17834 [6:58:56<2:00:03,  1.75s/it] 77%|███████▋  | 13726/17834 [6:58:58<1:59:28,  1.74s/it] 77%|███████▋  | 13727/17834 [6:59:00<1:58:55,  1.74s/it] 77%|███████▋  | 13728/17834 [6:59:01<1:58:06,  1.73s/it] 77%|███████▋  | 13729/17834 [6:59:03<1:59:31,  1.75s/it] 77%|███████▋  | 13730/17834 [6:59:05<2:00:51,  1.77s/it] 77%|███████▋  | 13731/17834 [6:59:07<2:02:48,  1.80s/it] 77%|███████▋  | 13732/17834 [6:59:09<2:01:32,  1.78s/it] 77%|███████▋  | 13733/17834 [6:59:10<2:00:47,  1.77s/it] 77%|███████▋  | 13734/17834 [6:59:12<1:59:22,  1.75s/it] 77%|███████▋  | 13735/17834 [6:59:14<1:58:20,  1.73s/it] 77%|███████▋  | 13736/17834 [6:59:15<1:58:46,  1.74s/it] 77%|███████▋  | 13737/17834 [6:59:17<1:57:49,  1.73s/it] 77%|███████▋  | 13738/17834 [6:59:19<1:58:05,  1.73s/it] 77%|███████▋  | 13739/17834 [6:59:21<1:57:53,  1.73s/it] 77%|███████▋  | 13740/17834 [6:59:22<1:59:22,  1.75s/it] 77%|███████▋  | 13741/17834 [6:59:24<1:59:43,  1.75s/it] 77%|███████▋  | 13742/17834 [6:59:26<1:59:05,  1.75s/it] 77%|███████▋  | 13743/17834 [6:59:28<1:59:54,  1.76s/it] 77%|███████▋  | 13744/17834 [6:59:29<1:58:32,  1.74s/it] 77%|███████▋  | 13745/17834 [6:59:31<1:58:47,  1.74s/it] 77%|███████▋  | 13746/17834 [6:59:33<1:58:27,  1.74s/it] 77%|███████▋  | 13747/17834 [6:59:35<1:59:24,  1.75s/it] 77%|███████▋  | 13748/17834 [6:59:36<2:00:07,  1.76s/it] 77%|███████▋  | 13749/17834 [6:59:38<2:01:44,  1.79s/it]08/31/2024 02:13:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1010029315948486, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05529189109802246, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.248438596725464, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.404733419418335}
 77%|███████▋  | 13750/17834 [6:59:40<1:59:39,  1.76s/it] 77%|███████▋  | 13751/17834 [6:59:42<1:59:55,  1.76s/it] 77%|███████▋  | 13752/17834 [6:59:43<1:59:46,  1.76s/it] 77%|███████▋  | 13753/17834 [6:59:45<1:59:14,  1.75s/it] 77%|███████▋  | 13754/17834 [6:59:47<1:58:48,  1.75s/it] 77%|███████▋  | 13755/17834 [6:59:49<1:57:58,  1.74s/it] 77%|███████▋  | 13756/17834 [6:59:50<1:58:45,  1.75s/it] 77%|███████▋  | 13757/17834 [6:59:52<1:59:33,  1.76s/it] 77%|███████▋  | 13758/17834 [6:59:54<1:59:14,  1.76s/it] 77%|███████▋  | 13759/17834 [6:59:56<1:59:44,  1.76s/it] 77%|███████▋  | 13760/17834 [6:59:58<1:59:43,  1.76s/it] 77%|███████▋  | 13761/17834 [6:59:59<1:58:52,  1.75s/it] 77%|███████▋  | 13762/17834 [7:00:01<1:58:06,  1.74s/it] 77%|███████▋  | 13763/17834 [7:00:03<1:56:47,  1.72s/it] 77%|███████▋  | 13764/17834 [7:00:04<1:57:25,  1.73s/it] 77%|███████▋  | 13765/17834 [7:00:06<1:57:16,  1.73s/it] 77%|███████▋  | 13766/17834 [7:00:08<1:58:19,  1.75s/it] 77%|███████▋  | 13767/17834 [7:00:10<1:59:49,  1.77s/it] 77%|███████▋  | 13768/17834 [7:00:11<1:57:42,  1.74s/it] 77%|███████▋  | 13769/17834 [7:00:13<1:58:54,  1.76s/it] 77%|███████▋  | 13770/17834 [7:00:15<1:57:18,  1.73s/it] 77%|███████▋  | 13771/17834 [7:00:17<1:57:03,  1.73s/it] 77%|███████▋  | 13772/17834 [7:00:18<1:59:05,  1.76s/it] 77%|███████▋  | 13773/17834 [7:00:20<1:58:36,  1.75s/it] 77%|███████▋  | 13774/17834 [7:00:22<1:56:56,  1.73s/it] 77%|███████▋  | 13775/17834 [7:00:24<1:57:49,  1.74s/it] 77%|███████▋  | 13776/17834 [7:00:25<1:58:49,  1.76s/it] 77%|███████▋  | 13777/17834 [7:00:27<1:56:31,  1.72s/it] 77%|███████▋  | 13778/17834 [7:00:29<1:58:48,  1.76s/it] 77%|███████▋  | 13779/17834 [7:00:31<1:58:12,  1.75s/it] 77%|███████▋  | 13780/17834 [7:00:32<1:57:32,  1.74s/it] 77%|███████▋  | 13781/17834 [7:00:34<1:57:43,  1.74s/it] 77%|███████▋  | 13782/17834 [7:00:36<1:57:33,  1.74s/it] 77%|███████▋  | 13783/17834 [7:00:38<1:59:35,  1.77s/it] 77%|███████▋  | 13784/17834 [7:00:39<1:58:18,  1.75s/it] 77%|███████▋  | 13785/17834 [7:00:41<1:57:13,  1.74s/it] 77%|███████▋  | 13786/17834 [7:00:43<1:57:32,  1.74s/it] 77%|███████▋  | 13787/17834 [7:00:44<1:56:38,  1.73s/it] 77%|███████▋  | 13788/17834 [7:00:46<1:57:28,  1.74s/it] 77%|███████▋  | 13789/17834 [7:00:48<1:58:00,  1.75s/it] 77%|███████▋  | 13790/17834 [7:00:50<1:59:44,  1.78s/it] 77%|███████▋  | 13791/17834 [7:00:52<1:57:51,  1.75s/it] 77%|███████▋  | 13792/17834 [7:00:53<1:58:39,  1.76s/it] 77%|███████▋  | 13793/17834 [7:00:55<1:58:08,  1.75s/it] 77%|███████▋  | 13794/17834 [7:00:57<1:59:47,  1.78s/it] 77%|███████▋  | 13795/17834 [7:00:59<1:59:08,  1.77s/it] 77%|███████▋  | 13796/17834 [7:01:00<1:57:25,  1.74s/it] 77%|███████▋  | 13797/17834 [7:01:02<1:58:50,  1.77s/it] 77%|███████▋  | 13798/17834 [7:01:04<1:59:33,  1.78s/it] 77%|███████▋  | 13799/17834 [7:01:06<1:58:40,  1.76s/it]08/31/2024 02:15:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9710528254508972, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.039587438106536865, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.144916296005249, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1555566787719727}
 77%|███████▋  | 13800/17834 [7:01:07<1:57:22,  1.75s/it] 77%|███████▋  | 13801/17834 [7:01:09<1:58:46,  1.77s/it] 77%|███████▋  | 13802/17834 [7:01:11<1:59:05,  1.77s/it] 77%|███████▋  | 13803/17834 [7:01:13<1:58:31,  1.76s/it] 77%|███████▋  | 13804/17834 [7:01:15<1:59:11,  1.77s/it] 77%|███████▋  | 13805/17834 [7:01:16<1:58:01,  1.76s/it] 77%|███████▋  | 13806/17834 [7:01:18<1:58:54,  1.77s/it] 77%|███████▋  | 13807/17834 [7:01:20<1:57:44,  1.75s/it] 77%|███████▋  | 13808/17834 [7:01:22<1:58:34,  1.77s/it] 77%|███████▋  | 13809/17834 [7:01:23<1:56:46,  1.74s/it] 77%|███████▋  | 13810/17834 [7:01:25<1:55:36,  1.72s/it] 77%|███████▋  | 13811/17834 [7:01:27<1:57:24,  1.75s/it] 77%|███████▋  | 13812/17834 [7:01:29<1:58:13,  1.76s/it] 77%|███████▋  | 13813/17834 [7:01:30<1:59:36,  1.78s/it] 77%|███████▋  | 13814/17834 [7:01:32<1:58:39,  1.77s/it] 77%|███████▋  | 13815/17834 [7:01:34<1:57:10,  1.75s/it] 77%|███████▋  | 13816/17834 [7:01:36<1:58:06,  1.76s/it] 77%|███████▋  | 13817/17834 [7:01:37<1:56:59,  1.75s/it] 77%|███████▋  | 13818/17834 [7:01:39<1:57:10,  1.75s/it] 77%|███████▋  | 13819/17834 [7:01:41<1:57:45,  1.76s/it] 77%|███████▋  | 13820/17834 [7:01:43<2:02:31,  1.83s/it] 77%|███████▋  | 13821/17834 [7:01:45<2:00:20,  1.80s/it] 78%|███████▊  | 13822/17834 [7:01:46<1:58:11,  1.77s/it] 78%|███████▊  | 13823/17834 [7:01:48<1:58:05,  1.77s/it] 78%|███████▊  | 13824/17834 [7:01:50<1:59:21,  1.79s/it] 78%|███████▊  | 13825/17834 [7:01:52<1:57:34,  1.76s/it] 78%|███████▊  | 13826/17834 [7:01:53<1:58:50,  1.78s/it] 78%|███████▊  | 13827/17834 [7:01:55<1:57:36,  1.76s/it] 78%|███████▊  | 13828/17834 [7:01:57<1:58:20,  1.77s/it] 78%|███████▊  | 13829/17834 [7:01:59<1:57:38,  1.76s/it] 78%|███████▊  | 13830/17834 [7:02:01<1:59:49,  1.80s/it] 78%|███████▊  | 13831/17834 [7:02:02<1:58:12,  1.77s/it] 78%|███████▊  | 13832/17834 [7:02:04<1:58:27,  1.78s/it] 78%|███████▊  | 13833/17834 [7:02:06<1:59:46,  1.80s/it] 78%|███████▊  | 13834/17834 [7:02:08<1:58:06,  1.77s/it] 78%|███████▊  | 13835/17834 [7:02:09<1:58:53,  1.78s/it] 78%|███████▊  | 13836/17834 [7:02:11<1:56:27,  1.75s/it] 78%|███████▊  | 13837/17834 [7:02:13<1:57:55,  1.77s/it] 78%|███████▊  | 13838/17834 [7:02:15<1:56:40,  1.75s/it] 78%|███████▊  | 13839/17834 [7:02:16<1:57:09,  1.76s/it] 78%|███████▊  | 13840/17834 [7:02:18<1:57:37,  1.77s/it] 78%|███████▊  | 13841/17834 [7:02:20<1:57:35,  1.77s/it] 78%|███████▊  | 13842/17834 [7:02:22<1:58:25,  1.78s/it] 78%|███████▊  | 13843/17834 [7:02:23<1:56:49,  1.76s/it] 78%|███████▊  | 13844/17834 [7:02:25<1:55:03,  1.73s/it] 78%|███████▊  | 13845/17834 [7:02:27<1:56:04,  1.75s/it] 78%|███████▊  | 13846/17834 [7:02:29<1:56:33,  1.75s/it] 78%|███████▊  | 13847/17834 [7:02:30<1:56:31,  1.75s/it] 78%|███████▊  | 13848/17834 [7:02:32<1:55:49,  1.74s/it] 78%|███████▊  | 13849/17834 [7:02:34<1:54:44,  1.73s/it]08/31/2024 02:16:53 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0515587329864502, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03103148564696312, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1460442543029785, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2286343574523926}
 78%|███████▊  | 13850/17834 [7:02:36<1:55:49,  1.74s/it] 78%|███████▊  | 13851/17834 [7:02:37<1:55:11,  1.74s/it] 78%|███████▊  | 13852/17834 [7:02:39<1:54:15,  1.72s/it] 78%|███████▊  | 13853/17834 [7:02:41<1:54:39,  1.73s/it] 78%|███████▊  | 13854/17834 [7:02:42<1:53:59,  1.72s/it] 78%|███████▊  | 13855/17834 [7:02:44<1:54:23,  1.72s/it] 78%|███████▊  | 13856/17834 [7:02:46<1:54:00,  1.72s/it] 78%|███████▊  | 13857/17834 [7:02:48<1:55:37,  1.74s/it] 78%|███████▊  | 13858/17834 [7:02:49<1:55:53,  1.75s/it] 78%|███████▊  | 13859/17834 [7:02:51<1:56:25,  1.76s/it] 78%|███████▊  | 13860/17834 [7:02:53<1:55:53,  1.75s/it] 78%|███████▊  | 13861/17834 [7:02:55<1:55:01,  1.74s/it] 78%|███████▊  | 13862/17834 [7:02:56<1:54:44,  1.73s/it] 78%|███████▊  | 13863/17834 [7:02:58<1:54:48,  1.73s/it] 78%|███████▊  | 13864/17834 [7:03:00<1:56:39,  1.76s/it] 78%|███████▊  | 13865/17834 [7:03:02<1:55:57,  1.75s/it] 78%|███████▊  | 13866/17834 [7:03:04<1:57:23,  1.78s/it] 78%|███████▊  | 13867/17834 [7:03:05<1:55:09,  1.74s/it] 78%|███████▊  | 13868/17834 [7:03:07<1:53:37,  1.72s/it] 78%|███████▊  | 13869/17834 [7:03:09<1:55:30,  1.75s/it] 78%|███████▊  | 13870/17834 [7:03:10<1:54:19,  1.73s/it] 78%|███████▊  | 13871/17834 [7:03:12<1:54:39,  1.74s/it] 78%|███████▊  | 13872/17834 [7:03:14<1:55:04,  1.74s/it] 78%|███████▊  | 13873/17834 [7:03:16<1:54:23,  1.73s/it] 78%|███████▊  | 13874/17834 [7:03:17<1:55:16,  1.75s/it] 78%|███████▊  | 13875/17834 [7:03:19<1:55:46,  1.75s/it] 78%|███████▊  | 13876/17834 [7:03:21<1:55:25,  1.75s/it] 78%|███████▊  | 13877/17834 [7:03:23<1:56:25,  1.77s/it] 78%|███████▊  | 13878/17834 [7:03:24<1:55:51,  1.76s/it] 78%|███████▊  | 13879/17834 [7:03:26<1:55:38,  1.75s/it] 78%|███████▊  | 13880/17834 [7:03:28<1:54:20,  1.74s/it] 78%|███████▊  | 13881/17834 [7:03:30<1:56:01,  1.76s/it] 78%|███████▊  | 13882/17834 [7:03:31<1:56:07,  1.76s/it] 78%|███████▊  | 13883/17834 [7:03:33<1:56:25,  1.77s/it] 78%|███████▊  | 13884/17834 [7:03:35<1:56:46,  1.77s/it] 78%|███████▊  | 13885/17834 [7:03:37<1:56:15,  1.77s/it] 78%|███████▊  | 13886/17834 [7:03:38<1:55:00,  1.75s/it] 78%|███████▊  | 13887/17834 [7:03:40<1:56:07,  1.77s/it] 78%|███████▊  | 13888/17834 [7:03:42<1:56:07,  1.77s/it] 78%|███████▊  | 13889/17834 [7:03:44<1:54:55,  1.75s/it] 78%|███████▊  | 13890/17834 [7:03:45<1:54:30,  1.74s/it] 78%|███████▊  | 13891/17834 [7:03:47<1:54:39,  1.74s/it] 78%|███████▊  | 13892/17834 [7:03:49<1:57:26,  1.79s/it] 78%|███████▊  | 13893/17834 [7:03:51<1:56:51,  1.78s/it] 78%|███████▊  | 13894/17834 [7:03:53<1:55:15,  1.76s/it] 78%|███████▊  | 13895/17834 [7:03:54<1:56:45,  1.78s/it] 78%|███████▊  | 13896/17834 [7:03:56<1:55:58,  1.77s/it] 78%|███████▊  | 13897/17834 [7:03:58<1:55:34,  1.76s/it] 78%|███████▊  | 13898/17834 [7:04:00<1:56:04,  1.77s/it] 78%|███████▊  | 13899/17834 [7:04:01<1:55:04,  1.75s/it]08/31/2024 02:18:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0395854711532593, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03352770209312439, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1735780239105225, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2466912269592285}
 78%|███████▊  | 13900/17834 [7:04:03<1:54:56,  1.75s/it] 78%|███████▊  | 13901/17834 [7:04:05<1:56:04,  1.77s/it] 78%|███████▊  | 13902/17834 [7:04:07<1:56:56,  1.78s/it] 78%|███████▊  | 13903/17834 [7:04:09<1:56:21,  1.78s/it] 78%|███████▊  | 13904/17834 [7:04:10<1:55:41,  1.77s/it] 78%|███████▊  | 13905/17834 [7:04:12<1:55:34,  1.76s/it] 78%|███████▊  | 13906/17834 [7:04:14<1:59:50,  1.83s/it] 78%|███████▊  | 13907/17834 [7:04:16<1:57:15,  1.79s/it] 78%|███████▊  | 13908/17834 [7:04:18<1:57:04,  1.79s/it] 78%|███████▊  | 13909/17834 [7:04:19<1:56:26,  1.78s/it] 78%|███████▊  | 13910/17834 [7:04:21<1:55:50,  1.77s/it] 78%|███████▊  | 13911/17834 [7:04:23<1:56:35,  1.78s/it] 78%|███████▊  | 13912/17834 [7:04:25<1:55:15,  1.76s/it] 78%|███████▊  | 13913/17834 [7:04:26<1:55:15,  1.76s/it] 78%|███████▊  | 13914/17834 [7:04:28<1:55:06,  1.76s/it] 78%|███████▊  | 13915/17834 [7:04:30<1:55:36,  1.77s/it] 78%|███████▊  | 13916/17834 [7:04:32<1:56:53,  1.79s/it] 78%|███████▊  | 13917/17834 [7:04:33<1:55:30,  1.77s/it] 78%|███████▊  | 13918/17834 [7:04:35<1:55:15,  1.77s/it] 78%|███████▊  | 13919/17834 [7:04:37<1:56:27,  1.78s/it] 78%|███████▊  | 13920/17834 [7:04:39<1:54:37,  1.76s/it] 78%|███████▊  | 13921/17834 [7:04:40<1:54:13,  1.75s/it] 78%|███████▊  | 13922/17834 [7:04:42<1:53:52,  1.75s/it] 78%|███████▊  | 13923/17834 [7:04:44<1:54:47,  1.76s/it] 78%|███████▊  | 13924/17834 [7:04:46<1:54:42,  1.76s/it] 78%|███████▊  | 13925/17834 [7:04:47<1:53:55,  1.75s/it] 78%|███████▊  | 13926/17834 [7:04:49<1:53:43,  1.75s/it] 78%|███████▊  | 13927/17834 [7:04:51<1:55:32,  1.77s/it] 78%|███████▊  | 13928/17834 [7:04:53<1:54:29,  1.76s/it] 78%|███████▊  | 13929/17834 [7:04:54<1:53:29,  1.74s/it] 78%|███████▊  | 13930/17834 [7:04:56<1:53:12,  1.74s/it] 78%|███████▊  | 13931/17834 [7:04:58<1:53:35,  1.75s/it] 78%|███████▊  | 13932/17834 [7:05:00<1:52:30,  1.73s/it] 78%|███████▊  | 13933/17834 [7:05:01<1:54:27,  1.76s/it] 78%|███████▊  | 13934/17834 [7:05:03<1:53:59,  1.75s/it] 78%|███████▊  | 13935/17834 [7:05:05<1:54:36,  1.76s/it] 78%|███████▊  | 13936/17834 [7:05:07<1:53:12,  1.74s/it] 78%|███████▊  | 13937/17834 [7:05:08<1:52:34,  1.73s/it] 78%|███████▊  | 13938/17834 [7:05:10<1:51:27,  1.72s/it] 78%|███████▊  | 13939/17834 [7:05:12<1:52:02,  1.73s/it] 78%|███████▊  | 13940/17834 [7:05:14<1:51:12,  1.71s/it] 78%|███████▊  | 13941/17834 [7:05:15<1:51:28,  1.72s/it] 78%|███████▊  | 13942/17834 [7:05:17<1:54:18,  1.76s/it] 78%|███████▊  | 13943/17834 [7:05:19<1:54:27,  1.76s/it] 78%|███████▊  | 13944/17834 [7:05:21<1:54:25,  1.77s/it] 78%|███████▊  | 13945/17834 [7:05:22<1:52:37,  1.74s/it] 78%|███████▊  | 13946/17834 [7:05:24<1:53:08,  1.75s/it] 78%|███████▊  | 13947/17834 [7:05:26<1:53:32,  1.75s/it] 78%|███████▊  | 13948/17834 [7:05:28<1:53:19,  1.75s/it] 78%|███████▊  | 13949/17834 [7:05:29<1:53:46,  1.76s/it]08/31/2024 02:19:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2167832851409912, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02913706749677658, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2106213569641113, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4565415382385254}
 78%|███████▊  | 13950/17834 [7:05:31<1:52:39,  1.74s/it] 78%|███████▊  | 13951/17834 [7:05:33<1:51:14,  1.72s/it] 78%|███████▊  | 13952/17834 [7:05:34<1:51:31,  1.72s/it] 78%|███████▊  | 13953/17834 [7:05:36<1:52:43,  1.74s/it] 78%|███████▊  | 13954/17834 [7:05:38<1:53:53,  1.76s/it] 78%|███████▊  | 13955/17834 [7:05:40<1:51:59,  1.73s/it] 78%|███████▊  | 13956/17834 [7:05:41<1:50:55,  1.72s/it] 78%|███████▊  | 13957/17834 [7:05:43<1:51:15,  1.72s/it] 78%|███████▊  | 13958/17834 [7:05:45<1:52:26,  1.74s/it] 78%|███████▊  | 13959/17834 [7:05:47<1:51:31,  1.73s/it] 78%|███████▊  | 13960/17834 [7:05:48<1:53:13,  1.75s/it] 78%|███████▊  | 13961/17834 [7:05:50<1:53:29,  1.76s/it] 78%|███████▊  | 13962/17834 [7:05:52<1:52:33,  1.74s/it] 78%|███████▊  | 13963/17834 [7:05:54<1:54:00,  1.77s/it] 78%|███████▊  | 13964/17834 [7:05:55<1:53:06,  1.75s/it] 78%|███████▊  | 13965/17834 [7:05:57<1:52:47,  1.75s/it] 78%|███████▊  | 13966/17834 [7:05:59<1:51:11,  1.72s/it] 78%|███████▊  | 13967/17834 [7:06:01<1:53:02,  1.75s/it] 78%|███████▊  | 13968/17834 [7:06:02<1:53:02,  1.75s/it] 78%|███████▊  | 13969/17834 [7:06:04<1:52:31,  1.75s/it] 78%|███████▊  | 13970/17834 [7:06:06<1:53:16,  1.76s/it] 78%|███████▊  | 13971/17834 [7:06:08<1:54:20,  1.78s/it] 78%|███████▊  | 13972/17834 [7:06:10<1:54:29,  1.78s/it] 78%|███████▊  | 13973/17834 [7:06:11<1:53:42,  1.77s/it] 78%|███████▊  | 13974/17834 [7:06:13<1:54:00,  1.77s/it] 78%|███████▊  | 13975/17834 [7:06:15<1:53:09,  1.76s/it] 78%|███████▊  | 13976/17834 [7:06:17<1:52:47,  1.75s/it] 78%|███████▊  | 13977/17834 [7:06:18<1:52:12,  1.75s/it] 78%|███████▊  | 13978/17834 [7:06:20<1:53:12,  1.76s/it] 78%|███████▊  | 13979/17834 [7:06:22<1:52:26,  1.75s/it] 78%|███████▊  | 13980/17834 [7:06:24<1:52:19,  1.75s/it] 78%|███████▊  | 13981/17834 [7:06:25<1:52:23,  1.75s/it] 78%|███████▊  | 13982/17834 [7:06:27<1:52:44,  1.76s/it] 78%|███████▊  | 13983/17834 [7:06:29<1:54:15,  1.78s/it] 78%|███████▊  | 13984/17834 [7:06:31<1:52:08,  1.75s/it] 78%|███████▊  | 13985/17834 [7:06:32<1:52:45,  1.76s/it] 78%|███████▊  | 13986/17834 [7:06:34<1:51:14,  1.73s/it] 78%|███████▊  | 13987/17834 [7:06:36<1:50:57,  1.73s/it] 78%|███████▊  | 13988/17834 [7:06:37<1:50:47,  1.73s/it] 78%|███████▊  | 13989/17834 [7:06:39<1:50:22,  1.72s/it] 78%|███████▊  | 13990/17834 [7:06:41<1:51:09,  1.74s/it] 78%|███████▊  | 13991/17834 [7:06:43<1:50:03,  1.72s/it] 78%|███████▊  | 13992/17834 [7:06:44<1:50:32,  1.73s/it] 78%|███████▊  | 13993/17834 [7:06:46<1:52:42,  1.76s/it] 78%|███████▊  | 13994/17834 [7:06:48<1:52:50,  1.76s/it] 78%|███████▊  | 13995/17834 [7:06:50<1:51:56,  1.75s/it] 78%|███████▊  | 13996/17834 [7:06:51<1:52:14,  1.75s/it] 78%|███████▊  | 13997/17834 [7:06:53<1:53:38,  1.78s/it] 78%|███████▊  | 13998/17834 [7:06:55<1:54:36,  1.79s/it] 78%|███████▊  | 13999/17834 [7:06:57<1:52:29,  1.76s/it]08/31/2024 02:21:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3707938194274902, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04356563091278076, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3173811435699463, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7317404747009277}
 79%|███████▊  | 14000/17834 [7:06:59<1:53:29,  1.78s/it] 79%|███████▊  | 14001/17834 [7:07:00<1:52:18,  1.76s/it] 79%|███████▊  | 14002/17834 [7:07:02<1:51:26,  1.74s/it] 79%|███████▊  | 14003/17834 [7:07:04<1:51:16,  1.74s/it] 79%|███████▊  | 14004/17834 [7:07:06<1:51:25,  1.75s/it] 79%|███████▊  | 14005/17834 [7:07:07<1:52:30,  1.76s/it] 79%|███████▊  | 14006/17834 [7:07:09<1:52:06,  1.76s/it] 79%|███████▊  | 14007/17834 [7:07:11<1:51:59,  1.76s/it] 79%|███████▊  | 14008/17834 [7:07:13<1:54:18,  1.79s/it] 79%|███████▊  | 14009/17834 [7:07:14<1:53:08,  1.77s/it] 79%|███████▊  | 14010/17834 [7:07:16<1:52:08,  1.76s/it] 79%|███████▊  | 14011/17834 [7:07:18<1:51:23,  1.75s/it] 79%|███████▊  | 14012/17834 [7:07:20<1:52:01,  1.76s/it] 79%|███████▊  | 14013/17834 [7:07:21<1:50:48,  1.74s/it] 79%|███████▊  | 14014/17834 [7:07:23<1:50:40,  1.74s/it] 79%|███████▊  | 14015/17834 [7:07:25<1:50:32,  1.74s/it] 79%|███████▊  | 14016/17834 [7:07:27<1:49:29,  1.72s/it] 79%|███████▊  | 14017/17834 [7:07:28<1:49:24,  1.72s/it] 79%|███████▊  | 14018/17834 [7:07:30<1:50:21,  1.74s/it] 79%|███████▊  | 14019/17834 [7:07:32<1:50:59,  1.75s/it] 79%|███████▊  | 14020/17834 [7:07:34<1:51:24,  1.75s/it] 79%|███████▊  | 14021/17834 [7:07:35<1:50:26,  1.74s/it] 79%|███████▊  | 14022/17834 [7:07:37<1:50:06,  1.73s/it] 79%|███████▊  | 14023/17834 [7:07:39<1:52:30,  1.77s/it] 79%|███████▊  | 14024/17834 [7:07:41<1:52:11,  1.77s/it] 79%|███████▊  | 14025/17834 [7:07:42<1:52:56,  1.78s/it] 79%|███████▊  | 14026/17834 [7:07:44<1:51:40,  1.76s/it] 79%|███████▊  | 14027/17834 [7:07:46<1:51:01,  1.75s/it] 79%|███████▊  | 14028/17834 [7:07:48<1:50:57,  1.75s/it] 79%|███████▊  | 14029/17834 [7:07:49<1:49:42,  1.73s/it] 79%|███████▊  | 14030/17834 [7:07:51<1:50:16,  1.74s/it] 79%|███████▊  | 14031/17834 [7:07:53<1:50:26,  1.74s/it] 79%|███████▊  | 14032/17834 [7:07:55<1:49:31,  1.73s/it] 79%|███████▊  | 14033/17834 [7:07:56<1:51:35,  1.76s/it] 79%|███████▊  | 14034/17834 [7:07:58<1:50:46,  1.75s/it] 79%|███████▊  | 14035/17834 [7:08:00<1:49:37,  1.73s/it] 79%|███████▊  | 14036/17834 [7:08:01<1:49:16,  1.73s/it] 79%|███████▊  | 14037/17834 [7:08:03<1:49:34,  1.73s/it] 79%|███████▊  | 14038/17834 [7:08:05<1:50:06,  1.74s/it] 79%|███████▊  | 14039/17834 [7:08:07<1:49:42,  1.73s/it] 79%|███████▊  | 14040/17834 [7:08:08<1:49:45,  1.74s/it] 79%|███████▊  | 14041/17834 [7:08:10<1:50:31,  1.75s/it] 79%|███████▊  | 14042/17834 [7:08:12<1:48:46,  1.72s/it] 79%|███████▊  | 14043/17834 [7:08:14<1:50:39,  1.75s/it] 79%|███████▊  | 14044/17834 [7:08:15<1:49:19,  1.73s/it] 79%|███████▉  | 14045/17834 [7:08:17<1:49:28,  1.73s/it] 79%|███████▉  | 14046/17834 [7:08:19<1:49:42,  1.74s/it] 79%|███████▉  | 14047/17834 [7:08:21<1:50:31,  1.75s/it] 79%|███████▉  | 14048/17834 [7:08:22<1:52:28,  1.78s/it] 79%|███████▉  | 14049/17834 [7:08:24<1:50:21,  1.75s/it]08/31/2024 02:22:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0673208236694336, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038544587790966034, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1505789756774902, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2564444541931152}
 79%|███████▉  | 14050/17834 [7:08:26<1:49:20,  1.73s/it] 79%|███████▉  | 14051/17834 [7:08:28<1:49:59,  1.74s/it] 79%|███████▉  | 14052/17834 [7:08:29<1:51:28,  1.77s/it] 79%|███████▉  | 14053/17834 [7:08:31<1:51:26,  1.77s/it] 79%|███████▉  | 14054/17834 [7:08:33<1:51:36,  1.77s/it] 79%|███████▉  | 14055/17834 [7:08:35<1:50:18,  1.75s/it] 79%|███████▉  | 14056/17834 [7:08:36<1:50:03,  1.75s/it] 79%|███████▉  | 14057/17834 [7:08:38<1:50:47,  1.76s/it] 79%|███████▉  | 14058/17834 [7:08:40<1:50:36,  1.76s/it] 79%|███████▉  | 14059/17834 [7:08:42<1:50:26,  1.76s/it] 79%|███████▉  | 14060/17834 [7:08:43<1:49:30,  1.74s/it] 79%|███████▉  | 14061/17834 [7:08:45<1:50:55,  1.76s/it] 79%|███████▉  | 14062/17834 [7:08:47<1:50:12,  1.75s/it] 79%|███████▉  | 14063/17834 [7:08:49<1:50:45,  1.76s/it] 79%|███████▉  | 14064/17834 [7:08:50<1:49:18,  1.74s/it] 79%|███████▉  | 14065/17834 [7:08:52<1:48:34,  1.73s/it] 79%|███████▉  | 14066/17834 [7:08:54<1:47:27,  1.71s/it] 79%|███████▉  | 14067/17834 [7:08:56<1:49:15,  1.74s/it] 79%|███████▉  | 14068/17834 [7:08:57<1:48:48,  1.73s/it] 79%|███████▉  | 14069/17834 [7:08:59<1:50:20,  1.76s/it] 79%|███████▉  | 14070/17834 [7:09:01<1:49:21,  1.74s/it] 79%|███████▉  | 14071/17834 [7:09:03<1:48:34,  1.73s/it] 79%|███████▉  | 14072/17834 [7:09:04<1:48:52,  1.74s/it] 79%|███████▉  | 14073/17834 [7:09:06<1:49:34,  1.75s/it] 79%|███████▉  | 14074/17834 [7:09:08<1:49:16,  1.74s/it] 79%|███████▉  | 14075/17834 [7:09:10<1:48:44,  1.74s/it] 79%|███████▉  | 14076/17834 [7:09:11<1:49:39,  1.75s/it] 79%|███████▉  | 14077/17834 [7:09:13<1:49:47,  1.75s/it] 79%|███████▉  | 14078/17834 [7:09:15<1:50:04,  1.76s/it] 79%|███████▉  | 14079/17834 [7:09:17<1:51:23,  1.78s/it] 79%|███████▉  | 14080/17834 [7:09:18<1:50:03,  1.76s/it] 79%|███████▉  | 14081/17834 [7:09:20<1:48:15,  1.73s/it] 79%|███████▉  | 14082/17834 [7:09:22<1:49:19,  1.75s/it] 79%|███████▉  | 14083/17834 [7:09:24<1:50:22,  1.77s/it] 79%|███████▉  | 14084/17834 [7:09:25<1:51:16,  1.78s/it] 79%|███████▉  | 14085/17834 [7:09:27<1:50:58,  1.78s/it] 79%|███████▉  | 14086/17834 [7:09:29<1:50:12,  1.76s/it] 79%|███████▉  | 14087/17834 [7:09:31<1:51:27,  1.78s/it] 79%|███████▉  | 14088/17834 [7:09:33<1:49:31,  1.75s/it] 79%|███████▉  | 14089/17834 [7:09:34<1:48:23,  1.74s/it] 79%|███████▉  | 14090/17834 [7:09:36<1:48:06,  1.73s/it] 79%|███████▉  | 14091/17834 [7:09:38<1:47:36,  1.72s/it] 79%|███████▉  | 14092/17834 [7:09:39<1:48:14,  1.74s/it] 79%|███████▉  | 14093/17834 [7:09:41<1:48:41,  1.74s/it] 79%|███████▉  | 14094/17834 [7:09:43<1:47:15,  1.72s/it] 79%|███████▉  | 14095/17834 [7:09:45<1:48:07,  1.74s/it] 79%|███████▉  | 14096/17834 [7:09:46<1:47:42,  1.73s/it] 79%|███████▉  | 14097/17834 [7:09:48<1:46:49,  1.72s/it] 79%|███████▉  | 14098/17834 [7:09:50<1:46:27,  1.71s/it] 79%|███████▉  | 14099/17834 [7:09:52<1:48:40,  1.75s/it]08/31/2024 02:24:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0221178531646729, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026980478316545486, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.227625846862793, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.276724338531494}
 79%|███████▉  | 14100/17834 [7:09:53<1:48:07,  1.74s/it] 79%|███████▉  | 14101/17834 [7:09:55<1:47:43,  1.73s/it] 79%|███████▉  | 14102/17834 [7:09:57<1:48:00,  1.74s/it] 79%|███████▉  | 14103/17834 [7:09:58<1:48:35,  1.75s/it] 79%|███████▉  | 14104/17834 [7:10:00<1:48:07,  1.74s/it] 79%|███████▉  | 14105/17834 [7:10:02<1:46:54,  1.72s/it] 79%|███████▉  | 14106/17834 [7:10:04<1:46:45,  1.72s/it] 79%|███████▉  | 14107/17834 [7:10:05<1:46:58,  1.72s/it] 79%|███████▉  | 14108/17834 [7:10:07<1:48:26,  1.75s/it] 79%|███████▉  | 14109/17834 [7:10:09<1:48:41,  1.75s/it] 79%|███████▉  | 14110/17834 [7:10:11<1:49:44,  1.77s/it] 79%|███████▉  | 14111/17834 [7:10:12<1:49:36,  1.77s/it] 79%|███████▉  | 14112/17834 [7:10:14<1:50:14,  1.78s/it] 79%|███████▉  | 14113/17834 [7:10:16<1:49:03,  1.76s/it] 79%|███████▉  | 14114/17834 [7:10:18<1:48:46,  1.75s/it] 79%|███████▉  | 14115/17834 [7:10:20<1:50:04,  1.78s/it] 79%|███████▉  | 14116/17834 [7:10:21<1:50:23,  1.78s/it] 79%|███████▉  | 14117/17834 [7:10:23<1:49:17,  1.76s/it] 79%|███████▉  | 14118/17834 [7:10:25<1:49:09,  1.76s/it] 79%|███████▉  | 14119/17834 [7:10:27<1:49:55,  1.78s/it] 79%|███████▉  | 14120/17834 [7:10:28<1:50:30,  1.79s/it] 79%|███████▉  | 14121/17834 [7:10:30<1:51:10,  1.80s/it] 79%|███████▉  | 14122/17834 [7:10:32<1:50:14,  1.78s/it] 79%|███████▉  | 14123/17834 [7:10:34<1:49:51,  1.78s/it] 79%|███████▉  | 14124/17834 [7:10:35<1:48:31,  1.76s/it] 79%|███████▉  | 14125/17834 [7:10:37<1:48:50,  1.76s/it] 79%|███████▉  | 14126/17834 [7:10:39<1:49:12,  1.77s/it] 79%|███████▉  | 14127/17834 [7:10:41<1:47:43,  1.74s/it] 79%|███████▉  | 14128/17834 [7:10:42<1:47:40,  1.74s/it] 79%|███████▉  | 14129/17834 [7:10:44<1:49:10,  1.77s/it] 79%|███████▉  | 14130/17834 [7:10:46<1:50:04,  1.78s/it] 79%|███████▉  | 14131/17834 [7:10:48<1:49:38,  1.78s/it] 79%|███████▉  | 14132/17834 [7:10:50<1:49:04,  1.77s/it] 79%|███████▉  | 14133/17834 [7:10:51<1:49:11,  1.77s/it] 79%|███████▉  | 14134/17834 [7:10:53<1:50:10,  1.79s/it] 79%|███████▉  | 14135/17834 [7:10:55<1:48:56,  1.77s/it] 79%|███████▉  | 14136/17834 [7:10:57<1:48:02,  1.75s/it] 79%|███████▉  | 14137/17834 [7:10:58<1:48:27,  1.76s/it] 79%|███████▉  | 14138/17834 [7:11:00<1:47:53,  1.75s/it] 79%|███████▉  | 14139/17834 [7:11:02<1:49:29,  1.78s/it] 79%|███████▉  | 14140/17834 [7:11:04<1:48:24,  1.76s/it] 79%|███████▉  | 14141/17834 [7:11:05<1:47:07,  1.74s/it] 79%|███████▉  | 14142/17834 [7:11:07<1:46:08,  1.73s/it] 79%|███████▉  | 14143/17834 [7:11:09<1:46:34,  1.73s/it] 79%|███████▉  | 14144/17834 [7:11:11<1:46:37,  1.73s/it] 79%|███████▉  | 14145/17834 [7:11:12<1:47:40,  1.75s/it] 79%|███████▉  | 14146/17834 [7:11:14<1:47:49,  1.75s/it] 79%|███████▉  | 14147/17834 [7:11:16<1:47:33,  1.75s/it] 79%|███████▉  | 14148/17834 [7:11:18<1:49:15,  1.78s/it] 79%|███████▉  | 14149/17834 [7:11:20<1:49:08,  1.78s/it]08/31/2024 02:25:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8668994903564453, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.014689454808831215, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1447694301605225, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0263583660125732}
 79%|███████▉  | 14150/17834 [7:11:21<1:49:01,  1.78s/it] 79%|███████▉  | 14151/17834 [7:11:23<1:48:26,  1.77s/it] 79%|███████▉  | 14152/17834 [7:11:25<1:47:51,  1.76s/it] 79%|███████▉  | 14153/17834 [7:11:26<1:45:55,  1.73s/it] 79%|███████▉  | 14154/17834 [7:11:28<1:45:53,  1.73s/it] 79%|███████▉  | 14155/17834 [7:11:30<1:46:03,  1.73s/it] 79%|███████▉  | 14156/17834 [7:11:32<1:48:34,  1.77s/it] 79%|███████▉  | 14157/17834 [7:11:33<1:48:14,  1.77s/it] 79%|███████▉  | 14158/17834 [7:11:35<1:47:23,  1.75s/it] 79%|███████▉  | 14159/17834 [7:11:37<1:47:02,  1.75s/it] 79%|███████▉  | 14160/17834 [7:11:39<1:48:08,  1.77s/it] 79%|███████▉  | 14161/17834 [7:11:41<1:48:03,  1.77s/it] 79%|███████▉  | 14162/17834 [7:11:42<1:47:39,  1.76s/it] 79%|███████▉  | 14163/17834 [7:11:44<1:50:21,  1.80s/it] 79%|███████▉  | 14164/17834 [7:11:46<1:48:00,  1.77s/it] 79%|███████▉  | 14165/17834 [7:11:48<1:47:14,  1.75s/it] 79%|███████▉  | 14166/17834 [7:11:49<1:46:20,  1.74s/it] 79%|███████▉  | 14167/17834 [7:11:51<1:48:08,  1.77s/it] 79%|███████▉  | 14168/17834 [7:11:53<1:48:04,  1.77s/it] 79%|███████▉  | 14169/17834 [7:11:55<1:48:39,  1.78s/it] 79%|███████▉  | 14170/17834 [7:11:56<1:48:20,  1.77s/it] 79%|███████▉  | 14171/17834 [7:11:58<1:47:26,  1.76s/it] 79%|███████▉  | 14172/17834 [7:12:00<1:47:43,  1.76s/it] 79%|███████▉  | 14173/17834 [7:12:02<1:48:05,  1.77s/it] 79%|███████▉  | 14174/17834 [7:12:04<1:49:03,  1.79s/it] 79%|███████▉  | 14175/17834 [7:12:05<1:49:01,  1.79s/it] 79%|███████▉  | 14176/17834 [7:12:07<1:47:10,  1.76s/it] 79%|███████▉  | 14177/17834 [7:12:09<1:46:49,  1.75s/it] 79%|███████▉  | 14178/17834 [7:12:11<1:46:07,  1.74s/it] 80%|███████▉  | 14179/17834 [7:12:12<1:46:14,  1.74s/it] 80%|███████▉  | 14180/17834 [7:12:14<1:46:12,  1.74s/it] 80%|███████▉  | 14181/17834 [7:12:16<1:47:14,  1.76s/it] 80%|███████▉  | 14182/17834 [7:12:18<1:47:23,  1.76s/it] 80%|███████▉  | 14183/17834 [7:12:19<1:47:16,  1.76s/it] 80%|███████▉  | 14184/17834 [7:12:21<1:46:25,  1.75s/it] 80%|███████▉  | 14185/17834 [7:12:23<1:45:47,  1.74s/it] 80%|███████▉  | 14186/17834 [7:12:24<1:45:07,  1.73s/it] 80%|███████▉  | 14187/17834 [7:12:26<1:46:09,  1.75s/it] 80%|███████▉  | 14188/17834 [7:12:28<1:46:04,  1.75s/it] 80%|███████▉  | 14189/17834 [7:12:30<1:49:05,  1.80s/it] 80%|███████▉  | 14190/17834 [7:12:32<1:48:06,  1.78s/it] 80%|███████▉  | 14191/17834 [7:12:33<1:47:40,  1.77s/it] 80%|███████▉  | 14192/17834 [7:12:35<1:47:34,  1.77s/it] 80%|███████▉  | 14193/17834 [7:12:37<1:47:48,  1.78s/it] 80%|███████▉  | 14194/17834 [7:12:39<1:46:44,  1.76s/it] 80%|███████▉  | 14195/17834 [7:12:40<1:46:02,  1.75s/it] 80%|███████▉  | 14196/17834 [7:12:42<1:45:31,  1.74s/it] 80%|███████▉  | 14197/17834 [7:12:44<1:46:39,  1.76s/it] 80%|███████▉  | 14198/17834 [7:12:46<1:45:49,  1.75s/it] 80%|███████▉  | 14199/17834 [7:12:47<1:45:33,  1.74s/it]08/31/2024 02:27:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1341025829315186, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02334710769355297, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.198673725128174, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.356123447418213}
 80%|███████▉  | 14200/17834 [7:12:49<1:45:21,  1.74s/it] 80%|███████▉  | 14201/17834 [7:12:51<1:45:22,  1.74s/it] 80%|███████▉  | 14202/17834 [7:12:53<1:44:56,  1.73s/it] 80%|███████▉  | 14203/17834 [7:12:54<1:44:20,  1.72s/it] 80%|███████▉  | 14204/17834 [7:12:56<1:46:21,  1.76s/it] 80%|███████▉  | 14205/17834 [7:12:58<1:46:33,  1.76s/it] 80%|███████▉  | 14206/17834 [7:13:00<1:47:08,  1.77s/it] 80%|███████▉  | 14207/17834 [7:13:01<1:46:39,  1.76s/it] 80%|███████▉  | 14208/17834 [7:13:03<1:46:17,  1.76s/it] 80%|███████▉  | 14209/17834 [7:13:05<1:46:50,  1.77s/it] 80%|███████▉  | 14210/17834 [7:13:07<1:46:14,  1.76s/it] 80%|███████▉  | 14211/17834 [7:13:08<1:46:03,  1.76s/it] 80%|███████▉  | 14212/17834 [7:13:10<1:44:03,  1.72s/it] 80%|███████▉  | 14213/17834 [7:13:12<1:45:52,  1.75s/it] 80%|███████▉  | 14214/17834 [7:13:14<1:45:03,  1.74s/it] 80%|███████▉  | 14215/17834 [7:13:15<1:45:57,  1.76s/it] 80%|███████▉  | 14216/17834 [7:13:17<1:45:58,  1.76s/it] 80%|███████▉  | 14217/17834 [7:13:19<1:45:23,  1.75s/it] 80%|███████▉  | 14218/17834 [7:13:21<1:43:43,  1.72s/it] 80%|███████▉  | 14219/17834 [7:13:22<1:45:42,  1.75s/it] 80%|███████▉  | 14220/17834 [7:13:24<1:44:08,  1.73s/it] 80%|███████▉  | 14221/17834 [7:13:26<1:44:46,  1.74s/it] 80%|███████▉  | 14222/17834 [7:13:28<1:44:13,  1.73s/it] 80%|███████▉  | 14223/17834 [7:13:29<1:44:04,  1.73s/it] 80%|███████▉  | 14224/17834 [7:13:31<1:46:09,  1.76s/it] 80%|███████▉  | 14225/17834 [7:13:33<1:44:20,  1.73s/it] 80%|███████▉  | 14226/17834 [7:13:35<1:44:46,  1.74s/it] 80%|███████▉  | 14227/17834 [7:13:36<1:43:56,  1.73s/it] 80%|███████▉  | 14228/17834 [7:13:38<1:43:51,  1.73s/it] 80%|███████▉  | 14229/17834 [7:13:40<1:45:16,  1.75s/it] 80%|███████▉  | 14230/17834 [7:13:42<1:45:22,  1.75s/it] 80%|███████▉  | 14231/17834 [7:13:43<1:46:03,  1.77s/it] 80%|███████▉  | 14232/17834 [7:13:45<1:46:26,  1.77s/it] 80%|███████▉  | 14233/17834 [7:13:47<1:45:56,  1.77s/it] 80%|███████▉  | 14234/17834 [7:13:49<1:44:40,  1.74s/it] 80%|███████▉  | 14235/17834 [7:13:50<1:44:30,  1.74s/it] 80%|███████▉  | 14236/17834 [7:13:52<1:43:43,  1.73s/it] 80%|███████▉  | 14237/17834 [7:13:54<1:43:29,  1.73s/it] 80%|███████▉  | 14238/17834 [7:13:56<1:44:19,  1.74s/it] 80%|███████▉  | 14239/17834 [7:13:57<1:43:39,  1.73s/it] 80%|███████▉  | 14240/17834 [7:13:59<1:43:39,  1.73s/it] 80%|███████▉  | 14241/17834 [7:14:01<1:43:15,  1.72s/it] 80%|███████▉  | 14242/17834 [7:14:02<1:44:15,  1.74s/it] 80%|███████▉  | 14243/17834 [7:14:04<1:44:08,  1.74s/it] 80%|███████▉  | 14244/17834 [7:14:06<1:46:55,  1.79s/it] 80%|███████▉  | 14245/17834 [7:14:08<1:46:22,  1.78s/it] 80%|███████▉  | 14246/17834 [7:14:10<1:45:46,  1.77s/it] 80%|███████▉  | 14247/17834 [7:14:11<1:43:50,  1.74s/it] 80%|███████▉  | 14248/17834 [7:14:13<1:44:30,  1.75s/it] 80%|███████▉  | 14249/17834 [7:14:15<1:44:51,  1.75s/it]08/31/2024 02:28:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3695831298828125, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03881104290485382, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3879804611206055, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.796374797821045}
 80%|███████▉  | 14250/17834 [7:14:16<1:43:28,  1.73s/it] 80%|███████▉  | 14251/17834 [7:14:18<1:42:50,  1.72s/it] 80%|███████▉  | 14252/17834 [7:14:20<1:42:26,  1.72s/it] 80%|███████▉  | 14253/17834 [7:14:22<1:43:26,  1.73s/it] 80%|███████▉  | 14254/17834 [7:14:23<1:43:08,  1.73s/it] 80%|███████▉  | 14255/17834 [7:14:25<1:43:37,  1.74s/it]08/31/2024 02:28:44 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/31/2024 02:28:44 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:52,  4.18it/s][A
  1%|▏         | 3/221 [00:00<00:56,  3.89it/s][A
  2%|▏         | 4/221 [00:00<00:46,  4.66it/s][A
  2%|▏         | 5/221 [00:01<00:54,  4.00it/s][A
  3%|▎         | 6/221 [00:01<00:52,  4.11it/s][A
  3%|▎         | 7/221 [00:01<00:58,  3.63it/s][A
  4%|▎         | 8/221 [00:02<01:01,  3.47it/s][A
  4%|▍         | 9/221 [00:02<01:12,  2.91it/s][A
  5%|▍         | 10/221 [00:02<01:17,  2.74it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.19it/s][A
  5%|▌         | 12/221 [00:03<00:56,  3.69it/s][A
  6%|▌         | 13/221 [00:03<00:47,  4.38it/s][A
  6%|▋         | 14/221 [00:03<00:49,  4.19it/s][A
  7%|▋         | 15/221 [00:03<00:44,  4.59it/s][A
  7%|▋         | 16/221 [00:04<00:50,  4.05it/s][A
  8%|▊         | 17/221 [00:04<00:48,  4.24it/s][A
  8%|▊         | 18/221 [00:04<00:41,  4.84it/s][A
  9%|▊         | 19/221 [00:04<00:43,  4.61it/s][A
  9%|▉         | 20/221 [00:05<00:41,  4.86it/s][A
 10%|▉         | 21/221 [00:05<00:42,  4.69it/s][A
 10%|▉         | 22/221 [00:05<00:48,  4.11it/s][A
 10%|█         | 23/221 [00:05<00:43,  4.56it/s][A
 11%|█         | 24/221 [00:05<00:36,  5.35it/s][A
 11%|█▏        | 25/221 [00:06<00:41,  4.70it/s][A
 12%|█▏        | 26/221 [00:06<00:39,  4.94it/s][A
 12%|█▏        | 27/221 [00:06<00:50,  3.83it/s][A
 13%|█▎        | 28/221 [00:07<01:01,  3.13it/s][A
 13%|█▎        | 29/221 [00:07<01:05,  2.92it/s][A
 14%|█▎        | 30/221 [00:07<01:08,  2.80it/s][A
 14%|█▍        | 31/221 [00:08<01:10,  2.69it/s][A
 14%|█▍        | 32/221 [00:08<01:08,  2.78it/s][A
 15%|█▍        | 33/221 [00:08<00:57,  3.29it/s][A
 15%|█▌        | 34/221 [00:09<00:53,  3.52it/s][A
 16%|█▌        | 35/221 [00:09<00:58,  3.20it/s][A
 16%|█▋        | 36/221 [00:09<00:57,  3.22it/s][A
 17%|█▋        | 37/221 [00:09<00:47,  3.84it/s][A
 17%|█▋        | 38/221 [00:10<00:48,  3.79it/s][A
 18%|█▊        | 39/221 [00:10<00:39,  4.62it/s][A
 18%|█▊        | 40/221 [00:10<00:44,  4.05it/s][A
 19%|█▊        | 41/221 [00:10<00:52,  3.44it/s][A
 19%|█▉        | 43/221 [00:11<00:41,  4.30it/s][A
 20%|█▉        | 44/221 [00:11<00:51,  3.43it/s][A
 20%|██        | 45/221 [00:12<00:57,  3.07it/s][A
 21%|██        | 46/221 [00:12<00:59,  2.93it/s][A
 21%|██▏       | 47/221 [00:12<00:53,  3.26it/s][A
 22%|██▏       | 48/221 [00:13<00:47,  3.62it/s][A
 22%|██▏       | 49/221 [00:13<00:45,  3.76it/s][A
 23%|██▎       | 50/221 [00:13<00:51,  3.30it/s][A
 23%|██▎       | 51/221 [00:14<00:54,  3.12it/s][A
 24%|██▎       | 52/221 [00:14<00:47,  3.55it/s][A
 24%|██▍       | 53/221 [00:14<00:44,  3.78it/s][A
 24%|██▍       | 54/221 [00:14<00:47,  3.55it/s][A
 25%|██▍       | 55/221 [00:14<00:44,  3.72it/s][A
 25%|██▌       | 56/221 [00:15<00:44,  3.71it/s][A
 26%|██▌       | 57/221 [00:15<00:55,  2.94it/s][A
 26%|██▌       | 58/221 [00:15<00:45,  3.59it/s][A
 27%|██▋       | 59/221 [00:16<00:37,  4.33it/s][A
 27%|██▋       | 60/221 [00:16<00:39,  4.08it/s][A
 28%|██▊       | 61/221 [00:16<00:42,  3.75it/s][A
 28%|██▊       | 62/221 [00:16<00:37,  4.20it/s][A
 29%|██▊       | 63/221 [00:17<00:42,  3.72it/s][A
 29%|██▉       | 64/221 [00:17<00:56,  2.79it/s][A
 29%|██▉       | 65/221 [00:18<01:00,  2.56it/s][A
 30%|██▉       | 66/221 [00:18<00:55,  2.77it/s][A
 30%|███       | 67/221 [00:18<00:53,  2.90it/s][A
 31%|███       | 68/221 [00:18<00:44,  3.41it/s][A
 31%|███       | 69/221 [00:19<00:54,  2.80it/s][A
 32%|███▏      | 70/221 [00:19<00:55,  2.71it/s][A
 32%|███▏      | 71/221 [00:20<00:47,  3.19it/s][A
 33%|███▎      | 72/221 [00:20<00:51,  2.87it/s][A
 33%|███▎      | 73/221 [00:20<00:51,  2.86it/s][A
 33%|███▎      | 74/221 [00:20<00:42,  3.48it/s][A
 34%|███▍      | 75/221 [00:21<00:39,  3.66it/s][A
 34%|███▍      | 76/221 [00:21<00:52,  2.74it/s][A
 35%|███▍      | 77/221 [00:22<01:00,  2.40it/s][A
 35%|███▌      | 78/221 [00:22<00:57,  2.49it/s][A
 36%|███▌      | 79/221 [00:23<01:03,  2.25it/s][A
 36%|███▌      | 80/221 [00:23<00:58,  2.40it/s][A
 37%|███▋      | 81/221 [00:23<00:46,  3.00it/s][A
 37%|███▋      | 82/221 [00:23<00:43,  3.18it/s][A
 38%|███▊      | 83/221 [00:24<00:43,  3.18it/s][A
 38%|███▊      | 84/221 [00:24<00:39,  3.46it/s][A
 38%|███▊      | 85/221 [00:24<00:40,  3.36it/s][A
 39%|███▉      | 86/221 [00:25<00:47,  2.86it/s][A
 39%|███▉      | 87/221 [00:25<00:51,  2.61it/s][A
 40%|███▉      | 88/221 [00:26<00:54,  2.46it/s][A
 40%|████      | 89/221 [00:26<00:50,  2.62it/s][A
 41%|████      | 90/221 [00:26<00:44,  2.95it/s][A
 41%|████      | 91/221 [00:26<00:37,  3.46it/s][A
 42%|████▏     | 92/221 [00:27<00:34,  3.73it/s][A
 42%|████▏     | 93/221 [00:27<00:43,  2.93it/s][A
 43%|████▎     | 94/221 [00:28<00:43,  2.94it/s][A
 43%|████▎     | 95/221 [00:28<00:36,  3.49it/s][A
 43%|████▎     | 96/221 [00:28<00:35,  3.50it/s][A
 44%|████▍     | 97/221 [00:28<00:36,  3.39it/s][A
 44%|████▍     | 98/221 [00:29<00:44,  2.75it/s][A
 45%|████▍     | 99/221 [00:29<00:43,  2.78it/s][A
 45%|████▌     | 100/221 [00:29<00:42,  2.88it/s][A
 46%|████▌     | 101/221 [00:30<00:34,  3.47it/s][A
 46%|████▌     | 102/221 [00:30<00:31,  3.74it/s][A
 47%|████▋     | 103/221 [00:30<00:34,  3.47it/s][A
 47%|████▋     | 104/221 [00:31<00:42,  2.76it/s][A
 48%|████▊     | 105/221 [00:31<00:45,  2.55it/s][A
 48%|████▊     | 106/221 [00:31<00:41,  2.80it/s][A
 48%|████▊     | 107/221 [00:32<00:38,  3.00it/s][A
 49%|████▉     | 108/221 [00:32<00:36,  3.13it/s][A
 49%|████▉     | 109/221 [00:32<00:32,  3.50it/s][A
 50%|████▉     | 110/221 [00:33<00:32,  3.37it/s][A
 50%|█████     | 111/221 [00:33<00:31,  3.45it/s][A
 51%|█████     | 112/221 [00:33<00:30,  3.55it/s][A
 51%|█████     | 113/221 [00:33<00:29,  3.71it/s][A
 52%|█████▏    | 114/221 [00:33<00:25,  4.16it/s][A
 52%|█████▏    | 115/221 [00:34<00:34,  3.05it/s][A
 52%|█████▏    | 116/221 [00:34<00:31,  3.35it/s][A
 53%|█████▎    | 117/221 [00:34<00:28,  3.68it/s][A
 53%|█████▎    | 118/221 [00:35<00:26,  3.91it/s][A
 54%|█████▍    | 119/221 [00:35<00:26,  3.88it/s][A
 54%|█████▍    | 120/221 [00:35<00:30,  3.26it/s][A
 55%|█████▍    | 121/221 [00:36<00:28,  3.53it/s][A
 55%|█████▌    | 122/221 [00:36<00:28,  3.52it/s][A
 56%|█████▌    | 123/221 [00:36<00:26,  3.73it/s][A
 56%|█████▌    | 124/221 [00:36<00:26,  3.67it/s][A
 57%|█████▋    | 125/221 [00:37<00:29,  3.27it/s][A
 57%|█████▋    | 126/221 [00:37<00:24,  3.93it/s][A
 57%|█████▋    | 127/221 [00:37<00:25,  3.62it/s][A
 58%|█████▊    | 128/221 [00:38<00:25,  3.61it/s][A
 58%|█████▊    | 129/221 [00:38<00:23,  3.95it/s][A
 59%|█████▉    | 130/221 [00:38<00:22,  4.02it/s][A
 59%|█████▉    | 131/221 [00:38<00:24,  3.61it/s][A
 60%|█████▉    | 132/221 [00:39<00:26,  3.34it/s][A
 60%|██████    | 133/221 [00:39<00:32,  2.74it/s][A
 61%|██████    | 134/221 [00:40<00:31,  2.80it/s][A
 61%|██████    | 135/221 [00:40<00:28,  2.99it/s][A
 62%|██████▏   | 136/221 [00:40<00:26,  3.22it/s][A
 62%|██████▏   | 137/221 [00:40<00:23,  3.61it/s][A
 62%|██████▏   | 138/221 [00:41<00:23,  3.59it/s][A
 63%|██████▎   | 139/221 [00:41<00:22,  3.71it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.79it/s][A
 64%|██████▍   | 141/221 [00:41<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:42<00:22,  3.53it/s][A
 65%|██████▍   | 143/221 [00:42<00:29,  2.63it/s][A
 65%|██████▌   | 144/221 [00:43<00:29,  2.64it/s][A
 66%|██████▌   | 145/221 [00:43<00:26,  2.87it/s][A
 66%|██████▌   | 146/221 [00:43<00:21,  3.46it/s][A
 67%|██████▋   | 147/221 [00:43<00:21,  3.47it/s][A
 67%|██████▋   | 148/221 [00:44<00:20,  3.57it/s][A
 67%|██████▋   | 149/221 [00:44<00:21,  3.40it/s][A
 68%|██████▊   | 150/221 [00:44<00:19,  3.72it/s][A
 68%|██████▊   | 151/221 [00:44<00:20,  3.49it/s][A
 69%|██████▉   | 152/221 [00:45<00:19,  3.60it/s][A
 69%|██████▉   | 153/221 [00:45<00:15,  4.39it/s][A
 70%|██████▉   | 154/221 [00:45<00:19,  3.37it/s][A
 70%|███████   | 155/221 [00:46<00:21,  3.06it/s][A
 71%|███████   | 156/221 [00:46<00:20,  3.12it/s][A
 71%|███████   | 157/221 [00:46<00:20,  3.06it/s][A
 71%|███████▏  | 158/221 [00:47<00:26,  2.41it/s][A
 72%|███████▏  | 159/221 [00:47<00:22,  2.71it/s][A
 72%|███████▏  | 160/221 [00:48<00:21,  2.78it/s][A
 73%|███████▎  | 161/221 [00:48<00:23,  2.52it/s][A
 73%|███████▎  | 162/221 [00:48<00:20,  2.88it/s][A
 74%|███████▍  | 163/221 [00:49<00:20,  2.80it/s][A
 74%|███████▍  | 164/221 [00:49<00:16,  3.50it/s][A
 75%|███████▍  | 165/221 [00:49<00:15,  3.59it/s][A
 75%|███████▌  | 166/221 [00:49<00:13,  4.10it/s][A
 76%|███████▌  | 167/221 [00:50<00:17,  3.13it/s][A
 76%|███████▌  | 168/221 [00:50<00:16,  3.27it/s][A
 76%|███████▋  | 169/221 [00:50<00:15,  3.34it/s][A
 77%|███████▋  | 170/221 [00:51<00:14,  3.50it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.43it/s][A
 78%|███████▊  | 172/221 [00:51<00:15,  3.19it/s][A
 78%|███████▊  | 173/221 [00:52<00:17,  2.78it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.25it/s][A
 79%|███████▉  | 175/221 [00:52<00:13,  3.48it/s][A
 80%|███████▉  | 176/221 [00:52<00:11,  3.85it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.28it/s][A
 81%|████████  | 178/221 [00:53<00:09,  4.50it/s][A
 81%|████████  | 179/221 [00:53<00:10,  4.16it/s][A
 81%|████████▏ | 180/221 [00:53<00:09,  4.33it/s][A
 82%|████████▏ | 181/221 [00:53<00:09,  4.32it/s][A
 82%|████████▏ | 182/221 [00:54<00:09,  4.15it/s][A
 83%|████████▎ | 183/221 [00:54<00:10,  3.53it/s][A
 83%|████████▎ | 184/221 [00:54<00:12,  3.00it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.22it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:55<00:10,  3.20it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:56<00:08,  3.71it/s][A
 86%|████████▌ | 190/221 [00:56<00:07,  3.88it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.04it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.36it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.50it/s][A
 88%|████████▊ | 194/221 [00:57<00:06,  3.87it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.53it/s][A
 89%|████████▊ | 196/221 [00:58<00:08,  3.09it/s][A
 89%|████████▉ | 197/221 [00:58<00:06,  3.57it/s][A
 90%|████████▉ | 198/221 [00:58<00:06,  3.47it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  3.99it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.98it/s][A
 91%|█████████ | 201/221 [00:59<00:04,  4.06it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.52it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  3.76it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.02it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.36it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.77it/s][A
 94%|█████████▎| 207/221 [01:00<00:02,  4.78it/s][A
 94%|█████████▍| 208/221 [01:01<00:02,  5.39it/s][A
 95%|█████████▍| 209/221 [01:01<00:02,  5.59it/s][A
 95%|█████████▌| 210/221 [01:01<00:01,  5.74it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  3.66it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.29it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.37it/s][A
 97%|█████████▋| 214/221 [01:02<00:02,  3.16it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.79it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.73it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.33it/s][A
 99%|█████████▊| 218/221 [01:04<00:01,  2.83it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  2.79it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.10it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.36it/s][A100%|██████████| 221/221 [01:05<00:00,  3.40it/s]
08/31/2024 02:31:08 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 14255--===========

08/31/2024 02:31:08 - INFO - __main__ -   {'area_r1': 5.0, 'area_recall': '5.0/14.6/19.3', 'area_ravg': 13.0}
08/31/2024 02:31:08 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 14255--===========

08/31/2024 02:31:08 - INFO - __main__ -   {'area_r1': 40.5, 'area_recall': '40.5/71.5/82.2', 'area_ravg': 64.7}
08/31/2024 02:31:08 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 14255--===========

08/31/2024 02:31:08 - INFO - __main__ -   {'video_r1': 34.2, 'video_recall': '34.2/66.2/76.7', 'video_ravg': 59.0}
08/31/2024 02:31:08 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/31/2024 02:31:08 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/31/2024 02:31:08 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 14255--===========

08/31/2024 02:31:08 - INFO - __main__ -   {'video_r1': 56.7, 'video_recall': '56.7/79.5/85.7', 'video_ravg': 74.0}
08/31/2024 02:31:08 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/31/2024 02:31:08 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
 80%|███████▉  | 14256/17834 [7:17:11<50:35:30, 50.90s/it] 80%|███████▉  | 14257/17834 [7:17:12<35:55:18, 36.15s/it] 80%|███████▉  | 14258/17834 [7:17:14<25:39:18, 25.83s/it] 80%|███████▉  | 14259/17834 [7:17:16<18:28:15, 18.60s/it] 80%|███████▉  | 14260/17834 [7:17:18<13:28:15, 13.57s/it] 80%|███████▉  | 14261/17834 [7:17:19<9:56:25, 10.02s/it]  80%|███████▉  | 14262/17834 [7:17:21<7:27:18,  7.51s/it] 80%|███████▉  | 14263/17834 [7:17:23<5:45:24,  5.80s/it] 80%|███████▉  | 14264/17834 [7:17:25<4:32:58,  4.59s/it] 80%|███████▉  | 14265/17834 [7:17:26<3:41:48,  3.73s/it] 80%|███████▉  | 14266/17834 [7:17:28<3:06:45,  3.14s/it] 80%|███████▉  | 14267/17834 [7:17:30<2:42:07,  2.73s/it] 80%|████████  | 14268/17834 [7:17:32<2:24:49,  2.44s/it] 80%|████████  | 14269/17834 [7:17:33<2:11:23,  2.21s/it] 80%|████████  | 14270/17834 [7:17:35<2:02:22,  2.06s/it] 80%|████████  | 14271/17834 [7:17:37<1:57:07,  1.97s/it] 80%|████████  | 14272/17834 [7:17:39<1:54:55,  1.94s/it] 80%|████████  | 14273/17834 [7:17:40<1:51:22,  1.88s/it] 80%|████████  | 14274/17834 [7:17:42<1:49:13,  1.84s/it] 80%|████████  | 14275/17834 [7:17:44<1:48:34,  1.83s/it] 80%|████████  | 14276/17834 [7:17:46<1:46:42,  1.80s/it] 80%|████████  | 14277/17834 [7:17:48<1:46:22,  1.79s/it] 80%|████████  | 14278/17834 [7:17:49<1:45:39,  1.78s/it] 80%|████████  | 14279/17834 [7:17:51<1:45:12,  1.78s/it] 80%|████████  | 14280/17834 [7:17:53<1:45:13,  1.78s/it] 80%|████████  | 14281/17834 [7:17:55<1:43:11,  1.74s/it] 80%|████████  | 14282/17834 [7:17:56<1:43:59,  1.76s/it] 80%|████████  | 14283/17834 [7:17:58<1:43:49,  1.75s/it] 80%|████████  | 14284/17834 [7:18:00<1:43:26,  1.75s/it] 80%|████████  | 14285/17834 [7:18:02<1:44:10,  1.76s/it] 80%|████████  | 14286/17834 [7:18:03<1:45:10,  1.78s/it] 80%|████████  | 14287/17834 [7:18:05<1:45:10,  1.78s/it] 80%|████████  | 14288/17834 [7:18:07<1:44:27,  1.77s/it] 80%|████████  | 14289/17834 [7:18:09<1:44:09,  1.76s/it] 80%|████████  | 14290/17834 [7:18:10<1:44:20,  1.77s/it] 80%|████████  | 14291/17834 [7:18:12<1:42:45,  1.74s/it] 80%|████████  | 14292/17834 [7:18:14<1:41:25,  1.72s/it] 80%|████████  | 14293/17834 [7:18:16<1:41:21,  1.72s/it] 80%|████████  | 14294/17834 [7:18:17<1:41:11,  1.72s/it] 80%|████████  | 14295/17834 [7:18:19<1:40:56,  1.71s/it] 80%|████████  | 14296/17834 [7:18:21<1:41:44,  1.73s/it] 80%|████████  | 14297/17834 [7:18:22<1:42:54,  1.75s/it] 80%|████████  | 14298/17834 [7:18:24<1:42:34,  1.74s/it] 80%|████████  | 14299/17834 [7:18:26<1:42:29,  1.74s/it]08/31/2024 02:32:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.7986035346984863, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.019319919869303703, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1433591842651367, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.961282730102539}
 80%|████████  | 14300/17834 [7:18:28<1:42:37,  1.74s/it] 80%|████████  | 14301/17834 [7:18:29<1:42:16,  1.74s/it] 80%|████████  | 14302/17834 [7:18:31<1:43:37,  1.76s/it] 80%|████████  | 14303/17834 [7:18:33<1:43:23,  1.76s/it] 80%|████████  | 14304/17834 [7:18:35<1:42:57,  1.75s/it] 80%|████████  | 14305/17834 [7:18:37<1:43:40,  1.76s/it] 80%|████████  | 14306/17834 [7:18:38<1:46:45,  1.82s/it] 80%|████████  | 14307/17834 [7:18:40<1:45:15,  1.79s/it] 80%|████████  | 14308/17834 [7:18:42<1:43:43,  1.77s/it] 80%|████████  | 14309/17834 [7:18:44<1:43:18,  1.76s/it] 80%|████████  | 14310/17834 [7:18:45<1:42:57,  1.75s/it] 80%|████████  | 14311/17834 [7:18:47<1:43:16,  1.76s/it] 80%|████████  | 14312/17834 [7:18:49<1:43:00,  1.75s/it] 80%|████████  | 14313/17834 [7:18:51<1:42:30,  1.75s/it] 80%|████████  | 14314/17834 [7:18:52<1:43:13,  1.76s/it] 80%|████████  | 14315/17834 [7:18:54<1:43:41,  1.77s/it] 80%|████████  | 14316/17834 [7:18:56<1:43:50,  1.77s/it] 80%|████████  | 14317/17834 [7:18:58<1:42:00,  1.74s/it] 80%|████████  | 14318/17834 [7:18:59<1:41:24,  1.73s/it] 80%|████████  | 14319/17834 [7:19:01<1:43:21,  1.76s/it] 80%|████████  | 14320/17834 [7:19:03<1:42:44,  1.75s/it] 80%|████████  | 14321/17834 [7:19:05<1:43:37,  1.77s/it] 80%|████████  | 14322/17834 [7:19:06<1:43:08,  1.76s/it] 80%|████████  | 14323/17834 [7:19:08<1:42:27,  1.75s/it] 80%|████████  | 14324/17834 [7:19:10<1:43:12,  1.76s/it] 80%|████████  | 14325/17834 [7:19:12<1:43:02,  1.76s/it] 80%|████████  | 14326/17834 [7:19:13<1:42:21,  1.75s/it] 80%|████████  | 14327/17834 [7:19:15<1:42:36,  1.76s/it] 80%|████████  | 14328/17834 [7:19:17<1:43:08,  1.77s/it] 80%|████████  | 14329/17834 [7:19:19<1:42:47,  1.76s/it] 80%|████████  | 14330/17834 [7:19:21<1:43:35,  1.77s/it] 80%|████████  | 14331/17834 [7:19:22<1:43:13,  1.77s/it] 80%|████████  | 14332/17834 [7:19:24<1:42:51,  1.76s/it] 80%|████████  | 14333/17834 [7:19:26<1:42:21,  1.75s/it] 80%|████████  | 14334/17834 [7:19:28<1:42:44,  1.76s/it] 80%|████████  | 14335/17834 [7:19:29<1:42:10,  1.75s/it] 80%|████████  | 14336/17834 [7:19:31<1:42:39,  1.76s/it] 80%|████████  | 14337/17834 [7:19:33<1:42:33,  1.76s/it] 80%|████████  | 14338/17834 [7:19:35<1:42:21,  1.76s/it] 80%|████████  | 14339/17834 [7:19:36<1:41:59,  1.75s/it] 80%|████████  | 14340/17834 [7:19:38<1:42:48,  1.77s/it] 80%|████████  | 14341/17834 [7:19:40<1:44:39,  1.80s/it] 80%|████████  | 14342/17834 [7:19:42<1:42:42,  1.76s/it] 80%|████████  | 14343/17834 [7:19:43<1:42:56,  1.77s/it] 80%|████████  | 14344/17834 [7:19:45<1:43:09,  1.77s/it] 80%|████████  | 14345/17834 [7:19:47<1:41:36,  1.75s/it] 80%|████████  | 14346/17834 [7:19:49<1:41:50,  1.75s/it] 80%|████████  | 14347/17834 [7:19:50<1:40:56,  1.74s/it] 80%|████████  | 14348/17834 [7:19:52<1:40:26,  1.73s/it] 80%|████████  | 14349/17834 [7:19:54<1:39:55,  1.72s/it]08/31/2024 02:34:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1719417572021484, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04413842037320137, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.19134521484375, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4074254035949707}
 80%|████████  | 14350/17834 [7:19:56<1:41:39,  1.75s/it] 80%|████████  | 14351/17834 [7:19:57<1:41:31,  1.75s/it] 80%|████████  | 14352/17834 [7:19:59<1:41:06,  1.74s/it] 80%|████████  | 14353/17834 [7:20:01<1:42:15,  1.76s/it] 80%|████████  | 14354/17834 [7:20:03<1:42:16,  1.76s/it] 80%|████████  | 14355/17834 [7:20:05<1:42:52,  1.77s/it] 80%|████████  | 14356/17834 [7:20:06<1:42:27,  1.77s/it] 81%|████████  | 14357/17834 [7:20:08<1:41:14,  1.75s/it] 81%|████████  | 14358/17834 [7:20:10<1:41:37,  1.75s/it] 81%|████████  | 14359/17834 [7:20:12<1:42:25,  1.77s/it] 81%|████████  | 14360/17834 [7:20:13<1:41:36,  1.75s/it] 81%|████████  | 14361/17834 [7:20:15<1:40:43,  1.74s/it] 81%|████████  | 14362/17834 [7:20:17<1:40:43,  1.74s/it] 81%|████████  | 14363/17834 [7:20:18<1:41:02,  1.75s/it] 81%|████████  | 14364/17834 [7:20:20<1:41:15,  1.75s/it] 81%|████████  | 14365/17834 [7:20:22<1:41:31,  1.76s/it] 81%|████████  | 14366/17834 [7:20:24<1:40:25,  1.74s/it] 81%|████████  | 14367/17834 [7:20:25<1:40:34,  1.74s/it] 81%|████████  | 14368/17834 [7:20:27<1:40:55,  1.75s/it] 81%|████████  | 14369/17834 [7:20:29<1:41:46,  1.76s/it] 81%|████████  | 14370/17834 [7:20:31<1:41:53,  1.76s/it] 81%|████████  | 14371/17834 [7:20:33<1:42:14,  1.77s/it] 81%|████████  | 14372/17834 [7:20:34<1:43:04,  1.79s/it] 81%|████████  | 14373/17834 [7:20:36<1:42:10,  1.77s/it] 81%|████████  | 14374/17834 [7:20:38<1:40:54,  1.75s/it] 81%|████████  | 14375/17834 [7:20:40<1:41:42,  1.76s/it] 81%|████████  | 14376/17834 [7:20:41<1:42:00,  1.77s/it] 81%|████████  | 14377/17834 [7:20:43<1:42:22,  1.78s/it] 81%|████████  | 14378/17834 [7:20:45<1:43:29,  1.80s/it] 81%|████████  | 14379/17834 [7:20:47<1:42:05,  1.77s/it] 81%|████████  | 14380/17834 [7:20:49<1:41:57,  1.77s/it] 81%|████████  | 14381/17834 [7:20:50<1:42:29,  1.78s/it] 81%|████████  | 14382/17834 [7:20:52<1:40:36,  1.75s/it] 81%|████████  | 14383/17834 [7:20:54<1:41:37,  1.77s/it] 81%|████████  | 14384/17834 [7:20:56<1:43:18,  1.80s/it] 81%|████████  | 14385/17834 [7:20:57<1:42:05,  1.78s/it] 81%|████████  | 14386/17834 [7:20:59<1:42:41,  1.79s/it] 81%|████████  | 14387/17834 [7:21:01<1:42:24,  1.78s/it] 81%|████████  | 14388/17834 [7:21:03<1:42:56,  1.79s/it] 81%|████████  | 14389/17834 [7:21:04<1:41:09,  1.76s/it] 81%|████████  | 14390/17834 [7:21:06<1:42:28,  1.79s/it] 81%|████████  | 14391/17834 [7:21:08<1:41:15,  1.76s/it] 81%|████████  | 14392/17834 [7:21:10<1:40:51,  1.76s/it] 81%|████████  | 14393/17834 [7:21:12<1:40:23,  1.75s/it] 81%|████████  | 14394/17834 [7:21:13<1:41:00,  1.76s/it] 81%|████████  | 14395/17834 [7:21:15<1:40:51,  1.76s/it] 81%|████████  | 14396/17834 [7:21:17<1:40:54,  1.76s/it] 81%|████████  | 14397/17834 [7:21:19<1:40:18,  1.75s/it] 81%|████████  | 14398/17834 [7:21:20<1:42:27,  1.79s/it] 81%|████████  | 14399/17834 [7:21:22<1:41:46,  1.78s/it]08/31/2024 02:35:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9176660776138306, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018895115703344345, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.10817551612854, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.044736623764038}
 81%|████████  | 14400/17834 [7:21:24<1:41:45,  1.78s/it] 81%|████████  | 14401/17834 [7:21:26<1:42:16,  1.79s/it] 81%|████████  | 14402/17834 [7:21:28<1:41:34,  1.78s/it] 81%|████████  | 14403/17834 [7:21:29<1:41:31,  1.78s/it] 81%|████████  | 14404/17834 [7:21:31<1:41:24,  1.77s/it] 81%|████████  | 14405/17834 [7:21:33<1:41:10,  1.77s/it] 81%|████████  | 14406/17834 [7:21:35<1:40:20,  1.76s/it] 81%|████████  | 14407/17834 [7:21:36<1:42:03,  1.79s/it] 81%|████████  | 14408/17834 [7:21:38<1:40:43,  1.76s/it] 81%|████████  | 14409/17834 [7:21:40<1:40:14,  1.76s/it] 81%|████████  | 14410/17834 [7:21:42<1:41:25,  1.78s/it] 81%|████████  | 14411/17834 [7:21:43<1:40:38,  1.76s/it] 81%|████████  | 14412/17834 [7:21:45<1:40:55,  1.77s/it] 81%|████████  | 14413/17834 [7:21:47<1:39:39,  1.75s/it] 81%|████████  | 14414/17834 [7:21:49<1:38:50,  1.73s/it] 81%|████████  | 14415/17834 [7:21:50<1:37:44,  1.72s/it] 81%|████████  | 14416/17834 [7:21:52<1:38:28,  1.73s/it] 81%|████████  | 14417/17834 [7:21:54<1:40:15,  1.76s/it] 81%|████████  | 14418/17834 [7:21:56<1:38:33,  1.73s/it] 81%|████████  | 14419/17834 [7:21:57<1:39:42,  1.75s/it] 81%|████████  | 14420/17834 [7:21:59<1:39:51,  1.76s/it] 81%|████████  | 14421/17834 [7:22:01<1:38:59,  1.74s/it] 81%|████████  | 14422/17834 [7:22:02<1:38:20,  1.73s/it] 81%|████████  | 14423/17834 [7:22:04<1:39:17,  1.75s/it] 81%|████████  | 14424/17834 [7:22:06<1:39:11,  1.75s/it] 81%|████████  | 14425/17834 [7:22:08<1:39:19,  1.75s/it] 81%|████████  | 14426/17834 [7:22:09<1:38:13,  1.73s/it] 81%|████████  | 14427/17834 [7:22:11<1:38:57,  1.74s/it] 81%|████████  | 14428/17834 [7:22:13<1:39:01,  1.74s/it] 81%|████████  | 14429/17834 [7:22:15<1:38:46,  1.74s/it] 81%|████████  | 14430/17834 [7:22:16<1:38:43,  1.74s/it] 81%|████████  | 14431/17834 [7:22:18<1:37:51,  1.73s/it] 81%|████████  | 14432/17834 [7:22:20<1:38:40,  1.74s/it] 81%|████████  | 14433/17834 [7:22:22<1:39:22,  1.75s/it] 81%|████████  | 14434/17834 [7:22:23<1:39:33,  1.76s/it] 81%|████████  | 14435/17834 [7:22:25<1:40:16,  1.77s/it] 81%|████████  | 14436/17834 [7:22:27<1:39:01,  1.75s/it] 81%|████████  | 14437/17834 [7:22:29<1:38:31,  1.74s/it] 81%|████████  | 14438/17834 [7:22:30<1:38:47,  1.75s/it] 81%|████████  | 14439/17834 [7:22:32<1:38:14,  1.74s/it] 81%|████████  | 14440/17834 [7:22:34<1:37:04,  1.72s/it] 81%|████████  | 14441/17834 [7:22:36<1:36:48,  1.71s/it] 81%|████████  | 14442/17834 [7:22:37<1:38:29,  1.74s/it] 81%|████████  | 14443/17834 [7:22:39<1:38:06,  1.74s/it] 81%|████████  | 14444/17834 [7:22:41<1:38:40,  1.75s/it] 81%|████████  | 14445/17834 [7:22:43<1:38:12,  1.74s/it] 81%|████████  | 14446/17834 [7:22:44<1:40:04,  1.77s/it] 81%|████████  | 14447/17834 [7:22:46<1:38:35,  1.75s/it] 81%|████████  | 14448/17834 [7:22:48<1:39:00,  1.75s/it] 81%|████████  | 14449/17834 [7:22:50<1:38:55,  1.75s/it]08/31/2024 02:37:09 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3567851781845093, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05336356908082962, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3059871196746826, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7161359786987305}
 81%|████████  | 14450/17834 [7:22:51<1:38:30,  1.75s/it] 81%|████████  | 14451/17834 [7:22:53<1:37:43,  1.73s/it] 81%|████████  | 14452/17834 [7:22:55<1:38:25,  1.75s/it] 81%|████████  | 14453/17834 [7:22:57<1:39:03,  1.76s/it] 81%|████████  | 14454/17834 [7:22:58<1:39:12,  1.76s/it] 81%|████████  | 14455/17834 [7:23:00<1:37:25,  1.73s/it] 81%|████████  | 14456/17834 [7:23:02<1:39:14,  1.76s/it] 81%|████████  | 14457/17834 [7:23:04<1:38:40,  1.75s/it] 81%|████████  | 14458/17834 [7:23:05<1:39:23,  1.77s/it] 81%|████████  | 14459/17834 [7:23:07<1:39:15,  1.76s/it] 81%|████████  | 14460/17834 [7:23:09<1:39:07,  1.76s/it] 81%|████████  | 14461/17834 [7:23:11<1:39:49,  1.78s/it] 81%|████████  | 14462/17834 [7:23:12<1:38:58,  1.76s/it] 81%|████████  | 14463/17834 [7:23:14<1:38:17,  1.75s/it] 81%|████████  | 14464/17834 [7:23:16<1:37:36,  1.74s/it] 81%|████████  | 14465/17834 [7:23:18<1:38:47,  1.76s/it] 81%|████████  | 14466/17834 [7:23:19<1:37:52,  1.74s/it] 81%|████████  | 14467/17834 [7:23:21<1:38:04,  1.75s/it] 81%|████████  | 14468/17834 [7:23:23<1:39:07,  1.77s/it] 81%|████████  | 14469/17834 [7:23:25<1:38:21,  1.75s/it] 81%|████████  | 14470/17834 [7:23:26<1:38:29,  1.76s/it] 81%|████████  | 14471/17834 [7:23:28<1:38:12,  1.75s/it] 81%|████████  | 14472/17834 [7:23:30<1:37:34,  1.74s/it] 81%|████████  | 14473/17834 [7:23:32<1:36:48,  1.73s/it] 81%|████████  | 14474/17834 [7:23:33<1:37:14,  1.74s/it] 81%|████████  | 14475/17834 [7:23:35<1:37:31,  1.74s/it] 81%|████████  | 14476/17834 [7:23:37<1:38:14,  1.76s/it] 81%|████████  | 14477/17834 [7:23:39<1:39:14,  1.77s/it] 81%|████████  | 14478/17834 [7:23:40<1:39:04,  1.77s/it] 81%|████████  | 14479/17834 [7:23:42<1:39:01,  1.77s/it] 81%|████████  | 14480/17834 [7:23:44<1:39:48,  1.79s/it] 81%|████████  | 14481/17834 [7:23:46<1:40:14,  1.79s/it] 81%|████████  | 14482/17834 [7:23:48<1:40:02,  1.79s/it] 81%|████████  | 14483/17834 [7:23:49<1:38:46,  1.77s/it] 81%|████████  | 14484/17834 [7:23:51<1:37:52,  1.75s/it] 81%|████████  | 14485/17834 [7:23:53<1:37:13,  1.74s/it] 81%|████████  | 14486/17834 [7:23:55<1:37:22,  1.75s/it] 81%|████████  | 14487/17834 [7:23:56<1:35:51,  1.72s/it] 81%|████████  | 14488/17834 [7:23:58<1:36:28,  1.73s/it] 81%|████████  | 14489/17834 [7:24:00<1:35:54,  1.72s/it] 81%|████████  | 14490/17834 [7:24:01<1:36:21,  1.73s/it] 81%|████████▏ | 14491/17834 [7:24:03<1:36:41,  1.74s/it] 81%|████████▏ | 14492/17834 [7:24:05<1:36:38,  1.74s/it] 81%|████████▏ | 14493/17834 [7:24:07<1:36:56,  1.74s/it] 81%|████████▏ | 14494/17834 [7:24:09<1:38:19,  1.77s/it] 81%|████████▏ | 14495/17834 [7:24:10<1:38:08,  1.76s/it] 81%|████████▏ | 14496/17834 [7:24:12<1:38:38,  1.77s/it] 81%|████████▏ | 14497/17834 [7:24:14<1:36:42,  1.74s/it] 81%|████████▏ | 14498/17834 [7:24:16<1:37:20,  1.75s/it] 81%|████████▏ | 14499/17834 [7:24:17<1:37:33,  1.76s/it]08/31/2024 02:38:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3651902675628662, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05267643555998802, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.349165916442871, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.7670326232910156}
 81%|████████▏ | 14500/17834 [7:24:19<1:38:17,  1.77s/it] 81%|████████▏ | 14501/17834 [7:24:21<1:38:04,  1.77s/it] 81%|████████▏ | 14502/17834 [7:24:22<1:36:20,  1.73s/it] 81%|████████▏ | 14503/17834 [7:24:24<1:35:58,  1.73s/it] 81%|████████▏ | 14504/17834 [7:24:26<1:35:58,  1.73s/it] 81%|████████▏ | 14505/17834 [7:24:28<1:36:50,  1.75s/it] 81%|████████▏ | 14506/17834 [7:24:29<1:36:32,  1.74s/it] 81%|████████▏ | 14507/17834 [7:24:31<1:37:48,  1.76s/it] 81%|████████▏ | 14508/17834 [7:24:33<1:36:51,  1.75s/it] 81%|████████▏ | 14509/17834 [7:24:35<1:36:57,  1.75s/it] 81%|████████▏ | 14510/17834 [7:24:37<1:38:57,  1.79s/it] 81%|████████▏ | 14511/17834 [7:24:38<1:37:26,  1.76s/it] 81%|████████▏ | 14512/17834 [7:24:40<1:36:13,  1.74s/it] 81%|████████▏ | 14513/17834 [7:24:42<1:37:12,  1.76s/it] 81%|████████▏ | 14514/17834 [7:24:44<1:36:47,  1.75s/it] 81%|████████▏ | 14515/17834 [7:24:45<1:38:19,  1.78s/it] 81%|████████▏ | 14516/17834 [7:24:47<1:36:51,  1.75s/it] 81%|████████▏ | 14517/17834 [7:24:49<1:36:55,  1.75s/it] 81%|████████▏ | 14518/17834 [7:24:51<1:36:01,  1.74s/it] 81%|████████▏ | 14519/17834 [7:24:52<1:35:47,  1.73s/it] 81%|████████▏ | 14520/17834 [7:24:54<1:37:04,  1.76s/it] 81%|████████▏ | 14521/17834 [7:24:56<1:36:41,  1.75s/it] 81%|████████▏ | 14522/17834 [7:24:58<1:36:48,  1.75s/it] 81%|████████▏ | 14523/17834 [7:24:59<1:36:13,  1.74s/it] 81%|████████▏ | 14524/17834 [7:25:01<1:37:32,  1.77s/it] 81%|████████▏ | 14525/17834 [7:25:03<1:35:41,  1.73s/it] 81%|████████▏ | 14526/17834 [7:25:05<1:36:16,  1.75s/it] 81%|████████▏ | 14527/17834 [7:25:06<1:37:21,  1.77s/it] 81%|████████▏ | 14528/17834 [7:25:08<1:36:28,  1.75s/it] 81%|████████▏ | 14529/17834 [7:25:10<1:35:22,  1.73s/it] 81%|████████▏ | 14530/17834 [7:25:12<1:36:02,  1.74s/it] 81%|████████▏ | 14531/17834 [7:25:13<1:36:17,  1.75s/it] 81%|████████▏ | 14532/17834 [7:25:15<1:36:14,  1.75s/it] 81%|████████▏ | 14533/17834 [7:25:17<1:35:28,  1.74s/it] 81%|████████▏ | 14534/17834 [7:25:19<1:36:27,  1.75s/it] 82%|████████▏ | 14535/17834 [7:25:20<1:37:18,  1.77s/it] 82%|████████▏ | 14536/17834 [7:25:22<1:36:47,  1.76s/it] 82%|████████▏ | 14537/17834 [7:25:24<1:36:50,  1.76s/it] 82%|████████▏ | 14538/17834 [7:25:26<1:35:47,  1.74s/it] 82%|████████▏ | 14539/17834 [7:25:27<1:36:54,  1.76s/it] 82%|████████▏ | 14540/17834 [7:25:29<1:35:39,  1.74s/it] 82%|████████▏ | 14541/17834 [7:25:31<1:36:21,  1.76s/it] 82%|████████▏ | 14542/17834 [7:25:33<1:35:38,  1.74s/it] 82%|████████▏ | 14543/17834 [7:25:34<1:35:40,  1.74s/it] 82%|████████▏ | 14544/17834 [7:25:36<1:35:50,  1.75s/it] 82%|████████▏ | 14545/17834 [7:25:38<1:37:03,  1.77s/it] 82%|████████▏ | 14546/17834 [7:25:40<1:35:29,  1.74s/it] 82%|████████▏ | 14547/17834 [7:25:41<1:35:48,  1.75s/it] 82%|████████▏ | 14548/17834 [7:25:43<1:35:56,  1.75s/it] 82%|████████▏ | 14549/17834 [7:25:45<1:35:46,  1.75s/it]08/31/2024 02:40:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1005098819732666, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03942276909947395, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.243985176086426, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.383917808532715}
 82%|████████▏ | 14550/17834 [7:25:47<1:35:24,  1.74s/it] 82%|████████▏ | 14551/17834 [7:25:48<1:34:53,  1.73s/it] 82%|████████▏ | 14552/17834 [7:25:50<1:37:02,  1.77s/it] 82%|████████▏ | 14553/17834 [7:25:52<1:36:38,  1.77s/it] 82%|████████▏ | 14554/17834 [7:25:54<1:35:42,  1.75s/it] 82%|████████▏ | 14555/17834 [7:25:55<1:35:16,  1.74s/it] 82%|████████▏ | 14556/17834 [7:25:57<1:36:02,  1.76s/it] 82%|████████▏ | 14557/17834 [7:25:59<1:35:02,  1.74s/it] 82%|████████▏ | 14558/17834 [7:26:01<1:34:51,  1.74s/it] 82%|████████▏ | 14559/17834 [7:26:02<1:36:09,  1.76s/it] 82%|████████▏ | 14560/17834 [7:26:04<1:35:39,  1.75s/it] 82%|████████▏ | 14561/17834 [7:26:06<1:35:14,  1.75s/it] 82%|████████▏ | 14562/17834 [7:26:08<1:35:42,  1.75s/it] 82%|████████▏ | 14563/17834 [7:26:09<1:35:55,  1.76s/it] 82%|████████▏ | 14564/17834 [7:26:11<1:35:18,  1.75s/it] 82%|████████▏ | 14565/17834 [7:26:13<1:34:16,  1.73s/it] 82%|████████▏ | 14566/17834 [7:26:14<1:33:21,  1.71s/it] 82%|████████▏ | 14567/17834 [7:26:16<1:33:43,  1.72s/it] 82%|████████▏ | 14568/17834 [7:26:18<1:34:41,  1.74s/it] 82%|████████▏ | 14569/17834 [7:26:20<1:34:39,  1.74s/it] 82%|████████▏ | 14570/17834 [7:26:21<1:35:05,  1.75s/it] 82%|████████▏ | 14571/17834 [7:26:23<1:35:41,  1.76s/it] 82%|████████▏ | 14572/17834 [7:26:25<1:35:32,  1.76s/it] 82%|████████▏ | 14573/17834 [7:26:27<1:36:25,  1.77s/it] 82%|████████▏ | 14574/17834 [7:26:29<1:35:25,  1.76s/it] 82%|████████▏ | 14575/17834 [7:26:30<1:35:56,  1.77s/it] 82%|████████▏ | 14576/17834 [7:26:32<1:35:18,  1.76s/it] 82%|████████▏ | 14577/17834 [7:26:34<1:33:18,  1.72s/it] 82%|████████▏ | 14578/17834 [7:26:35<1:33:09,  1.72s/it] 82%|████████▏ | 14579/17834 [7:26:37<1:33:39,  1.73s/it] 82%|████████▏ | 14580/17834 [7:26:39<1:34:31,  1.74s/it] 82%|████████▏ | 14581/17834 [7:26:41<1:35:29,  1.76s/it] 82%|████████▏ | 14582/17834 [7:26:42<1:34:17,  1.74s/it] 82%|████████▏ | 14583/17834 [7:26:44<1:34:26,  1.74s/it] 82%|████████▏ | 14584/17834 [7:26:46<1:35:00,  1.75s/it] 82%|████████▏ | 14585/17834 [7:26:48<1:34:42,  1.75s/it] 82%|████████▏ | 14586/17834 [7:26:49<1:34:20,  1.74s/it] 82%|████████▏ | 14587/17834 [7:26:51<1:33:06,  1.72s/it] 82%|████████▏ | 14588/17834 [7:26:53<1:33:24,  1.73s/it] 82%|████████▏ | 14589/17834 [7:26:55<1:35:38,  1.77s/it] 82%|████████▏ | 14590/17834 [7:26:56<1:33:38,  1.73s/it] 82%|████████▏ | 14591/17834 [7:26:58<1:35:25,  1.77s/it] 82%|████████▏ | 14592/17834 [7:27:00<1:34:50,  1.76s/it] 82%|████████▏ | 14593/17834 [7:27:02<1:34:06,  1.74s/it] 82%|████████▏ | 14594/17834 [7:27:03<1:35:42,  1.77s/it] 82%|████████▏ | 14595/17834 [7:27:05<1:35:48,  1.77s/it] 82%|████████▏ | 14596/17834 [7:27:07<1:35:59,  1.78s/it] 82%|████████▏ | 14597/17834 [7:27:09<1:35:48,  1.78s/it] 82%|████████▏ | 14598/17834 [7:27:11<1:34:57,  1.76s/it] 82%|████████▏ | 14599/17834 [7:27:12<1:34:51,  1.76s/it]08/31/2024 02:41:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1319937705993652, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.027173355221748352, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.254410982131958, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4135780334472656}
 82%|████████▏ | 14600/17834 [7:27:14<1:35:47,  1.78s/it] 82%|████████▏ | 14601/17834 [7:27:16<1:35:52,  1.78s/it] 82%|████████▏ | 14602/17834 [7:27:18<1:34:34,  1.76s/it] 82%|████████▏ | 14603/17834 [7:27:19<1:33:39,  1.74s/it] 82%|████████▏ | 14604/17834 [7:27:21<1:35:10,  1.77s/it] 82%|████████▏ | 14605/17834 [7:27:23<1:35:43,  1.78s/it] 82%|████████▏ | 14606/17834 [7:27:25<1:34:30,  1.76s/it] 82%|████████▏ | 14607/17834 [7:27:26<1:34:22,  1.75s/it] 82%|████████▏ | 14608/17834 [7:27:28<1:35:10,  1.77s/it] 82%|████████▏ | 14609/17834 [7:27:30<1:34:29,  1.76s/it] 82%|████████▏ | 14610/17834 [7:27:32<1:34:17,  1.75s/it] 82%|████████▏ | 14611/17834 [7:27:33<1:35:12,  1.77s/it] 82%|████████▏ | 14612/17834 [7:27:35<1:36:16,  1.79s/it] 82%|████████▏ | 14613/17834 [7:27:37<1:35:24,  1.78s/it] 82%|████████▏ | 14614/17834 [7:27:39<1:34:33,  1.76s/it] 82%|████████▏ | 14615/17834 [7:27:41<1:37:04,  1.81s/it] 82%|████████▏ | 14616/17834 [7:27:42<1:35:45,  1.79s/it] 82%|████████▏ | 14617/17834 [7:27:44<1:34:27,  1.76s/it] 82%|████████▏ | 14618/17834 [7:27:46<1:34:00,  1.75s/it] 82%|████████▏ | 14619/17834 [7:27:48<1:33:07,  1.74s/it] 82%|████████▏ | 14620/17834 [7:27:49<1:33:34,  1.75s/it] 82%|████████▏ | 14621/17834 [7:27:51<1:33:32,  1.75s/it] 82%|████████▏ | 14622/17834 [7:27:53<1:32:42,  1.73s/it] 82%|████████▏ | 14623/17834 [7:27:55<1:32:26,  1.73s/it] 82%|████████▏ | 14624/17834 [7:27:56<1:33:23,  1.75s/it] 82%|████████▏ | 14625/17834 [7:27:58<1:32:37,  1.73s/it] 82%|████████▏ | 14626/17834 [7:28:00<1:33:25,  1.75s/it] 82%|████████▏ | 14627/17834 [7:28:02<1:33:01,  1.74s/it] 82%|████████▏ | 14628/17834 [7:28:03<1:33:36,  1.75s/it] 82%|████████▏ | 14629/17834 [7:28:05<1:32:37,  1.73s/it] 82%|████████▏ | 14630/17834 [7:28:07<1:33:12,  1.75s/it] 82%|████████▏ | 14631/17834 [7:28:09<1:34:58,  1.78s/it] 82%|████████▏ | 14632/17834 [7:28:10<1:33:16,  1.75s/it] 82%|████████▏ | 14633/17834 [7:28:12<1:33:42,  1.76s/it] 82%|████████▏ | 14634/17834 [7:28:14<1:34:42,  1.78s/it] 82%|████████▏ | 14635/17834 [7:28:16<1:33:37,  1.76s/it] 82%|████████▏ | 14636/17834 [7:28:17<1:33:18,  1.75s/it] 82%|████████▏ | 14637/17834 [7:28:19<1:32:41,  1.74s/it] 82%|████████▏ | 14638/17834 [7:28:21<1:34:25,  1.77s/it] 82%|████████▏ | 14639/17834 [7:28:23<1:33:36,  1.76s/it] 82%|████████▏ | 14640/17834 [7:28:24<1:34:15,  1.77s/it] 82%|████████▏ | 14641/17834 [7:28:26<1:33:45,  1.76s/it] 82%|████████▏ | 14642/17834 [7:28:28<1:33:10,  1.75s/it] 82%|████████▏ | 14643/17834 [7:28:30<1:35:19,  1.79s/it] 82%|████████▏ | 14644/17834 [7:28:32<1:34:37,  1.78s/it] 82%|████████▏ | 14645/17834 [7:28:33<1:33:47,  1.76s/it] 82%|████████▏ | 14646/17834 [7:28:35<1:34:02,  1.77s/it] 82%|████████▏ | 14647/17834 [7:28:37<1:35:10,  1.79s/it] 82%|████████▏ | 14648/17834 [7:28:39<1:33:22,  1.76s/it] 82%|████████▏ | 14649/17834 [7:28:40<1:33:41,  1.76s/it]08/31/2024 02:43:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.248492956161499, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03672222048044205, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.227175712585449, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5123908519744873}
 82%|████████▏ | 14650/17834 [7:28:42<1:33:17,  1.76s/it] 82%|████████▏ | 14651/17834 [7:28:44<1:33:27,  1.76s/it] 82%|████████▏ | 14652/17834 [7:28:46<1:32:57,  1.75s/it] 82%|████████▏ | 14653/17834 [7:28:47<1:33:47,  1.77s/it] 82%|████████▏ | 14654/17834 [7:28:49<1:34:55,  1.79s/it] 82%|████████▏ | 14655/17834 [7:28:51<1:33:06,  1.76s/it] 82%|████████▏ | 14656/17834 [7:28:53<1:32:17,  1.74s/it] 82%|████████▏ | 14657/17834 [7:28:54<1:33:09,  1.76s/it] 82%|████████▏ | 14658/17834 [7:28:56<1:33:56,  1.77s/it] 82%|████████▏ | 14659/17834 [7:28:58<1:33:46,  1.77s/it] 82%|████████▏ | 14660/17834 [7:29:00<1:34:17,  1.78s/it] 82%|████████▏ | 14661/17834 [7:29:02<1:33:11,  1.76s/it] 82%|████████▏ | 14662/17834 [7:29:03<1:32:53,  1.76s/it] 82%|████████▏ | 14663/17834 [7:29:05<1:32:23,  1.75s/it] 82%|████████▏ | 14664/17834 [7:29:07<1:32:41,  1.75s/it] 82%|████████▏ | 14665/17834 [7:29:09<1:32:38,  1.75s/it] 82%|████████▏ | 14666/17834 [7:29:10<1:32:42,  1.76s/it] 82%|████████▏ | 14667/17834 [7:29:12<1:31:31,  1.73s/it] 82%|████████▏ | 14668/17834 [7:29:14<1:31:08,  1.73s/it] 82%|████████▏ | 14669/17834 [7:29:15<1:31:49,  1.74s/it] 82%|████████▏ | 14670/17834 [7:29:17<1:33:03,  1.76s/it] 82%|████████▏ | 14671/17834 [7:29:19<1:32:13,  1.75s/it] 82%|████████▏ | 14672/17834 [7:29:21<1:32:15,  1.75s/it] 82%|████████▏ | 14673/17834 [7:29:23<1:32:54,  1.76s/it] 82%|████████▏ | 14674/17834 [7:29:24<1:32:11,  1.75s/it] 82%|████████▏ | 14675/17834 [7:29:26<1:30:36,  1.72s/it] 82%|████████▏ | 14676/17834 [7:29:28<1:30:15,  1.71s/it] 82%|████████▏ | 14677/17834 [7:29:29<1:30:03,  1.71s/it] 82%|████████▏ | 14678/17834 [7:29:31<1:31:01,  1.73s/it] 82%|████████▏ | 14679/17834 [7:29:33<1:30:39,  1.72s/it] 82%|████████▏ | 14680/17834 [7:29:35<1:31:16,  1.74s/it] 82%|████████▏ | 14681/17834 [7:29:36<1:31:11,  1.74s/it] 82%|████████▏ | 14682/17834 [7:29:38<1:30:29,  1.72s/it] 82%|████████▏ | 14683/17834 [7:29:40<1:32:16,  1.76s/it] 82%|████████▏ | 14684/17834 [7:29:42<1:33:07,  1.77s/it] 82%|████████▏ | 14685/17834 [7:29:43<1:32:30,  1.76s/it] 82%|████████▏ | 14686/17834 [7:29:45<1:33:19,  1.78s/it] 82%|████████▏ | 14687/17834 [7:29:47<1:32:40,  1.77s/it] 82%|████████▏ | 14688/17834 [7:29:49<1:31:10,  1.74s/it] 82%|████████▏ | 14689/17834 [7:29:50<1:32:08,  1.76s/it] 82%|████████▏ | 14690/17834 [7:29:52<1:32:04,  1.76s/it] 82%|████████▏ | 14691/17834 [7:29:54<1:32:15,  1.76s/it] 82%|████████▏ | 14692/17834 [7:29:56<1:32:36,  1.77s/it] 82%|████████▏ | 14693/17834 [7:29:57<1:31:49,  1.75s/it] 82%|████████▏ | 14694/17834 [7:29:59<1:32:03,  1.76s/it] 82%|████████▏ | 14695/17834 [7:30:01<1:30:58,  1.74s/it] 82%|████████▏ | 14696/17834 [7:30:03<1:32:38,  1.77s/it] 82%|████████▏ | 14697/17834 [7:30:04<1:31:40,  1.75s/it] 82%|████████▏ | 14698/17834 [7:30:06<1:32:09,  1.76s/it] 82%|████████▏ | 14699/17834 [7:30:08<1:31:34,  1.75s/it]08/31/2024 02:44:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.32277512550354, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0318499431014061, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.211787700653076, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.566412925720215}
 82%|████████▏ | 14700/17834 [7:30:10<1:32:09,  1.76s/it] 82%|████████▏ | 14701/17834 [7:30:11<1:31:29,  1.75s/it] 82%|████████▏ | 14702/17834 [7:30:13<1:31:51,  1.76s/it] 82%|████████▏ | 14703/17834 [7:30:15<1:32:34,  1.77s/it] 82%|████████▏ | 14704/17834 [7:30:17<1:32:28,  1.77s/it] 82%|████████▏ | 14705/17834 [7:30:19<1:32:19,  1.77s/it] 82%|████████▏ | 14706/17834 [7:30:20<1:30:59,  1.75s/it] 82%|████████▏ | 14707/17834 [7:30:22<1:31:47,  1.76s/it] 82%|████████▏ | 14708/17834 [7:30:24<1:32:02,  1.77s/it] 82%|████████▏ | 14709/17834 [7:30:26<1:31:39,  1.76s/it] 82%|████████▏ | 14710/17834 [7:30:27<1:32:07,  1.77s/it] 82%|████████▏ | 14711/17834 [7:30:29<1:31:40,  1.76s/it] 82%|████████▏ | 14712/17834 [7:30:31<1:31:12,  1.75s/it] 82%|████████▏ | 14713/17834 [7:30:33<1:29:42,  1.72s/it] 83%|████████▎ | 14714/17834 [7:30:34<1:31:26,  1.76s/it] 83%|████████▎ | 14715/17834 [7:30:36<1:30:18,  1.74s/it] 83%|████████▎ | 14716/17834 [7:30:38<1:30:29,  1.74s/it] 83%|████████▎ | 14717/17834 [7:30:40<1:31:49,  1.77s/it] 83%|████████▎ | 14718/17834 [7:30:41<1:31:07,  1.75s/it] 83%|████████▎ | 14719/17834 [7:30:43<1:30:21,  1.74s/it] 83%|████████▎ | 14720/17834 [7:30:45<1:30:53,  1.75s/it] 83%|████████▎ | 14721/17834 [7:30:47<1:30:14,  1.74s/it] 83%|████████▎ | 14722/17834 [7:30:48<1:29:55,  1.73s/it] 83%|████████▎ | 14723/17834 [7:30:50<1:31:56,  1.77s/it] 83%|████████▎ | 14724/17834 [7:30:52<1:31:23,  1.76s/it] 83%|████████▎ | 14725/17834 [7:30:54<1:30:59,  1.76s/it] 83%|████████▎ | 14726/17834 [7:30:55<1:30:00,  1.74s/it] 83%|████████▎ | 14727/17834 [7:30:57<1:30:08,  1.74s/it] 83%|████████▎ | 14728/17834 [7:30:59<1:29:30,  1.73s/it] 83%|████████▎ | 14729/17834 [7:31:01<1:29:51,  1.74s/it] 83%|████████▎ | 14730/17834 [7:31:02<1:30:37,  1.75s/it] 83%|████████▎ | 14731/17834 [7:31:04<1:29:52,  1.74s/it] 83%|████████▎ | 14732/17834 [7:31:06<1:30:31,  1.75s/it] 83%|████████▎ | 14733/17834 [7:31:07<1:29:40,  1.73s/it] 83%|████████▎ | 14734/17834 [7:31:09<1:29:14,  1.73s/it] 83%|████████▎ | 14735/17834 [7:31:11<1:29:50,  1.74s/it] 83%|████████▎ | 14736/17834 [7:31:13<1:31:48,  1.78s/it] 83%|████████▎ | 14737/17834 [7:31:15<1:31:29,  1.77s/it] 83%|████████▎ | 14738/17834 [7:31:16<1:31:32,  1.77s/it] 83%|████████▎ | 14739/17834 [7:31:18<1:29:57,  1.74s/it] 83%|████████▎ | 14740/17834 [7:31:20<1:29:53,  1.74s/it] 83%|████████▎ | 14741/17834 [7:31:22<1:29:49,  1.74s/it] 83%|████████▎ | 14742/17834 [7:31:23<1:29:21,  1.73s/it] 83%|████████▎ | 14743/17834 [7:31:25<1:29:58,  1.75s/it] 83%|████████▎ | 14744/17834 [7:31:27<1:30:43,  1.76s/it] 83%|████████▎ | 14745/17834 [7:31:29<1:30:06,  1.75s/it] 83%|████████▎ | 14746/17834 [7:31:30<1:29:34,  1.74s/it] 83%|████████▎ | 14747/17834 [7:31:32<1:29:25,  1.74s/it] 83%|████████▎ | 14748/17834 [7:31:34<1:30:08,  1.75s/it] 83%|████████▎ | 14749/17834 [7:31:36<1:30:26,  1.76s/it]08/31/2024 02:45:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3917746543884277, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03496256470680237, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.344064235687256, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.770801544189453}
 83%|████████▎ | 14750/17834 [7:31:37<1:30:00,  1.75s/it] 83%|████████▎ | 14751/17834 [7:31:39<1:29:51,  1.75s/it] 83%|████████▎ | 14752/17834 [7:31:41<1:31:59,  1.79s/it] 83%|████████▎ | 14753/17834 [7:31:43<1:30:45,  1.77s/it] 83%|████████▎ | 14754/17834 [7:31:44<1:30:17,  1.76s/it] 83%|████████▎ | 14755/17834 [7:31:46<1:30:01,  1.75s/it] 83%|████████▎ | 14756/17834 [7:31:48<1:29:13,  1.74s/it] 83%|████████▎ | 14757/17834 [7:31:50<1:28:48,  1.73s/it] 83%|████████▎ | 14758/17834 [7:31:51<1:31:37,  1.79s/it] 83%|████████▎ | 14759/17834 [7:31:53<1:31:46,  1.79s/it] 83%|████████▎ | 14760/17834 [7:31:55<1:31:14,  1.78s/it] 83%|████████▎ | 14761/17834 [7:31:57<1:30:38,  1.77s/it] 83%|████████▎ | 14762/17834 [7:31:58<1:29:33,  1.75s/it] 83%|████████▎ | 14763/17834 [7:32:00<1:29:03,  1.74s/it] 83%|████████▎ | 14764/17834 [7:32:02<1:29:30,  1.75s/it] 83%|████████▎ | 14765/17834 [7:32:04<1:29:36,  1.75s/it] 83%|████████▎ | 14766/17834 [7:32:05<1:28:47,  1.74s/it] 83%|████████▎ | 14767/17834 [7:32:07<1:28:50,  1.74s/it] 83%|████████▎ | 14768/17834 [7:32:09<1:29:50,  1.76s/it] 83%|████████▎ | 14769/17834 [7:32:11<1:28:48,  1.74s/it] 83%|████████▎ | 14770/17834 [7:32:12<1:30:21,  1.77s/it] 83%|████████▎ | 14771/17834 [7:32:14<1:29:13,  1.75s/it] 83%|████████▎ | 14772/17834 [7:32:16<1:29:14,  1.75s/it] 83%|████████▎ | 14773/17834 [7:32:18<1:30:30,  1.77s/it] 83%|████████▎ | 14774/17834 [7:32:19<1:29:24,  1.75s/it] 83%|████████▎ | 14775/17834 [7:32:21<1:30:28,  1.77s/it] 83%|████████▎ | 14776/17834 [7:32:23<1:30:24,  1.77s/it] 83%|████████▎ | 14777/17834 [7:32:25<1:29:12,  1.75s/it] 83%|████████▎ | 14778/17834 [7:32:27<1:30:15,  1.77s/it] 83%|████████▎ | 14779/17834 [7:32:28<1:28:41,  1.74s/it] 83%|████████▎ | 14780/17834 [7:32:30<1:28:24,  1.74s/it] 83%|████████▎ | 14781/17834 [7:32:32<1:29:24,  1.76s/it] 83%|████████▎ | 14782/17834 [7:32:34<1:29:16,  1.75s/it] 83%|████████▎ | 14783/17834 [7:32:35<1:28:17,  1.74s/it] 83%|████████▎ | 14784/17834 [7:32:37<1:28:29,  1.74s/it] 83%|████████▎ | 14785/17834 [7:32:39<1:27:43,  1.73s/it] 83%|████████▎ | 14786/17834 [7:32:40<1:28:42,  1.75s/it] 83%|████████▎ | 14787/17834 [7:32:42<1:28:34,  1.74s/it] 83%|████████▎ | 14788/17834 [7:32:44<1:28:49,  1.75s/it] 83%|████████▎ | 14789/17834 [7:32:46<1:29:09,  1.76s/it] 83%|████████▎ | 14790/17834 [7:32:48<1:29:57,  1.77s/it] 83%|████████▎ | 14791/17834 [7:32:49<1:28:49,  1.75s/it] 83%|████████▎ | 14792/17834 [7:32:51<1:29:56,  1.77s/it] 83%|████████▎ | 14793/17834 [7:32:53<1:28:19,  1.74s/it] 83%|████████▎ | 14794/17834 [7:32:54<1:28:13,  1.74s/it] 83%|████████▎ | 14795/17834 [7:32:56<1:29:10,  1.76s/it] 83%|████████▎ | 14796/17834 [7:32:58<1:28:47,  1.75s/it] 83%|████████▎ | 14797/17834 [7:33:00<1:27:41,  1.73s/it] 83%|████████▎ | 14798/17834 [7:33:02<1:29:10,  1.76s/it] 83%|████████▎ | 14799/17834 [7:33:03<1:29:06,  1.76s/it]08/31/2024 02:47:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1421551704406738, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03951860964298248, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.171975612640381, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.353649377822876}
 83%|████████▎ | 14800/17834 [7:33:05<1:28:00,  1.74s/it] 83%|████████▎ | 14801/17834 [7:33:07<1:27:43,  1.74s/it] 83%|████████▎ | 14802/17834 [7:33:08<1:26:53,  1.72s/it] 83%|████████▎ | 14803/17834 [7:33:10<1:27:26,  1.73s/it] 83%|████████▎ | 14804/17834 [7:33:12<1:27:40,  1.74s/it] 83%|████████▎ | 14805/17834 [7:33:14<1:28:10,  1.75s/it] 83%|████████▎ | 14806/17834 [7:33:15<1:29:24,  1.77s/it] 83%|████████▎ | 14807/17834 [7:33:17<1:27:51,  1.74s/it] 83%|████████▎ | 14808/17834 [7:33:19<1:28:11,  1.75s/it] 83%|████████▎ | 14809/17834 [7:33:21<1:27:57,  1.74s/it] 83%|████████▎ | 14810/17834 [7:33:22<1:28:56,  1.76s/it] 83%|████████▎ | 14811/17834 [7:33:24<1:28:11,  1.75s/it] 83%|████████▎ | 14812/17834 [7:33:26<1:27:27,  1.74s/it] 83%|████████▎ | 14813/17834 [7:33:28<1:27:02,  1.73s/it] 83%|████████▎ | 14814/17834 [7:33:29<1:26:42,  1.72s/it] 83%|████████▎ | 14815/17834 [7:33:31<1:26:53,  1.73s/it] 83%|████████▎ | 14816/17834 [7:33:33<1:27:06,  1.73s/it] 83%|████████▎ | 14817/17834 [7:33:34<1:26:25,  1.72s/it] 83%|████████▎ | 14818/17834 [7:33:36<1:27:38,  1.74s/it] 83%|████████▎ | 14819/17834 [7:33:38<1:27:58,  1.75s/it] 83%|████████▎ | 14820/17834 [7:33:40<1:27:52,  1.75s/it] 83%|████████▎ | 14821/17834 [7:33:42<1:28:01,  1.75s/it] 83%|████████▎ | 14822/17834 [7:33:43<1:28:22,  1.76s/it] 83%|████████▎ | 14823/17834 [7:33:45<1:27:40,  1.75s/it] 83%|████████▎ | 14824/17834 [7:33:47<1:27:56,  1.75s/it] 83%|████████▎ | 14825/17834 [7:33:49<1:27:34,  1.75s/it] 83%|████████▎ | 14826/17834 [7:33:50<1:28:36,  1.77s/it] 83%|████████▎ | 14827/17834 [7:33:52<1:27:30,  1.75s/it] 83%|████████▎ | 14828/17834 [7:33:54<1:27:44,  1.75s/it] 83%|████████▎ | 14829/17834 [7:33:56<1:26:31,  1.73s/it] 83%|████████▎ | 14830/17834 [7:33:57<1:27:12,  1.74s/it] 83%|████████▎ | 14831/17834 [7:33:59<1:26:44,  1.73s/it] 83%|████████▎ | 14832/17834 [7:34:01<1:27:30,  1.75s/it] 83%|████████▎ | 14833/17834 [7:34:03<1:28:05,  1.76s/it] 83%|████████▎ | 14834/17834 [7:34:04<1:28:25,  1.77s/it] 83%|████████▎ | 14835/17834 [7:34:06<1:28:49,  1.78s/it] 83%|████████▎ | 14836/17834 [7:34:08<1:27:52,  1.76s/it] 83%|████████▎ | 14837/17834 [7:34:10<1:27:33,  1.75s/it] 83%|████████▎ | 14838/17834 [7:34:11<1:26:42,  1.74s/it] 83%|████████▎ | 14839/17834 [7:34:13<1:26:19,  1.73s/it] 83%|████████▎ | 14840/17834 [7:34:15<1:26:59,  1.74s/it] 83%|████████▎ | 14841/17834 [7:34:17<1:26:32,  1.74s/it] 83%|████████▎ | 14842/17834 [7:34:18<1:26:30,  1.73s/it] 83%|████████▎ | 14843/17834 [7:34:20<1:27:48,  1.76s/it] 83%|████████▎ | 14844/17834 [7:34:22<1:26:37,  1.74s/it] 83%|████████▎ | 14845/17834 [7:34:23<1:26:38,  1.74s/it] 83%|████████▎ | 14846/17834 [7:34:25<1:25:07,  1.71s/it] 83%|████████▎ | 14847/17834 [7:34:27<1:23:30,  1.68s/it] 83%|████████▎ | 14848/17834 [7:34:28<1:22:20,  1.65s/it] 83%|████████▎ | 14849/17834 [7:34:30<1:21:22,  1.64s/it]08/31/2024 02:48:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.140955924987793, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03302072361111641, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.241583824157715, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.415560483932495}
 83%|████████▎ | 14850/17834 [7:34:32<1:20:45,  1.62s/it] 83%|████████▎ | 14851/17834 [7:34:33<1:20:19,  1.62s/it] 83%|████████▎ | 14852/17834 [7:34:35<1:20:04,  1.61s/it] 83%|████████▎ | 14853/17834 [7:34:36<1:19:54,  1.61s/it] 83%|████████▎ | 14854/17834 [7:34:38<1:19:47,  1.61s/it] 83%|████████▎ | 14855/17834 [7:34:40<1:19:37,  1.60s/it] 83%|████████▎ | 14856/17834 [7:34:41<1:19:26,  1.60s/it] 83%|████████▎ | 14857/17834 [7:34:43<1:19:25,  1.60s/it] 83%|████████▎ | 14858/17834 [7:34:44<1:19:19,  1.60s/it] 83%|████████▎ | 14859/17834 [7:34:46<1:19:18,  1.60s/it] 83%|████████▎ | 14860/17834 [7:34:48<1:19:14,  1.60s/it] 83%|████████▎ | 14861/17834 [7:34:49<1:19:07,  1.60s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
 83%|████████▎ | 14862/17834 [7:35:14<7:06:36,  8.61s/it] 83%|████████▎ | 14863/17834 [7:35:17<5:39:15,  6.85s/it] 83%|████████▎ | 14864/17834 [7:35:19<4:28:07,  5.42s/it] 83%|████████▎ | 14865/17834 [7:35:21<3:33:39,  4.32s/it] 83%|████████▎ | 14866/17834 [7:35:23<2:57:55,  3.60s/it] 83%|████████▎ | 14867/17834 [7:35:24<2:29:26,  3.02s/it] 83%|████████▎ | 14868/17834 [7:35:26<2:11:59,  2.67s/it] 83%|████████▎ | 14869/17834 [7:35:28<1:57:49,  2.38s/it] 83%|████████▎ | 14870/17834 [7:35:29<1:47:26,  2.18s/it] 83%|████████▎ | 14871/17834 [7:35:31<1:41:53,  2.06s/it] 83%|████████▎ | 14872/17834 [7:35:33<1:37:20,  1.97s/it] 83%|████████▎ | 14873/17834 [7:35:35<1:34:21,  1.91s/it] 83%|████████▎ | 14874/17834 [7:35:37<1:32:34,  1.88s/it] 83%|████████▎ | 14875/17834 [7:35:38<1:31:14,  1.85s/it] 83%|████████▎ | 14876/17834 [7:35:40<1:29:00,  1.81s/it] 83%|████████▎ | 14877/17834 [7:35:42<1:28:36,  1.80s/it] 83%|████████▎ | 14878/17834 [7:35:44<1:29:06,  1.81s/it] 83%|████████▎ | 14879/17834 [7:35:45<1:27:57,  1.79s/it] 83%|████████▎ | 14880/17834 [7:35:47<1:28:11,  1.79s/it] 83%|████████▎ | 14881/17834 [7:35:49<1:27:10,  1.77s/it] 83%|████████▎ | 14882/17834 [7:35:51<1:27:11,  1.77s/it] 83%|████████▎ | 14883/17834 [7:35:52<1:26:22,  1.76s/it] 83%|████████▎ | 14884/17834 [7:35:54<1:25:59,  1.75s/it] 83%|████████▎ | 14885/17834 [7:35:56<1:25:21,  1.74s/it] 83%|████████▎ | 14886/17834 [7:35:58<1:25:26,  1.74s/it] 83%|████████▎ | 14887/17834 [7:35:59<1:24:33,  1.72s/it] 83%|████████▎ | 14888/17834 [7:36:01<1:26:24,  1.76s/it] 83%|████████▎ | 14889/17834 [7:36:03<1:26:37,  1.76s/it] 83%|████████▎ | 14890/17834 [7:36:05<1:27:50,  1.79s/it] 83%|████████▎ | 14891/17834 [7:36:07<1:28:11,  1.80s/it] 84%|████████▎ | 14892/17834 [7:36:08<1:26:03,  1.75s/it] 84%|████████▎ | 14893/17834 [7:36:10<1:25:48,  1.75s/it] 84%|████████▎ | 14894/17834 [7:36:12<1:25:59,  1.75s/it] 84%|████████▎ | 14895/17834 [7:36:14<1:25:53,  1.75s/it] 84%|████████▎ | 14896/17834 [7:36:15<1:25:49,  1.75s/it] 84%|████████▎ | 14897/17834 [7:36:17<1:24:47,  1.73s/it] 84%|████████▎ | 14898/17834 [7:36:19<1:24:47,  1.73s/it] 84%|████████▎ | 14899/17834 [7:36:21<1:25:51,  1.76s/it]08/31/2024 02:50:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4040417671203613, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04139166325330734, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2860307693481445, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.731464147567749}
 84%|████████▎ | 14900/17834 [7:36:22<1:25:29,  1.75s/it] 84%|████████▎ | 14901/17834 [7:36:24<1:25:33,  1.75s/it] 84%|████████▎ | 14902/17834 [7:36:26<1:25:34,  1.75s/it] 84%|████████▎ | 14903/17834 [7:36:28<1:26:42,  1.77s/it] 84%|████████▎ | 14904/17834 [7:36:29<1:27:57,  1.80s/it] 84%|████████▎ | 14905/17834 [7:36:31<1:26:41,  1.78s/it] 84%|████████▎ | 14906/17834 [7:36:33<1:26:26,  1.77s/it] 84%|████████▎ | 14907/17834 [7:36:35<1:26:47,  1.78s/it] 84%|████████▎ | 14908/17834 [7:36:36<1:25:55,  1.76s/it] 84%|████████▎ | 14909/17834 [7:36:38<1:25:41,  1.76s/it] 84%|████████▎ | 14910/17834 [7:36:40<1:26:38,  1.78s/it] 84%|████████▎ | 14911/17834 [7:36:42<1:28:14,  1.81s/it] 84%|████████▎ | 14912/17834 [7:36:44<1:27:36,  1.80s/it] 84%|████████▎ | 14913/17834 [7:36:45<1:26:47,  1.78s/it] 84%|████████▎ | 14914/17834 [7:36:47<1:27:53,  1.81s/it] 84%|████████▎ | 14915/17834 [7:36:49<1:27:25,  1.80s/it] 84%|████████▎ | 14916/17834 [7:36:51<1:28:21,  1.82s/it] 84%|████████▎ | 14917/17834 [7:36:53<1:26:09,  1.77s/it] 84%|████████▎ | 14918/17834 [7:36:54<1:26:00,  1.77s/it] 84%|████████▎ | 14919/17834 [7:36:56<1:26:44,  1.79s/it] 84%|████████▎ | 14920/17834 [7:36:58<1:26:09,  1.77s/it] 84%|████████▎ | 14921/17834 [7:37:00<1:25:18,  1.76s/it] 84%|████████▎ | 14922/17834 [7:37:01<1:25:39,  1.76s/it] 84%|████████▎ | 14923/17834 [7:37:03<1:25:27,  1.76s/it] 84%|████████▎ | 14924/17834 [7:37:05<1:25:39,  1.77s/it] 84%|████████▎ | 14925/17834 [7:37:07<1:26:02,  1.77s/it] 84%|████████▎ | 14926/17834 [7:37:09<1:25:49,  1.77s/it] 84%|████████▎ | 14927/17834 [7:37:10<1:24:39,  1.75s/it] 84%|████████▎ | 14928/17834 [7:37:12<1:24:58,  1.75s/it] 84%|████████▎ | 14929/17834 [7:37:14<1:24:58,  1.76s/it] 84%|████████▎ | 14930/17834 [7:37:15<1:24:16,  1.74s/it] 84%|████████▎ | 14931/17834 [7:37:17<1:23:43,  1.73s/it] 84%|████████▎ | 14932/17834 [7:37:19<1:24:23,  1.74s/it] 84%|████████▎ | 14933/17834 [7:37:21<1:25:17,  1.76s/it] 84%|████████▎ | 14934/17834 [7:37:23<1:25:16,  1.76s/it] 84%|████████▎ | 14935/17834 [7:37:24<1:25:04,  1.76s/it] 84%|████████▍ | 14936/17834 [7:37:26<1:24:32,  1.75s/it] 84%|████████▍ | 14937/17834 [7:37:28<1:24:52,  1.76s/it] 84%|████████▍ | 14938/17834 [7:37:30<1:24:36,  1.75s/it] 84%|████████▍ | 14939/17834 [7:37:31<1:24:21,  1.75s/it] 84%|████████▍ | 14940/17834 [7:37:33<1:25:53,  1.78s/it] 84%|████████▍ | 14941/17834 [7:37:35<1:24:37,  1.75s/it] 84%|████████▍ | 14942/17834 [7:37:37<1:24:49,  1.76s/it] 84%|████████▍ | 14943/17834 [7:37:38<1:23:47,  1.74s/it] 84%|████████▍ | 14944/17834 [7:37:40<1:23:23,  1.73s/it] 84%|████████▍ | 14945/17834 [7:37:42<1:22:53,  1.72s/it] 84%|████████▍ | 14946/17834 [7:37:43<1:22:37,  1.72s/it] 84%|████████▍ | 14947/17834 [7:37:45<1:22:45,  1.72s/it] 84%|████████▍ | 14948/17834 [7:37:47<1:23:35,  1.74s/it] 84%|████████▍ | 14949/17834 [7:37:49<1:23:40,  1.74s/it]08/31/2024 02:52:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.959664523601532, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025185011327266693, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1344752311706543, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1193246841430664}
 84%|████████▍ | 14950/17834 [7:37:50<1:23:24,  1.74s/it] 84%|████████▍ | 14951/17834 [7:37:52<1:24:07,  1.75s/it] 84%|████████▍ | 14952/17834 [7:37:54<1:23:39,  1.74s/it] 84%|████████▍ | 14953/17834 [7:37:56<1:22:31,  1.72s/it] 84%|████████▍ | 14954/17834 [7:37:57<1:23:27,  1.74s/it] 84%|████████▍ | 14955/17834 [7:37:59<1:24:51,  1.77s/it] 84%|████████▍ | 14956/17834 [7:38:01<1:24:01,  1.75s/it] 84%|████████▍ | 14957/17834 [7:38:03<1:24:13,  1.76s/it] 84%|████████▍ | 14958/17834 [7:38:04<1:23:31,  1.74s/it] 84%|████████▍ | 14959/17834 [7:38:06<1:23:06,  1.73s/it] 84%|████████▍ | 14960/17834 [7:38:08<1:24:17,  1.76s/it] 84%|████████▍ | 14961/17834 [7:38:10<1:23:10,  1.74s/it] 84%|████████▍ | 14962/17834 [7:38:11<1:23:00,  1.73s/it] 84%|████████▍ | 14963/17834 [7:38:13<1:22:47,  1.73s/it] 84%|████████▍ | 14964/17834 [7:38:15<1:23:12,  1.74s/it] 84%|████████▍ | 14965/17834 [7:38:17<1:24:04,  1.76s/it] 84%|████████▍ | 14966/17834 [7:38:18<1:23:17,  1.74s/it] 84%|████████▍ | 14967/17834 [7:38:20<1:24:22,  1.77s/it] 84%|████████▍ | 14968/17834 [7:38:22<1:23:38,  1.75s/it] 84%|████████▍ | 14969/17834 [7:38:24<1:22:57,  1.74s/it] 84%|████████▍ | 14970/17834 [7:38:25<1:23:58,  1.76s/it] 84%|████████▍ | 14971/17834 [7:38:27<1:23:38,  1.75s/it] 84%|████████▍ | 14972/17834 [7:38:29<1:24:08,  1.76s/it] 84%|████████▍ | 14973/17834 [7:38:31<1:23:24,  1.75s/it] 84%|████████▍ | 14974/17834 [7:38:32<1:24:11,  1.77s/it] 84%|████████▍ | 14975/17834 [7:38:34<1:25:06,  1.79s/it] 84%|████████▍ | 14976/17834 [7:38:36<1:24:13,  1.77s/it] 84%|████████▍ | 14977/17834 [7:38:38<1:23:23,  1.75s/it] 84%|████████▍ | 14978/17834 [7:38:39<1:22:32,  1.73s/it] 84%|████████▍ | 14979/17834 [7:38:41<1:22:30,  1.73s/it] 84%|████████▍ | 14980/17834 [7:38:43<1:23:23,  1.75s/it] 84%|████████▍ | 14981/17834 [7:38:45<1:24:00,  1.77s/it] 84%|████████▍ | 14982/17834 [7:38:46<1:23:33,  1.76s/it] 84%|████████▍ | 14983/17834 [7:38:48<1:22:46,  1.74s/it] 84%|████████▍ | 14984/17834 [7:38:50<1:22:43,  1.74s/it] 84%|████████▍ | 14985/17834 [7:38:52<1:22:36,  1.74s/it] 84%|████████▍ | 14986/17834 [7:38:53<1:23:13,  1.75s/it] 84%|████████▍ | 14987/17834 [7:38:55<1:23:27,  1.76s/it] 84%|████████▍ | 14988/17834 [7:38:57<1:23:26,  1.76s/it] 84%|████████▍ | 14989/17834 [7:38:59<1:21:44,  1.72s/it] 84%|████████▍ | 14990/17834 [7:39:00<1:21:52,  1.73s/it] 84%|████████▍ | 14991/17834 [7:39:02<1:21:35,  1.72s/it] 84%|████████▍ | 14992/17834 [7:39:04<1:21:40,  1.72s/it] 84%|████████▍ | 14993/17834 [7:39:05<1:21:38,  1.72s/it] 84%|████████▍ | 14994/17834 [7:39:07<1:21:17,  1.72s/it] 84%|████████▍ | 14995/17834 [7:39:09<1:23:05,  1.76s/it] 84%|████████▍ | 14996/17834 [7:39:11<1:22:41,  1.75s/it] 84%|████████▍ | 14997/17834 [7:39:13<1:23:18,  1.76s/it] 84%|████████▍ | 14998/17834 [7:39:14<1:23:15,  1.76s/it] 84%|████████▍ | 14999/17834 [7:39:16<1:22:53,  1.75s/it]08/31/2024 02:53:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.094728946685791, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03674978017807007, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.222719669342041, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.354198455810547}
 84%|████████▍ | 15000/17834 [7:39:18<1:22:30,  1.75s/it] 84%|████████▍ | 15001/17834 [7:39:20<1:22:51,  1.75s/it] 84%|████████▍ | 15002/17834 [7:39:21<1:22:39,  1.75s/it] 84%|████████▍ | 15003/17834 [7:39:23<1:22:34,  1.75s/it] 84%|████████▍ | 15004/17834 [7:39:25<1:23:01,  1.76s/it] 84%|████████▍ | 15005/17834 [7:39:27<1:22:47,  1.76s/it] 84%|████████▍ | 15006/17834 [7:39:28<1:22:28,  1.75s/it] 84%|████████▍ | 15007/17834 [7:39:30<1:22:20,  1.75s/it] 84%|████████▍ | 15008/17834 [7:39:32<1:22:41,  1.76s/it] 84%|████████▍ | 15009/17834 [7:39:34<1:23:01,  1.76s/it] 84%|████████▍ | 15010/17834 [7:39:35<1:22:31,  1.75s/it] 84%|████████▍ | 15011/17834 [7:39:37<1:22:42,  1.76s/it] 84%|████████▍ | 15012/17834 [7:39:39<1:22:04,  1.74s/it] 84%|████████▍ | 15013/17834 [7:39:41<1:22:28,  1.75s/it] 84%|████████▍ | 15014/17834 [7:39:42<1:21:17,  1.73s/it] 84%|████████▍ | 15015/17834 [7:39:44<1:22:04,  1.75s/it] 84%|████████▍ | 15016/17834 [7:39:46<1:23:31,  1.78s/it] 84%|████████▍ | 15017/17834 [7:39:48<1:24:24,  1.80s/it] 84%|████████▍ | 15018/17834 [7:39:49<1:23:36,  1.78s/it] 84%|████████▍ | 15019/17834 [7:39:51<1:22:31,  1.76s/it] 84%|████████▍ | 15020/17834 [7:39:53<1:23:04,  1.77s/it] 84%|████████▍ | 15021/17834 [7:39:55<1:22:36,  1.76s/it] 84%|████████▍ | 15022/17834 [7:39:56<1:22:50,  1.77s/it] 84%|████████▍ | 15023/17834 [7:39:58<1:21:57,  1.75s/it] 84%|████████▍ | 15024/17834 [7:40:00<1:22:16,  1.76s/it] 84%|████████▍ | 15025/17834 [7:40:02<1:21:05,  1.73s/it] 84%|████████▍ | 15026/17834 [7:40:03<1:21:13,  1.74s/it] 84%|████████▍ | 15027/17834 [7:40:05<1:20:52,  1.73s/it] 84%|████████▍ | 15028/17834 [7:40:07<1:20:38,  1.72s/it] 84%|████████▍ | 15029/17834 [7:40:09<1:20:48,  1.73s/it] 84%|████████▍ | 15030/17834 [7:40:10<1:20:25,  1.72s/it] 84%|████████▍ | 15031/17834 [7:40:12<1:21:39,  1.75s/it] 84%|████████▍ | 15032/17834 [7:40:14<1:20:36,  1.73s/it] 84%|████████▍ | 15033/17834 [7:40:16<1:21:07,  1.74s/it] 84%|████████▍ | 15034/17834 [7:40:17<1:21:16,  1.74s/it] 84%|████████▍ | 15035/17834 [7:40:19<1:20:24,  1.72s/it] 84%|████████▍ | 15036/17834 [7:40:21<1:19:34,  1.71s/it] 84%|████████▍ | 15037/17834 [7:40:22<1:20:58,  1.74s/it] 84%|████████▍ | 15038/17834 [7:40:24<1:21:25,  1.75s/it] 84%|████████▍ | 15039/17834 [7:40:26<1:22:00,  1.76s/it] 84%|████████▍ | 15040/17834 [7:40:28<1:21:07,  1.74s/it] 84%|████████▍ | 15041/17834 [7:40:29<1:21:11,  1.74s/it] 84%|████████▍ | 15042/17834 [7:40:31<1:21:16,  1.75s/it] 84%|████████▍ | 15043/17834 [7:40:33<1:21:11,  1.75s/it] 84%|████████▍ | 15044/17834 [7:40:35<1:21:09,  1.75s/it] 84%|████████▍ | 15045/17834 [7:40:36<1:20:43,  1.74s/it] 84%|████████▍ | 15046/17834 [7:40:38<1:20:22,  1.73s/it] 84%|████████▍ | 15047/17834 [7:40:40<1:20:28,  1.73s/it] 84%|████████▍ | 15048/17834 [7:40:42<1:21:18,  1.75s/it] 84%|████████▍ | 15049/17834 [7:40:43<1:20:42,  1.74s/it]08/31/2024 02:55:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0230759382247925, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03680456802248955, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.11671781539917, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.176598310470581}
 84%|████████▍ | 15050/17834 [7:40:45<1:21:29,  1.76s/it] 84%|████████▍ | 15051/17834 [7:40:47<1:20:58,  1.75s/it] 84%|████████▍ | 15052/17834 [7:40:49<1:21:02,  1.75s/it] 84%|████████▍ | 15053/17834 [7:40:50<1:21:01,  1.75s/it] 84%|████████▍ | 15054/17834 [7:40:52<1:20:31,  1.74s/it] 84%|████████▍ | 15055/17834 [7:40:54<1:20:04,  1.73s/it] 84%|████████▍ | 15056/17834 [7:40:55<1:19:57,  1.73s/it] 84%|████████▍ | 15057/17834 [7:40:57<1:20:20,  1.74s/it] 84%|████████▍ | 15058/17834 [7:40:59<1:21:26,  1.76s/it] 84%|████████▍ | 15059/17834 [7:41:01<1:21:49,  1.77s/it] 84%|████████▍ | 15060/17834 [7:41:03<1:20:53,  1.75s/it] 84%|████████▍ | 15061/17834 [7:41:04<1:21:17,  1.76s/it] 84%|████████▍ | 15062/17834 [7:41:06<1:22:51,  1.79s/it] 84%|████████▍ | 15063/17834 [7:41:08<1:22:02,  1.78s/it] 84%|████████▍ | 15064/17834 [7:41:10<1:22:05,  1.78s/it] 84%|████████▍ | 15065/17834 [7:41:11<1:21:37,  1.77s/it] 84%|████████▍ | 15066/17834 [7:41:13<1:21:20,  1.76s/it] 84%|████████▍ | 15067/17834 [7:41:15<1:20:52,  1.75s/it] 84%|████████▍ | 15068/17834 [7:41:17<1:21:23,  1.77s/it] 84%|████████▍ | 15069/17834 [7:41:18<1:20:10,  1.74s/it] 85%|████████▍ | 15070/17834 [7:41:20<1:20:51,  1.76s/it] 85%|████████▍ | 15071/17834 [7:41:22<1:20:37,  1.75s/it] 85%|████████▍ | 15072/17834 [7:41:24<1:19:41,  1.73s/it] 85%|████████▍ | 15073/17834 [7:41:25<1:19:16,  1.72s/it] 85%|████████▍ | 15074/17834 [7:41:27<1:21:21,  1.77s/it] 85%|████████▍ | 15075/17834 [7:41:29<1:20:42,  1.76s/it] 85%|████████▍ | 15076/17834 [7:41:31<1:20:40,  1.76s/it] 85%|████████▍ | 15077/17834 [7:41:33<1:21:24,  1.77s/it] 85%|████████▍ | 15078/17834 [7:41:34<1:21:21,  1.77s/it] 85%|████████▍ | 15079/17834 [7:41:36<1:20:44,  1.76s/it] 85%|████████▍ | 15080/17834 [7:41:38<1:19:58,  1.74s/it] 85%|████████▍ | 15081/17834 [7:41:40<1:20:35,  1.76s/it] 85%|████████▍ | 15082/17834 [7:41:41<1:19:42,  1.74s/it] 85%|████████▍ | 15083/17834 [7:41:43<1:19:12,  1.73s/it] 85%|████████▍ | 15084/17834 [7:41:45<1:21:01,  1.77s/it] 85%|████████▍ | 15085/17834 [7:41:47<1:21:34,  1.78s/it] 85%|████████▍ | 15086/17834 [7:41:48<1:22:18,  1.80s/it] 85%|████████▍ | 15087/17834 [7:41:50<1:23:02,  1.81s/it] 85%|████████▍ | 15088/17834 [7:41:52<1:21:59,  1.79s/it] 85%|████████▍ | 15089/17834 [7:41:54<1:21:08,  1.77s/it] 85%|████████▍ | 15090/17834 [7:41:56<1:21:04,  1.77s/it] 85%|████████▍ | 15091/17834 [7:41:57<1:19:47,  1.75s/it] 85%|████████▍ | 15092/17834 [7:41:59<1:19:26,  1.74s/it] 85%|████████▍ | 15093/17834 [7:42:01<1:20:13,  1.76s/it] 85%|████████▍ | 15094/17834 [7:42:02<1:19:38,  1.74s/it] 85%|████████▍ | 15095/17834 [7:42:04<1:20:52,  1.77s/it] 85%|████████▍ | 15096/17834 [7:42:06<1:20:46,  1.77s/it] 85%|████████▍ | 15097/17834 [7:42:08<1:20:18,  1.76s/it] 85%|████████▍ | 15098/17834 [7:42:10<1:20:00,  1.75s/it] 85%|████████▍ | 15099/17834 [7:42:11<1:20:48,  1.77s/it]08/31/2024 02:56:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0590095520019531, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029101205989718437, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1270558834075928, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2151665687561035}
 85%|████████▍ | 15100/17834 [7:42:13<1:20:46,  1.77s/it] 85%|████████▍ | 15101/17834 [7:42:15<1:20:10,  1.76s/it] 85%|████████▍ | 15102/17834 [7:42:17<1:19:36,  1.75s/it] 85%|████████▍ | 15103/17834 [7:42:18<1:18:25,  1.72s/it] 85%|████████▍ | 15104/17834 [7:42:20<1:18:18,  1.72s/it] 85%|████████▍ | 15105/17834 [7:42:22<1:19:25,  1.75s/it] 85%|████████▍ | 15106/17834 [7:42:24<1:19:39,  1.75s/it] 85%|████████▍ | 15107/17834 [7:42:25<1:19:15,  1.74s/it] 85%|████████▍ | 15108/17834 [7:42:27<1:20:06,  1.76s/it] 85%|████████▍ | 15109/17834 [7:42:29<1:19:20,  1.75s/it] 85%|████████▍ | 15110/17834 [7:42:31<1:20:21,  1.77s/it] 85%|████████▍ | 15111/17834 [7:42:32<1:20:58,  1.78s/it] 85%|████████▍ | 15112/17834 [7:42:34<1:19:45,  1.76s/it] 85%|████████▍ | 15113/17834 [7:42:36<1:19:19,  1.75s/it] 85%|████████▍ | 15114/17834 [7:42:38<1:18:33,  1.73s/it] 85%|████████▍ | 15115/17834 [7:42:39<1:18:41,  1.74s/it] 85%|████████▍ | 15116/17834 [7:42:41<1:19:19,  1.75s/it] 85%|████████▍ | 15117/17834 [7:42:43<1:18:39,  1.74s/it] 85%|████████▍ | 15118/17834 [7:42:44<1:18:01,  1.72s/it] 85%|████████▍ | 15119/17834 [7:42:46<1:18:08,  1.73s/it] 85%|████████▍ | 15120/17834 [7:42:48<1:18:49,  1.74s/it] 85%|████████▍ | 15121/17834 [7:42:50<1:18:30,  1.74s/it] 85%|████████▍ | 15122/17834 [7:42:51<1:19:12,  1.75s/it] 85%|████████▍ | 15123/17834 [7:42:53<1:19:08,  1.75s/it] 85%|████████▍ | 15124/17834 [7:42:55<1:19:37,  1.76s/it] 85%|████████▍ | 15125/17834 [7:42:57<1:18:11,  1.73s/it] 85%|████████▍ | 15126/17834 [7:42:58<1:19:31,  1.76s/it] 85%|████████▍ | 15127/17834 [7:43:00<1:19:20,  1.76s/it] 85%|████████▍ | 15128/17834 [7:43:02<1:19:23,  1.76s/it] 85%|████████▍ | 15129/17834 [7:43:04<1:18:21,  1.74s/it] 85%|████████▍ | 15130/17834 [7:43:05<1:17:25,  1.72s/it] 85%|████████▍ | 15131/17834 [7:43:07<1:18:35,  1.74s/it] 85%|████████▍ | 15132/17834 [7:43:09<1:18:00,  1.73s/it] 85%|████████▍ | 15133/17834 [7:43:11<1:18:26,  1.74s/it] 85%|████████▍ | 15134/17834 [7:43:12<1:19:14,  1.76s/it] 85%|████████▍ | 15135/17834 [7:43:14<1:19:36,  1.77s/it] 85%|████████▍ | 15136/17834 [7:43:16<1:19:13,  1.76s/it] 85%|████████▍ | 15137/17834 [7:43:18<1:19:07,  1.76s/it] 85%|████████▍ | 15138/17834 [7:43:20<1:19:41,  1.77s/it] 85%|████████▍ | 15139/17834 [7:43:21<1:20:12,  1.79s/it] 85%|████████▍ | 15140/17834 [7:43:23<1:18:39,  1.75s/it] 85%|████████▍ | 15141/17834 [7:43:25<1:18:23,  1.75s/it] 85%|████████▍ | 15142/17834 [7:43:27<1:18:36,  1.75s/it] 85%|████████▍ | 15143/17834 [7:43:28<1:18:58,  1.76s/it] 85%|████████▍ | 15144/17834 [7:43:30<1:19:18,  1.77s/it] 85%|████████▍ | 15145/17834 [7:43:32<1:19:16,  1.77s/it] 85%|████████▍ | 15146/17834 [7:43:34<1:18:17,  1.75s/it] 85%|████████▍ | 15147/17834 [7:43:35<1:19:18,  1.77s/it] 85%|████████▍ | 15148/17834 [7:43:37<1:17:50,  1.74s/it] 85%|████████▍ | 15149/17834 [7:43:39<1:19:18,  1.77s/it]08/31/2024 02:57:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1186329126358032, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.037329792976379395, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.249114990234375, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4050776958465576}
 85%|████████▍ | 15150/17834 [7:43:41<1:19:18,  1.77s/it] 85%|████████▍ | 15151/17834 [7:43:42<1:18:36,  1.76s/it] 85%|████████▍ | 15152/17834 [7:43:44<1:17:25,  1.73s/it] 85%|████████▍ | 15153/17834 [7:43:46<1:17:44,  1.74s/it] 85%|████████▍ | 15154/17834 [7:43:48<1:18:17,  1.75s/it] 85%|████████▍ | 15155/17834 [7:43:49<1:17:36,  1.74s/it] 85%|████████▍ | 15156/17834 [7:43:51<1:17:31,  1.74s/it] 85%|████████▍ | 15157/17834 [7:43:53<1:17:29,  1.74s/it] 85%|████████▍ | 15158/17834 [7:43:54<1:17:07,  1.73s/it] 85%|████████▌ | 15159/17834 [7:43:56<1:17:31,  1.74s/it] 85%|████████▌ | 15160/17834 [7:43:58<1:18:18,  1.76s/it] 85%|████████▌ | 15161/17834 [7:44:00<1:17:49,  1.75s/it] 85%|████████▌ | 15162/17834 [7:44:01<1:17:20,  1.74s/it] 85%|████████▌ | 15163/17834 [7:44:03<1:17:38,  1.74s/it] 85%|████████▌ | 15164/17834 [7:44:05<1:18:00,  1.75s/it] 85%|████████▌ | 15165/17834 [7:44:07<1:17:30,  1.74s/it] 85%|████████▌ | 15166/17834 [7:44:09<1:18:06,  1.76s/it] 85%|████████▌ | 15167/17834 [7:44:10<1:19:30,  1.79s/it] 85%|████████▌ | 15168/17834 [7:44:12<1:18:08,  1.76s/it] 85%|████████▌ | 15169/17834 [7:44:14<1:19:16,  1.78s/it] 85%|████████▌ | 15170/17834 [7:44:16<1:18:23,  1.77s/it] 85%|████████▌ | 15171/17834 [7:44:17<1:17:29,  1.75s/it] 85%|████████▌ | 15172/17834 [7:44:19<1:17:24,  1.74s/it] 85%|████████▌ | 15173/17834 [7:44:21<1:18:00,  1.76s/it] 85%|████████▌ | 15174/17834 [7:44:23<1:16:35,  1.73s/it] 85%|████████▌ | 15175/17834 [7:44:24<1:15:53,  1.71s/it] 85%|████████▌ | 15176/17834 [7:44:26<1:15:53,  1.71s/it] 85%|████████▌ | 15177/17834 [7:44:28<1:16:59,  1.74s/it] 85%|████████▌ | 15178/17834 [7:44:29<1:17:10,  1.74s/it] 85%|████████▌ | 15179/17834 [7:44:31<1:16:53,  1.74s/it] 85%|████████▌ | 15180/17834 [7:44:33<1:17:19,  1.75s/it] 85%|████████▌ | 15181/17834 [7:44:35<1:17:17,  1.75s/it] 85%|████████▌ | 15182/17834 [7:44:36<1:17:16,  1.75s/it] 85%|████████▌ | 15183/17834 [7:44:38<1:18:00,  1.77s/it] 85%|████████▌ | 15184/17834 [7:44:40<1:18:04,  1.77s/it] 85%|████████▌ | 15185/17834 [7:44:42<1:17:07,  1.75s/it] 85%|████████▌ | 15186/17834 [7:44:43<1:16:52,  1.74s/it] 85%|████████▌ | 15187/17834 [7:44:45<1:15:44,  1.72s/it] 85%|████████▌ | 15188/17834 [7:44:47<1:16:59,  1.75s/it] 85%|████████▌ | 15189/17834 [7:44:49<1:17:35,  1.76s/it] 85%|████████▌ | 15190/17834 [7:44:50<1:17:05,  1.75s/it] 85%|████████▌ | 15191/17834 [7:44:52<1:17:09,  1.75s/it] 85%|████████▌ | 15192/17834 [7:44:54<1:17:02,  1.75s/it] 85%|████████▌ | 15193/17834 [7:44:56<1:17:59,  1.77s/it] 85%|████████▌ | 15194/17834 [7:44:58<1:18:14,  1.78s/it] 85%|████████▌ | 15195/17834 [7:44:59<1:18:05,  1.78s/it] 85%|████████▌ | 15196/17834 [7:45:01<1:18:00,  1.77s/it] 85%|████████▌ | 15197/17834 [7:45:03<1:17:37,  1.77s/it] 85%|████████▌ | 15198/17834 [7:45:05<1:17:17,  1.76s/it] 85%|████████▌ | 15199/17834 [7:45:06<1:17:21,  1.76s/it]08/31/2024 02:59:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9261395931243896, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028628002852201462, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.141090154647827, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.095857620239258}
 85%|████████▌ | 15200/17834 [7:45:08<1:17:31,  1.77s/it] 85%|████████▌ | 15201/17834 [7:45:10<1:18:18,  1.78s/it] 85%|████████▌ | 15202/17834 [7:45:12<1:17:39,  1.77s/it] 85%|████████▌ | 15203/17834 [7:45:13<1:16:40,  1.75s/it] 85%|████████▌ | 15204/17834 [7:45:15<1:16:48,  1.75s/it] 85%|████████▌ | 15205/17834 [7:45:17<1:16:40,  1.75s/it] 85%|████████▌ | 15206/17834 [7:45:19<1:15:51,  1.73s/it] 85%|████████▌ | 15207/17834 [7:45:20<1:16:06,  1.74s/it] 85%|████████▌ | 15208/17834 [7:45:22<1:15:28,  1.72s/it] 85%|████████▌ | 15209/17834 [7:45:24<1:15:14,  1.72s/it] 85%|████████▌ | 15210/17834 [7:45:25<1:15:07,  1.72s/it] 85%|████████▌ | 15211/17834 [7:45:27<1:16:12,  1.74s/it] 85%|████████▌ | 15212/17834 [7:45:29<1:15:55,  1.74s/it] 85%|████████▌ | 15213/17834 [7:45:31<1:16:58,  1.76s/it] 85%|████████▌ | 15214/17834 [7:45:33<1:17:00,  1.76s/it] 85%|████████▌ | 15215/17834 [7:45:34<1:16:11,  1.75s/it] 85%|████████▌ | 15216/17834 [7:45:36<1:15:19,  1.73s/it] 85%|████████▌ | 15217/17834 [7:45:38<1:15:22,  1.73s/it] 85%|████████▌ | 15218/17834 [7:45:40<1:15:54,  1.74s/it] 85%|████████▌ | 15219/17834 [7:45:41<1:15:30,  1.73s/it] 85%|████████▌ | 15220/17834 [7:45:43<1:15:38,  1.74s/it] 85%|████████▌ | 15221/17834 [7:45:45<1:17:46,  1.79s/it] 85%|████████▌ | 15222/17834 [7:45:47<1:17:21,  1.78s/it] 85%|████████▌ | 15223/17834 [7:45:48<1:17:28,  1.78s/it] 85%|████████▌ | 15224/17834 [7:45:50<1:17:23,  1.78s/it] 85%|████████▌ | 15225/17834 [7:45:52<1:18:41,  1.81s/it] 85%|████████▌ | 15226/17834 [7:45:54<1:18:06,  1.80s/it] 85%|████████▌ | 15227/17834 [7:45:56<1:16:55,  1.77s/it] 85%|████████▌ | 15228/17834 [7:45:57<1:17:23,  1.78s/it] 85%|████████▌ | 15229/17834 [7:45:59<1:17:19,  1.78s/it] 85%|████████▌ | 15230/17834 [7:46:01<1:17:19,  1.78s/it] 85%|████████▌ | 15231/17834 [7:46:03<1:17:57,  1.80s/it] 85%|████████▌ | 15232/17834 [7:46:04<1:16:52,  1.77s/it] 85%|████████▌ | 15233/17834 [7:46:06<1:16:58,  1.78s/it] 85%|████████▌ | 15234/17834 [7:46:08<1:17:50,  1.80s/it] 85%|████████▌ | 15235/17834 [7:46:10<1:17:31,  1.79s/it] 85%|████████▌ | 15236/17834 [7:46:12<1:17:13,  1.78s/it] 85%|████████▌ | 15237/17834 [7:46:13<1:16:20,  1.76s/it] 85%|████████▌ | 15238/17834 [7:46:15<1:16:42,  1.77s/it] 85%|████████▌ | 15239/17834 [7:46:17<1:16:08,  1.76s/it] 85%|████████▌ | 15240/17834 [7:46:19<1:16:04,  1.76s/it] 85%|████████▌ | 15241/17834 [7:46:20<1:16:28,  1.77s/it] 85%|████████▌ | 15242/17834 [7:46:22<1:15:40,  1.75s/it] 85%|████████▌ | 15243/17834 [7:46:24<1:15:18,  1.74s/it] 85%|████████▌ | 15244/17834 [7:46:26<1:16:16,  1.77s/it] 85%|████████▌ | 15245/17834 [7:46:27<1:15:31,  1.75s/it] 85%|████████▌ | 15246/17834 [7:46:29<1:15:18,  1.75s/it] 85%|████████▌ | 15247/17834 [7:46:31<1:15:13,  1.74s/it] 85%|████████▌ | 15248/17834 [7:46:33<1:14:24,  1.73s/it] 86%|████████▌ | 15249/17834 [7:46:34<1:14:27,  1.73s/it]08/31/2024 03:00:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0587780475616455, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02906886115670204, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1492698192596436, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.237116813659668}
 86%|████████▌ | 15250/17834 [7:46:36<1:14:47,  1.74s/it] 86%|████████▌ | 15251/17834 [7:46:38<1:14:38,  1.73s/it] 86%|████████▌ | 15252/17834 [7:46:39<1:14:32,  1.73s/it] 86%|████████▌ | 15253/17834 [7:46:41<1:14:23,  1.73s/it] 86%|████████▌ | 15254/17834 [7:46:43<1:14:42,  1.74s/it] 86%|████████▌ | 15255/17834 [7:46:45<1:14:31,  1.73s/it] 86%|████████▌ | 15256/17834 [7:46:46<1:13:52,  1.72s/it] 86%|████████▌ | 15257/17834 [7:46:48<1:15:09,  1.75s/it] 86%|████████▌ | 15258/17834 [7:46:50<1:14:58,  1.75s/it] 86%|████████▌ | 15259/17834 [7:46:52<1:14:40,  1.74s/it] 86%|████████▌ | 15260/17834 [7:46:53<1:14:22,  1.73s/it] 86%|████████▌ | 15261/17834 [7:46:55<1:14:31,  1.74s/it] 86%|████████▌ | 15262/17834 [7:46:57<1:14:34,  1.74s/it] 86%|████████▌ | 15263/17834 [7:46:59<1:15:12,  1.76s/it] 86%|████████▌ | 15264/17834 [7:47:00<1:15:35,  1.76s/it] 86%|████████▌ | 15265/17834 [7:47:02<1:15:22,  1.76s/it] 86%|████████▌ | 15266/17834 [7:47:04<1:14:48,  1.75s/it] 86%|████████▌ | 15267/17834 [7:47:06<1:15:00,  1.75s/it] 86%|████████▌ | 15268/17834 [7:47:07<1:15:02,  1.75s/it] 86%|████████▌ | 15269/17834 [7:47:09<1:15:13,  1.76s/it] 86%|████████▌ | 15270/17834 [7:47:11<1:14:38,  1.75s/it] 86%|████████▌ | 15271/17834 [7:47:13<1:15:17,  1.76s/it] 86%|████████▌ | 15272/17834 [7:47:14<1:14:49,  1.75s/it] 86%|████████▌ | 15273/17834 [7:47:16<1:15:05,  1.76s/it] 86%|████████▌ | 15274/17834 [7:47:18<1:13:27,  1.72s/it] 86%|████████▌ | 15275/17834 [7:47:20<1:13:53,  1.73s/it] 86%|████████▌ | 15276/17834 [7:47:21<1:13:46,  1.73s/it] 86%|████████▌ | 15277/17834 [7:47:23<1:13:19,  1.72s/it] 86%|████████▌ | 15278/17834 [7:47:25<1:14:01,  1.74s/it] 86%|████████▌ | 15279/17834 [7:47:27<1:14:04,  1.74s/it] 86%|████████▌ | 15280/17834 [7:47:28<1:13:41,  1.73s/it] 86%|████████▌ | 15281/17834 [7:47:30<1:14:18,  1.75s/it] 86%|████████▌ | 15282/17834 [7:47:32<1:13:37,  1.73s/it] 86%|████████▌ | 15283/17834 [7:47:34<1:13:47,  1.74s/it] 86%|████████▌ | 15284/17834 [7:47:35<1:13:55,  1.74s/it] 86%|████████▌ | 15285/17834 [7:47:37<1:14:34,  1.76s/it] 86%|████████▌ | 15286/17834 [7:47:39<1:14:06,  1.75s/it] 86%|████████▌ | 15287/17834 [7:47:41<1:15:42,  1.78s/it] 86%|████████▌ | 15288/17834 [7:47:42<1:15:33,  1.78s/it] 86%|████████▌ | 15289/17834 [7:47:44<1:15:10,  1.77s/it] 86%|████████▌ | 15290/17834 [7:47:46<1:15:20,  1.78s/it] 86%|████████▌ | 15291/17834 [7:47:48<1:14:28,  1.76s/it] 86%|████████▌ | 15292/17834 [7:47:49<1:14:21,  1.76s/it] 86%|████████▌ | 15293/17834 [7:47:51<1:14:33,  1.76s/it] 86%|████████▌ | 15294/17834 [7:47:53<1:14:50,  1.77s/it] 86%|████████▌ | 15295/17834 [7:47:55<1:14:32,  1.76s/it] 86%|████████▌ | 15296/17834 [7:47:56<1:14:30,  1.76s/it] 86%|████████▌ | 15297/17834 [7:47:58<1:13:25,  1.74s/it] 86%|████████▌ | 15298/17834 [7:48:00<1:13:03,  1.73s/it] 86%|████████▌ | 15299/17834 [7:48:02<1:13:10,  1.73s/it]08/31/2024 03:02:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.06308913230896, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03495371341705322, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1340548992156982, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.232097625732422}
 86%|████████▌ | 15300/17834 [7:48:03<1:13:46,  1.75s/it] 86%|████████▌ | 15301/17834 [7:48:05<1:13:43,  1.75s/it] 86%|████████▌ | 15302/17834 [7:48:07<1:13:56,  1.75s/it] 86%|████████▌ | 15303/17834 [7:48:09<1:13:54,  1.75s/it] 86%|████████▌ | 15304/17834 [7:48:10<1:14:05,  1.76s/it] 86%|████████▌ | 15305/17834 [7:48:12<1:14:07,  1.76s/it] 86%|████████▌ | 15306/17834 [7:48:14<1:14:53,  1.78s/it] 86%|████████▌ | 15307/17834 [7:48:16<1:15:07,  1.78s/it] 86%|████████▌ | 15308/17834 [7:48:18<1:16:50,  1.83s/it] 86%|████████▌ | 15309/17834 [7:48:19<1:15:34,  1.80s/it] 86%|████████▌ | 15310/17834 [7:48:21<1:14:38,  1.77s/it] 86%|████████▌ | 15311/17834 [7:48:23<1:13:36,  1.75s/it] 86%|████████▌ | 15312/17834 [7:48:25<1:13:25,  1.75s/it] 86%|████████▌ | 15313/17834 [7:48:26<1:14:01,  1.76s/it] 86%|████████▌ | 15314/17834 [7:48:28<1:13:44,  1.76s/it] 86%|████████▌ | 15315/17834 [7:48:30<1:14:06,  1.77s/it] 86%|████████▌ | 15316/17834 [7:48:32<1:14:51,  1.78s/it] 86%|████████▌ | 15317/17834 [7:48:34<1:14:38,  1.78s/it] 86%|████████▌ | 15318/17834 [7:48:35<1:14:16,  1.77s/it] 86%|████████▌ | 15319/17834 [7:48:37<1:14:51,  1.79s/it] 86%|████████▌ | 15320/17834 [7:48:39<1:15:06,  1.79s/it] 86%|████████▌ | 15321/17834 [7:48:41<1:14:16,  1.77s/it] 86%|████████▌ | 15322/17834 [7:48:42<1:13:58,  1.77s/it] 86%|████████▌ | 15323/17834 [7:48:44<1:13:21,  1.75s/it] 86%|████████▌ | 15324/17834 [7:48:46<1:12:55,  1.74s/it] 86%|████████▌ | 15325/17834 [7:48:48<1:12:45,  1.74s/it] 86%|████████▌ | 15326/17834 [7:48:49<1:12:48,  1.74s/it] 86%|████████▌ | 15327/17834 [7:48:51<1:12:55,  1.75s/it] 86%|████████▌ | 15328/17834 [7:48:53<1:12:01,  1.72s/it] 86%|████████▌ | 15329/17834 [7:48:55<1:12:46,  1.74s/it] 86%|████████▌ | 15330/17834 [7:48:56<1:12:55,  1.75s/it] 86%|████████▌ | 15331/17834 [7:48:58<1:13:15,  1.76s/it] 86%|████████▌ | 15332/17834 [7:49:00<1:12:00,  1.73s/it] 86%|████████▌ | 15333/17834 [7:49:01<1:11:44,  1.72s/it] 86%|████████▌ | 15334/17834 [7:49:03<1:11:51,  1.72s/it] 86%|████████▌ | 15335/17834 [7:49:05<1:11:24,  1.71s/it] 86%|████████▌ | 15336/17834 [7:49:07<1:11:30,  1.72s/it] 86%|████████▌ | 15337/17834 [7:49:08<1:12:32,  1.74s/it] 86%|████████▌ | 15338/17834 [7:49:10<1:11:35,  1.72s/it] 86%|████████▌ | 15339/17834 [7:49:12<1:11:50,  1.73s/it] 86%|████████▌ | 15340/17834 [7:49:14<1:12:12,  1.74s/it] 86%|████████▌ | 15341/17834 [7:49:15<1:11:35,  1.72s/it] 86%|████████▌ | 15342/17834 [7:49:17<1:11:52,  1.73s/it] 86%|████████▌ | 15343/17834 [7:49:19<1:12:04,  1.74s/it] 86%|████████▌ | 15344/17834 [7:49:20<1:11:36,  1.73s/it] 86%|████████▌ | 15345/17834 [7:49:22<1:11:26,  1.72s/it] 86%|████████▌ | 15346/17834 [7:49:24<1:11:49,  1.73s/it] 86%|████████▌ | 15347/17834 [7:49:26<1:11:54,  1.73s/it] 86%|████████▌ | 15348/17834 [7:49:27<1:12:37,  1.75s/it] 86%|████████▌ | 15349/17834 [7:49:29<1:12:07,  1.74s/it]08/31/2024 03:03:48 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3254870176315308, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03552559018135071, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.275775194168091, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6367878913879395}
 86%|████████▌ | 15350/17834 [7:49:31<1:11:45,  1.73s/it] 86%|████████▌ | 15351/17834 [7:49:33<1:11:42,  1.73s/it] 86%|████████▌ | 15352/17834 [7:49:34<1:11:54,  1.74s/it] 86%|████████▌ | 15353/17834 [7:49:36<1:12:22,  1.75s/it] 86%|████████▌ | 15354/17834 [7:49:38<1:12:47,  1.76s/it] 86%|████████▌ | 15355/17834 [7:49:40<1:11:58,  1.74s/it] 86%|████████▌ | 15356/17834 [7:49:41<1:11:50,  1.74s/it] 86%|████████▌ | 15357/17834 [7:49:43<1:11:09,  1.72s/it] 86%|████████▌ | 15358/17834 [7:49:45<1:13:06,  1.77s/it] 86%|████████▌ | 15359/17834 [7:49:47<1:12:56,  1.77s/it] 86%|████████▌ | 15360/17834 [7:49:48<1:12:20,  1.75s/it] 86%|████████▌ | 15361/17834 [7:49:50<1:11:31,  1.74s/it] 86%|████████▌ | 15362/17834 [7:49:52<1:11:37,  1.74s/it] 86%|████████▌ | 15363/17834 [7:49:54<1:12:05,  1.75s/it] 86%|████████▌ | 15364/17834 [7:49:55<1:13:07,  1.78s/it] 86%|████████▌ | 15365/17834 [7:49:57<1:11:22,  1.73s/it] 86%|████████▌ | 15366/17834 [7:49:59<1:11:57,  1.75s/it] 86%|████████▌ | 15367/17834 [7:50:01<1:11:52,  1.75s/it] 86%|████████▌ | 15368/17834 [7:50:02<1:11:48,  1.75s/it] 86%|████████▌ | 15369/17834 [7:50:04<1:12:17,  1.76s/it] 86%|████████▌ | 15370/17834 [7:50:06<1:11:47,  1.75s/it] 86%|████████▌ | 15371/17834 [7:50:08<1:11:41,  1.75s/it] 86%|████████▌ | 15372/17834 [7:50:09<1:11:55,  1.75s/it] 86%|████████▌ | 15373/17834 [7:50:11<1:11:19,  1.74s/it] 86%|████████▌ | 15374/17834 [7:50:13<1:11:00,  1.73s/it] 86%|████████▌ | 15375/17834 [7:50:15<1:11:18,  1.74s/it] 86%|████████▌ | 15376/17834 [7:50:16<1:11:10,  1.74s/it] 86%|████████▌ | 15377/17834 [7:50:18<1:11:36,  1.75s/it] 86%|████████▌ | 15378/17834 [7:50:20<1:12:20,  1.77s/it] 86%|████████▌ | 15379/17834 [7:50:22<1:12:18,  1.77s/it] 86%|████████▌ | 15380/17834 [7:50:23<1:11:53,  1.76s/it] 86%|████████▌ | 15381/17834 [7:50:25<1:13:52,  1.81s/it] 86%|████████▋ | 15382/17834 [7:50:27<1:12:41,  1.78s/it] 86%|████████▋ | 15383/17834 [7:50:29<1:12:47,  1.78s/it] 86%|████████▋ | 15384/17834 [7:50:31<1:12:00,  1.76s/it] 86%|████████▋ | 15385/17834 [7:50:32<1:11:02,  1.74s/it] 86%|████████▋ | 15386/17834 [7:50:34<1:10:50,  1.74s/it] 86%|████████▋ | 15387/17834 [7:50:36<1:10:31,  1.73s/it] 86%|████████▋ | 15388/17834 [7:50:37<1:11:36,  1.76s/it] 86%|████████▋ | 15389/17834 [7:50:39<1:12:07,  1.77s/it] 86%|████████▋ | 15390/17834 [7:50:41<1:11:22,  1.75s/it] 86%|████████▋ | 15391/17834 [7:50:43<1:11:58,  1.77s/it] 86%|████████▋ | 15392/17834 [7:50:44<1:10:59,  1.74s/it] 86%|████████▋ | 15393/17834 [7:50:46<1:10:41,  1.74s/it] 86%|████████▋ | 15394/17834 [7:50:48<1:10:14,  1.73s/it] 86%|████████▋ | 15395/17834 [7:50:50<1:10:07,  1.73s/it] 86%|████████▋ | 15396/17834 [7:50:51<1:10:35,  1.74s/it] 86%|████████▋ | 15397/17834 [7:50:53<1:10:31,  1.74s/it] 86%|████████▋ | 15398/17834 [7:50:55<1:11:02,  1.75s/it] 86%|████████▋ | 15399/17834 [7:50:57<1:10:48,  1.74s/it]08/31/2024 03:05:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2606842517852783, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0557844340801239, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3131508827209473, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.629619598388672}
 86%|████████▋ | 15400/17834 [7:50:58<1:10:43,  1.74s/it] 86%|████████▋ | 15401/17834 [7:51:00<1:10:07,  1.73s/it] 86%|████████▋ | 15402/17834 [7:51:02<1:10:09,  1.73s/it] 86%|████████▋ | 15403/17834 [7:51:04<1:09:46,  1.72s/it] 86%|████████▋ | 15404/17834 [7:51:05<1:09:41,  1.72s/it] 86%|████████▋ | 15405/17834 [7:51:07<1:10:16,  1.74s/it] 86%|████████▋ | 15406/17834 [7:51:09<1:10:02,  1.73s/it] 86%|████████▋ | 15407/17834 [7:51:11<1:10:28,  1.74s/it] 86%|████████▋ | 15408/17834 [7:51:12<1:11:19,  1.76s/it] 86%|████████▋ | 15409/17834 [7:51:14<1:10:31,  1.74s/it] 86%|████████▋ | 15410/17834 [7:51:16<1:10:36,  1.75s/it] 86%|████████▋ | 15411/17834 [7:51:18<1:10:21,  1.74s/it] 86%|████████▋ | 15412/17834 [7:51:19<1:10:33,  1.75s/it] 86%|████████▋ | 15413/17834 [7:51:21<1:10:08,  1.74s/it] 86%|████████▋ | 15414/17834 [7:51:23<1:09:38,  1.73s/it] 86%|████████▋ | 15415/17834 [7:51:24<1:09:51,  1.73s/it] 86%|████████▋ | 15416/17834 [7:51:26<1:12:18,  1.79s/it] 86%|████████▋ | 15417/17834 [7:51:28<1:11:29,  1.77s/it] 86%|████████▋ | 15418/17834 [7:51:30<1:11:05,  1.77s/it] 86%|████████▋ | 15419/17834 [7:51:32<1:10:59,  1.76s/it] 86%|████████▋ | 15420/17834 [7:51:33<1:10:43,  1.76s/it] 86%|████████▋ | 15421/17834 [7:51:35<1:09:55,  1.74s/it] 86%|████████▋ | 15422/17834 [7:51:37<1:10:00,  1.74s/it] 86%|████████▋ | 15423/17834 [7:51:39<1:10:38,  1.76s/it] 86%|████████▋ | 15424/17834 [7:51:40<1:10:13,  1.75s/it] 86%|████████▋ | 15425/17834 [7:51:42<1:09:48,  1.74s/it] 86%|████████▋ | 15426/17834 [7:51:44<1:08:59,  1.72s/it] 87%|████████▋ | 15427/17834 [7:51:45<1:08:32,  1.71s/it] 87%|████████▋ | 15428/17834 [7:51:47<1:08:20,  1.70s/it] 87%|████████▋ | 15429/17834 [7:51:49<1:08:29,  1.71s/it] 87%|████████▋ | 15430/17834 [7:51:51<1:09:40,  1.74s/it] 87%|████████▋ | 15431/17834 [7:51:52<1:09:56,  1.75s/it] 87%|████████▋ | 15432/17834 [7:51:54<1:09:42,  1.74s/it] 87%|████████▋ | 15433/17834 [7:51:56<1:11:08,  1.78s/it] 87%|████████▋ | 15434/17834 [7:51:58<1:09:42,  1.74s/it] 87%|████████▋ | 15435/17834 [7:51:59<1:09:27,  1.74s/it] 87%|████████▋ | 15436/17834 [7:52:01<1:08:54,  1.72s/it] 87%|████████▋ | 15437/17834 [7:52:03<1:09:55,  1.75s/it] 87%|████████▋ | 15438/17834 [7:52:05<1:09:38,  1.74s/it] 87%|████████▋ | 15439/17834 [7:52:06<1:09:32,  1.74s/it] 87%|████████▋ | 15440/17834 [7:52:08<1:09:56,  1.75s/it] 87%|████████▋ | 15441/17834 [7:52:10<1:10:31,  1.77s/it] 87%|████████▋ | 15442/17834 [7:52:12<1:09:47,  1.75s/it] 87%|████████▋ | 15443/17834 [7:52:13<1:09:33,  1.75s/it] 87%|████████▋ | 15444/17834 [7:52:15<1:09:08,  1.74s/it] 87%|████████▋ | 15445/17834 [7:52:17<1:08:56,  1.73s/it] 87%|████████▋ | 15446/17834 [7:52:19<1:09:24,  1.74s/it] 87%|████████▋ | 15447/17834 [7:52:20<1:09:26,  1.75s/it] 87%|████████▋ | 15448/17834 [7:52:22<1:08:40,  1.73s/it] 87%|████████▋ | 15449/17834 [7:52:24<1:08:57,  1.73s/it]08/31/2024 03:06:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0113322734832764, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029048826545476913, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.139310359954834, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1796913146972656}
 87%|████████▋ | 15450/17834 [7:52:25<1:09:14,  1.74s/it] 87%|████████▋ | 15451/17834 [7:52:27<1:09:55,  1.76s/it] 87%|████████▋ | 15452/17834 [7:52:29<1:09:06,  1.74s/it] 87%|████████▋ | 15453/17834 [7:52:31<1:08:54,  1.74s/it] 87%|████████▋ | 15454/17834 [7:52:32<1:09:03,  1.74s/it] 87%|████████▋ | 15455/17834 [7:52:34<1:09:14,  1.75s/it] 87%|████████▋ | 15456/17834 [7:52:36<1:09:04,  1.74s/it] 87%|████████▋ | 15457/17834 [7:52:38<1:09:49,  1.76s/it] 87%|████████▋ | 15458/17834 [7:52:40<1:10:23,  1.78s/it] 87%|████████▋ | 15459/17834 [7:52:41<1:09:38,  1.76s/it] 87%|████████▋ | 15460/17834 [7:52:43<1:08:58,  1.74s/it] 87%|████████▋ | 15461/17834 [7:52:45<1:08:52,  1.74s/it] 87%|████████▋ | 15462/17834 [7:52:47<1:09:21,  1.75s/it] 87%|████████▋ | 15463/17834 [7:52:48<1:09:10,  1.75s/it] 87%|████████▋ | 15464/17834 [7:52:50<1:09:29,  1.76s/it] 87%|████████▋ | 15465/17834 [7:52:52<1:09:56,  1.77s/it] 87%|████████▋ | 15466/17834 [7:52:54<1:10:13,  1.78s/it] 87%|████████▋ | 15467/17834 [7:52:55<1:09:13,  1.75s/it] 87%|████████▋ | 15468/17834 [7:52:57<1:09:17,  1.76s/it] 87%|████████▋ | 15469/17834 [7:52:59<1:10:12,  1.78s/it] 87%|████████▋ | 15470/17834 [7:53:01<1:11:31,  1.82s/it] 87%|████████▋ | 15471/17834 [7:53:03<1:09:47,  1.77s/it] 87%|████████▋ | 15472/17834 [7:53:04<1:09:45,  1.77s/it] 87%|████████▋ | 15473/17834 [7:53:06<1:09:32,  1.77s/it] 87%|████████▋ | 15474/17834 [7:53:08<1:09:22,  1.76s/it] 87%|████████▋ | 15475/17834 [7:53:10<1:09:21,  1.76s/it] 87%|████████▋ | 15476/17834 [7:53:11<1:09:14,  1.76s/it] 87%|████████▋ | 15477/17834 [7:53:13<1:09:11,  1.76s/it] 87%|████████▋ | 15478/17834 [7:53:15<1:09:07,  1.76s/it] 87%|████████▋ | 15479/17834 [7:53:17<1:09:33,  1.77s/it] 87%|████████▋ | 15480/17834 [7:53:18<1:08:34,  1.75s/it] 87%|████████▋ | 15481/17834 [7:53:20<1:09:35,  1.77s/it] 87%|████████▋ | 15482/17834 [7:53:22<1:08:47,  1.76s/it] 87%|████████▋ | 15483/17834 [7:53:24<1:09:25,  1.77s/it] 87%|████████▋ | 15484/17834 [7:53:25<1:09:08,  1.77s/it] 87%|████████▋ | 15485/17834 [7:53:27<1:08:23,  1.75s/it] 87%|████████▋ | 15486/17834 [7:53:29<1:08:34,  1.75s/it] 87%|████████▋ | 15487/17834 [7:53:31<1:08:46,  1.76s/it] 87%|████████▋ | 15488/17834 [7:53:33<1:10:29,  1.80s/it] 87%|████████▋ | 15489/17834 [7:53:34<1:10:12,  1.80s/it] 87%|████████▋ | 15490/17834 [7:53:36<1:08:46,  1.76s/it] 87%|████████▋ | 15491/17834 [7:53:38<1:09:05,  1.77s/it] 87%|████████▋ | 15492/17834 [7:53:40<1:10:13,  1.80s/it] 87%|████████▋ | 15493/17834 [7:53:41<1:09:01,  1.77s/it] 87%|████████▋ | 15494/17834 [7:53:43<1:09:31,  1.78s/it] 87%|████████▋ | 15495/17834 [7:53:45<1:09:05,  1.77s/it] 87%|████████▋ | 15496/17834 [7:53:47<1:08:02,  1.75s/it] 87%|████████▋ | 15497/17834 [7:53:48<1:08:41,  1.76s/it] 87%|████████▋ | 15498/17834 [7:53:50<1:08:03,  1.75s/it] 87%|████████▋ | 15499/17834 [7:53:52<1:07:26,  1.73s/it]08/31/2024 03:08:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2760677337646484, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029873250052332878, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3438096046447754, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6497507095336914}
 87%|████████▋ | 15500/17834 [7:53:54<1:06:59,  1.72s/it] 87%|████████▋ | 15501/17834 [7:53:55<1:06:39,  1.71s/it] 87%|████████▋ | 15502/17834 [7:53:57<1:06:26,  1.71s/it] 87%|████████▋ | 15503/17834 [7:53:59<1:07:06,  1.73s/it] 87%|████████▋ | 15504/17834 [7:54:00<1:07:26,  1.74s/it] 87%|████████▋ | 15505/17834 [7:54:02<1:07:44,  1.75s/it] 87%|████████▋ | 15506/17834 [7:54:04<1:07:52,  1.75s/it] 87%|████████▋ | 15507/17834 [7:54:06<1:08:14,  1.76s/it] 87%|████████▋ | 15508/17834 [7:54:07<1:07:31,  1.74s/it] 87%|████████▋ | 15509/17834 [7:54:09<1:07:58,  1.75s/it] 87%|████████▋ | 15510/17834 [7:54:11<1:07:30,  1.74s/it] 87%|████████▋ | 15511/17834 [7:54:13<1:08:39,  1.77s/it] 87%|████████▋ | 15512/17834 [7:54:15<1:08:26,  1.77s/it] 87%|████████▋ | 15513/17834 [7:54:16<1:08:07,  1.76s/it] 87%|████████▋ | 15514/17834 [7:54:18<1:08:29,  1.77s/it] 87%|████████▋ | 15515/17834 [7:54:20<1:08:09,  1.76s/it] 87%|████████▋ | 15516/17834 [7:54:22<1:08:04,  1.76s/it] 87%|████████▋ | 15517/17834 [7:54:23<1:08:07,  1.76s/it] 87%|████████▋ | 15518/17834 [7:54:25<1:07:20,  1.74s/it] 87%|████████▋ | 15519/17834 [7:54:27<1:07:13,  1.74s/it] 87%|████████▋ | 15520/17834 [7:54:29<1:07:13,  1.74s/it] 87%|████████▋ | 15521/17834 [7:54:30<1:07:17,  1.75s/it] 87%|████████▋ | 15522/17834 [7:54:32<1:07:43,  1.76s/it] 87%|████████▋ | 15523/17834 [7:54:34<1:07:37,  1.76s/it] 87%|████████▋ | 15524/17834 [7:54:36<1:07:21,  1.75s/it] 87%|████████▋ | 15525/17834 [7:54:37<1:07:32,  1.76s/it] 87%|████████▋ | 15526/17834 [7:54:39<1:07:29,  1.75s/it] 87%|████████▋ | 15527/17834 [7:54:41<1:07:42,  1.76s/it] 87%|████████▋ | 15528/17834 [7:54:43<1:07:25,  1.75s/it] 87%|████████▋ | 15529/17834 [7:54:44<1:07:04,  1.75s/it] 87%|████████▋ | 15530/17834 [7:54:46<1:07:08,  1.75s/it] 87%|████████▋ | 15531/17834 [7:54:48<1:06:27,  1.73s/it] 87%|████████▋ | 15532/17834 [7:54:50<1:06:40,  1.74s/it] 87%|████████▋ | 15533/17834 [7:54:51<1:06:14,  1.73s/it] 87%|████████▋ | 15534/17834 [7:54:53<1:05:57,  1.72s/it] 87%|████████▋ | 15535/17834 [7:54:55<1:06:30,  1.74s/it] 87%|████████▋ | 15536/17834 [7:54:56<1:06:31,  1.74s/it] 87%|████████▋ | 15537/17834 [7:54:58<1:06:47,  1.74s/it] 87%|████████▋ | 15538/17834 [7:55:00<1:06:57,  1.75s/it] 87%|████████▋ | 15539/17834 [7:55:02<1:06:17,  1.73s/it] 87%|████████▋ | 15540/17834 [7:55:03<1:06:33,  1.74s/it] 87%|████████▋ | 15541/17834 [7:55:05<1:06:48,  1.75s/it] 87%|████████▋ | 15542/17834 [7:55:07<1:06:32,  1.74s/it] 87%|████████▋ | 15543/17834 [7:55:09<1:06:42,  1.75s/it] 87%|████████▋ | 15544/17834 [7:55:10<1:06:24,  1.74s/it] 87%|████████▋ | 15545/17834 [7:55:12<1:06:15,  1.74s/it] 87%|████████▋ | 15546/17834 [7:55:14<1:06:45,  1.75s/it] 87%|████████▋ | 15547/17834 [7:55:16<1:07:12,  1.76s/it] 87%|████████▋ | 15548/17834 [7:55:18<1:07:09,  1.76s/it] 87%|████████▋ | 15549/17834 [7:55:19<1:07:21,  1.77s/it]08/31/2024 03:09:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9125033617019653, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0305550005286932, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1254591941833496, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0685176849365234}
 87%|████████▋ | 15550/17834 [7:55:21<1:08:11,  1.79s/it] 87%|████████▋ | 15551/17834 [7:55:23<1:07:22,  1.77s/it] 87%|████████▋ | 15552/17834 [7:55:25<1:07:40,  1.78s/it] 87%|████████▋ | 15553/17834 [7:55:26<1:07:23,  1.77s/it] 87%|████████▋ | 15554/17834 [7:55:28<1:06:51,  1.76s/it] 87%|████████▋ | 15555/17834 [7:55:30<1:06:39,  1.75s/it] 87%|████████▋ | 15556/17834 [7:55:32<1:06:50,  1.76s/it] 87%|████████▋ | 15557/17834 [7:55:33<1:06:39,  1.76s/it] 87%|████████▋ | 15558/17834 [7:55:35<1:06:01,  1.74s/it] 87%|████████▋ | 15559/17834 [7:55:37<1:06:13,  1.75s/it] 87%|████████▋ | 15560/17834 [7:55:39<1:06:33,  1.76s/it] 87%|████████▋ | 15561/17834 [7:55:40<1:06:16,  1.75s/it] 87%|████████▋ | 15562/17834 [7:55:42<1:06:01,  1.74s/it] 87%|████████▋ | 15563/17834 [7:55:44<1:05:50,  1.74s/it] 87%|████████▋ | 15564/17834 [7:55:46<1:06:06,  1.75s/it] 87%|████████▋ | 15565/17834 [7:55:47<1:05:34,  1.73s/it] 87%|████████▋ | 15566/17834 [7:55:49<1:06:49,  1.77s/it] 87%|████████▋ | 15567/17834 [7:55:51<1:06:52,  1.77s/it] 87%|████████▋ | 15568/17834 [7:55:53<1:06:21,  1.76s/it] 87%|████████▋ | 15569/17834 [7:55:54<1:06:03,  1.75s/it] 87%|████████▋ | 15570/17834 [7:55:56<1:06:42,  1.77s/it] 87%|████████▋ | 15571/17834 [7:55:58<1:06:29,  1.76s/it] 87%|████████▋ | 15572/17834 [7:56:00<1:05:40,  1.74s/it] 87%|████████▋ | 15573/17834 [7:56:01<1:04:45,  1.72s/it] 87%|████████▋ | 15574/17834 [7:56:03<1:05:12,  1.73s/it] 87%|████████▋ | 15575/17834 [7:56:05<1:05:13,  1.73s/it] 87%|████████▋ | 15576/17834 [7:56:07<1:04:57,  1.73s/it] 87%|████████▋ | 15577/17834 [7:56:08<1:06:24,  1.77s/it] 87%|████████▋ | 15578/17834 [7:56:10<1:05:31,  1.74s/it] 87%|████████▋ | 15579/17834 [7:56:12<1:05:28,  1.74s/it] 87%|████████▋ | 15580/17834 [7:56:14<1:05:52,  1.75s/it] 87%|████████▋ | 15581/17834 [7:56:15<1:05:55,  1.76s/it] 87%|████████▋ | 15582/17834 [7:56:17<1:04:45,  1.73s/it] 87%|████████▋ | 15583/17834 [7:56:19<1:04:59,  1.73s/it] 87%|████████▋ | 15584/17834 [7:56:20<1:04:54,  1.73s/it] 87%|████████▋ | 15585/17834 [7:56:22<1:04:47,  1.73s/it] 87%|████████▋ | 15586/17834 [7:56:24<1:05:16,  1.74s/it] 87%|████████▋ | 15587/17834 [7:56:26<1:06:12,  1.77s/it] 87%|████████▋ | 15588/17834 [7:56:28<1:05:47,  1.76s/it] 87%|████████▋ | 15589/17834 [7:56:29<1:05:13,  1.74s/it] 87%|████████▋ | 15590/17834 [7:56:31<1:05:38,  1.76s/it] 87%|████████▋ | 15591/17834 [7:56:33<1:05:34,  1.75s/it] 87%|████████▋ | 15592/17834 [7:56:35<1:05:48,  1.76s/it] 87%|████████▋ | 15593/17834 [7:56:36<1:06:13,  1.77s/it] 87%|████████▋ | 15594/17834 [7:56:38<1:06:31,  1.78s/it] 87%|████████▋ | 15595/17834 [7:56:40<1:05:16,  1.75s/it] 87%|████████▋ | 15596/17834 [7:56:42<1:05:49,  1.76s/it] 87%|████████▋ | 15597/17834 [7:56:43<1:06:08,  1.77s/it] 87%|████████▋ | 15598/17834 [7:56:45<1:05:46,  1.76s/it] 87%|████████▋ | 15599/17834 [7:56:47<1:04:37,  1.73s/it]08/31/2024 03:11:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.202603816986084, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03179318457841873, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.196981430053711, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4313783645629883}
 87%|████████▋ | 15600/17834 [7:56:49<1:04:29,  1.73s/it] 87%|████████▋ | 15601/17834 [7:56:50<1:04:18,  1.73s/it] 87%|████████▋ | 15602/17834 [7:56:52<1:05:04,  1.75s/it] 87%|████████▋ | 15603/17834 [7:56:54<1:04:46,  1.74s/it] 87%|████████▋ | 15604/17834 [7:56:56<1:04:49,  1.74s/it] 88%|████████▊ | 15605/17834 [7:56:57<1:04:33,  1.74s/it] 88%|████████▊ | 15606/17834 [7:56:59<1:05:39,  1.77s/it] 88%|████████▊ | 15607/17834 [7:57:01<1:05:29,  1.76s/it] 88%|████████▊ | 15608/17834 [7:57:03<1:04:50,  1.75s/it] 88%|████████▊ | 15609/17834 [7:57:04<1:04:28,  1.74s/it] 88%|████████▊ | 15610/17834 [7:57:06<1:04:22,  1.74s/it] 88%|████████▊ | 15611/17834 [7:57:08<1:04:00,  1.73s/it] 88%|████████▊ | 15612/17834 [7:57:10<1:04:36,  1.74s/it] 88%|████████▊ | 15613/17834 [7:57:11<1:04:26,  1.74s/it] 88%|████████▊ | 15614/17834 [7:57:13<1:04:31,  1.74s/it] 88%|████████▊ | 15615/17834 [7:57:15<1:04:25,  1.74s/it] 88%|████████▊ | 15616/17834 [7:57:17<1:05:38,  1.78s/it] 88%|████████▊ | 15617/17834 [7:57:18<1:05:19,  1.77s/it] 88%|████████▊ | 15618/17834 [7:57:20<1:03:48,  1.73s/it] 88%|████████▊ | 15619/17834 [7:57:22<1:04:10,  1.74s/it] 88%|████████▊ | 15620/17834 [7:57:23<1:03:55,  1.73s/it] 88%|████████▊ | 15621/17834 [7:57:25<1:04:07,  1.74s/it] 88%|████████▊ | 15622/17834 [7:57:27<1:04:30,  1.75s/it] 88%|████████▊ | 15623/17834 [7:57:29<1:04:38,  1.75s/it] 88%|████████▊ | 15624/17834 [7:57:31<1:04:35,  1.75s/it] 88%|████████▊ | 15625/17834 [7:57:32<1:04:14,  1.74s/it] 88%|████████▊ | 15626/17834 [7:57:34<1:03:55,  1.74s/it] 88%|████████▊ | 15627/17834 [7:57:36<1:04:09,  1.74s/it] 88%|████████▊ | 15628/17834 [7:57:37<1:03:55,  1.74s/it] 88%|████████▊ | 15629/17834 [7:57:39<1:03:53,  1.74s/it] 88%|████████▊ | 15630/17834 [7:57:41<1:04:18,  1.75s/it] 88%|████████▊ | 15631/17834 [7:57:43<1:03:42,  1.74s/it] 88%|████████▊ | 15632/17834 [7:57:44<1:03:21,  1.73s/it] 88%|████████▊ | 15633/17834 [7:57:46<1:03:40,  1.74s/it] 88%|████████▊ | 15634/17834 [7:57:48<1:04:24,  1.76s/it] 88%|████████▊ | 15635/17834 [7:57:50<1:03:51,  1.74s/it] 88%|████████▊ | 15636/17834 [7:57:52<1:05:41,  1.79s/it] 88%|████████▊ | 15637/17834 [7:57:53<1:04:44,  1.77s/it] 88%|████████▊ | 15638/17834 [7:57:55<1:04:21,  1.76s/it] 88%|████████▊ | 15639/17834 [7:57:57<1:04:17,  1.76s/it] 88%|████████▊ | 15640/17834 [7:57:59<1:04:28,  1.76s/it] 88%|████████▊ | 15641/17834 [7:58:00<1:04:13,  1.76s/it] 88%|████████▊ | 15642/17834 [7:58:02<1:03:54,  1.75s/it] 88%|████████▊ | 15643/17834 [7:58:04<1:03:46,  1.75s/it] 88%|████████▊ | 15644/17834 [7:58:05<1:03:04,  1.73s/it] 88%|████████▊ | 15645/17834 [7:58:07<1:02:44,  1.72s/it] 88%|████████▊ | 15646/17834 [7:58:09<1:03:02,  1.73s/it] 88%|████████▊ | 15647/17834 [7:58:11<1:02:35,  1.72s/it] 88%|████████▊ | 15648/17834 [7:58:12<1:04:01,  1.76s/it] 88%|████████▊ | 15649/17834 [7:58:14<1:05:31,  1.80s/it]08/31/2024 03:12:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2796916961669922, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028445368632674217, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2502002716064453, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5583372116088867}
 88%|████████▊ | 15650/17834 [7:58:16<1:04:37,  1.78s/it] 88%|████████▊ | 15651/17834 [7:58:18<1:03:21,  1.74s/it] 88%|████████▊ | 15652/17834 [7:58:20<1:04:31,  1.77s/it] 88%|████████▊ | 15653/17834 [7:58:21<1:04:17,  1.77s/it] 88%|████████▊ | 15654/17834 [7:58:23<1:03:26,  1.75s/it] 88%|████████▊ | 15655/17834 [7:58:25<1:03:06,  1.74s/it] 88%|████████▊ | 15656/17834 [7:58:26<1:03:07,  1.74s/it] 88%|████████▊ | 15657/17834 [7:58:28<1:03:36,  1.75s/it] 88%|████████▊ | 15658/17834 [7:58:30<1:03:04,  1.74s/it] 88%|████████▊ | 15659/17834 [7:58:32<1:02:57,  1.74s/it] 88%|████████▊ | 15660/17834 [7:58:33<1:02:50,  1.73s/it] 88%|████████▊ | 15661/17834 [7:58:35<1:03:05,  1.74s/it] 88%|████████▊ | 15662/17834 [7:58:37<1:02:59,  1.74s/it] 88%|████████▊ | 15663/17834 [7:58:39<1:02:47,  1.74s/it] 88%|████████▊ | 15664/17834 [7:58:40<1:03:17,  1.75s/it] 88%|████████▊ | 15665/17834 [7:58:42<1:03:28,  1.76s/it] 88%|████████▊ | 15666/17834 [7:58:44<1:03:15,  1.75s/it] 88%|████████▊ | 15667/17834 [7:58:46<1:03:44,  1.76s/it] 88%|████████▊ | 15668/17834 [7:58:47<1:03:36,  1.76s/it] 88%|████████▊ | 15669/17834 [7:58:49<1:02:57,  1.74s/it] 88%|████████▊ | 15670/17834 [7:58:51<1:03:20,  1.76s/it] 88%|████████▊ | 15671/17834 [7:58:53<1:03:32,  1.76s/it] 88%|████████▊ | 15672/17834 [7:58:54<1:03:19,  1.76s/it] 88%|████████▊ | 15673/17834 [7:58:56<1:02:50,  1.74s/it] 88%|████████▊ | 15674/17834 [7:58:58<1:02:42,  1.74s/it] 88%|████████▊ | 15675/17834 [7:59:00<1:02:02,  1.72s/it] 88%|████████▊ | 15676/17834 [7:59:01<1:02:05,  1.73s/it] 88%|████████▊ | 15677/17834 [7:59:03<1:02:47,  1.75s/it] 88%|████████▊ | 15678/17834 [7:59:05<1:02:50,  1.75s/it] 88%|████████▊ | 15679/17834 [7:59:07<1:02:14,  1.73s/it] 88%|████████▊ | 15680/17834 [7:59:08<1:01:49,  1.72s/it] 88%|████████▊ | 15681/17834 [7:59:10<1:02:30,  1.74s/it] 88%|████████▊ | 15682/17834 [7:59:12<1:01:40,  1.72s/it] 88%|████████▊ | 15683/17834 [7:59:13<1:01:49,  1.72s/it] 88%|████████▊ | 15684/17834 [7:59:15<1:01:59,  1.73s/it] 88%|████████▊ | 15685/17834 [7:59:17<1:01:39,  1.72s/it] 88%|████████▊ | 15686/17834 [7:59:19<1:02:12,  1.74s/it] 88%|████████▊ | 15687/17834 [7:59:20<1:02:23,  1.74s/it] 88%|████████▊ | 15688/17834 [7:59:22<1:02:02,  1.73s/it] 88%|████████▊ | 15689/17834 [7:59:24<1:03:06,  1.77s/it] 88%|████████▊ | 15690/17834 [7:59:26<1:02:28,  1.75s/it] 88%|████████▊ | 15691/17834 [7:59:28<1:03:09,  1.77s/it] 88%|████████▊ | 15692/17834 [7:59:29<1:02:40,  1.76s/it] 88%|████████▊ | 15693/17834 [7:59:31<1:02:38,  1.76s/it] 88%|████████▊ | 15694/17834 [7:59:33<1:01:41,  1.73s/it] 88%|████████▊ | 15695/17834 [7:59:34<1:01:51,  1.73s/it] 88%|████████▊ | 15696/17834 [7:59:36<1:01:02,  1.71s/it] 88%|████████▊ | 15697/17834 [7:59:38<1:00:43,  1.70s/it] 88%|████████▊ | 15698/17834 [7:59:40<1:01:51,  1.74s/it] 88%|████████▊ | 15699/17834 [7:59:41<1:02:47,  1.76s/it]08/31/2024 03:14:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9724322557449341, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026888366788625717, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.212949514389038, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2122702598571777}
 88%|████████▊ | 15700/17834 [7:59:43<1:02:12,  1.75s/it] 88%|████████▊ | 15701/17834 [7:59:45<1:02:12,  1.75s/it] 88%|████████▊ | 15702/17834 [7:59:47<1:02:18,  1.75s/it] 88%|████████▊ | 15703/17834 [7:59:48<1:02:21,  1.76s/it] 88%|████████▊ | 15704/17834 [7:59:50<1:01:46,  1.74s/it] 88%|████████▊ | 15705/17834 [7:59:52<1:01:47,  1.74s/it] 88%|████████▊ | 15706/17834 [7:59:54<1:02:08,  1.75s/it] 88%|████████▊ | 15707/17834 [7:59:55<1:01:45,  1.74s/it] 88%|████████▊ | 15708/17834 [7:59:57<1:02:16,  1.76s/it] 88%|████████▊ | 15709/17834 [7:59:59<1:02:16,  1.76s/it] 88%|████████▊ | 15710/17834 [8:00:01<1:02:18,  1.76s/it] 88%|████████▊ | 15711/17834 [8:00:02<1:02:07,  1.76s/it] 88%|████████▊ | 15712/17834 [8:00:04<1:01:59,  1.75s/it] 88%|████████▊ | 15713/17834 [8:00:06<1:02:16,  1.76s/it] 88%|████████▊ | 15714/17834 [8:00:08<1:02:40,  1.77s/it] 88%|████████▊ | 15715/17834 [8:00:09<1:02:25,  1.77s/it] 88%|████████▊ | 15716/17834 [8:00:11<1:01:56,  1.75s/it] 88%|████████▊ | 15717/17834 [8:00:13<1:01:32,  1.74s/it] 88%|████████▊ | 15718/17834 [8:00:15<1:01:21,  1.74s/it] 88%|████████▊ | 15719/17834 [8:00:16<1:01:33,  1.75s/it] 88%|████████▊ | 15720/17834 [8:00:18<1:01:06,  1.73s/it] 88%|████████▊ | 15721/17834 [8:00:20<1:00:59,  1.73s/it] 88%|████████▊ | 15722/17834 [8:00:22<1:01:16,  1.74s/it] 88%|████████▊ | 15723/17834 [8:00:23<1:01:47,  1.76s/it] 88%|████████▊ | 15724/17834 [8:00:25<1:02:15,  1.77s/it] 88%|████████▊ | 15725/17834 [8:00:27<1:03:04,  1.79s/it] 88%|████████▊ | 15726/17834 [8:00:29<1:02:32,  1.78s/it] 88%|████████▊ | 15727/17834 [8:00:31<1:02:26,  1.78s/it] 88%|████████▊ | 15728/17834 [8:00:32<1:02:02,  1.77s/it] 88%|████████▊ | 15729/17834 [8:00:34<1:01:34,  1.76s/it] 88%|████████▊ | 15730/17834 [8:00:36<1:01:16,  1.75s/it] 88%|████████▊ | 15731/17834 [8:00:38<1:01:03,  1.74s/it] 88%|████████▊ | 15732/17834 [8:00:39<1:00:06,  1.72s/it] 88%|████████▊ | 15733/17834 [8:00:41<1:00:06,  1.72s/it] 88%|████████▊ | 15734/17834 [8:00:43<1:00:42,  1.73s/it] 88%|████████▊ | 15735/17834 [8:00:44<1:00:54,  1.74s/it] 88%|████████▊ | 15736/17834 [8:00:46<1:01:18,  1.75s/it] 88%|████████▊ | 15737/17834 [8:00:48<1:00:46,  1.74s/it] 88%|████████▊ | 15738/17834 [8:00:50<1:01:05,  1.75s/it] 88%|████████▊ | 15739/17834 [8:00:51<1:01:14,  1.75s/it] 88%|████████▊ | 15740/17834 [8:00:53<1:01:24,  1.76s/it] 88%|████████▊ | 15741/17834 [8:00:55<1:01:59,  1.78s/it] 88%|████████▊ | 15742/17834 [8:00:57<1:01:19,  1.76s/it] 88%|████████▊ | 15743/17834 [8:00:58<1:00:44,  1.74s/it] 88%|████████▊ | 15744/17834 [8:01:00<1:00:30,  1.74s/it] 88%|████████▊ | 15745/17834 [8:01:02<1:00:56,  1.75s/it] 88%|████████▊ | 15746/17834 [8:01:04<1:01:03,  1.75s/it] 88%|████████▊ | 15747/17834 [8:01:05<1:01:03,  1.76s/it] 88%|████████▊ | 15748/17834 [8:01:07<1:01:41,  1.77s/it] 88%|████████▊ | 15749/17834 [8:01:09<1:01:49,  1.78s/it]08/31/2024 03:15:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1055121421813965, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04239954799413681, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2545523643493652, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.402463912963867}
 88%|████████▊ | 15750/17834 [8:01:11<1:01:03,  1.76s/it] 88%|████████▊ | 15751/17834 [8:01:13<1:01:50,  1.78s/it] 88%|████████▊ | 15752/17834 [8:01:14<1:01:01,  1.76s/it] 88%|████████▊ | 15753/17834 [8:01:16<1:00:45,  1.75s/it] 88%|████████▊ | 15754/17834 [8:01:18<1:00:47,  1.75s/it] 88%|████████▊ | 15755/17834 [8:01:20<1:00:14,  1.74s/it] 88%|████████▊ | 15756/17834 [8:01:21<59:58,  1.73s/it]   88%|████████▊ | 15757/17834 [8:01:23<1:00:45,  1.76s/it] 88%|████████▊ | 15758/17834 [8:01:25<1:00:01,  1.74s/it] 88%|████████▊ | 15759/17834 [8:01:26<59:51,  1.73s/it]   88%|████████▊ | 15760/17834 [8:01:28<59:46,  1.73s/it] 88%|████████▊ | 15761/17834 [8:01:30<1:00:06,  1.74s/it] 88%|████████▊ | 15762/17834 [8:01:32<59:52,  1.73s/it]   88%|████████▊ | 15763/17834 [8:01:33<59:53,  1.74s/it] 88%|████████▊ | 15764/17834 [8:01:35<59:53,  1.74s/it] 88%|████████▊ | 15765/17834 [8:01:37<1:00:47,  1.76s/it] 88%|████████▊ | 15766/17834 [8:01:39<1:00:54,  1.77s/it] 88%|████████▊ | 15767/17834 [8:01:40<1:00:16,  1.75s/it] 88%|████████▊ | 15768/17834 [8:01:42<1:00:31,  1.76s/it] 88%|████████▊ | 15769/17834 [8:01:44<1:00:14,  1.75s/it] 88%|████████▊ | 15770/17834 [8:01:46<59:37,  1.73s/it]   88%|████████▊ | 15771/17834 [8:01:47<59:08,  1.72s/it] 88%|████████▊ | 15772/17834 [8:01:49<59:14,  1.72s/it] 88%|████████▊ | 15773/17834 [8:01:51<59:03,  1.72s/it] 88%|████████▊ | 15774/17834 [8:01:53<58:49,  1.71s/it] 88%|████████▊ | 15775/17834 [8:01:54<59:10,  1.72s/it] 88%|████████▊ | 15776/17834 [8:01:56<59:21,  1.73s/it] 88%|████████▊ | 15777/17834 [8:01:58<1:00:29,  1.76s/it] 88%|████████▊ | 15778/17834 [8:02:00<1:00:50,  1.78s/it] 88%|████████▊ | 15779/17834 [8:02:01<1:00:40,  1.77s/it] 88%|████████▊ | 15780/17834 [8:02:03<1:00:39,  1.77s/it] 88%|████████▊ | 15781/17834 [8:02:05<1:00:16,  1.76s/it] 88%|████████▊ | 15782/17834 [8:02:07<1:01:23,  1.79s/it] 88%|████████▊ | 15783/17834 [8:02:08<59:54,  1.75s/it]   89%|████████▊ | 15784/17834 [8:02:10<1:00:23,  1.77s/it] 89%|████████▊ | 15785/17834 [8:02:12<59:23,  1.74s/it]   89%|████████▊ | 15786/17834 [8:02:14<59:33,  1.75s/it] 89%|████████▊ | 15787/17834 [8:02:15<59:27,  1.74s/it] 89%|████████▊ | 15788/17834 [8:02:17<59:35,  1.75s/it] 89%|████████▊ | 15789/17834 [8:02:19<59:05,  1.73s/it] 89%|████████▊ | 15790/17834 [8:02:21<59:17,  1.74s/it] 89%|████████▊ | 15791/17834 [8:02:22<59:07,  1.74s/it] 89%|████████▊ | 15792/17834 [8:02:24<1:00:11,  1.77s/it] 89%|████████▊ | 15793/17834 [8:02:26<59:55,  1.76s/it]   89%|████████▊ | 15794/17834 [8:02:28<58:53,  1.73s/it] 89%|████████▊ | 15795/17834 [8:02:29<59:00,  1.74s/it] 89%|████████▊ | 15796/17834 [8:02:31<58:38,  1.73s/it] 89%|████████▊ | 15797/17834 [8:02:33<58:48,  1.73s/it] 89%|████████▊ | 15798/17834 [8:02:35<59:07,  1.74s/it] 89%|████████▊ | 15799/17834 [8:02:36<58:38,  1.73s/it]08/31/2024 03:16:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.110982894897461, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028919372707605362, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1495001316070557, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.289402484893799}
 89%|████████▊ | 15800/17834 [8:02:38<1:00:16,  1.78s/it] 89%|████████▊ | 15801/17834 [8:02:40<59:29,  1.76s/it]   89%|████████▊ | 15802/17834 [8:02:42<59:09,  1.75s/it] 89%|████████▊ | 15803/17834 [8:02:43<58:30,  1.73s/it] 89%|████████▊ | 15804/17834 [8:02:45<58:57,  1.74s/it] 89%|████████▊ | 15805/17834 [8:02:47<59:04,  1.75s/it] 89%|████████▊ | 15806/17834 [8:02:49<58:42,  1.74s/it] 89%|████████▊ | 15807/17834 [8:02:50<58:13,  1.72s/it] 89%|████████▊ | 15808/17834 [8:02:52<58:54,  1.74s/it] 89%|████████▊ | 15809/17834 [8:02:54<59:44,  1.77s/it] 89%|████████▊ | 15810/17834 [8:02:56<58:48,  1.74s/it] 89%|████████▊ | 15811/17834 [8:02:57<58:54,  1.75s/it] 89%|████████▊ | 15812/17834 [8:02:59<58:45,  1.74s/it] 89%|████████▊ | 15813/17834 [8:03:01<58:41,  1.74s/it] 89%|████████▊ | 15814/17834 [8:03:02<58:31,  1.74s/it] 89%|████████▊ | 15815/17834 [8:03:04<58:26,  1.74s/it] 89%|████████▊ | 15816/17834 [8:03:06<58:26,  1.74s/it] 89%|████████▊ | 15817/17834 [8:03:08<57:55,  1.72s/it] 89%|████████▊ | 15818/17834 [8:03:09<57:50,  1.72s/it] 89%|████████▊ | 15819/17834 [8:03:11<57:23,  1.71s/it] 89%|████████▊ | 15820/17834 [8:03:13<59:18,  1.77s/it] 89%|████████▊ | 15821/17834 [8:03:15<59:05,  1.76s/it] 89%|████████▊ | 15822/17834 [8:03:16<58:56,  1.76s/it] 89%|████████▊ | 15823/17834 [8:03:18<58:35,  1.75s/it] 89%|████████▊ | 15824/17834 [8:03:20<58:05,  1.73s/it] 89%|████████▊ | 15825/17834 [8:03:22<58:44,  1.75s/it] 89%|████████▊ | 15826/17834 [8:03:23<58:26,  1.75s/it] 89%|████████▊ | 15827/17834 [8:03:25<58:13,  1.74s/it] 89%|████████▉ | 15828/17834 [8:03:27<58:24,  1.75s/it] 89%|████████▉ | 15829/17834 [8:03:29<59:08,  1.77s/it] 89%|████████▉ | 15830/17834 [8:03:30<59:16,  1.77s/it] 89%|████████▉ | 15831/17834 [8:03:32<58:58,  1.77s/it] 89%|████████▉ | 15832/17834 [8:03:34<57:46,  1.73s/it] 89%|████████▉ | 15833/17834 [8:03:36<57:23,  1.72s/it] 89%|████████▉ | 15834/17834 [8:03:37<58:04,  1.74s/it] 89%|████████▉ | 15835/17834 [8:03:39<57:43,  1.73s/it] 89%|████████▉ | 15836/17834 [8:03:41<58:10,  1.75s/it] 89%|████████▉ | 15837/17834 [8:03:43<57:37,  1.73s/it] 89%|████████▉ | 15838/17834 [8:03:44<57:20,  1.72s/it] 89%|████████▉ | 15839/17834 [8:03:46<57:32,  1.73s/it] 89%|████████▉ | 15840/17834 [8:03:48<58:14,  1.75s/it] 89%|████████▉ | 15841/17834 [8:03:50<58:04,  1.75s/it] 89%|████████▉ | 15842/17834 [8:03:51<58:37,  1.77s/it] 89%|████████▉ | 15843/17834 [8:03:53<58:00,  1.75s/it] 89%|████████▉ | 15844/17834 [8:03:55<58:06,  1.75s/it] 89%|████████▉ | 15845/17834 [8:03:57<58:05,  1.75s/it] 89%|████████▉ | 15846/17834 [8:03:58<58:02,  1.75s/it] 89%|████████▉ | 15847/17834 [8:04:00<57:50,  1.75s/it] 89%|████████▉ | 15848/17834 [8:04:02<57:57,  1.75s/it] 89%|████████▉ | 15849/17834 [8:04:04<57:12,  1.73s/it]08/31/2024 03:18:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8572497367858887, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023400839418172836, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1003682613372803, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.9810187816619873}
 89%|████████▉ | 15850/17834 [8:04:05<57:38,  1.74s/it] 89%|████████▉ | 15851/17834 [8:04:07<58:15,  1.76s/it] 89%|████████▉ | 15852/17834 [8:04:09<59:11,  1.79s/it] 89%|████████▉ | 15853/17834 [8:04:11<58:50,  1.78s/it] 89%|████████▉ | 15854/17834 [8:04:12<58:11,  1.76s/it] 89%|████████▉ | 15855/17834 [8:04:14<57:35,  1.75s/it] 89%|████████▉ | 15856/17834 [8:04:16<57:00,  1.73s/it] 89%|████████▉ | 15857/17834 [8:04:18<57:18,  1.74s/it] 89%|████████▉ | 15858/17834 [8:04:19<56:47,  1.72s/it] 89%|████████▉ | 15859/17834 [8:04:21<57:18,  1.74s/it] 89%|████████▉ | 15860/17834 [8:04:23<57:14,  1.74s/it] 89%|████████▉ | 15861/17834 [8:04:25<57:37,  1.75s/it] 89%|████████▉ | 15862/17834 [8:04:26<57:10,  1.74s/it] 89%|████████▉ | 15863/17834 [8:04:28<58:32,  1.78s/it] 89%|████████▉ | 15864/17834 [8:04:30<58:41,  1.79s/it] 89%|████████▉ | 15865/17834 [8:04:32<58:01,  1.77s/it] 89%|████████▉ | 15866/17834 [8:04:33<57:07,  1.74s/it] 89%|████████▉ | 15867/17834 [8:04:35<57:07,  1.74s/it] 89%|████████▉ | 15868/17834 [8:04:37<57:28,  1.75s/it] 89%|████████▉ | 15869/17834 [8:04:39<57:05,  1.74s/it] 89%|████████▉ | 15870/17834 [8:04:40<57:08,  1.75s/it] 89%|████████▉ | 15871/17834 [8:04:42<56:44,  1.73s/it] 89%|████████▉ | 15872/17834 [8:04:44<56:23,  1.72s/it] 89%|████████▉ | 15873/17834 [8:04:46<56:42,  1.73s/it] 89%|████████▉ | 15874/17834 [8:04:47<56:34,  1.73s/it] 89%|████████▉ | 15875/17834 [8:04:49<56:54,  1.74s/it] 89%|████████▉ | 15876/17834 [8:04:51<57:30,  1.76s/it] 89%|████████▉ | 15877/17834 [8:04:53<57:31,  1.76s/it] 89%|████████▉ | 15878/17834 [8:04:54<57:33,  1.77s/it] 89%|████████▉ | 15879/17834 [8:04:56<57:08,  1.75s/it] 89%|████████▉ | 15880/17834 [8:04:58<56:23,  1.73s/it] 89%|████████▉ | 15881/17834 [8:05:00<56:39,  1.74s/it] 89%|████████▉ | 15882/17834 [8:05:01<56:54,  1.75s/it] 89%|████████▉ | 15883/17834 [8:05:03<56:47,  1.75s/it] 89%|████████▉ | 15884/17834 [8:05:05<56:24,  1.74s/it] 89%|████████▉ | 15885/17834 [8:05:07<56:51,  1.75s/it] 89%|████████▉ | 15886/17834 [8:05:08<57:33,  1.77s/it] 89%|████████▉ | 15887/17834 [8:05:10<56:44,  1.75s/it] 89%|████████▉ | 15888/17834 [8:05:12<57:23,  1.77s/it] 89%|████████▉ | 15889/17834 [8:05:14<56:47,  1.75s/it] 89%|████████▉ | 15890/17834 [8:05:15<57:27,  1.77s/it] 89%|████████▉ | 15891/17834 [8:05:17<56:26,  1.74s/it] 89%|████████▉ | 15892/17834 [8:05:19<57:37,  1.78s/it] 89%|████████▉ | 15893/17834 [8:05:21<57:30,  1.78s/it] 89%|████████▉ | 15894/17834 [8:05:22<56:53,  1.76s/it] 89%|████████▉ | 15895/17834 [8:05:24<56:30,  1.75s/it] 89%|████████▉ | 15896/17834 [8:05:26<56:17,  1.74s/it] 89%|████████▉ | 15897/17834 [8:05:28<55:56,  1.73s/it] 89%|████████▉ | 15898/17834 [8:05:29<56:05,  1.74s/it] 89%|████████▉ | 15899/17834 [8:05:31<56:37,  1.76s/it]08/31/2024 03:19:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8908388614654541, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020615199580788612, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.126800060272217, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0382542610168457}
 89%|████████▉ | 15900/17834 [8:05:33<56:26,  1.75s/it] 89%|████████▉ | 15901/17834 [8:05:35<55:47,  1.73s/it] 89%|████████▉ | 15902/17834 [8:05:36<55:54,  1.74s/it] 89%|████████▉ | 15903/17834 [8:05:38<56:03,  1.74s/it] 89%|████████▉ | 15904/17834 [8:05:40<55:34,  1.73s/it] 89%|████████▉ | 15905/17834 [8:05:42<56:00,  1.74s/it] 89%|████████▉ | 15906/17834 [8:05:43<56:05,  1.75s/it] 89%|████████▉ | 15907/17834 [8:05:45<56:02,  1.74s/it] 89%|████████▉ | 15908/17834 [8:05:47<55:09,  1.72s/it] 89%|████████▉ | 15909/17834 [8:05:48<55:23,  1.73s/it] 89%|████████▉ | 15910/17834 [8:05:50<55:27,  1.73s/it] 89%|████████▉ | 15911/17834 [8:05:52<55:10,  1.72s/it] 89%|████████▉ | 15912/17834 [8:05:54<54:45,  1.71s/it] 89%|████████▉ | 15913/17834 [8:05:55<56:24,  1.76s/it] 89%|████████▉ | 15914/17834 [8:05:57<56:06,  1.75s/it] 89%|████████▉ | 15915/17834 [8:05:59<56:20,  1.76s/it] 89%|████████▉ | 15916/17834 [8:06:01<56:22,  1.76s/it] 89%|████████▉ | 15917/17834 [8:06:03<56:41,  1.77s/it] 89%|████████▉ | 15918/17834 [8:06:04<56:39,  1.77s/it] 89%|████████▉ | 15919/17834 [8:06:06<56:26,  1.77s/it] 89%|████████▉ | 15920/17834 [8:06:08<55:12,  1.73s/it] 89%|████████▉ | 15921/17834 [8:06:09<55:11,  1.73s/it] 89%|████████▉ | 15922/17834 [8:06:11<55:14,  1.73s/it] 89%|████████▉ | 15923/17834 [8:06:13<55:06,  1.73s/it] 89%|████████▉ | 15924/17834 [8:06:15<54:36,  1.72s/it] 89%|████████▉ | 15925/17834 [8:06:16<55:25,  1.74s/it] 89%|████████▉ | 15926/17834 [8:06:18<55:20,  1.74s/it] 89%|████████▉ | 15927/17834 [8:06:20<55:33,  1.75s/it] 89%|████████▉ | 15928/17834 [8:06:22<56:03,  1.76s/it] 89%|████████▉ | 15929/17834 [8:06:23<55:35,  1.75s/it] 89%|████████▉ | 15930/17834 [8:06:25<55:01,  1.73s/it] 89%|████████▉ | 15931/17834 [8:06:27<55:23,  1.75s/it] 89%|████████▉ | 15932/17834 [8:06:29<55:24,  1.75s/it] 89%|████████▉ | 15933/17834 [8:06:30<55:09,  1.74s/it] 89%|████████▉ | 15934/17834 [8:06:32<54:46,  1.73s/it] 89%|████████▉ | 15935/17834 [8:06:34<54:50,  1.73s/it] 89%|████████▉ | 15936/17834 [8:06:36<55:13,  1.75s/it] 89%|████████▉ | 15937/17834 [8:06:37<56:03,  1.77s/it] 89%|████████▉ | 15938/17834 [8:06:39<55:32,  1.76s/it] 89%|████████▉ | 15939/17834 [8:06:41<55:43,  1.76s/it] 89%|████████▉ | 15940/17834 [8:06:43<55:22,  1.75s/it] 89%|████████▉ | 15941/17834 [8:06:44<55:25,  1.76s/it] 89%|████████▉ | 15942/17834 [8:06:46<55:07,  1.75s/it] 89%|████████▉ | 15943/17834 [8:06:48<55:09,  1.75s/it] 89%|████████▉ | 15944/17834 [8:06:50<56:10,  1.78s/it] 89%|████████▉ | 15945/17834 [8:06:52<56:06,  1.78s/it] 89%|████████▉ | 15946/17834 [8:06:53<55:47,  1.77s/it] 89%|████████▉ | 15947/17834 [8:06:55<55:25,  1.76s/it] 89%|████████▉ | 15948/17834 [8:06:57<55:26,  1.76s/it] 89%|████████▉ | 15949/17834 [8:06:59<55:17,  1.76s/it]08/31/2024 03:21:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8431453704833984, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015929969027638435, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.125974416732788, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.9850497245788574}
 89%|████████▉ | 15950/17834 [8:07:00<54:46,  1.74s/it] 89%|████████▉ | 15951/17834 [8:07:02<54:36,  1.74s/it] 89%|████████▉ | 15952/17834 [8:07:04<55:23,  1.77s/it] 89%|████████▉ | 15953/17834 [8:07:05<54:37,  1.74s/it] 89%|████████▉ | 15954/17834 [8:07:07<54:34,  1.74s/it] 89%|████████▉ | 15955/17834 [8:07:09<54:31,  1.74s/it] 89%|████████▉ | 15956/17834 [8:07:11<53:45,  1.72s/it] 89%|████████▉ | 15957/17834 [8:07:12<54:06,  1.73s/it] 89%|████████▉ | 15958/17834 [8:07:14<54:34,  1.75s/it] 89%|████████▉ | 15959/17834 [8:07:16<54:10,  1.73s/it] 89%|████████▉ | 15960/17834 [8:07:18<54:36,  1.75s/it] 89%|████████▉ | 15961/17834 [8:07:19<54:06,  1.73s/it] 90%|████████▉ | 15962/17834 [8:07:21<54:12,  1.74s/it] 90%|████████▉ | 15963/17834 [8:07:23<54:04,  1.73s/it] 90%|████████▉ | 15964/17834 [8:07:25<55:00,  1.76s/it] 90%|████████▉ | 15965/17834 [8:07:26<53:59,  1.73s/it] 90%|████████▉ | 15966/17834 [8:07:28<54:03,  1.74s/it] 90%|████████▉ | 15967/17834 [8:07:30<54:06,  1.74s/it] 90%|████████▉ | 15968/17834 [8:07:32<54:22,  1.75s/it] 90%|████████▉ | 15969/17834 [8:07:33<54:49,  1.76s/it] 90%|████████▉ | 15970/17834 [8:07:35<54:18,  1.75s/it] 90%|████████▉ | 15971/17834 [8:07:37<55:00,  1.77s/it] 90%|████████▉ | 15972/17834 [8:07:39<54:15,  1.75s/it] 90%|████████▉ | 15973/17834 [8:07:40<54:50,  1.77s/it] 90%|████████▉ | 15974/17834 [8:07:42<54:09,  1.75s/it] 90%|████████▉ | 15975/17834 [8:07:44<54:01,  1.74s/it] 90%|████████▉ | 15976/17834 [8:07:46<53:45,  1.74s/it] 90%|████████▉ | 15977/17834 [8:07:47<54:09,  1.75s/it] 90%|████████▉ | 15978/17834 [8:07:49<54:16,  1.75s/it] 90%|████████▉ | 15979/17834 [8:07:51<54:26,  1.76s/it] 90%|████████▉ | 15980/17834 [8:07:53<54:16,  1.76s/it] 90%|████████▉ | 15981/17834 [8:07:54<54:16,  1.76s/it] 90%|████████▉ | 15982/17834 [8:07:56<54:13,  1.76s/it] 90%|████████▉ | 15983/17834 [8:07:58<53:47,  1.74s/it] 90%|████████▉ | 15984/17834 [8:08:00<53:33,  1.74s/it] 90%|████████▉ | 15985/17834 [8:08:01<53:32,  1.74s/it] 90%|████████▉ | 15986/17834 [8:08:03<53:26,  1.74s/it] 90%|████████▉ | 15987/17834 [8:08:05<53:56,  1.75s/it] 90%|████████▉ | 15988/17834 [8:08:07<54:29,  1.77s/it] 90%|████████▉ | 15989/17834 [8:08:08<53:48,  1.75s/it] 90%|████████▉ | 15990/17834 [8:08:10<53:39,  1.75s/it] 90%|████████▉ | 15991/17834 [8:08:12<54:07,  1.76s/it] 90%|████████▉ | 15992/17834 [8:08:14<54:18,  1.77s/it] 90%|████████▉ | 15993/17834 [8:08:15<54:07,  1.76s/it] 90%|████████▉ | 15994/17834 [8:08:17<53:55,  1.76s/it] 90%|████████▉ | 15995/17834 [8:08:19<53:40,  1.75s/it] 90%|████████▉ | 15996/17834 [8:08:21<53:36,  1.75s/it] 90%|████████▉ | 15997/17834 [8:08:22<53:34,  1.75s/it] 90%|████████▉ | 15998/17834 [8:08:24<52:42,  1.72s/it] 90%|████████▉ | 15999/17834 [8:08:26<52:34,  1.72s/it]08/31/2024 03:22:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9963729381561279, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024744484573602676, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.116419553756714, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1375370025634766}
 90%|████████▉ | 16000/17834 [8:08:28<53:32,  1.75s/it] 90%|████████▉ | 16001/17834 [8:08:29<53:09,  1.74s/it] 90%|████████▉ | 16002/17834 [8:08:31<52:37,  1.72s/it] 90%|████████▉ | 16003/17834 [8:08:33<53:36,  1.76s/it] 90%|████████▉ | 16004/17834 [8:08:35<52:45,  1.73s/it] 90%|████████▉ | 16005/17834 [8:08:36<52:26,  1.72s/it] 90%|████████▉ | 16006/17834 [8:08:38<52:46,  1.73s/it] 90%|████████▉ | 16007/17834 [8:08:40<52:40,  1.73s/it] 90%|████████▉ | 16008/17834 [8:08:42<53:16,  1.75s/it] 90%|████████▉ | 16009/17834 [8:08:43<53:08,  1.75s/it] 90%|████████▉ | 16010/17834 [8:08:45<52:52,  1.74s/it] 90%|████████▉ | 16011/17834 [8:08:47<53:32,  1.76s/it] 90%|████████▉ | 16012/17834 [8:08:48<52:55,  1.74s/it] 90%|████████▉ | 16013/17834 [8:08:50<52:46,  1.74s/it] 90%|████████▉ | 16014/17834 [8:08:52<54:09,  1.79s/it] 90%|████████▉ | 16015/17834 [8:08:54<53:24,  1.76s/it] 90%|████████▉ | 16016/17834 [8:08:56<53:27,  1.76s/it] 90%|████████▉ | 16017/17834 [8:08:57<53:15,  1.76s/it] 90%|████████▉ | 16018/17834 [8:08:59<53:28,  1.77s/it] 90%|████████▉ | 16019/17834 [8:09:01<52:46,  1.74s/it] 90%|████████▉ | 16020/17834 [8:09:03<52:58,  1.75s/it] 90%|████████▉ | 16021/17834 [8:09:04<52:37,  1.74s/it] 90%|████████▉ | 16022/17834 [8:09:06<52:50,  1.75s/it] 90%|████████▉ | 16023/17834 [8:09:08<52:29,  1.74s/it] 90%|████████▉ | 16024/17834 [8:09:10<52:49,  1.75s/it] 90%|████████▉ | 16025/17834 [8:09:11<52:56,  1.76s/it] 90%|████████▉ | 16026/17834 [8:09:13<52:41,  1.75s/it] 90%|████████▉ | 16027/17834 [8:09:15<53:56,  1.79s/it] 90%|████████▉ | 16028/17834 [8:09:17<53:00,  1.76s/it] 90%|████████▉ | 16029/17834 [8:09:18<52:59,  1.76s/it] 90%|████████▉ | 16030/17834 [8:09:20<52:34,  1.75s/it] 90%|████████▉ | 16031/17834 [8:09:22<51:55,  1.73s/it] 90%|████████▉ | 16032/17834 [8:09:24<52:23,  1.74s/it] 90%|████████▉ | 16033/17834 [8:09:25<52:06,  1.74s/it] 90%|████████▉ | 16034/17834 [8:09:27<51:38,  1.72s/it] 90%|████████▉ | 16035/17834 [8:09:29<52:16,  1.74s/it] 90%|████████▉ | 16036/17834 [8:09:31<52:02,  1.74s/it] 90%|████████▉ | 16037/17834 [8:09:32<52:41,  1.76s/it]08/31/2024 03:23:51 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/31/2024 03:23:51 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:48,  4.56it/s][A
  1%|▏         | 3/221 [00:00<00:56,  3.84it/s][A
  2%|▏         | 4/221 [00:00<00:49,  4.41it/s][A
  2%|▏         | 5/221 [00:01<00:52,  4.09it/s][A
  3%|▎         | 6/221 [00:01<00:47,  4.54it/s][A
  3%|▎         | 7/221 [00:01<00:54,  3.90it/s][A
  4%|▎         | 8/221 [00:02<00:57,  3.73it/s][A
  4%|▍         | 9/221 [00:02<01:07,  3.16it/s][A
  5%|▍         | 10/221 [00:02<01:10,  3.01it/s][A
  5%|▍         | 11/221 [00:03<01:02,  3.36it/s][A
  5%|▌         | 12/221 [00:03<00:54,  3.84it/s][A
  6%|▌         | 13/221 [00:03<00:46,  4.50it/s][A
  6%|▋         | 14/221 [00:03<00:47,  4.34it/s][A
  7%|▋         | 15/221 [00:03<00:43,  4.76it/s][A
  7%|▋         | 16/221 [00:04<00:49,  4.10it/s][A
  8%|▊         | 17/221 [00:04<00:49,  4.12it/s][A
  8%|▊         | 18/221 [00:04<00:45,  4.44it/s][A
  9%|▊         | 19/221 [00:04<00:45,  4.45it/s][A
  9%|▉         | 20/221 [00:04<00:42,  4.77it/s][A
 10%|▉         | 21/221 [00:05<00:44,  4.49it/s][A
 10%|▉         | 22/221 [00:05<00:47,  4.18it/s][A
 10%|█         | 23/221 [00:05<00:44,  4.43it/s][A
 11%|█         | 24/221 [00:05<00:38,  5.16it/s][A
 11%|█▏        | 25/221 [00:05<00:41,  4.69it/s][A
 12%|█▏        | 26/221 [00:06<00:38,  5.04it/s][A
 12%|█▏        | 27/221 [00:06<00:48,  3.97it/s][A
 13%|█▎        | 28/221 [00:06<00:58,  3.28it/s][A
 13%|█▎        | 29/221 [00:07<01:05,  2.93it/s][A
 14%|█▎        | 30/221 [00:07<01:10,  2.72it/s][A
 14%|█▍        | 31/221 [00:08<01:10,  2.69it/s][A
 14%|█▍        | 32/221 [00:08<01:07,  2.79it/s][A
 15%|█▍        | 33/221 [00:08<00:57,  3.29it/s][A
 15%|█▌        | 34/221 [00:08<00:53,  3.50it/s][A
 16%|█▌        | 35/221 [00:09<00:57,  3.21it/s][A
 16%|█▋        | 36/221 [00:09<00:57,  3.19it/s][A
 17%|█▋        | 37/221 [00:09<00:51,  3.57it/s][A
 17%|█▋        | 38/221 [00:10<00:50,  3.64it/s][A
 18%|█▊        | 39/221 [00:10<00:42,  4.25it/s][A
 18%|█▊        | 40/221 [00:10<00:48,  3.74it/s][A
 19%|█▊        | 41/221 [00:10<00:56,  3.19it/s][A
 19%|█▉        | 43/221 [00:11<00:44,  4.01it/s][A
 20%|█▉        | 44/221 [00:11<00:51,  3.41it/s][A
 20%|██        | 45/221 [00:12<00:55,  3.16it/s][A
 21%|██        | 46/221 [00:12<00:57,  3.03it/s][A
 21%|██▏       | 47/221 [00:12<00:55,  3.15it/s][A
 22%|██▏       | 48/221 [00:12<00:48,  3.58it/s][A
 22%|██▏       | 49/221 [00:13<00:46,  3.73it/s][A
 23%|██▎       | 50/221 [00:13<00:50,  3.38it/s][A
 23%|██▎       | 51/221 [00:13<00:53,  3.20it/s][A
 24%|██▎       | 52/221 [00:14<00:48,  3.50it/s][A
 24%|██▍       | 53/221 [00:14<00:45,  3.71it/s][A
 24%|██▍       | 54/221 [00:14<00:45,  3.68it/s][A
 25%|██▍       | 55/221 [00:15<00:48,  3.45it/s][A
 25%|██▌       | 56/221 [00:15<00:46,  3.55it/s][A
 26%|██▌       | 57/221 [00:15<00:56,  2.92it/s][A
 26%|██▌       | 58/221 [00:15<00:46,  3.54it/s][A
 27%|██▋       | 59/221 [00:16<00:39,  4.15it/s][A
 27%|██▋       | 60/221 [00:16<00:42,  3.81it/s][A
 28%|██▊       | 61/221 [00:16<00:44,  3.60it/s][A
 28%|██▊       | 62/221 [00:16<00:39,  4.05it/s][A
 29%|██▊       | 63/221 [00:17<00:43,  3.63it/s][A
 29%|██▉       | 64/221 [00:17<00:51,  3.02it/s][A
 29%|██▉       | 65/221 [00:18<00:54,  2.84it/s][A
 30%|██▉       | 66/221 [00:18<00:53,  2.88it/s][A
 30%|███       | 67/221 [00:18<00:54,  2.81it/s][A
 31%|███       | 68/221 [00:18<00:47,  3.23it/s][A
 31%|███       | 69/221 [00:19<00:55,  2.74it/s][A
 32%|███▏      | 70/221 [00:19<00:54,  2.75it/s][A
 32%|███▏      | 71/221 [00:20<00:47,  3.15it/s][A
 33%|███▎      | 72/221 [00:20<00:52,  2.83it/s][A
 33%|███▎      | 73/221 [00:20<00:52,  2.84it/s][A
 33%|███▎      | 74/221 [00:20<00:42,  3.50it/s][A
 34%|███▍      | 75/221 [00:21<00:39,  3.68it/s][A
 34%|███▍      | 76/221 [00:21<00:51,  2.83it/s][A
 35%|███▍      | 77/221 [00:22<00:55,  2.57it/s][A
 35%|███▌      | 78/221 [00:22<00:52,  2.72it/s][A
 36%|███▌      | 79/221 [00:23<01:01,  2.32it/s][A
 36%|███▌      | 80/221 [00:23<00:56,  2.49it/s][A
 37%|███▋      | 81/221 [00:23<00:44,  3.13it/s][A
 37%|███▋      | 82/221 [00:23<00:43,  3.21it/s][A
 38%|███▊      | 83/221 [00:24<00:45,  3.05it/s][A
 38%|███▊      | 84/221 [00:24<00:41,  3.31it/s][A
 38%|███▊      | 85/221 [00:24<00:39,  3.48it/s][A
 39%|███▉      | 86/221 [00:25<00:47,  2.86it/s][A
 39%|███▉      | 87/221 [00:25<00:50,  2.68it/s][A
 40%|███▉      | 88/221 [00:26<00:53,  2.47it/s][A
 40%|████      | 89/221 [00:26<00:50,  2.64it/s][A
 41%|████      | 90/221 [00:26<00:44,  2.96it/s][A
 41%|████      | 91/221 [00:26<00:37,  3.50it/s][A
 42%|████▏     | 92/221 [00:27<00:36,  3.51it/s][A
 42%|████▏     | 93/221 [00:27<00:47,  2.70it/s][A
 43%|████▎     | 94/221 [00:28<00:46,  2.75it/s][A
 43%|████▎     | 95/221 [00:28<00:39,  3.20it/s][A
 43%|████▎     | 96/221 [00:28<00:39,  3.17it/s][A
 44%|████▍     | 97/221 [00:28<00:38,  3.24it/s][A
 44%|████▍     | 98/221 [00:29<00:45,  2.68it/s][A
 45%|████▍     | 99/221 [00:29<00:44,  2.77it/s][A
 45%|████▌     | 100/221 [00:30<00:41,  2.90it/s][A
 46%|████▌     | 101/221 [00:30<00:37,  3.20it/s][A
 46%|████▌     | 102/221 [00:30<00:32,  3.61it/s][A
 47%|████▋     | 103/221 [00:30<00:34,  3.44it/s][A
 47%|████▋     | 104/221 [00:31<00:44,  2.65it/s][A
 48%|████▊     | 105/221 [00:31<00:45,  2.55it/s][A
 48%|████▊     | 106/221 [00:32<00:41,  2.79it/s][A
 48%|████▊     | 107/221 [00:32<00:40,  2.84it/s][A
 49%|████▉     | 108/221 [00:32<00:37,  3.01it/s][A
 49%|████▉     | 109/221 [00:32<00:32,  3.44it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.33it/s][A
 50%|█████     | 111/221 [00:33<00:33,  3.26it/s][A
 51%|█████     | 112/221 [00:33<00:31,  3.46it/s][A
 51%|█████     | 113/221 [00:33<00:28,  3.73it/s][A
 52%|█████▏    | 114/221 [00:34<00:25,  4.17it/s][A
 52%|█████▏    | 115/221 [00:34<00:31,  3.35it/s][A
 52%|█████▏    | 116/221 [00:34<00:30,  3.40it/s][A
 53%|█████▎    | 117/221 [00:35<00:27,  3.73it/s][A
 53%|█████▎    | 118/221 [00:35<00:25,  3.99it/s][A
 54%|█████▍    | 119/221 [00:35<00:25,  4.00it/s][A
 54%|█████▍    | 120/221 [00:35<00:29,  3.38it/s][A
 55%|█████▍    | 121/221 [00:36<00:27,  3.67it/s][A
 55%|█████▌    | 122/221 [00:36<00:27,  3.59it/s][A
 56%|█████▌    | 123/221 [00:36<00:25,  3.77it/s][A
 56%|█████▌    | 124/221 [00:36<00:26,  3.61it/s][A
 57%|█████▋    | 125/221 [00:37<00:29,  3.24it/s][A
 57%|█████▋    | 126/221 [00:37<00:25,  3.74it/s][A
 57%|█████▋    | 127/221 [00:37<00:25,  3.68it/s][A
 58%|█████▊    | 128/221 [00:38<00:25,  3.62it/s][A
 58%|█████▊    | 129/221 [00:38<00:22,  4.13it/s][A
 59%|█████▉    | 130/221 [00:38<00:21,  4.22it/s][A
 59%|█████▉    | 131/221 [00:38<00:23,  3.85it/s][A
 60%|█████▉    | 132/221 [00:39<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:39<00:32,  2.69it/s][A
 61%|██████    | 134/221 [00:40<00:31,  2.76it/s][A
 61%|██████    | 135/221 [00:40<00:29,  2.87it/s][A
 62%|██████▏   | 136/221 [00:40<00:27,  3.14it/s][A
 62%|██████▏   | 137/221 [00:40<00:23,  3.50it/s][A
 62%|██████▏   | 138/221 [00:41<00:22,  3.62it/s][A
 63%|██████▎   | 139/221 [00:41<00:21,  3.81it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.83it/s][A
 64%|██████▍   | 141/221 [00:41<00:23,  3.38it/s][A
 64%|██████▍   | 142/221 [00:42<00:22,  3.59it/s][A
 65%|██████▍   | 143/221 [00:42<00:30,  2.57it/s][A
 65%|██████▌   | 144/221 [00:43<00:29,  2.64it/s][A
 66%|██████▌   | 145/221 [00:43<00:27,  2.79it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.30it/s][A
 67%|██████▋   | 147/221 [00:44<00:23,  3.21it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.32it/s][A
 67%|██████▋   | 149/221 [00:44<00:23,  3.01it/s][A
 68%|██████▊   | 150/221 [00:44<00:19,  3.65it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:45<00:20,  3.41it/s][A
 69%|██████▉   | 153/221 [00:45<00:16,  4.22it/s][A
 70%|██████▉   | 154/221 [00:46<00:21,  3.15it/s][A
 70%|███████   | 155/221 [00:46<00:21,  3.05it/s][A
 71%|███████   | 156/221 [00:46<00:21,  3.09it/s][A
 71%|███████   | 157/221 [00:47<00:20,  3.10it/s][A
 71%|███████▏  | 158/221 [00:47<00:25,  2.48it/s][A
 72%|███████▏  | 159/221 [00:47<00:21,  2.82it/s][A
 72%|███████▏  | 160/221 [00:48<00:22,  2.77it/s][A
 73%|███████▎  | 161/221 [00:48<00:24,  2.47it/s][A
 73%|███████▎  | 162/221 [00:49<00:21,  2.81it/s][A
 74%|███████▍  | 163/221 [00:49<00:21,  2.73it/s][A
 74%|███████▍  | 164/221 [00:49<00:16,  3.38it/s][A
 75%|███████▍  | 165/221 [00:49<00:15,  3.65it/s][A
 75%|███████▌  | 166/221 [00:49<00:13,  4.11it/s][A
 76%|███████▌  | 167/221 [00:50<00:17,  3.07it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.34it/s][A
 76%|███████▋  | 169/221 [00:50<00:15,  3.43it/s][A
 77%|███████▋  | 170/221 [00:51<00:14,  3.63it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.56it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.43it/s][A
 78%|███████▊  | 173/221 [00:52<00:17,  2.80it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.24it/s][A
 79%|███████▉  | 175/221 [00:52<00:13,  3.36it/s][A
 80%|███████▉  | 176/221 [00:53<00:12,  3.73it/s][A
 80%|████████  | 177/221 [00:53<00:10,  4.30it/s][A
 81%|████████  | 178/221 [00:53<00:10,  4.27it/s][A
 81%|████████  | 179/221 [00:53<00:09,  4.29it/s][A
 81%|████████▏ | 180/221 [00:53<00:09,  4.49it/s][A
 82%|████████▏ | 181/221 [00:54<00:09,  4.43it/s][A
 82%|████████▏ | 182/221 [00:54<00:09,  4.22it/s][A
 83%|████████▎ | 183/221 [00:54<00:10,  3.69it/s][A
 83%|████████▎ | 184/221 [00:55<00:12,  2.99it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.23it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.24it/s][A
 85%|████████▍ | 187/221 [00:56<00:11,  3.08it/s][A
 85%|████████▌ | 188/221 [00:56<00:10,  3.23it/s][A
 86%|████████▌ | 189/221 [00:56<00:08,  3.74it/s][A
 86%|████████▌ | 190/221 [00:56<00:07,  3.90it/s][A
 86%|████████▋ | 191/221 [00:57<00:10,  2.96it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.29it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.48it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.79it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.71it/s][A
 89%|████████▊ | 196/221 [00:58<00:08,  2.78it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.20it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  3.14it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  3.68it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.50it/s][A
 91%|█████████ | 201/221 [01:00<00:05,  3.77it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.56it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  3.88it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.14it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.46it/s][A
 93%|█████████▎| 206/221 [01:01<00:03,  4.85it/s][A
 94%|█████████▎| 207/221 [01:01<00:02,  4.90it/s][A
 94%|█████████▍| 208/221 [01:01<00:02,  5.46it/s][A
 95%|█████████▍| 209/221 [01:01<00:02,  5.64it/s][A
 95%|█████████▌| 210/221 [01:01<00:01,  5.78it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  4.07it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.37it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.57it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.24it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.74it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.70it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.34it/s][A
 99%|█████████▊| 218/221 [01:04<00:01,  2.84it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  2.80it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  2.98it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.27it/s][A100%|██████████| 221/221 [01:05<00:00,  3.38it/s]
08/31/2024 03:26:16 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 16037--===========

08/31/2024 03:26:16 - INFO - __main__ -   {'area_r1': 5.9, 'area_recall': '5.9/14.3/19.0', 'area_ravg': 13.0}
08/31/2024 03:26:16 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 16037--===========

08/31/2024 03:26:16 - INFO - __main__ -   {'area_r1': 39.8, 'area_recall': '39.8/71.6/82.4', 'area_ravg': 64.6}
08/31/2024 03:26:16 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 16037--===========

08/31/2024 03:26:16 - INFO - __main__ -   {'video_r1': 34.5, 'video_recall': '34.5/67.1/77.0', 'video_ravg': 59.5}
08/31/2024 03:26:16 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/31/2024 03:26:16 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/31/2024 03:26:16 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 16037--===========

08/31/2024 03:26:16 - INFO - __main__ -   {'video_r1': 56.7, 'video_recall': '56.7/80.0/86.4', 'video_ravg': 74.4}
08/31/2024 03:26:16 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/31/2024 03:26:16 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
 90%|████████▉ | 16038/17834 [8:12:16<25:09:26, 50.43s/it] 90%|████████▉ | 16039/17834 [8:12:18<17:51:06, 35.80s/it] 90%|████████▉ | 16040/17834 [8:12:20<12:44:49, 25.58s/it] 90%|████████▉ | 16041/17834 [8:12:21<9:10:50, 18.43s/it]  90%|████████▉ | 16042/17834 [8:12:23<6:40:48, 13.42s/it] 90%|████████▉ | 16043/17834 [8:12:25<4:55:37,  9.90s/it] 90%|████████▉ | 16044/17834 [8:12:27<3:42:05,  7.44s/it] 90%|████████▉ | 16045/17834 [8:12:28<2:51:00,  5.74s/it] 90%|████████▉ | 16046/17834 [8:12:30<2:15:01,  4.53s/it] 90%|████████▉ | 16047/17834 [8:12:32<1:49:46,  3.69s/it] 90%|████████▉ | 16048/17834 [8:12:33<1:31:58,  3.09s/it] 90%|████████▉ | 16049/17834 [8:12:35<1:19:41,  2.68s/it]08/31/2024 03:26:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0192592144012451, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02637643553316593, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.211686134338379, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.257321834564209}
 90%|████████▉ | 16050/17834 [8:12:37<1:11:12,  2.39s/it] 90%|█████████ | 16051/17834 [8:12:39<1:06:12,  2.23s/it] 90%|█████████ | 16052/17834 [8:12:40<1:01:32,  2.07s/it] 90%|█████████ | 16053/17834 [8:12:42<58:29,  1.97s/it]   90%|█████████ | 16054/17834 [8:12:44<56:52,  1.92s/it] 90%|█████████ | 16055/17834 [8:12:46<55:06,  1.86s/it] 90%|█████████ | 16056/17834 [8:12:47<53:48,  1.82s/it] 90%|█████████ | 16057/17834 [8:12:49<53:08,  1.79s/it] 90%|█████████ | 16058/17834 [8:12:51<53:16,  1.80s/it] 90%|█████████ | 16059/17834 [8:12:53<52:27,  1.77s/it] 90%|█████████ | 16060/17834 [8:12:54<51:55,  1.76s/it] 90%|█████████ | 16061/17834 [8:12:56<51:25,  1.74s/it] 90%|█████████ | 16062/17834 [8:12:58<52:06,  1.76s/it] 90%|█████████ | 16063/17834 [8:13:00<51:23,  1.74s/it] 90%|█████████ | 16064/17834 [8:13:01<51:13,  1.74s/it] 90%|█████████ | 16065/17834 [8:13:03<51:06,  1.73s/it] 90%|█████████ | 16066/17834 [8:13:05<51:35,  1.75s/it] 90%|█████████ | 16067/17834 [8:13:07<51:23,  1.75s/it] 90%|█████████ | 16068/17834 [8:13:08<51:08,  1.74s/it] 90%|█████████ | 16069/17834 [8:13:10<51:13,  1.74s/it] 90%|█████████ | 16070/17834 [8:13:12<50:20,  1.71s/it] 90%|█████████ | 16071/17834 [8:13:13<50:19,  1.71s/it] 90%|█████████ | 16072/17834 [8:13:15<51:34,  1.76s/it] 90%|█████████ | 16073/17834 [8:13:17<50:45,  1.73s/it] 90%|█████████ | 16074/17834 [8:13:19<50:26,  1.72s/it] 90%|█████████ | 16075/17834 [8:13:20<50:46,  1.73s/it] 90%|█████████ | 16076/17834 [8:13:22<50:18,  1.72s/it] 90%|█████████ | 16077/17834 [8:13:24<50:39,  1.73s/it] 90%|█████████ | 16078/17834 [8:13:26<51:22,  1.76s/it] 90%|█████████ | 16079/17834 [8:13:27<50:51,  1.74s/it] 90%|█████████ | 16080/17834 [8:13:29<50:41,  1.73s/it] 90%|█████████ | 16081/17834 [8:13:31<50:45,  1.74s/it] 90%|█████████ | 16082/17834 [8:13:33<51:51,  1.78s/it] 90%|█████████ | 16083/17834 [8:13:35<52:09,  1.79s/it] 90%|█████████ | 16084/17834 [8:13:36<51:31,  1.77s/it] 90%|█████████ | 16085/17834 [8:13:38<51:25,  1.76s/it] 90%|█████████ | 16086/17834 [8:13:40<51:39,  1.77s/it] 90%|█████████ | 16087/17834 [8:13:42<51:27,  1.77s/it] 90%|█████████ | 16088/17834 [8:13:43<50:40,  1.74s/it] 90%|█████████ | 16089/17834 [8:13:45<50:36,  1.74s/it] 90%|█████████ | 16090/17834 [8:13:47<50:36,  1.74s/it] 90%|█████████ | 16091/17834 [8:13:48<50:38,  1.74s/it] 90%|█████████ | 16092/17834 [8:13:50<50:42,  1.75s/it] 90%|█████████ | 16093/17834 [8:13:52<50:04,  1.73s/it] 90%|█████████ | 16094/17834 [8:13:54<50:01,  1.72s/it] 90%|█████████ | 16095/17834 [8:13:55<50:40,  1.75s/it] 90%|█████████ | 16096/17834 [8:13:57<50:46,  1.75s/it] 90%|█████████ | 16097/17834 [8:13:59<50:07,  1.73s/it] 90%|█████████ | 16098/17834 [8:14:01<50:39,  1.75s/it] 90%|█████████ | 16099/17834 [8:14:02<50:38,  1.75s/it]08/31/2024 03:28:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8382729887962341, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015994155779480934, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1435437202453613, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.9978108406066895}
 90%|█████████ | 16100/17834 [8:14:04<51:23,  1.78s/it] 90%|█████████ | 16101/17834 [8:14:06<50:59,  1.77s/it] 90%|█████████ | 16102/17834 [8:14:08<50:48,  1.76s/it] 90%|█████████ | 16103/17834 [8:14:09<50:00,  1.73s/it] 90%|█████████ | 16104/17834 [8:14:11<49:53,  1.73s/it] 90%|█████████ | 16105/17834 [8:14:13<49:51,  1.73s/it] 90%|█████████ | 16106/17834 [8:14:15<50:14,  1.74s/it] 90%|█████████ | 16107/17834 [8:14:16<50:24,  1.75s/it] 90%|█████████ | 16108/17834 [8:14:18<51:11,  1.78s/it] 90%|█████████ | 16109/17834 [8:14:20<50:39,  1.76s/it] 90%|█████████ | 16110/17834 [8:14:22<50:24,  1.75s/it] 90%|█████████ | 16111/17834 [8:14:23<50:01,  1.74s/it] 90%|█████████ | 16112/17834 [8:14:25<50:44,  1.77s/it] 90%|█████████ | 16113/17834 [8:14:27<50:31,  1.76s/it] 90%|█████████ | 16114/17834 [8:14:29<50:28,  1.76s/it] 90%|█████████ | 16115/17834 [8:14:30<50:12,  1.75s/it] 90%|█████████ | 16116/17834 [8:14:32<52:04,  1.82s/it] 90%|█████████ | 16117/17834 [8:14:34<50:58,  1.78s/it] 90%|█████████ | 16118/17834 [8:14:36<50:55,  1.78s/it] 90%|█████████ | 16119/17834 [8:14:38<51:23,  1.80s/it] 90%|█████████ | 16120/17834 [8:14:40<50:55,  1.78s/it] 90%|█████████ | 16121/17834 [8:14:41<50:49,  1.78s/it] 90%|█████████ | 16122/17834 [8:14:43<49:57,  1.75s/it] 90%|█████████ | 16123/17834 [8:14:45<49:19,  1.73s/it] 90%|█████████ | 16124/17834 [8:14:46<49:53,  1.75s/it] 90%|█████████ | 16125/17834 [8:14:48<49:21,  1.73s/it] 90%|█████████ | 16126/17834 [8:14:50<49:47,  1.75s/it] 90%|█████████ | 16127/17834 [8:14:52<49:22,  1.74s/it] 90%|█████████ | 16128/17834 [8:14:53<49:13,  1.73s/it] 90%|█████████ | 16129/17834 [8:14:55<49:03,  1.73s/it] 90%|█████████ | 16130/17834 [8:14:57<49:09,  1.73s/it] 90%|█████████ | 16131/17834 [8:14:59<49:28,  1.74s/it] 90%|█████████ | 16132/17834 [8:15:00<49:12,  1.73s/it] 90%|█████████ | 16133/17834 [8:15:02<49:40,  1.75s/it] 90%|█████████ | 16134/17834 [8:15:04<49:25,  1.74s/it] 90%|█████████ | 16135/17834 [8:15:06<49:06,  1.73s/it] 90%|█████████ | 16136/17834 [8:15:07<48:57,  1.73s/it] 90%|█████████ | 16137/17834 [8:15:09<49:25,  1.75s/it] 90%|█████████ | 16138/17834 [8:15:11<49:14,  1.74s/it] 90%|█████████ | 16139/17834 [8:15:12<48:54,  1.73s/it] 91%|█████████ | 16140/17834 [8:15:14<49:34,  1.76s/it] 91%|█████████ | 16141/17834 [8:15:16<49:23,  1.75s/it] 91%|█████████ | 16142/17834 [8:15:18<49:32,  1.76s/it] 91%|█████████ | 16143/17834 [8:15:20<49:44,  1.77s/it] 91%|█████████ | 16144/17834 [8:15:21<49:32,  1.76s/it] 91%|█████████ | 16145/17834 [8:15:23<49:23,  1.75s/it] 91%|█████████ | 16146/17834 [8:15:25<48:41,  1.73s/it] 91%|█████████ | 16147/17834 [8:15:27<49:16,  1.75s/it] 91%|█████████ | 16148/17834 [8:15:28<49:28,  1.76s/it] 91%|█████████ | 16149/17834 [8:15:30<48:54,  1.74s/it]08/31/2024 03:29:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9874749183654785, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032244350761175156, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.146179676055908, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1658987998962402}
 91%|█████████ | 16150/17834 [8:15:32<48:59,  1.75s/it] 91%|█████████ | 16151/17834 [8:15:34<49:07,  1.75s/it] 91%|█████████ | 16152/17834 [8:15:35<49:31,  1.77s/it] 91%|█████████ | 16153/17834 [8:15:37<49:33,  1.77s/it] 91%|█████████ | 16154/17834 [8:15:39<49:45,  1.78s/it] 91%|█████████ | 16155/17834 [8:15:41<49:45,  1.78s/it] 91%|█████████ | 16156/17834 [8:15:42<49:35,  1.77s/it] 91%|█████████ | 16157/17834 [8:15:44<49:56,  1.79s/it] 91%|█████████ | 16158/17834 [8:15:46<49:40,  1.78s/it] 91%|█████████ | 16159/17834 [8:15:48<48:47,  1.75s/it] 91%|█████████ | 16160/17834 [8:15:49<48:44,  1.75s/it] 91%|█████████ | 16161/17834 [8:15:51<48:27,  1.74s/it] 91%|█████████ | 16162/17834 [8:15:53<48:20,  1.73s/it] 91%|█████████ | 16163/17834 [8:15:55<48:22,  1.74s/it] 91%|█████████ | 16164/17834 [8:15:56<48:32,  1.74s/it] 91%|█████████ | 16165/17834 [8:15:58<48:49,  1.76s/it] 91%|█████████ | 16166/17834 [8:16:00<48:50,  1.76s/it] 91%|█████████ | 16167/17834 [8:16:02<49:01,  1.76s/it] 91%|█████████ | 16168/17834 [8:16:03<48:24,  1.74s/it] 91%|█████████ | 16169/17834 [8:16:05<48:15,  1.74s/it] 91%|█████████ | 16170/17834 [8:16:07<47:55,  1.73s/it] 91%|█████████ | 16171/17834 [8:16:09<48:25,  1.75s/it] 91%|█████████ | 16172/17834 [8:16:10<48:07,  1.74s/it] 91%|█████████ | 16173/17834 [8:16:12<48:08,  1.74s/it] 91%|█████████ | 16174/17834 [8:16:14<47:50,  1.73s/it] 91%|█████████ | 16175/17834 [8:16:16<47:32,  1.72s/it] 91%|█████████ | 16176/17834 [8:16:17<47:36,  1.72s/it] 91%|█████████ | 16177/17834 [8:16:19<48:14,  1.75s/it] 91%|█████████ | 16178/17834 [8:16:21<48:35,  1.76s/it] 91%|█████████ | 16179/17834 [8:16:23<48:02,  1.74s/it] 91%|█████████ | 16180/17834 [8:16:24<48:31,  1.76s/it] 91%|█████████ | 16181/17834 [8:16:26<47:55,  1.74s/it] 91%|█████████ | 16182/17834 [8:16:28<48:37,  1.77s/it] 91%|█████████ | 16183/17834 [8:16:30<49:09,  1.79s/it] 91%|█████████ | 16184/17834 [8:16:31<49:19,  1.79s/it] 91%|█████████ | 16185/17834 [8:16:33<49:02,  1.78s/it] 91%|█████████ | 16186/17834 [8:16:35<49:01,  1.78s/it] 91%|█████████ | 16187/17834 [8:16:37<48:39,  1.77s/it] 91%|█████████ | 16188/17834 [8:16:39<48:37,  1.77s/it] 91%|█████████ | 16189/17834 [8:16:40<48:21,  1.76s/it] 91%|█████████ | 16190/17834 [8:16:42<47:50,  1.75s/it] 91%|█████████ | 16191/17834 [8:16:44<47:48,  1.75s/it] 91%|█████████ | 16192/17834 [8:16:46<48:21,  1.77s/it] 91%|█████████ | 16193/17834 [8:16:47<48:20,  1.77s/it] 91%|█████████ | 16194/17834 [8:16:49<48:05,  1.76s/it] 91%|█████████ | 16195/17834 [8:16:51<47:56,  1.76s/it] 91%|█████████ | 16196/17834 [8:16:53<48:32,  1.78s/it] 91%|█████████ | 16197/17834 [8:16:54<47:48,  1.75s/it] 91%|█████████ | 16198/17834 [8:16:56<48:40,  1.79s/it] 91%|█████████ | 16199/17834 [8:16:58<47:55,  1.76s/it]08/31/2024 03:31:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2633095979690552, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04498286545276642, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1956379413604736, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.5039305686950684}
 91%|█████████ | 16200/17834 [8:17:00<47:49,  1.76s/it] 91%|█████████ | 16201/17834 [8:17:01<47:24,  1.74s/it] 91%|█████████ | 16202/17834 [8:17:03<46:53,  1.72s/it] 91%|█████████ | 16203/17834 [8:17:05<47:10,  1.74s/it] 91%|█████████ | 16204/17834 [8:17:07<46:53,  1.73s/it] 91%|█████████ | 16205/17834 [8:17:08<47:09,  1.74s/it] 91%|█████████ | 16206/17834 [8:17:10<47:38,  1.76s/it] 91%|█████████ | 16207/17834 [8:17:12<47:11,  1.74s/it] 91%|█████████ | 16208/17834 [8:17:13<46:40,  1.72s/it] 91%|█████████ | 16209/17834 [8:17:15<46:40,  1.72s/it] 91%|█████████ | 16210/17834 [8:17:17<46:39,  1.72s/it] 91%|█████████ | 16211/17834 [8:17:19<47:06,  1.74s/it] 91%|█████████ | 16212/17834 [8:17:20<46:56,  1.74s/it] 91%|█████████ | 16213/17834 [8:17:22<47:27,  1.76s/it] 91%|█████████ | 16214/17834 [8:17:24<47:28,  1.76s/it] 91%|█████████ | 16215/17834 [8:17:26<47:19,  1.75s/it] 91%|█████████ | 16216/17834 [8:17:28<47:41,  1.77s/it] 91%|█████████ | 16217/17834 [8:17:29<47:59,  1.78s/it] 91%|█████████ | 16218/17834 [8:17:31<48:09,  1.79s/it] 91%|█████████ | 16219/17834 [8:17:33<46:51,  1.74s/it] 91%|█████████ | 16220/17834 [8:17:35<46:53,  1.74s/it] 91%|█████████ | 16221/17834 [8:17:36<46:56,  1.75s/it] 91%|█████████ | 16222/17834 [8:17:38<47:18,  1.76s/it] 91%|█████████ | 16223/17834 [8:17:40<46:53,  1.75s/it] 91%|█████████ | 16224/17834 [8:17:42<48:13,  1.80s/it] 91%|█████████ | 16225/17834 [8:17:43<47:58,  1.79s/it] 91%|█████████ | 16226/17834 [8:17:45<47:38,  1.78s/it] 91%|█████████ | 16227/17834 [8:17:47<47:43,  1.78s/it] 91%|█████████ | 16228/17834 [8:17:49<46:59,  1.76s/it] 91%|█████████ | 16229/17834 [8:17:51<47:22,  1.77s/it] 91%|█████████ | 16230/17834 [8:17:52<47:27,  1.78s/it] 91%|█████████ | 16231/17834 [8:17:54<47:04,  1.76s/it] 91%|█████████ | 16232/17834 [8:17:56<46:53,  1.76s/it] 91%|█████████ | 16233/17834 [8:17:58<47:02,  1.76s/it] 91%|█████████ | 16234/17834 [8:17:59<47:28,  1.78s/it] 91%|█████████ | 16235/17834 [8:18:01<47:00,  1.76s/it] 91%|█████████ | 16236/17834 [8:18:03<46:32,  1.75s/it] 91%|█████████ | 16237/17834 [8:18:05<46:49,  1.76s/it] 91%|█████████ | 16238/17834 [8:18:06<46:06,  1.73s/it] 91%|█████████ | 16239/17834 [8:18:08<46:57,  1.77s/it] 91%|█████████ | 16240/17834 [8:18:10<46:40,  1.76s/it] 91%|█████████ | 16241/17834 [8:18:12<46:15,  1.74s/it] 91%|█████████ | 16242/17834 [8:18:13<46:27,  1.75s/it] 91%|█████████ | 16243/17834 [8:18:15<46:33,  1.76s/it] 91%|█████████ | 16244/17834 [8:18:17<45:58,  1.73s/it] 91%|█████████ | 16245/17834 [8:18:19<46:49,  1.77s/it] 91%|█████████ | 16246/17834 [8:18:20<46:45,  1.77s/it] 91%|█████████ | 16247/17834 [8:18:22<47:00,  1.78s/it] 91%|█████████ | 16248/17834 [8:18:24<47:17,  1.79s/it] 91%|█████████ | 16249/17834 [8:18:26<47:09,  1.79s/it]08/31/2024 03:32:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8047125935554504, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024190422147512436, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1187455654144287, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.947648525238037}
 91%|█████████ | 16250/17834 [8:18:28<46:49,  1.77s/it] 91%|█████████ | 16251/17834 [8:18:29<46:37,  1.77s/it] 91%|█████████ | 16252/17834 [8:18:31<45:55,  1.74s/it] 91%|█████████ | 16253/17834 [8:18:33<47:33,  1.80s/it] 91%|█████████ | 16254/17834 [8:18:35<46:51,  1.78s/it] 91%|█████████ | 16255/17834 [8:18:36<46:00,  1.75s/it] 91%|█████████ | 16256/17834 [8:18:38<46:02,  1.75s/it] 91%|█████████ | 16257/17834 [8:18:40<46:12,  1.76s/it] 91%|█████████ | 16258/17834 [8:18:42<46:22,  1.77s/it] 91%|█████████ | 16259/17834 [8:18:43<46:16,  1.76s/it] 91%|█████████ | 16260/17834 [8:18:45<45:54,  1.75s/it] 91%|█████████ | 16261/17834 [8:18:47<46:31,  1.77s/it] 91%|█████████ | 16262/17834 [8:18:49<46:05,  1.76s/it] 91%|█████████ | 16263/17834 [8:18:50<46:09,  1.76s/it] 91%|█████████ | 16264/17834 [8:18:52<45:57,  1.76s/it] 91%|█████████ | 16265/17834 [8:18:54<45:50,  1.75s/it] 91%|█████████ | 16266/17834 [8:18:56<46:32,  1.78s/it] 91%|█████████ | 16267/17834 [8:18:57<45:51,  1.76s/it] 91%|█████████ | 16268/17834 [8:18:59<45:30,  1.74s/it] 91%|█████████ | 16269/17834 [8:19:01<45:34,  1.75s/it] 91%|█████████ | 16270/17834 [8:19:03<45:39,  1.75s/it] 91%|█████████ | 16271/17834 [8:19:04<45:39,  1.75s/it] 91%|█████████ | 16272/17834 [8:19:06<45:32,  1.75s/it] 91%|█████████ | 16273/17834 [8:19:08<45:07,  1.73s/it] 91%|█████████▏| 16274/17834 [8:19:10<44:44,  1.72s/it] 91%|█████████▏| 16275/17834 [8:19:11<44:58,  1.73s/it] 91%|█████████▏| 16276/17834 [8:19:13<44:49,  1.73s/it] 91%|█████████▏| 16277/17834 [8:19:15<44:46,  1.73s/it] 91%|█████████▏| 16278/17834 [8:19:17<45:40,  1.76s/it] 91%|█████████▏| 16279/17834 [8:19:18<45:55,  1.77s/it] 91%|█████████▏| 16280/17834 [8:19:20<45:57,  1.77s/it] 91%|█████████▏| 16281/17834 [8:19:22<45:37,  1.76s/it] 91%|█████████▏| 16282/17834 [8:19:24<45:53,  1.77s/it] 91%|█████████▏| 16283/17834 [8:19:26<45:56,  1.78s/it] 91%|█████████▏| 16284/17834 [8:19:27<45:55,  1.78s/it] 91%|█████████▏| 16285/17834 [8:19:29<45:39,  1.77s/it] 91%|█████████▏| 16286/17834 [8:19:31<45:02,  1.75s/it] 91%|█████████▏| 16287/17834 [8:19:32<44:55,  1.74s/it] 91%|█████████▏| 16288/17834 [8:19:34<44:24,  1.72s/it] 91%|█████████▏| 16289/17834 [8:19:36<44:42,  1.74s/it] 91%|█████████▏| 16290/17834 [8:19:38<44:26,  1.73s/it] 91%|█████████▏| 16291/17834 [8:19:39<43:58,  1.71s/it] 91%|█████████▏| 16292/17834 [8:19:41<44:13,  1.72s/it] 91%|█████████▏| 16293/17834 [8:19:43<44:29,  1.73s/it] 91%|█████████▏| 16294/17834 [8:19:45<44:33,  1.74s/it] 91%|█████████▏| 16295/17834 [8:19:46<44:06,  1.72s/it] 91%|█████████▏| 16296/17834 [8:19:48<44:38,  1.74s/it] 91%|█████████▏| 16297/17834 [8:19:50<45:02,  1.76s/it] 91%|█████████▏| 16298/17834 [8:19:52<44:47,  1.75s/it] 91%|█████████▏| 16299/17834 [8:19:53<45:11,  1.77s/it]08/31/2024 03:34:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0834288597106934, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03357435017824173, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2022705078125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3192737102508545}
 91%|█████████▏| 16300/17834 [8:19:55<45:07,  1.77s/it] 91%|█████████▏| 16301/17834 [8:19:57<44:51,  1.76s/it] 91%|█████████▏| 16302/17834 [8:19:59<45:19,  1.77s/it] 91%|█████████▏| 16303/17834 [8:20:00<45:10,  1.77s/it] 91%|█████████▏| 16304/17834 [8:20:02<44:53,  1.76s/it] 91%|█████████▏| 16305/17834 [8:20:04<44:49,  1.76s/it] 91%|█████████▏| 16306/17834 [8:20:06<45:11,  1.77s/it] 91%|█████████▏| 16307/17834 [8:20:07<45:01,  1.77s/it] 91%|█████████▏| 16308/17834 [8:20:09<44:26,  1.75s/it] 91%|█████████▏| 16309/17834 [8:20:11<44:34,  1.75s/it] 91%|█████████▏| 16310/17834 [8:20:13<45:06,  1.78s/it] 91%|█████████▏| 16311/17834 [8:20:15<44:54,  1.77s/it] 91%|█████████▏| 16312/17834 [8:20:16<45:01,  1.78s/it] 91%|█████████▏| 16313/17834 [8:20:18<44:41,  1.76s/it] 91%|█████████▏| 16314/17834 [8:20:20<44:17,  1.75s/it] 91%|█████████▏| 16315/17834 [8:20:22<44:50,  1.77s/it] 91%|█████████▏| 16316/17834 [8:20:23<44:46,  1.77s/it] 91%|█████████▏| 16317/17834 [8:20:25<44:34,  1.76s/it] 91%|█████████▏| 16318/17834 [8:20:27<44:36,  1.77s/it] 92%|█████████▏| 16319/17834 [8:20:29<44:02,  1.74s/it] 92%|█████████▏| 16320/17834 [8:20:30<44:43,  1.77s/it] 92%|█████████▏| 16321/17834 [8:20:32<44:07,  1.75s/it] 92%|█████████▏| 16322/17834 [8:20:34<44:30,  1.77s/it] 92%|█████████▏| 16323/17834 [8:20:36<44:08,  1.75s/it] 92%|█████████▏| 16324/17834 [8:20:37<44:06,  1.75s/it] 92%|█████████▏| 16325/17834 [8:20:39<44:13,  1.76s/it] 92%|█████████▏| 16326/17834 [8:20:41<44:22,  1.77s/it] 92%|█████████▏| 16327/17834 [8:20:43<43:47,  1.74s/it] 92%|█████████▏| 16328/17834 [8:20:44<43:42,  1.74s/it] 92%|█████████▏| 16329/17834 [8:20:46<43:54,  1.75s/it] 92%|█████████▏| 16330/17834 [8:20:48<43:15,  1.73s/it] 92%|█████████▏| 16331/17834 [8:20:50<43:47,  1.75s/it] 92%|█████████▏| 16332/17834 [8:20:51<43:06,  1.72s/it] 92%|█████████▏| 16333/17834 [8:20:53<45:06,  1.80s/it] 92%|█████████▏| 16334/17834 [8:20:55<44:32,  1.78s/it] 92%|█████████▏| 16335/17834 [8:20:57<44:12,  1.77s/it] 92%|█████████▏| 16336/17834 [8:20:58<43:47,  1.75s/it] 92%|█████████▏| 16337/17834 [8:21:00<43:39,  1.75s/it] 92%|█████████▏| 16338/17834 [8:21:02<43:02,  1.73s/it] 92%|█████████▏| 16339/17834 [8:21:04<43:53,  1.76s/it] 92%|█████████▏| 16340/17834 [8:21:05<43:38,  1.75s/it] 92%|█████████▏| 16341/17834 [8:21:07<43:44,  1.76s/it] 92%|█████████▏| 16342/17834 [8:21:09<43:29,  1.75s/it] 92%|█████████▏| 16343/17834 [8:21:11<43:29,  1.75s/it] 92%|█████████▏| 16344/17834 [8:21:12<43:13,  1.74s/it] 92%|█████████▏| 16345/17834 [8:21:14<44:00,  1.77s/it] 92%|█████████▏| 16346/17834 [8:21:16<43:17,  1.75s/it] 92%|█████████▏| 16347/17834 [8:21:18<43:46,  1.77s/it] 92%|█████████▏| 16348/17834 [8:21:20<44:01,  1.78s/it] 92%|█████████▏| 16349/17834 [8:21:21<44:13,  1.79s/it]08/31/2024 03:35:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8302491903305054, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018131259828805923, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1289072036743164, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.977287769317627}
 92%|█████████▏| 16350/17834 [8:21:23<44:08,  1.78s/it] 92%|█████████▏| 16351/17834 [8:21:25<43:34,  1.76s/it] 92%|█████████▏| 16352/17834 [8:21:27<43:14,  1.75s/it] 92%|█████████▏| 16353/17834 [8:21:28<43:05,  1.75s/it] 92%|█████████▏| 16354/17834 [8:21:30<43:16,  1.75s/it] 92%|█████████▏| 16355/17834 [8:21:32<43:05,  1.75s/it] 92%|█████████▏| 16356/17834 [8:21:34<42:58,  1.74s/it] 92%|█████████▏| 16357/17834 [8:21:35<42:50,  1.74s/it] 92%|█████████▏| 16358/17834 [8:21:37<43:14,  1.76s/it] 92%|█████████▏| 16359/17834 [8:21:39<43:01,  1.75s/it] 92%|█████████▏| 16360/17834 [8:21:41<43:08,  1.76s/it] 92%|█████████▏| 16361/17834 [8:21:42<42:44,  1.74s/it] 92%|█████████▏| 16362/17834 [8:21:44<42:50,  1.75s/it] 92%|█████████▏| 16363/17834 [8:21:46<42:19,  1.73s/it] 92%|█████████▏| 16364/17834 [8:21:48<42:35,  1.74s/it] 92%|█████████▏| 16365/17834 [8:21:49<42:37,  1.74s/it] 92%|█████████▏| 16366/17834 [8:21:51<42:35,  1.74s/it] 92%|█████████▏| 16367/17834 [8:21:53<42:25,  1.74s/it] 92%|█████████▏| 16368/17834 [8:21:54<42:41,  1.75s/it] 92%|█████████▏| 16369/17834 [8:21:56<42:07,  1.72s/it] 92%|█████████▏| 16370/17834 [8:21:58<42:24,  1.74s/it] 92%|█████████▏| 16371/17834 [8:22:00<41:57,  1.72s/it] 92%|█████████▏| 16372/17834 [8:22:01<42:05,  1.73s/it] 92%|█████████▏| 16373/17834 [8:22:03<42:25,  1.74s/it] 92%|█████████▏| 16374/17834 [8:22:05<42:32,  1.75s/it] 92%|█████████▏| 16375/17834 [8:22:07<42:05,  1.73s/it] 92%|█████████▏| 16376/17834 [8:22:08<42:27,  1.75s/it] 92%|█████████▏| 16377/17834 [8:22:10<42:28,  1.75s/it] 92%|█████████▏| 16378/17834 [8:22:12<42:13,  1.74s/it] 92%|█████████▏| 16379/17834 [8:22:14<42:10,  1.74s/it] 92%|█████████▏| 16380/17834 [8:22:15<42:30,  1.75s/it] 92%|█████████▏| 16381/17834 [8:22:17<42:36,  1.76s/it] 92%|█████████▏| 16382/17834 [8:22:19<41:53,  1.73s/it] 92%|█████████▏| 16383/17834 [8:22:21<42:02,  1.74s/it] 92%|█████████▏| 16384/17834 [8:22:22<41:49,  1.73s/it] 92%|█████████▏| 16385/17834 [8:22:24<41:50,  1.73s/it] 92%|█████████▏| 16386/17834 [8:22:26<41:50,  1.73s/it] 92%|█████████▏| 16387/17834 [8:22:28<42:21,  1.76s/it] 92%|█████████▏| 16388/17834 [8:22:29<42:14,  1.75s/it] 92%|█████████▏| 16389/17834 [8:22:31<42:06,  1.75s/it] 92%|█████████▏| 16390/17834 [8:22:33<42:27,  1.76s/it] 92%|█████████▏| 16391/17834 [8:22:35<42:47,  1.78s/it] 92%|█████████▏| 16392/17834 [8:22:36<42:19,  1.76s/it] 92%|█████████▏| 16393/17834 [8:22:38<42:55,  1.79s/it] 92%|█████████▏| 16394/17834 [8:22:40<42:26,  1.77s/it] 92%|█████████▏| 16395/17834 [8:22:42<42:58,  1.79s/it] 92%|█████████▏| 16396/17834 [8:22:44<42:35,  1.78s/it] 92%|█████████▏| 16397/17834 [8:22:45<42:17,  1.77s/it] 92%|█████████▏| 16398/17834 [8:22:47<41:58,  1.75s/it] 92%|█████████▏| 16399/17834 [8:22:49<41:35,  1.74s/it]08/31/2024 03:37:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8706561326980591, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018825598061084747, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1495609283447266, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0390427112579346}
 92%|█████████▏| 16400/17834 [8:22:50<41:56,  1.76s/it] 92%|█████████▏| 16401/17834 [8:22:52<41:40,  1.75s/it] 92%|█████████▏| 16402/17834 [8:22:54<42:07,  1.76s/it] 92%|█████████▏| 16403/17834 [8:22:56<41:18,  1.73s/it] 92%|█████████▏| 16404/17834 [8:22:57<41:10,  1.73s/it] 92%|█████████▏| 16405/17834 [8:22:59<41:05,  1.73s/it] 92%|█████████▏| 16406/17834 [8:23:01<41:19,  1.74s/it] 92%|█████████▏| 16407/17834 [8:23:03<41:01,  1.72s/it] 92%|█████████▏| 16408/17834 [8:23:04<40:30,  1.70s/it] 92%|█████████▏| 16409/17834 [8:23:06<41:10,  1.73s/it] 92%|█████████▏| 16410/17834 [8:23:08<41:23,  1.74s/it] 92%|█████████▏| 16411/17834 [8:23:10<41:25,  1.75s/it] 92%|█████████▏| 16412/17834 [8:23:11<41:09,  1.74s/it] 92%|█████████▏| 16413/17834 [8:23:13<41:40,  1.76s/it] 92%|█████████▏| 16414/17834 [8:23:15<41:00,  1.73s/it] 92%|█████████▏| 16415/17834 [8:23:17<41:06,  1.74s/it] 92%|█████████▏| 16416/17834 [8:23:18<40:43,  1.72s/it] 92%|█████████▏| 16417/17834 [8:23:20<41:41,  1.77s/it] 92%|█████████▏| 16418/17834 [8:23:22<42:11,  1.79s/it] 92%|█████████▏| 16419/17834 [8:23:24<41:51,  1.77s/it] 92%|█████████▏| 16420/17834 [8:23:25<41:13,  1.75s/it] 92%|█████████▏| 16421/17834 [8:23:27<41:06,  1.75s/it] 92%|█████████▏| 16422/17834 [8:23:29<41:08,  1.75s/it] 92%|█████████▏| 16423/17834 [8:23:31<41:05,  1.75s/it] 92%|█████████▏| 16424/17834 [8:23:32<40:59,  1.74s/it] 92%|█████████▏| 16425/17834 [8:23:34<40:57,  1.74s/it] 92%|█████████▏| 16426/17834 [8:23:36<41:45,  1.78s/it] 92%|█████████▏| 16427/17834 [8:23:38<41:30,  1.77s/it] 92%|█████████▏| 16428/17834 [8:23:39<41:53,  1.79s/it] 92%|█████████▏| 16429/17834 [8:23:41<40:56,  1.75s/it] 92%|█████████▏| 16430/17834 [8:23:43<41:17,  1.76s/it] 92%|█████████▏| 16431/17834 [8:23:45<40:54,  1.75s/it] 92%|█████████▏| 16432/17834 [8:23:46<40:57,  1.75s/it] 92%|█████████▏| 16433/17834 [8:23:48<41:28,  1.78s/it] 92%|█████████▏| 16434/17834 [8:23:50<41:50,  1.79s/it] 92%|█████████▏| 16435/17834 [8:23:52<41:09,  1.77s/it] 92%|█████████▏| 16436/17834 [8:23:54<41:12,  1.77s/it] 92%|█████████▏| 16437/17834 [8:23:55<40:33,  1.74s/it] 92%|█████████▏| 16438/17834 [8:23:57<40:30,  1.74s/it] 92%|█████████▏| 16439/17834 [8:23:59<40:18,  1.73s/it] 92%|█████████▏| 16440/17834 [8:24:01<40:46,  1.76s/it] 92%|█████████▏| 16441/17834 [8:24:02<40:51,  1.76s/it] 92%|█████████▏| 16442/17834 [8:24:04<40:30,  1.75s/it] 92%|█████████▏| 16443/17834 [8:24:06<40:35,  1.75s/it] 92%|█████████▏| 16444/17834 [8:24:07<40:27,  1.75s/it] 92%|█████████▏| 16445/17834 [8:24:09<40:36,  1.75s/it] 92%|█████████▏| 16446/17834 [8:24:11<40:06,  1.73s/it] 92%|█████████▏| 16447/17834 [8:24:13<40:23,  1.75s/it] 92%|█████████▏| 16448/17834 [8:24:14<40:31,  1.75s/it] 92%|█████████▏| 16449/17834 [8:24:16<40:33,  1.76s/it]08/31/2024 03:38:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9940988421440125, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.022884415462613106, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.187598466873169, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2045817375183105}
 92%|█████████▏| 16450/17834 [8:24:18<40:44,  1.77s/it] 92%|█████████▏| 16451/17834 [8:24:20<40:25,  1.75s/it] 92%|█████████▏| 16452/17834 [8:24:22<40:42,  1.77s/it] 92%|█████████▏| 16453/17834 [8:24:23<40:20,  1.75s/it] 92%|█████████▏| 16454/17834 [8:24:25<40:09,  1.75s/it] 92%|█████████▏| 16455/17834 [8:24:27<40:19,  1.75s/it] 92%|█████████▏| 16456/17834 [8:24:28<39:47,  1.73s/it] 92%|█████████▏| 16457/17834 [8:24:30<39:35,  1.73s/it] 92%|█████████▏| 16458/17834 [8:24:32<39:50,  1.74s/it] 92%|█████████▏| 16459/17834 [8:24:34<40:28,  1.77s/it] 92%|█████████▏| 16460/17834 [8:24:36<40:29,  1.77s/it] 92%|█████████▏| 16461/17834 [8:24:37<40:20,  1.76s/it] 92%|█████████▏| 16462/17834 [8:24:39<40:47,  1.78s/it] 92%|█████████▏| 16463/17834 [8:24:41<41:05,  1.80s/it] 92%|█████████▏| 16464/17834 [8:24:43<40:50,  1.79s/it] 92%|█████████▏| 16465/17834 [8:24:44<40:13,  1.76s/it] 92%|█████████▏| 16466/17834 [8:24:46<41:11,  1.81s/it] 92%|█████████▏| 16467/17834 [8:24:48<39:57,  1.75s/it] 92%|█████████▏| 16468/17834 [8:24:50<40:11,  1.77s/it] 92%|█████████▏| 16469/17834 [8:24:52<39:58,  1.76s/it] 92%|█████████▏| 16470/17834 [8:24:53<39:46,  1.75s/it] 92%|█████████▏| 16471/17834 [8:24:55<39:22,  1.73s/it] 92%|█████████▏| 16472/17834 [8:24:57<39:23,  1.74s/it] 92%|█████████▏| 16473/17834 [8:24:58<39:17,  1.73s/it] 92%|█████████▏| 16474/17834 [8:25:00<39:32,  1.74s/it] 92%|█████████▏| 16475/17834 [8:25:02<39:10,  1.73s/it] 92%|█████████▏| 16476/17834 [8:25:04<39:25,  1.74s/it] 92%|█████████▏| 16477/17834 [8:25:05<39:28,  1.75s/it] 92%|█████████▏| 16478/17834 [8:25:07<39:04,  1.73s/it] 92%|█████████▏| 16479/17834 [8:25:09<38:48,  1.72s/it] 92%|█████████▏| 16480/17834 [8:25:11<38:56,  1.73s/it] 92%|█████████▏| 16481/17834 [8:25:12<38:56,  1.73s/it] 92%|█████████▏| 16482/17834 [8:25:14<38:40,  1.72s/it] 92%|█████████▏| 16483/17834 [8:25:16<38:31,  1.71s/it] 92%|█████████▏| 16484/17834 [8:25:17<39:03,  1.74s/it] 92%|█████████▏| 16485/17834 [8:25:19<39:21,  1.75s/it] 92%|█████████▏| 16486/17834 [8:25:21<39:08,  1.74s/it] 92%|█████████▏| 16487/17834 [8:25:23<38:52,  1.73s/it] 92%|█████████▏| 16488/17834 [8:25:24<39:21,  1.75s/it] 92%|█████████▏| 16489/17834 [8:25:26<38:51,  1.73s/it] 92%|█████████▏| 16490/17834 [8:25:28<39:04,  1.74s/it] 92%|█████████▏| 16491/17834 [8:25:30<39:38,  1.77s/it] 92%|█████████▏| 16492/17834 [8:25:31<39:12,  1.75s/it] 92%|█████████▏| 16493/17834 [8:25:33<39:02,  1.75s/it] 92%|█████████▏| 16494/17834 [8:25:35<39:11,  1.75s/it] 92%|█████████▏| 16495/17834 [8:25:37<38:50,  1.74s/it] 92%|█████████▏| 16496/17834 [8:25:38<38:38,  1.73s/it] 93%|█████████▎| 16497/17834 [8:25:40<38:38,  1.73s/it] 93%|█████████▎| 16498/17834 [8:25:42<38:55,  1.75s/it] 93%|█████████▎| 16499/17834 [8:25:44<38:56,  1.75s/it]08/31/2024 03:40:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.930202841758728, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.039208825677633286, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1702442169189453, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.139655828475952}
 93%|█████████▎| 16500/17834 [8:25:45<39:14,  1.77s/it] 93%|█████████▎| 16501/17834 [8:25:47<39:27,  1.78s/it] 93%|█████████▎| 16502/17834 [8:25:49<38:49,  1.75s/it] 93%|█████████▎| 16503/17834 [8:25:51<38:22,  1.73s/it] 93%|█████████▎| 16504/17834 [8:25:52<39:04,  1.76s/it] 93%|█████████▎| 16505/17834 [8:25:54<38:47,  1.75s/it] 93%|█████████▎| 16506/17834 [8:25:56<38:27,  1.74s/it] 93%|█████████▎| 16507/17834 [8:25:58<38:19,  1.73s/it] 93%|█████████▎| 16508/17834 [8:25:59<38:21,  1.74s/it] 93%|█████████▎| 16509/17834 [8:26:01<38:07,  1.73s/it] 93%|█████████▎| 16510/17834 [8:26:03<38:10,  1.73s/it] 93%|█████████▎| 16511/17834 [8:26:05<39:34,  1.79s/it] 93%|█████████▎| 16512/17834 [8:26:06<38:55,  1.77s/it] 93%|█████████▎| 16513/17834 [8:26:08<39:05,  1.78s/it] 93%|█████████▎| 16514/17834 [8:26:10<38:50,  1.77s/it] 93%|█████████▎| 16515/17834 [8:26:12<38:34,  1.75s/it] 93%|█████████▎| 16516/17834 [8:26:14<38:51,  1.77s/it] 93%|█████████▎| 16517/17834 [8:26:15<38:49,  1.77s/it] 93%|█████████▎| 16518/17834 [8:26:17<38:31,  1.76s/it] 93%|█████████▎| 16519/17834 [8:26:19<39:42,  1.81s/it] 93%|█████████▎| 16520/17834 [8:26:21<38:55,  1.78s/it] 93%|█████████▎| 16521/17834 [8:26:22<38:31,  1.76s/it] 93%|█████████▎| 16522/17834 [8:26:24<38:39,  1.77s/it] 93%|█████████▎| 16523/17834 [8:26:26<38:02,  1.74s/it] 93%|█████████▎| 16524/17834 [8:26:28<38:03,  1.74s/it] 93%|█████████▎| 16525/17834 [8:26:29<38:16,  1.75s/it] 93%|█████████▎| 16526/17834 [8:26:31<38:02,  1.74s/it] 93%|█████████▎| 16527/17834 [8:26:33<38:13,  1.75s/it] 93%|█████████▎| 16528/17834 [8:26:35<38:02,  1.75s/it] 93%|█████████▎| 16529/17834 [8:26:36<37:40,  1.73s/it] 93%|█████████▎| 16530/17834 [8:26:38<38:05,  1.75s/it] 93%|█████████▎| 16531/17834 [8:26:40<37:33,  1.73s/it] 93%|█████████▎| 16532/17834 [8:26:41<37:22,  1.72s/it] 93%|█████████▎| 16533/17834 [8:26:43<37:35,  1.73s/it] 93%|█████████▎| 16534/17834 [8:26:45<37:32,  1.73s/it] 93%|█████████▎| 16535/17834 [8:26:47<37:25,  1.73s/it] 93%|█████████▎| 16536/17834 [8:26:48<37:41,  1.74s/it] 93%|█████████▎| 16537/17834 [8:26:50<37:53,  1.75s/it] 93%|█████████▎| 16538/17834 [8:26:52<37:41,  1.74s/it] 93%|█████████▎| 16539/17834 [8:26:54<37:48,  1.75s/it] 93%|█████████▎| 16540/17834 [8:26:55<37:38,  1.75s/it] 93%|█████████▎| 16541/17834 [8:26:57<38:02,  1.77s/it] 93%|█████████▎| 16542/17834 [8:26:59<37:49,  1.76s/it] 93%|█████████▎| 16543/17834 [8:27:01<37:34,  1.75s/it] 93%|█████████▎| 16544/17834 [8:27:03<37:53,  1.76s/it] 93%|█████████▎| 16545/17834 [8:27:04<37:26,  1.74s/it] 93%|█████████▎| 16546/17834 [8:27:06<37:46,  1.76s/it] 93%|█████████▎| 16547/17834 [8:27:08<37:30,  1.75s/it] 93%|█████████▎| 16548/17834 [8:27:10<38:08,  1.78s/it] 93%|█████████▎| 16549/17834 [8:27:11<37:48,  1.77s/it]08/31/2024 03:41:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9071676135063171, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025759369134902954, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1215009689331055, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0544281005859375}
 93%|█████████▎| 16550/17834 [8:27:13<38:01,  1.78s/it] 93%|█████████▎| 16551/17834 [8:27:15<37:47,  1.77s/it] 93%|█████████▎| 16552/17834 [8:27:17<37:06,  1.74s/it] 93%|█████████▎| 16553/17834 [8:27:18<37:21,  1.75s/it] 93%|█████████▎| 16554/17834 [8:27:20<37:55,  1.78s/it] 93%|█████████▎| 16555/17834 [8:27:22<38:10,  1.79s/it] 93%|█████████▎| 16556/17834 [8:27:24<38:00,  1.78s/it] 93%|█████████▎| 16557/17834 [8:27:26<38:19,  1.80s/it] 93%|█████████▎| 16558/17834 [8:27:27<37:42,  1.77s/it] 93%|█████████▎| 16559/17834 [8:27:29<37:23,  1.76s/it] 93%|█████████▎| 16560/17834 [8:27:31<36:57,  1.74s/it] 93%|█████████▎| 16561/17834 [8:27:33<38:29,  1.81s/it] 93%|█████████▎| 16562/17834 [8:27:34<37:27,  1.77s/it] 93%|█████████▎| 16563/17834 [8:27:36<37:09,  1.75s/it] 93%|█████████▎| 16564/17834 [8:27:38<36:55,  1.74s/it] 93%|█████████▎| 16565/17834 [8:27:40<37:47,  1.79s/it] 93%|█████████▎| 16566/17834 [8:27:42<37:49,  1.79s/it] 93%|█████████▎| 16567/17834 [8:27:43<37:46,  1.79s/it] 93%|█████████▎| 16568/17834 [8:27:45<37:17,  1.77s/it] 93%|█████████▎| 16569/17834 [8:27:47<37:01,  1.76s/it] 93%|█████████▎| 16570/17834 [8:27:49<37:01,  1.76s/it] 93%|█████████▎| 16571/17834 [8:27:50<36:52,  1.75s/it] 93%|█████████▎| 16572/17834 [8:27:52<36:34,  1.74s/it] 93%|█████████▎| 16573/17834 [8:27:54<36:53,  1.76s/it] 93%|█████████▎| 16574/17834 [8:27:55<36:48,  1.75s/it] 93%|█████████▎| 16575/17834 [8:27:57<37:15,  1.78s/it] 93%|█████████▎| 16576/17834 [8:27:59<37:14,  1.78s/it] 93%|█████████▎| 16577/17834 [8:28:01<37:06,  1.77s/it] 93%|█████████▎| 16578/17834 [8:28:03<36:42,  1.75s/it] 93%|█████████▎| 16579/17834 [8:28:04<36:24,  1.74s/it] 93%|█████████▎| 16580/17834 [8:28:06<36:12,  1.73s/it] 93%|█████████▎| 16581/17834 [8:28:08<35:53,  1.72s/it] 93%|█████████▎| 16582/17834 [8:28:09<36:07,  1.73s/it] 93%|█████████▎| 16583/17834 [8:28:11<36:12,  1.74s/it] 93%|█████████▎| 16584/17834 [8:28:13<36:27,  1.75s/it] 93%|█████████▎| 16585/17834 [8:28:15<36:25,  1.75s/it] 93%|█████████▎| 16586/17834 [8:28:16<36:17,  1.74s/it] 93%|█████████▎| 16587/17834 [8:28:18<36:25,  1.75s/it] 93%|█████████▎| 16588/17834 [8:28:20<36:24,  1.75s/it] 93%|█████████▎| 16589/17834 [8:28:22<35:53,  1.73s/it] 93%|█████████▎| 16590/17834 [8:28:23<35:42,  1.72s/it] 93%|█████████▎| 16591/17834 [8:28:25<35:54,  1.73s/it] 93%|█████████▎| 16592/17834 [8:28:27<36:55,  1.78s/it] 93%|█████████▎| 16593/17834 [8:28:29<36:39,  1.77s/it] 93%|█████████▎| 16594/17834 [8:28:30<36:10,  1.75s/it] 93%|█████████▎| 16595/17834 [8:28:32<36:44,  1.78s/it] 93%|█████████▎| 16596/17834 [8:28:34<36:20,  1.76s/it] 93%|█████████▎| 16597/17834 [8:28:36<36:13,  1.76s/it] 93%|█████████▎| 16598/17834 [8:28:38<36:04,  1.75s/it] 93%|█████████▎| 16599/17834 [8:28:39<35:54,  1.74s/it]08/31/2024 03:42:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1906230449676514, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026828546077013016, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.241137981414795, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.458589553833008}
 93%|█████████▎| 16600/17834 [8:28:41<36:35,  1.78s/it] 93%|█████████▎| 16601/17834 [8:28:43<36:11,  1.76s/it] 93%|█████████▎| 16602/17834 [8:28:45<36:15,  1.77s/it] 93%|█████████▎| 16603/17834 [8:28:46<36:10,  1.76s/it] 93%|█████████▎| 16604/17834 [8:28:48<35:52,  1.75s/it] 93%|█████████▎| 16605/17834 [8:28:50<35:35,  1.74s/it] 93%|█████████▎| 16606/17834 [8:28:52<35:41,  1.74s/it] 93%|█████████▎| 16607/17834 [8:28:53<35:31,  1.74s/it] 93%|█████████▎| 16608/17834 [8:28:55<35:42,  1.75s/it] 93%|█████████▎| 16609/17834 [8:28:57<35:41,  1.75s/it] 93%|█████████▎| 16610/17834 [8:28:59<35:52,  1.76s/it] 93%|█████████▎| 16611/17834 [8:29:00<35:46,  1.75s/it] 93%|█████████▎| 16612/17834 [8:29:02<35:35,  1.75s/it] 93%|█████████▎| 16613/17834 [8:29:04<35:43,  1.76s/it] 93%|█████████▎| 16614/17834 [8:29:06<36:12,  1.78s/it] 93%|█████████▎| 16615/17834 [8:29:07<35:47,  1.76s/it] 93%|█████████▎| 16616/17834 [8:29:09<36:10,  1.78s/it] 93%|█████████▎| 16617/17834 [8:29:11<35:38,  1.76s/it] 93%|█████████▎| 16618/17834 [8:29:13<35:56,  1.77s/it] 93%|█████████▎| 16619/17834 [8:29:14<35:33,  1.76s/it] 93%|█████████▎| 16620/17834 [8:29:16<35:48,  1.77s/it] 93%|█████████▎| 16621/17834 [8:29:18<35:20,  1.75s/it] 93%|█████████▎| 16622/17834 [8:29:20<35:05,  1.74s/it] 93%|█████████▎| 16623/17834 [8:29:21<35:00,  1.73s/it] 93%|█████████▎| 16624/17834 [8:29:23<35:05,  1.74s/it] 93%|█████████▎| 16625/17834 [8:29:25<34:49,  1.73s/it] 93%|█████████▎| 16626/17834 [8:29:27<34:34,  1.72s/it] 93%|█████████▎| 16627/17834 [8:29:28<34:23,  1.71s/it] 93%|█████████▎| 16628/17834 [8:29:30<34:29,  1.72s/it] 93%|█████████▎| 16629/17834 [8:29:32<34:39,  1.73s/it] 93%|█████████▎| 16630/17834 [8:29:34<35:16,  1.76s/it] 93%|█████████▎| 16631/17834 [8:29:35<35:24,  1.77s/it] 93%|█████████▎| 16632/17834 [8:29:37<34:46,  1.74s/it] 93%|█████████▎| 16633/17834 [8:29:39<35:23,  1.77s/it] 93%|█████████▎| 16634/17834 [8:29:41<35:17,  1.76s/it] 93%|█████████▎| 16635/17834 [8:29:42<34:48,  1.74s/it] 93%|█████████▎| 16636/17834 [8:29:44<35:26,  1.77s/it] 93%|█████████▎| 16637/17834 [8:29:46<35:20,  1.77s/it] 93%|█████████▎| 16638/17834 [8:29:48<35:12,  1.77s/it] 93%|█████████▎| 16639/17834 [8:29:49<35:29,  1.78s/it] 93%|█████████▎| 16640/17834 [8:29:51<35:35,  1.79s/it] 93%|█████████▎| 16641/17834 [8:29:53<35:01,  1.76s/it] 93%|█████████▎| 16642/17834 [8:29:55<35:07,  1.77s/it] 93%|█████████▎| 16643/17834 [8:29:56<34:45,  1.75s/it] 93%|█████████▎| 16644/17834 [8:29:58<34:36,  1.75s/it] 93%|█████████▎| 16645/17834 [8:30:00<34:46,  1.76s/it] 93%|█████████▎| 16646/17834 [8:30:02<34:32,  1.74s/it] 93%|█████████▎| 16647/17834 [8:30:03<34:56,  1.77s/it] 93%|█████████▎| 16648/17834 [8:30:05<34:41,  1.75s/it] 93%|█████████▎| 16649/17834 [8:30:07<34:32,  1.75s/it]08/31/2024 03:44:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9361235499382019, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02588699385523796, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1440749168395996, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1060855388641357}
 93%|█████████▎| 16650/17834 [8:30:09<34:17,  1.74s/it] 93%|█████████▎| 16651/17834 [8:30:10<34:12,  1.74s/it] 93%|█████████▎| 16652/17834 [8:30:12<34:29,  1.75s/it] 93%|█████████▎| 16653/17834 [8:30:14<34:26,  1.75s/it] 93%|█████████▎| 16654/17834 [8:30:16<34:26,  1.75s/it] 93%|█████████▎| 16655/17834 [8:30:18<34:49,  1.77s/it] 93%|█████████▎| 16656/17834 [8:30:19<35:06,  1.79s/it] 93%|█████████▎| 16657/17834 [8:30:21<34:59,  1.78s/it] 93%|█████████▎| 16658/17834 [8:30:23<34:35,  1.77s/it] 93%|█████████▎| 16659/17834 [8:30:25<34:37,  1.77s/it] 93%|█████████▎| 16660/17834 [8:30:26<34:30,  1.76s/it] 93%|█████████▎| 16661/17834 [8:30:28<34:14,  1.75s/it] 93%|█████████▎| 16662/17834 [8:30:30<33:44,  1.73s/it] 93%|█████████▎| 16663/17834 [8:30:31<33:25,  1.71s/it] 93%|█████████▎| 16664/17834 [8:30:33<33:23,  1.71s/it] 93%|█████████▎| 16665/17834 [8:30:35<33:12,  1.70s/it] 93%|█████████▎| 16666/17834 [8:30:37<33:31,  1.72s/it] 93%|█████████▎| 16667/17834 [8:30:38<33:39,  1.73s/it] 93%|█████████▎| 16668/17834 [8:30:40<33:21,  1.72s/it] 93%|█████████▎| 16669/17834 [8:30:42<33:16,  1.71s/it] 93%|█████████▎| 16670/17834 [8:30:43<33:19,  1.72s/it] 93%|█████████▎| 16671/17834 [8:30:45<33:51,  1.75s/it] 93%|█████████▎| 16672/17834 [8:30:47<33:40,  1.74s/it] 93%|█████████▎| 16673/17834 [8:30:49<33:23,  1.73s/it] 93%|█████████▎| 16674/17834 [8:30:50<33:38,  1.74s/it] 94%|█████████▎| 16675/17834 [8:30:52<33:57,  1.76s/it] 94%|█████████▎| 16676/17834 [8:30:54<33:54,  1.76s/it] 94%|█████████▎| 16677/17834 [8:30:56<33:58,  1.76s/it] 94%|█████████▎| 16678/17834 [8:30:57<33:32,  1.74s/it] 94%|█████████▎| 16679/17834 [8:30:59<33:30,  1.74s/it] 94%|█████████▎| 16680/17834 [8:31:01<33:38,  1.75s/it] 94%|█████████▎| 16681/17834 [8:31:03<33:30,  1.74s/it] 94%|█████████▎| 16682/17834 [8:31:04<33:34,  1.75s/it] 94%|█████████▎| 16683/17834 [8:31:06<33:41,  1.76s/it] 94%|█████████▎| 16684/17834 [8:31:08<33:34,  1.75s/it] 94%|█████████▎| 16685/17834 [8:31:10<33:29,  1.75s/it] 94%|█████████▎| 16686/17834 [8:31:12<33:41,  1.76s/it] 94%|█████████▎| 16687/17834 [8:31:13<33:29,  1.75s/it] 94%|█████████▎| 16688/17834 [8:31:15<33:29,  1.75s/it] 94%|█████████▎| 16689/17834 [8:31:17<33:10,  1.74s/it] 94%|█████████▎| 16690/17834 [8:31:19<33:39,  1.76s/it] 94%|█████████▎| 16691/17834 [8:31:20<33:20,  1.75s/it] 94%|█████████▎| 16692/17834 [8:31:22<33:22,  1.75s/it] 94%|█████████▎| 16693/17834 [8:31:24<33:14,  1.75s/it] 94%|█████████▎| 16694/17834 [8:31:26<33:59,  1.79s/it] 94%|█████████▎| 16695/17834 [8:31:27<33:38,  1.77s/it] 94%|█████████▎| 16696/17834 [8:31:29<34:02,  1.79s/it] 94%|█████████▎| 16697/17834 [8:31:31<33:45,  1.78s/it] 94%|█████████▎| 16698/17834 [8:31:33<33:32,  1.77s/it] 94%|█████████▎| 16699/17834 [8:31:34<33:11,  1.75s/it]08/31/2024 03:45:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2241255044937134, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03967546671628952, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.199885845184326, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.463686943054199}
 94%|█████████▎| 16700/17834 [8:31:36<33:19,  1.76s/it] 94%|█████████▎| 16701/17834 [8:31:38<33:36,  1.78s/it] 94%|█████████▎| 16702/17834 [8:31:40<33:22,  1.77s/it] 94%|█████████▎| 16703/17834 [8:31:42<33:14,  1.76s/it] 94%|█████████▎| 16704/17834 [8:31:43<33:30,  1.78s/it] 94%|█████████▎| 16705/17834 [8:31:45<33:13,  1.77s/it] 94%|█████████▎| 16706/17834 [8:31:47<33:19,  1.77s/it] 94%|█████████▎| 16707/17834 [8:31:49<33:23,  1.78s/it] 94%|█████████▎| 16708/17834 [8:31:50<33:12,  1.77s/it] 94%|█████████▎| 16709/17834 [8:31:52<33:18,  1.78s/it] 94%|█████████▎| 16710/17834 [8:31:54<33:46,  1.80s/it] 94%|█████████▎| 16711/17834 [8:31:56<33:31,  1.79s/it] 94%|█████████▎| 16712/17834 [8:31:58<33:01,  1.77s/it] 94%|█████████▎| 16713/17834 [8:31:59<33:04,  1.77s/it] 94%|█████████▎| 16714/17834 [8:32:01<32:54,  1.76s/it] 94%|█████████▎| 16715/17834 [8:32:03<32:34,  1.75s/it] 94%|█████████▎| 16716/17834 [8:32:04<32:16,  1.73s/it] 94%|█████████▎| 16717/17834 [8:32:06<33:06,  1.78s/it] 94%|█████████▎| 16718/17834 [8:32:08<32:55,  1.77s/it] 94%|█████████▎| 16719/17834 [8:32:10<33:09,  1.78s/it] 94%|█████████▍| 16720/17834 [8:32:12<32:30,  1.75s/it] 94%|█████████▍| 16721/17834 [8:32:13<32:27,  1.75s/it] 94%|█████████▍| 16722/17834 [8:32:15<32:46,  1.77s/it] 94%|█████████▍| 16723/17834 [8:32:17<32:12,  1.74s/it] 94%|█████████▍| 16724/17834 [8:32:19<32:14,  1.74s/it] 94%|█████████▍| 16725/17834 [8:32:20<32:16,  1.75s/it] 94%|█████████▍| 16726/17834 [8:32:22<32:49,  1.78s/it] 94%|█████████▍| 16727/17834 [8:32:24<32:04,  1.74s/it] 94%|█████████▍| 16728/17834 [8:32:26<32:14,  1.75s/it] 94%|█████████▍| 16729/17834 [8:32:27<31:58,  1.74s/it] 94%|█████████▍| 16730/17834 [8:32:29<32:11,  1.75s/it] 94%|█████████▍| 16731/17834 [8:32:31<32:20,  1.76s/it] 94%|█████████▍| 16732/17834 [8:32:33<32:05,  1.75s/it] 94%|█████████▍| 16733/17834 [8:32:34<31:53,  1.74s/it] 94%|█████████▍| 16734/17834 [8:32:36<31:52,  1.74s/it] 94%|█████████▍| 16735/17834 [8:32:38<31:51,  1.74s/it] 94%|█████████▍| 16736/17834 [8:32:40<32:12,  1.76s/it] 94%|█████████▍| 16737/17834 [8:32:41<32:23,  1.77s/it] 94%|█████████▍| 16738/17834 [8:32:43<31:50,  1.74s/it] 94%|█████████▍| 16739/17834 [8:32:45<32:00,  1.75s/it] 94%|█████████▍| 16740/17834 [8:32:47<32:13,  1.77s/it] 94%|█████████▍| 16741/17834 [8:32:48<32:20,  1.78s/it] 94%|█████████▍| 16742/17834 [8:32:50<32:03,  1.76s/it] 94%|█████████▍| 16743/17834 [8:32:52<31:41,  1.74s/it] 94%|█████████▍| 16744/17834 [8:32:54<31:35,  1.74s/it] 94%|█████████▍| 16745/17834 [8:32:55<31:31,  1.74s/it] 94%|█████████▍| 16746/17834 [8:32:57<31:36,  1.74s/it] 94%|█████████▍| 16747/17834 [8:32:59<31:41,  1.75s/it] 94%|█████████▍| 16748/17834 [8:33:01<31:23,  1.73s/it] 94%|█████████▍| 16749/17834 [8:33:02<31:07,  1.72s/it]08/31/2024 03:47:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1413242816925049, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02881946787238121, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.188239097595215, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3583827018737793}
 94%|█████████▍| 16750/17834 [8:33:04<30:56,  1.71s/it] 94%|█████████▍| 16751/17834 [8:33:06<31:13,  1.73s/it] 94%|█████████▍| 16752/17834 [8:33:08<31:40,  1.76s/it] 94%|█████████▍| 16753/17834 [8:33:09<31:43,  1.76s/it] 94%|█████████▍| 16754/17834 [8:33:11<31:52,  1.77s/it] 94%|█████████▍| 16755/17834 [8:33:13<31:51,  1.77s/it] 94%|█████████▍| 16756/17834 [8:33:15<31:33,  1.76s/it] 94%|█████████▍| 16757/17834 [8:33:16<31:35,  1.76s/it] 94%|█████████▍| 16758/17834 [8:33:18<31:38,  1.76s/it] 94%|█████████▍| 16759/17834 [8:33:20<31:33,  1.76s/it] 94%|█████████▍| 16760/17834 [8:33:22<31:45,  1.77s/it] 94%|█████████▍| 16761/17834 [8:33:23<31:38,  1.77s/it] 94%|█████████▍| 16762/17834 [8:33:25<31:32,  1.77s/it] 94%|█████████▍| 16763/17834 [8:33:27<31:17,  1.75s/it] 94%|█████████▍| 16764/17834 [8:33:29<31:32,  1.77s/it] 94%|█████████▍| 16765/17834 [8:33:30<31:22,  1.76s/it] 94%|█████████▍| 16766/17834 [8:33:32<31:21,  1.76s/it] 94%|█████████▍| 16767/17834 [8:33:34<31:14,  1.76s/it] 94%|█████████▍| 16768/17834 [8:33:36<31:03,  1.75s/it] 94%|█████████▍| 16769/17834 [8:33:38<31:25,  1.77s/it] 94%|█████████▍| 16770/17834 [8:33:39<31:01,  1.75s/it] 94%|█████████▍| 16771/17834 [8:33:41<31:12,  1.76s/it] 94%|█████████▍| 16772/17834 [8:33:43<31:18,  1.77s/it] 94%|█████████▍| 16773/17834 [8:33:45<30:57,  1.75s/it] 94%|█████████▍| 16774/17834 [8:33:46<31:05,  1.76s/it] 94%|█████████▍| 16775/17834 [8:33:48<30:57,  1.75s/it] 94%|█████████▍| 16776/17834 [8:33:50<31:28,  1.78s/it] 94%|█████████▍| 16777/17834 [8:33:52<30:54,  1.75s/it] 94%|█████████▍| 16778/17834 [8:33:53<31:10,  1.77s/it] 94%|█████████▍| 16779/17834 [8:33:55<31:22,  1.78s/it] 94%|█████████▍| 16780/17834 [8:33:57<31:00,  1.77s/it] 94%|█████████▍| 16781/17834 [8:33:59<30:43,  1.75s/it] 94%|█████████▍| 16782/17834 [8:34:00<30:42,  1.75s/it] 94%|█████████▍| 16783/17834 [8:34:02<30:54,  1.76s/it] 94%|█████████▍| 16784/17834 [8:34:04<30:35,  1.75s/it] 94%|█████████▍| 16785/17834 [8:34:06<30:37,  1.75s/it] 94%|█████████▍| 16786/17834 [8:34:07<30:28,  1.75s/it] 94%|█████████▍| 16787/17834 [8:34:09<30:27,  1.75s/it] 94%|█████████▍| 16788/17834 [8:34:11<31:02,  1.78s/it] 94%|█████████▍| 16789/17834 [8:34:13<30:33,  1.75s/it] 94%|█████████▍| 16790/17834 [8:34:14<30:23,  1.75s/it] 94%|█████████▍| 16791/17834 [8:34:16<29:53,  1.72s/it] 94%|█████████▍| 16792/17834 [8:34:18<30:21,  1.75s/it] 94%|█████████▍| 16793/17834 [8:34:20<30:27,  1.76s/it] 94%|█████████▍| 16794/17834 [8:34:21<30:16,  1.75s/it] 94%|█████████▍| 16795/17834 [8:34:23<30:08,  1.74s/it] 94%|█████████▍| 16796/17834 [8:34:25<29:54,  1.73s/it] 94%|█████████▍| 16797/17834 [8:34:27<29:52,  1.73s/it] 94%|█████████▍| 16798/17834 [8:34:28<30:29,  1.77s/it] 94%|█████████▍| 16799/17834 [8:34:30<30:32,  1.77s/it]08/31/2024 03:48:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8683045506477356, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.021191991865634918, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.118175745010376, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0076723098754883}
 94%|█████████▍| 16800/17834 [8:34:32<30:27,  1.77s/it] 94%|█████████▍| 16801/17834 [8:34:34<30:18,  1.76s/it] 94%|█████████▍| 16802/17834 [8:34:36<30:41,  1.78s/it] 94%|█████████▍| 16803/17834 [8:34:37<30:39,  1.78s/it] 94%|█████████▍| 16804/17834 [8:34:39<30:20,  1.77s/it] 94%|█████████▍| 16805/17834 [8:34:41<30:14,  1.76s/it] 94%|█████████▍| 16806/17834 [8:34:43<30:37,  1.79s/it] 94%|█████████▍| 16807/17834 [8:34:44<30:11,  1.76s/it] 94%|█████████▍| 16808/17834 [8:34:46<29:52,  1.75s/it] 94%|█████████▍| 16809/17834 [8:34:48<29:49,  1.75s/it] 94%|█████████▍| 16810/17834 [8:34:50<30:03,  1.76s/it] 94%|█████████▍| 16811/17834 [8:34:51<30:03,  1.76s/it] 94%|█████████▍| 16812/17834 [8:34:53<30:07,  1.77s/it] 94%|█████████▍| 16813/17834 [8:34:55<29:47,  1.75s/it] 94%|█████████▍| 16814/17834 [8:34:57<29:36,  1.74s/it] 94%|█████████▍| 16815/17834 [8:34:58<29:28,  1.74s/it] 94%|█████████▍| 16816/17834 [8:35:00<29:26,  1.74s/it] 94%|█████████▍| 16817/17834 [8:35:02<29:23,  1.73s/it] 94%|█████████▍| 16818/17834 [8:35:04<29:53,  1.76s/it] 94%|█████████▍| 16819/17834 [8:35:05<29:47,  1.76s/it] 94%|█████████▍| 16820/17834 [8:35:07<30:03,  1.78s/it] 94%|█████████▍| 16821/17834 [8:35:09<29:38,  1.76s/it] 94%|█████████▍| 16822/17834 [8:35:11<29:37,  1.76s/it] 94%|█████████▍| 16823/17834 [8:35:13<30:17,  1.80s/it] 94%|█████████▍| 16824/17834 [8:35:14<29:57,  1.78s/it] 94%|█████████▍| 16825/17834 [8:35:16<29:50,  1.77s/it] 94%|█████████▍| 16826/17834 [8:35:18<29:42,  1.77s/it] 94%|█████████▍| 16827/17834 [8:35:20<29:29,  1.76s/it] 94%|█████████▍| 16828/17834 [8:35:21<29:48,  1.78s/it] 94%|█████████▍| 16829/17834 [8:35:23<29:34,  1.77s/it] 94%|█████████▍| 16830/17834 [8:35:25<30:01,  1.79s/it] 94%|█████████▍| 16831/17834 [8:35:27<29:49,  1.78s/it] 94%|█████████▍| 16832/17834 [8:35:28<29:26,  1.76s/it] 94%|█████████▍| 16833/17834 [8:35:30<29:24,  1.76s/it] 94%|█████████▍| 16834/17834 [8:35:32<29:10,  1.75s/it] 94%|█████████▍| 16835/17834 [8:35:34<29:03,  1.75s/it] 94%|█████████▍| 16836/17834 [8:35:35<28:54,  1.74s/it] 94%|█████████▍| 16837/17834 [8:35:37<29:03,  1.75s/it] 94%|█████████▍| 16838/17834 [8:35:39<28:54,  1.74s/it] 94%|█████████▍| 16839/17834 [8:35:41<28:38,  1.73s/it] 94%|█████████▍| 16840/17834 [8:35:42<28:27,  1.72s/it] 94%|█████████▍| 16841/17834 [8:35:44<29:02,  1.75s/it] 94%|█████████▍| 16842/17834 [8:35:46<28:51,  1.75s/it] 94%|█████████▍| 16843/17834 [8:35:48<28:51,  1.75s/it] 94%|█████████▍| 16844/17834 [8:35:49<28:53,  1.75s/it] 94%|█████████▍| 16845/17834 [8:35:51<28:48,  1.75s/it] 94%|█████████▍| 16846/17834 [8:35:53<28:31,  1.73s/it] 94%|█████████▍| 16847/17834 [8:35:55<28:51,  1.75s/it] 94%|█████████▍| 16848/17834 [8:35:56<28:51,  1.76s/it] 94%|█████████▍| 16849/17834 [8:35:58<29:03,  1.77s/it]08/31/2024 03:50:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1986963748931885, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025333967059850693, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.199634552001953, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.4236650466918945}
 94%|█████████▍| 16850/17834 [8:36:00<28:29,  1.74s/it] 94%|█████████▍| 16851/17834 [8:36:02<28:49,  1.76s/it] 94%|█████████▍| 16852/17834 [8:36:03<28:32,  1.74s/it] 94%|█████████▍| 16853/17834 [8:36:05<28:31,  1.74s/it] 95%|█████████▍| 16854/17834 [8:36:07<28:25,  1.74s/it] 95%|█████████▍| 16855/17834 [8:36:09<28:39,  1.76s/it] 95%|█████████▍| 16856/17834 [8:36:10<28:30,  1.75s/it] 95%|█████████▍| 16857/17834 [8:36:12<28:26,  1.75s/it] 95%|█████████▍| 16858/17834 [8:36:14<28:52,  1.77s/it] 95%|█████████▍| 16859/17834 [8:36:16<28:20,  1.74s/it] 95%|█████████▍| 16860/17834 [8:36:17<28:27,  1.75s/it] 95%|█████████▍| 16861/17834 [8:36:19<28:50,  1.78s/it] 95%|█████████▍| 16862/17834 [8:36:21<28:48,  1.78s/it] 95%|█████████▍| 16863/17834 [8:36:23<28:30,  1.76s/it] 95%|█████████▍| 16864/17834 [8:36:24<28:22,  1.75s/it] 95%|█████████▍| 16865/17834 [8:36:26<28:21,  1.76s/it] 95%|█████████▍| 16866/17834 [8:36:28<28:42,  1.78s/it] 95%|█████████▍| 16867/17834 [8:36:30<28:25,  1.76s/it] 95%|█████████▍| 16868/17834 [8:36:32<28:27,  1.77s/it] 95%|█████████▍| 16869/17834 [8:36:33<28:48,  1.79s/it] 95%|█████████▍| 16870/17834 [8:36:35<28:51,  1.80s/it] 95%|█████████▍| 16871/17834 [8:36:37<28:29,  1.77s/it] 95%|█████████▍| 16872/17834 [8:36:39<28:14,  1.76s/it] 95%|█████████▍| 16873/17834 [8:36:40<28:09,  1.76s/it] 95%|█████████▍| 16874/17834 [8:36:42<27:43,  1.73s/it] 95%|█████████▍| 16875/17834 [8:36:44<27:33,  1.72s/it] 95%|█████████▍| 16876/17834 [8:36:45<27:25,  1.72s/it] 95%|█████████▍| 16877/17834 [8:36:47<27:24,  1.72s/it] 95%|█████████▍| 16878/17834 [8:36:49<27:39,  1.74s/it] 95%|█████████▍| 16879/17834 [8:36:51<27:39,  1.74s/it] 95%|█████████▍| 16880/17834 [8:36:52<27:39,  1.74s/it] 95%|█████████▍| 16881/17834 [8:36:54<27:36,  1.74s/it] 95%|█████████▍| 16882/17834 [8:36:56<27:47,  1.75s/it] 95%|█████████▍| 16883/17834 [8:36:58<28:06,  1.77s/it] 95%|█████████▍| 16884/17834 [8:37:00<28:38,  1.81s/it] 95%|█████████▍| 16885/17834 [8:37:01<28:05,  1.78s/it] 95%|█████████▍| 16886/17834 [8:37:03<27:46,  1.76s/it] 95%|█████████▍| 16887/17834 [8:37:05<28:02,  1.78s/it] 95%|█████████▍| 16888/17834 [8:37:07<27:42,  1.76s/it] 95%|█████████▍| 16889/17834 [8:37:08<27:41,  1.76s/it] 95%|█████████▍| 16890/17834 [8:37:10<27:20,  1.74s/it] 95%|█████████▍| 16891/17834 [8:37:12<27:13,  1.73s/it] 95%|█████████▍| 16892/17834 [8:37:14<27:30,  1.75s/it] 95%|█████████▍| 16893/17834 [8:37:15<27:20,  1.74s/it] 95%|█████████▍| 16894/17834 [8:37:17<27:35,  1.76s/it] 95%|█████████▍| 16895/17834 [8:37:19<27:15,  1.74s/it] 95%|█████████▍| 16896/17834 [8:37:21<27:34,  1.76s/it] 95%|█████████▍| 16897/17834 [8:37:22<27:27,  1.76s/it] 95%|█████████▍| 16898/17834 [8:37:24<27:46,  1.78s/it] 95%|█████████▍| 16899/17834 [8:37:26<27:27,  1.76s/it]08/31/2024 03:51:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8681139349937439, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.019256900995969772, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1447317600250244, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.032102584838867}
 95%|█████████▍| 16900/17834 [8:37:28<27:11,  1.75s/it] 95%|█████████▍| 16901/17834 [8:37:30<27:48,  1.79s/it] 95%|█████████▍| 16902/17834 [8:37:31<27:15,  1.75s/it] 95%|█████████▍| 16903/17834 [8:37:33<27:14,  1.76s/it] 95%|█████████▍| 16904/17834 [8:37:35<26:56,  1.74s/it] 95%|█████████▍| 16905/17834 [8:37:36<26:57,  1.74s/it] 95%|█████████▍| 16906/17834 [8:37:38<27:09,  1.76s/it] 95%|█████████▍| 16907/17834 [8:37:40<27:13,  1.76s/it] 95%|█████████▍| 16908/17834 [8:37:42<27:12,  1.76s/it] 95%|█████████▍| 16909/17834 [8:37:43<26:52,  1.74s/it] 95%|█████████▍| 16910/17834 [8:37:45<26:58,  1.75s/it] 95%|█████████▍| 16911/17834 [8:37:47<27:04,  1.76s/it] 95%|█████████▍| 16912/17834 [8:37:49<27:01,  1.76s/it] 95%|█████████▍| 16913/17834 [8:37:51<27:24,  1.79s/it] 95%|█████████▍| 16914/17834 [8:37:52<26:55,  1.76s/it] 95%|█████████▍| 16915/17834 [8:37:54<26:54,  1.76s/it] 95%|█████████▍| 16916/17834 [8:37:56<27:01,  1.77s/it] 95%|█████████▍| 16917/17834 [8:37:58<27:00,  1.77s/it] 95%|█████████▍| 16918/17834 [8:37:59<26:50,  1.76s/it] 95%|█████████▍| 16919/17834 [8:38:01<26:37,  1.75s/it] 95%|█████████▍| 16920/17834 [8:38:03<26:32,  1.74s/it] 95%|█████████▍| 16921/17834 [8:38:04<26:17,  1.73s/it] 95%|█████████▍| 16922/17834 [8:38:06<26:19,  1.73s/it] 95%|█████████▍| 16923/17834 [8:38:08<26:21,  1.74s/it] 95%|█████████▍| 16924/17834 [8:38:10<26:17,  1.73s/it] 95%|█████████▍| 16925/17834 [8:38:11<26:39,  1.76s/it] 95%|█████████▍| 16926/17834 [8:38:13<26:45,  1.77s/it] 95%|█████████▍| 16927/17834 [8:38:15<26:26,  1.75s/it] 95%|█████████▍| 16928/17834 [8:38:17<26:39,  1.77s/it] 95%|█████████▍| 16929/17834 [8:38:19<26:38,  1.77s/it] 95%|█████████▍| 16930/17834 [8:38:20<26:34,  1.76s/it] 95%|█████████▍| 16931/17834 [8:38:22<26:16,  1.75s/it] 95%|█████████▍| 16932/17834 [8:38:24<26:18,  1.75s/it] 95%|█████████▍| 16933/17834 [8:38:26<26:22,  1.76s/it] 95%|█████████▍| 16934/17834 [8:38:27<26:10,  1.74s/it] 95%|█████████▍| 16935/17834 [8:38:29<26:05,  1.74s/it] 95%|█████████▍| 16936/17834 [8:38:31<26:13,  1.75s/it] 95%|█████████▍| 16937/17834 [8:38:33<26:08,  1.75s/it] 95%|█████████▍| 16938/17834 [8:38:34<26:11,  1.75s/it] 95%|█████████▍| 16939/17834 [8:38:36<26:02,  1.75s/it] 95%|█████████▍| 16940/17834 [8:38:38<25:42,  1.73s/it] 95%|█████████▍| 16941/17834 [8:38:39<25:43,  1.73s/it] 95%|█████████▍| 16942/17834 [8:38:41<26:14,  1.77s/it] 95%|█████████▌| 16943/17834 [8:38:43<25:51,  1.74s/it] 95%|█████████▌| 16944/17834 [8:38:45<25:41,  1.73s/it] 95%|█████████▌| 16945/17834 [8:38:46<25:40,  1.73s/it] 95%|█████████▌| 16946/17834 [8:38:48<25:28,  1.72s/it] 95%|█████████▌| 16947/17834 [8:38:50<25:33,  1.73s/it] 95%|█████████▌| 16948/17834 [8:38:52<26:00,  1.76s/it] 95%|█████████▌| 16949/17834 [8:38:53<25:46,  1.75s/it]08/31/2024 03:53:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.957937479019165, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02038831263780594, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1469764709472656, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.125302314758301}
 95%|█████████▌| 16950/17834 [8:38:55<25:41,  1.74s/it] 95%|█████████▌| 16951/17834 [8:38:57<25:40,  1.74s/it] 95%|█████████▌| 16952/17834 [8:38:59<26:27,  1.80s/it] 95%|█████████▌| 16953/17834 [8:39:01<26:07,  1.78s/it] 95%|█████████▌| 16954/17834 [8:39:02<25:52,  1.76s/it] 95%|█████████▌| 16955/17834 [8:39:04<25:43,  1.76s/it] 95%|█████████▌| 16956/17834 [8:39:06<25:47,  1.76s/it] 95%|█████████▌| 16957/17834 [8:39:07<25:28,  1.74s/it] 95%|█████████▌| 16958/17834 [8:39:09<25:27,  1.74s/it] 95%|█████████▌| 16959/17834 [8:39:11<25:21,  1.74s/it] 95%|█████████▌| 16960/17834 [8:39:13<25:50,  1.77s/it] 95%|█████████▌| 16961/17834 [8:39:15<25:35,  1.76s/it] 95%|█████████▌| 16962/17834 [8:39:16<25:21,  1.74s/it] 95%|█████████▌| 16963/17834 [8:39:18<25:32,  1.76s/it] 95%|█████████▌| 16964/17834 [8:39:20<25:21,  1.75s/it] 95%|█████████▌| 16965/17834 [8:39:22<25:29,  1.76s/it] 95%|█████████▌| 16966/17834 [8:39:23<25:44,  1.78s/it] 95%|█████████▌| 16967/17834 [8:39:25<25:16,  1.75s/it] 95%|█████████▌| 16968/17834 [8:39:27<25:23,  1.76s/it] 95%|█████████▌| 16969/17834 [8:39:29<25:13,  1.75s/it] 95%|█████████▌| 16970/17834 [8:39:30<25:14,  1.75s/it] 95%|█████████▌| 16971/17834 [8:39:32<25:27,  1.77s/it] 95%|█████████▌| 16972/17834 [8:39:34<25:09,  1.75s/it] 95%|█████████▌| 16973/17834 [8:39:36<25:14,  1.76s/it] 95%|█████████▌| 16974/17834 [8:39:37<25:00,  1.74s/it] 95%|█████████▌| 16975/17834 [8:39:39<24:58,  1.74s/it] 95%|█████████▌| 16976/17834 [8:39:41<24:52,  1.74s/it] 95%|█████████▌| 16977/17834 [8:39:43<25:13,  1.77s/it] 95%|█████████▌| 16978/17834 [8:39:44<24:58,  1.75s/it] 95%|█████████▌| 16979/17834 [8:39:46<24:56,  1.75s/it] 95%|█████████▌| 16980/17834 [8:39:48<25:03,  1.76s/it] 95%|█████████▌| 16981/17834 [8:39:50<24:41,  1.74s/it] 95%|█████████▌| 16982/17834 [8:39:51<25:04,  1.77s/it] 95%|█████████▌| 16983/17834 [8:39:53<24:49,  1.75s/it] 95%|█████████▌| 16984/17834 [8:39:55<24:41,  1.74s/it] 95%|█████████▌| 16985/17834 [8:39:57<25:05,  1.77s/it] 95%|█████████▌| 16986/17834 [8:39:58<25:02,  1.77s/it] 95%|█████████▌| 16987/17834 [8:40:00<24:42,  1.75s/it] 95%|█████████▌| 16988/17834 [8:40:02<24:52,  1.76s/it] 95%|█████████▌| 16989/17834 [8:40:04<24:51,  1.77s/it] 95%|█████████▌| 16990/17834 [8:40:05<24:56,  1.77s/it] 95%|█████████▌| 16991/17834 [8:40:07<24:25,  1.74s/it] 95%|█████████▌| 16992/17834 [8:40:09<24:22,  1.74s/it] 95%|█████████▌| 16993/17834 [8:40:11<24:37,  1.76s/it] 95%|█████████▌| 16994/17834 [8:40:12<24:46,  1.77s/it] 95%|█████████▌| 16995/17834 [8:40:14<24:41,  1.77s/it] 95%|█████████▌| 16996/17834 [8:40:16<24:26,  1.75s/it] 95%|█████████▌| 16997/17834 [8:40:18<24:26,  1.75s/it] 95%|█████████▌| 16998/17834 [8:40:19<24:20,  1.75s/it] 95%|█████████▌| 16999/17834 [8:40:21<24:28,  1.76s/it]08/31/2024 03:54:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0094380378723145, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02555149607360363, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1587233543395996, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.1937127113342285}
 95%|█████████▌| 17000/17834 [8:40:23<24:26,  1.76s/it] 95%|█████████▌| 17001/17834 [8:40:25<24:37,  1.77s/it] 95%|█████████▌| 17002/17834 [8:40:27<24:30,  1.77s/it] 95%|█████████▌| 17003/17834 [8:40:28<24:23,  1.76s/it] 95%|█████████▌| 17004/17834 [8:40:30<24:09,  1.75s/it] 95%|█████████▌| 17005/17834 [8:40:32<24:09,  1.75s/it] 95%|█████████▌| 17006/17834 [8:40:34<24:09,  1.75s/it] 95%|█████████▌| 17007/17834 [8:40:35<24:14,  1.76s/it] 95%|█████████▌| 17008/17834 [8:40:37<24:17,  1.76s/it] 95%|█████████▌| 17009/17834 [8:40:39<24:23,  1.77s/it] 95%|█████████▌| 17010/17834 [8:40:41<24:24,  1.78s/it] 95%|█████████▌| 17011/17834 [8:40:42<24:18,  1.77s/it] 95%|█████████▌| 17012/17834 [8:40:44<23:52,  1.74s/it] 95%|█████████▌| 17013/17834 [8:40:46<23:41,  1.73s/it] 95%|█████████▌| 17014/17834 [8:40:48<23:34,  1.73s/it] 95%|█████████▌| 17015/17834 [8:40:49<24:01,  1.76s/it] 95%|█████████▌| 17016/17834 [8:40:51<23:55,  1.75s/it] 95%|█████████▌| 17017/17834 [8:40:53<24:01,  1.76s/it] 95%|█████████▌| 17018/17834 [8:40:55<24:04,  1.77s/it] 95%|█████████▌| 17019/17834 [8:40:56<24:10,  1.78s/it] 95%|█████████▌| 17020/17834 [8:40:58<24:02,  1.77s/it] 95%|█████████▌| 17021/17834 [8:41:00<23:43,  1.75s/it] 95%|█████████▌| 17022/17834 [8:41:02<23:45,  1.76s/it] 95%|█████████▌| 17023/17834 [8:41:03<23:49,  1.76s/it] 95%|█████████▌| 17024/17834 [8:41:05<23:50,  1.77s/it] 95%|█████████▌| 17025/17834 [8:41:07<23:38,  1.75s/it] 95%|█████████▌| 17026/17834 [8:41:09<23:28,  1.74s/it] 95%|█████████▌| 17027/17834 [8:41:10<23:29,  1.75s/it] 95%|█████████▌| 17028/17834 [8:41:12<23:27,  1.75s/it] 95%|█████████▌| 17029/17834 [8:41:14<23:22,  1.74s/it] 95%|█████████▌| 17030/17834 [8:41:16<23:50,  1.78s/it] 95%|█████████▌| 17031/17834 [8:41:18<23:53,  1.79s/it] 96%|█████████▌| 17032/17834 [8:41:19<23:38,  1.77s/it] 96%|█████████▌| 17033/17834 [8:41:21<23:31,  1.76s/it] 96%|█████████▌| 17034/17834 [8:41:23<23:21,  1.75s/it] 96%|█████████▌| 17035/17834 [8:41:25<23:14,  1.75s/it] 96%|█████████▌| 17036/17834 [8:41:26<23:39,  1.78s/it] 96%|█████████▌| 17037/17834 [8:41:28<23:13,  1.75s/it] 96%|█████████▌| 17038/17834 [8:41:30<23:01,  1.74s/it] 96%|█████████▌| 17039/17834 [8:41:32<23:11,  1.75s/it] 96%|█████████▌| 17040/17834 [8:41:33<23:22,  1.77s/it] 96%|█████████▌| 17041/17834 [8:41:35<23:23,  1.77s/it] 96%|█████████▌| 17042/17834 [8:41:37<23:18,  1.77s/it] 96%|█████████▌| 17043/17834 [8:41:39<22:56,  1.74s/it] 96%|█████████▌| 17044/17834 [8:41:40<23:01,  1.75s/it] 96%|█████████▌| 17045/17834 [8:41:42<23:01,  1.75s/it] 96%|█████████▌| 17046/17834 [8:41:44<23:02,  1.75s/it] 96%|█████████▌| 17047/17834 [8:41:46<22:46,  1.74s/it] 96%|█████████▌| 17048/17834 [8:41:47<23:24,  1.79s/it] 96%|█████████▌| 17049/17834 [8:41:49<23:01,  1.76s/it]08/31/2024 03:56:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0630321502685547, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03678714856505394, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1796727180480957, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.279491901397705}
 96%|█████████▌| 17050/17834 [8:41:51<23:08,  1.77s/it] 96%|█████████▌| 17051/17834 [8:41:53<23:05,  1.77s/it] 96%|█████████▌| 17052/17834 [8:41:55<23:11,  1.78s/it] 96%|█████████▌| 17053/17834 [8:41:56<23:19,  1.79s/it] 96%|█████████▌| 17054/17834 [8:41:58<23:05,  1.78s/it] 96%|█████████▌| 17055/17834 [8:42:00<22:54,  1.76s/it] 96%|█████████▌| 17056/17834 [8:42:02<23:46,  1.83s/it] 96%|█████████▌| 17057/17834 [8:42:03<23:06,  1.78s/it] 96%|█████████▌| 17058/17834 [8:42:05<22:48,  1.76s/it] 96%|█████████▌| 17059/17834 [8:42:07<22:52,  1.77s/it] 96%|█████████▌| 17060/17834 [8:42:09<22:43,  1.76s/it] 96%|█████████▌| 17061/17834 [8:42:10<22:44,  1.77s/it] 96%|█████████▌| 17062/17834 [8:42:12<22:27,  1.75s/it] 96%|█████████▌| 17063/17834 [8:42:14<22:26,  1.75s/it] 96%|█████████▌| 17064/17834 [8:42:16<22:17,  1.74s/it] 96%|█████████▌| 17065/17834 [8:42:17<22:18,  1.74s/it] 96%|█████████▌| 17066/17834 [8:42:19<22:06,  1.73s/it] 96%|█████████▌| 17067/17834 [8:42:21<22:02,  1.72s/it] 96%|█████████▌| 17068/17834 [8:42:23<22:09,  1.74s/it] 96%|█████████▌| 17069/17834 [8:42:24<21:53,  1.72s/it] 96%|█████████▌| 17070/17834 [8:42:26<22:13,  1.75s/it] 96%|█████████▌| 17071/17834 [8:42:28<22:05,  1.74s/it] 96%|█████████▌| 17072/17834 [8:42:29<21:52,  1.72s/it] 96%|█████████▌| 17073/17834 [8:42:31<21:57,  1.73s/it] 96%|█████████▌| 17074/17834 [8:42:33<22:00,  1.74s/it] 96%|█████████▌| 17075/17834 [8:42:35<22:04,  1.75s/it] 96%|█████████▌| 17076/17834 [8:42:36<22:02,  1.74s/it] 96%|█████████▌| 17077/17834 [8:42:38<21:49,  1.73s/it] 96%|█████████▌| 17078/17834 [8:42:40<22:05,  1.75s/it] 96%|█████████▌| 17079/17834 [8:42:42<21:50,  1.74s/it] 96%|█████████▌| 17080/17834 [8:42:43<21:46,  1.73s/it] 96%|█████████▌| 17081/17834 [8:42:45<21:37,  1.72s/it] 96%|█████████▌| 17082/17834 [8:42:47<21:37,  1.73s/it] 96%|█████████▌| 17083/17834 [8:42:49<21:49,  1.74s/it] 96%|█████████▌| 17084/17834 [8:42:50<22:04,  1.77s/it] 96%|█████████▌| 17085/17834 [8:42:52<22:11,  1.78s/it] 96%|█████████▌| 17086/17834 [8:42:54<21:48,  1.75s/it] 96%|█████████▌| 17087/17834 [8:42:56<21:59,  1.77s/it] 96%|█████████▌| 17088/17834 [8:42:58<22:11,  1.78s/it] 96%|█████████▌| 17089/17834 [8:42:59<21:51,  1.76s/it] 96%|█████████▌| 17090/17834 [8:43:01<21:53,  1.77s/it] 96%|█████████▌| 17091/17834 [8:43:03<21:52,  1.77s/it] 96%|█████████▌| 17092/17834 [8:43:05<21:43,  1.76s/it] 96%|█████████▌| 17093/17834 [8:43:06<21:35,  1.75s/it] 96%|█████████▌| 17094/17834 [8:43:08<21:34,  1.75s/it] 96%|█████████▌| 17095/17834 [8:43:10<21:35,  1.75s/it] 96%|█████████▌| 17096/17834 [8:43:12<21:32,  1.75s/it] 96%|█████████▌| 17097/17834 [8:43:13<21:25,  1.74s/it] 96%|█████████▌| 17098/17834 [8:43:15<21:19,  1.74s/it] 96%|█████████▌| 17099/17834 [8:43:17<21:25,  1.75s/it]08/31/2024 03:57:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.239084243774414, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04947502166032791, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3069844245910645, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.59554386138916}
 96%|█████████▌| 17100/17834 [8:43:18<21:14,  1.74s/it] 96%|█████████▌| 17101/17834 [8:43:20<21:38,  1.77s/it] 96%|█████████▌| 17102/17834 [8:43:22<21:23,  1.75s/it] 96%|█████████▌| 17103/17834 [8:43:24<21:15,  1.74s/it] 96%|█████████▌| 17104/17834 [8:43:26<21:19,  1.75s/it] 96%|█████████▌| 17105/17834 [8:43:27<21:08,  1.74s/it] 96%|█████████▌| 17106/17834 [8:43:29<21:12,  1.75s/it] 96%|█████████▌| 17107/17834 [8:43:31<21:26,  1.77s/it] 96%|█████████▌| 17108/17834 [8:43:33<21:19,  1.76s/it] 96%|█████████▌| 17109/17834 [8:43:34<21:14,  1.76s/it] 96%|█████████▌| 17110/17834 [8:43:36<21:20,  1.77s/it] 96%|█████████▌| 17111/17834 [8:43:38<21:47,  1.81s/it] 96%|█████████▌| 17112/17834 [8:43:40<21:30,  1.79s/it] 96%|█████████▌| 17113/17834 [8:43:41<21:14,  1.77s/it] 96%|█████████▌| 17114/17834 [8:43:43<21:07,  1.76s/it] 96%|█████████▌| 17115/17834 [8:43:45<20:47,  1.74s/it] 96%|█████████▌| 17116/17834 [8:43:47<20:53,  1.75s/it] 96%|█████████▌| 17117/17834 [8:43:48<20:46,  1.74s/it] 96%|█████████▌| 17118/17834 [8:43:50<20:35,  1.73s/it] 96%|█████████▌| 17119/17834 [8:43:52<20:39,  1.73s/it] 96%|█████████▌| 17120/17834 [8:43:54<20:52,  1.75s/it] 96%|█████████▌| 17121/17834 [8:43:55<20:35,  1.73s/it] 96%|█████████▌| 17122/17834 [8:43:57<20:30,  1.73s/it] 96%|█████████▌| 17123/17834 [8:43:59<20:31,  1.73s/it] 96%|█████████▌| 17124/17834 [8:44:01<20:34,  1.74s/it] 96%|█████████▌| 17125/17834 [8:44:02<20:23,  1.73s/it] 96%|█████████▌| 17126/17834 [8:44:04<20:24,  1.73s/it] 96%|█████████▌| 17127/17834 [8:44:06<20:26,  1.74s/it] 96%|█████████▌| 17128/17834 [8:44:07<20:22,  1.73s/it] 96%|█████████▌| 17129/17834 [8:44:09<20:34,  1.75s/it] 96%|█████████▌| 17130/17834 [8:44:11<21:20,  1.82s/it] 96%|█████████▌| 17131/17834 [8:44:13<20:52,  1.78s/it] 96%|█████████▌| 17132/17834 [8:44:15<20:41,  1.77s/it] 96%|█████████▌| 17133/17834 [8:44:16<20:48,  1.78s/it] 96%|█████████▌| 17134/17834 [8:44:18<20:34,  1.76s/it] 96%|█████████▌| 17135/17834 [8:44:20<20:30,  1.76s/it] 96%|█████████▌| 17136/17834 [8:44:22<20:35,  1.77s/it] 96%|█████████▌| 17137/17834 [8:44:23<20:37,  1.77s/it] 96%|█████████▌| 17138/17834 [8:44:25<20:32,  1.77s/it] 96%|█████████▌| 17139/17834 [8:44:27<20:26,  1.76s/it] 96%|█████████▌| 17140/17834 [8:44:29<20:23,  1.76s/it] 96%|█████████▌| 17141/17834 [8:44:30<20:10,  1.75s/it] 96%|█████████▌| 17142/17834 [8:44:32<20:17,  1.76s/it] 96%|█████████▌| 17143/17834 [8:44:34<20:09,  1.75s/it] 96%|█████████▌| 17144/17834 [8:44:36<20:06,  1.75s/it] 96%|█████████▌| 17145/17834 [8:44:38<20:14,  1.76s/it] 96%|█████████▌| 17146/17834 [8:44:39<19:56,  1.74s/it] 96%|█████████▌| 17147/17834 [8:44:41<19:47,  1.73s/it] 96%|█████████▌| 17148/17834 [8:44:43<19:44,  1.73s/it] 96%|█████████▌| 17149/17834 [8:44:44<19:50,  1.74s/it]08/31/2024 03:59:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.0641132593154907, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03136830031871796, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2209362983703613, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.316417694091797}
 96%|█████████▌| 17150/17834 [8:44:46<19:42,  1.73s/it] 96%|█████████▌| 17151/17834 [8:44:48<20:03,  1.76s/it] 96%|█████████▌| 17152/17834 [8:44:50<20:03,  1.76s/it] 96%|█████████▌| 17153/17834 [8:44:51<19:54,  1.75s/it] 96%|█████████▌| 17154/17834 [8:44:53<19:38,  1.73s/it] 96%|█████████▌| 17155/17834 [8:44:55<19:57,  1.76s/it] 96%|█████████▌| 17156/17834 [8:44:57<19:57,  1.77s/it] 96%|█████████▌| 17157/17834 [8:44:59<20:22,  1.81s/it] 96%|█████████▌| 17158/17834 [8:45:00<20:02,  1.78s/it] 96%|█████████▌| 17159/17834 [8:45:02<20:05,  1.79s/it] 96%|█████████▌| 17160/17834 [8:45:04<19:48,  1.76s/it] 96%|█████████▌| 17161/17834 [8:45:06<19:36,  1.75s/it] 96%|█████████▌| 17162/17834 [8:45:07<19:28,  1.74s/it] 96%|█████████▌| 17163/17834 [8:45:09<19:32,  1.75s/it] 96%|█████████▌| 17164/17834 [8:45:11<19:32,  1.75s/it] 96%|█████████▌| 17165/17834 [8:45:13<19:30,  1.75s/it] 96%|█████████▋| 17166/17834 [8:45:14<19:36,  1.76s/it] 96%|█████████▋| 17167/17834 [8:45:16<19:30,  1.76s/it] 96%|█████████▋| 17168/17834 [8:45:18<19:28,  1.76s/it] 96%|█████████▋| 17169/17834 [8:45:20<19:17,  1.74s/it] 96%|█████████▋| 17170/17834 [8:45:21<19:06,  1.73s/it] 96%|█████████▋| 17171/17834 [8:45:23<19:16,  1.74s/it] 96%|█████████▋| 17172/17834 [8:45:25<19:09,  1.74s/it] 96%|█████████▋| 17173/17834 [8:45:26<19:06,  1.73s/it] 96%|█████████▋| 17174/17834 [8:45:28<19:05,  1.74s/it] 96%|█████████▋| 17175/17834 [8:45:30<19:11,  1.75s/it] 96%|█████████▋| 17176/17834 [8:45:32<19:10,  1.75s/it] 96%|█████████▋| 17177/17834 [8:45:33<19:01,  1.74s/it] 96%|█████████▋| 17178/17834 [8:45:35<18:55,  1.73s/it] 96%|█████████▋| 17179/17834 [8:45:37<18:46,  1.72s/it] 96%|█████████▋| 17180/17834 [8:45:39<18:49,  1.73s/it] 96%|█████████▋| 17181/17834 [8:45:40<19:04,  1.75s/it] 96%|█████████▋| 17182/17834 [8:45:42<18:50,  1.73s/it] 96%|█████████▋| 17183/17834 [8:45:44<18:48,  1.73s/it] 96%|█████████▋| 17184/17834 [8:45:46<18:56,  1.75s/it] 96%|█████████▋| 17185/17834 [8:45:47<19:04,  1.76s/it] 96%|█████████▋| 17186/17834 [8:45:49<19:03,  1.76s/it] 96%|█████████▋| 17187/17834 [8:45:51<18:48,  1.74s/it] 96%|█████████▋| 17188/17834 [8:45:53<18:53,  1.75s/it] 96%|█████████▋| 17189/17834 [8:45:54<18:43,  1.74s/it] 96%|█████████▋| 17190/17834 [8:45:56<18:33,  1.73s/it] 96%|█████████▋| 17191/17834 [8:45:58<18:29,  1.73s/it] 96%|█████████▋| 17192/17834 [8:46:00<18:37,  1.74s/it] 96%|█████████▋| 17193/17834 [8:46:01<18:49,  1.76s/it] 96%|█████████▋| 17194/17834 [8:46:03<18:34,  1.74s/it] 96%|█████████▋| 17195/17834 [8:46:05<18:29,  1.74s/it] 96%|█████████▋| 17196/17834 [8:46:07<18:28,  1.74s/it] 96%|█████████▋| 17197/17834 [8:46:08<18:19,  1.73s/it] 96%|█████████▋| 17198/17834 [8:46:10<18:05,  1.71s/it] 96%|█████████▋| 17199/17834 [8:46:12<18:24,  1.74s/it]08/31/2024 04:00:31 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.122864842414856, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02818692848086357, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.221362590789795, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3724143505096436}
 96%|█████████▋| 17200/17834 [8:46:13<18:13,  1.72s/it] 96%|█████████▋| 17201/17834 [8:46:15<18:12,  1.73s/it] 96%|█████████▋| 17202/17834 [8:46:17<18:13,  1.73s/it] 96%|█████████▋| 17203/17834 [8:46:19<18:16,  1.74s/it] 96%|█████████▋| 17204/17834 [8:46:20<18:31,  1.76s/it] 96%|█████████▋| 17205/17834 [8:46:22<18:25,  1.76s/it] 96%|█████████▋| 17206/17834 [8:46:24<18:14,  1.74s/it] 96%|█████████▋| 17207/17834 [8:46:26<18:19,  1.75s/it] 96%|█████████▋| 17208/17834 [8:46:27<18:07,  1.74s/it] 96%|█████████▋| 17209/17834 [8:46:29<18:13,  1.75s/it] 97%|█████████▋| 17210/17834 [8:46:31<17:56,  1.73s/it] 97%|█████████▋| 17211/17834 [8:46:33<17:56,  1.73s/it] 97%|█████████▋| 17212/17834 [8:46:34<17:58,  1.73s/it] 97%|█████████▋| 17213/17834 [8:46:36<18:06,  1.75s/it] 97%|█████████▋| 17214/17834 [8:46:38<17:54,  1.73s/it] 97%|█████████▋| 17215/17834 [8:46:40<17:56,  1.74s/it] 97%|█████████▋| 17216/17834 [8:46:41<17:51,  1.73s/it] 97%|█████████▋| 17217/17834 [8:46:43<17:42,  1.72s/it] 97%|█████████▋| 17218/17834 [8:46:45<18:00,  1.75s/it] 97%|█████████▋| 17219/17834 [8:46:47<17:52,  1.74s/it] 97%|█████████▋| 17220/17834 [8:46:48<17:54,  1.75s/it] 97%|█████████▋| 17221/17834 [8:46:50<17:42,  1.73s/it] 97%|█████████▋| 17222/17834 [8:46:52<17:55,  1.76s/it] 97%|█████████▋| 17223/17834 [8:46:54<17:43,  1.74s/it] 97%|█████████▋| 17224/17834 [8:46:55<17:36,  1.73s/it] 97%|█████████▋| 17225/17834 [8:46:57<17:39,  1.74s/it] 97%|█████████▋| 17226/17834 [8:46:59<17:36,  1.74s/it] 97%|█████████▋| 17227/17834 [8:47:01<17:47,  1.76s/it] 97%|█████████▋| 17228/17834 [8:47:02<17:46,  1.76s/it] 97%|█████████▋| 17229/17834 [8:47:04<17:50,  1.77s/it] 97%|█████████▋| 17230/17834 [8:47:06<17:50,  1.77s/it] 97%|█████████▋| 17231/17834 [8:47:08<17:43,  1.76s/it] 97%|█████████▋| 17232/17834 [8:47:09<17:41,  1.76s/it] 97%|█████████▋| 17233/17834 [8:47:11<17:33,  1.75s/it] 97%|█████████▋| 17234/17834 [8:47:13<17:38,  1.76s/it] 97%|█████████▋| 17235/17834 [8:47:15<17:26,  1.75s/it] 97%|█████████▋| 17236/17834 [8:47:16<17:32,  1.76s/it] 97%|█████████▋| 17237/17834 [8:47:18<17:31,  1.76s/it] 97%|█████████▋| 17238/17834 [8:47:20<17:25,  1.75s/it] 97%|█████████▋| 17239/17834 [8:47:22<17:19,  1.75s/it] 97%|█████████▋| 17240/17834 [8:47:23<17:18,  1.75s/it] 97%|█████████▋| 17241/17834 [8:47:25<16:59,  1.72s/it] 97%|█████████▋| 17242/17834 [8:47:27<17:02,  1.73s/it] 97%|█████████▋| 17243/17834 [8:47:29<17:25,  1.77s/it] 97%|█████████▋| 17244/17834 [8:47:30<17:16,  1.76s/it] 97%|█████████▋| 17245/17834 [8:47:32<17:12,  1.75s/it] 97%|█████████▋| 17246/17834 [8:47:34<17:13,  1.76s/it] 97%|█████████▋| 17247/17834 [8:47:36<16:57,  1.73s/it] 97%|█████████▋| 17248/17834 [8:47:37<17:08,  1.75s/it] 97%|█████████▋| 17249/17834 [8:47:39<17:02,  1.75s/it]08/31/2024 04:01:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.1163930892944336, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05266112461686134, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.152395009994507, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3214492797851562}
 97%|█████████▋| 17250/17834 [8:47:41<17:14,  1.77s/it] 97%|█████████▋| 17251/17834 [8:47:43<17:10,  1.77s/it] 97%|█████████▋| 17252/17834 [8:47:44<17:03,  1.76s/it] 97%|█████████▋| 17253/17834 [8:47:46<16:55,  1.75s/it] 97%|█████████▋| 17254/17834 [8:47:48<17:25,  1.80s/it] 97%|█████████▋| 17255/17834 [8:47:50<17:03,  1.77s/it] 97%|█████████▋| 17256/17834 [8:47:52<17:02,  1.77s/it] 97%|█████████▋| 17257/17834 [8:47:53<16:46,  1.74s/it] 97%|█████████▋| 17258/17834 [8:47:55<16:49,  1.75s/it] 97%|█████████▋| 17259/17834 [8:47:57<16:44,  1.75s/it] 97%|█████████▋| 17260/17834 [8:47:58<16:38,  1.74s/it] 97%|█████████▋| 17261/17834 [8:48:00<16:35,  1.74s/it] 97%|█████████▋| 17262/17834 [8:48:02<16:34,  1.74s/it] 97%|█████████▋| 17263/17834 [8:48:04<16:23,  1.72s/it] 97%|█████████▋| 17264/17834 [8:48:05<16:31,  1.74s/it] 97%|█████████▋| 17265/17834 [8:48:07<16:38,  1.75s/it] 97%|█████████▋| 17266/17834 [8:48:09<16:32,  1.75s/it] 97%|█████████▋| 17267/17834 [8:48:11<16:22,  1.73s/it] 97%|█████████▋| 17268/17834 [8:48:12<16:19,  1.73s/it] 97%|█████████▋| 17269/17834 [8:48:14<16:13,  1.72s/it] 97%|█████████▋| 17270/17834 [8:48:16<16:11,  1.72s/it] 97%|█████████▋| 17271/17834 [8:48:17<16:01,  1.71s/it] 97%|█████████▋| 17272/17834 [8:48:19<15:54,  1.70s/it] 97%|█████████▋| 17273/17834 [8:48:21<16:02,  1.72s/it] 97%|█████████▋| 17274/17834 [8:48:23<16:17,  1.75s/it] 97%|█████████▋| 17275/17834 [8:48:24<16:10,  1.74s/it] 97%|█████████▋| 17276/17834 [8:48:26<16:22,  1.76s/it] 97%|█████████▋| 17277/17834 [8:48:28<16:05,  1.73s/it] 97%|█████████▋| 17278/17834 [8:48:30<16:17,  1.76s/it] 97%|█████████▋| 17279/17834 [8:48:31<16:17,  1.76s/it] 97%|█████████▋| 17280/17834 [8:48:33<16:25,  1.78s/it] 97%|█████████▋| 17281/17834 [8:48:35<16:08,  1.75s/it] 97%|█████████▋| 17282/17834 [8:48:37<16:09,  1.76s/it] 97%|█████████▋| 17283/17834 [8:48:38<15:59,  1.74s/it] 97%|█████████▋| 17284/17834 [8:48:40<16:09,  1.76s/it] 97%|█████████▋| 17285/17834 [8:48:42<16:21,  1.79s/it] 97%|█████████▋| 17286/17834 [8:48:44<16:29,  1.81s/it] 97%|█████████▋| 17287/17834 [8:48:46<16:15,  1.78s/it] 97%|█████████▋| 17288/17834 [8:48:47<16:23,  1.80s/it] 97%|█████████▋| 17289/17834 [8:48:49<16:05,  1.77s/it] 97%|█████████▋| 17290/17834 [8:48:51<15:56,  1.76s/it] 97%|█████████▋| 17291/17834 [8:48:53<15:42,  1.74s/it] 97%|█████████▋| 17292/17834 [8:48:54<15:51,  1.76s/it] 97%|█████████▋| 17293/17834 [8:48:56<15:42,  1.74s/it] 97%|█████████▋| 17294/17834 [8:48:58<15:36,  1.73s/it] 97%|█████████▋| 17295/17834 [8:49:00<15:33,  1.73s/it] 97%|█████████▋| 17296/17834 [8:49:01<15:32,  1.73s/it] 97%|█████████▋| 17297/17834 [8:49:03<15:31,  1.73s/it] 97%|█████████▋| 17298/17834 [8:49:05<15:29,  1.73s/it] 97%|█████████▋| 17299/17834 [8:49:07<15:33,  1.75s/it]08/31/2024 04:03:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.2234687805175781, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0343644842505455, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.181821584701538, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.43965482711792}
 97%|█████████▋| 17300/17834 [8:49:08<15:33,  1.75s/it] 97%|█████████▋| 17301/17834 [8:49:10<15:30,  1.75s/it] 97%|█████████▋| 17302/17834 [8:49:12<15:33,  1.75s/it] 97%|█████████▋| 17303/17834 [8:49:14<15:28,  1.75s/it] 97%|█████████▋| 17304/17834 [8:49:15<15:23,  1.74s/it] 97%|█████████▋| 17305/17834 [8:49:17<15:20,  1.74s/it] 97%|█████████▋| 17306/17834 [8:49:19<15:30,  1.76s/it] 97%|█████████▋| 17307/17834 [8:49:21<15:38,  1.78s/it] 97%|█████████▋| 17308/17834 [8:49:22<15:42,  1.79s/it] 97%|█████████▋| 17309/17834 [8:49:24<15:29,  1.77s/it] 97%|█████████▋| 17310/17834 [8:49:26<15:28,  1.77s/it] 97%|█████████▋| 17311/17834 [8:49:28<15:22,  1.76s/it] 97%|█████████▋| 17312/17834 [8:49:29<15:08,  1.74s/it] 97%|█████████▋| 17313/17834 [8:49:31<14:59,  1.73s/it] 97%|█████████▋| 17314/17834 [8:49:33<14:56,  1.72s/it] 97%|█████████▋| 17315/17834 [8:49:35<14:51,  1.72s/it] 97%|█████████▋| 17316/17834 [8:49:36<14:53,  1.73s/it] 97%|█████████▋| 17317/17834 [8:49:38<14:52,  1.73s/it] 97%|█████████▋| 17318/17834 [8:49:40<15:02,  1.75s/it] 97%|█████████▋| 17319/17834 [8:49:42<15:05,  1.76s/it] 97%|█████████▋| 17320/17834 [8:49:43<15:07,  1.77s/it] 97%|█████████▋| 17321/17834 [8:49:45<15:00,  1.75s/it] 97%|█████████▋| 17322/17834 [8:49:47<14:52,  1.74s/it] 97%|█████████▋| 17323/17834 [8:49:49<14:47,  1.74s/it] 97%|█████████▋| 17324/17834 [8:49:50<14:48,  1.74s/it] 97%|█████████▋| 17325/17834 [8:49:52<14:53,  1.76s/it] 97%|█████████▋| 17326/17834 [8:49:54<14:50,  1.75s/it] 97%|█████████▋| 17327/17834 [8:49:56<15:00,  1.78s/it] 97%|█████████▋| 17328/17834 [8:49:57<14:51,  1.76s/it] 97%|█████████▋| 17329/17834 [8:49:59<14:46,  1.76s/it] 97%|█████████▋| 17330/17834 [8:50:01<14:57,  1.78s/it] 97%|█████████▋| 17331/17834 [8:50:03<14:48,  1.77s/it] 97%|█████████▋| 17332/17834 [8:50:04<14:37,  1.75s/it] 97%|█████████▋| 17333/17834 [8:50:06<14:32,  1.74s/it] 97%|█████████▋| 17334/17834 [8:50:08<14:25,  1.73s/it] 97%|█████████▋| 17335/17834 [8:50:10<14:24,  1.73s/it] 97%|█████████▋| 17336/17834 [8:50:11<14:28,  1.74s/it] 97%|█████████▋| 17337/17834 [8:50:13<14:14,  1.72s/it] 97%|█████████▋| 17338/17834 [8:50:15<14:15,  1.72s/it] 97%|█████████▋| 17339/17834 [8:50:16<14:21,  1.74s/it] 97%|█████████▋| 17340/17834 [8:50:18<14:21,  1.74s/it] 97%|█████████▋| 17341/17834 [8:50:20<14:23,  1.75s/it] 97%|█████████▋| 17342/17834 [8:50:22<14:09,  1.73s/it] 97%|█████████▋| 17343/17834 [8:50:23<14:18,  1.75s/it] 97%|█████████▋| 17344/17834 [8:50:25<14:23,  1.76s/it] 97%|█████████▋| 17345/17834 [8:50:27<14:12,  1.74s/it] 97%|█████████▋| 17346/17834 [8:50:29<14:16,  1.75s/it] 97%|█████████▋| 17347/17834 [8:50:31<14:14,  1.75s/it] 97%|█████████▋| 17348/17834 [8:50:32<14:17,  1.76s/it] 97%|█████████▋| 17349/17834 [8:50:34<14:33,  1.80s/it]08/31/2024 04:04:53 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4251787662506104, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05736859142780304, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3459370136260986, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.828484535217285}
 97%|█████████▋| 17350/17834 [8:50:36<14:16,  1.77s/it] 97%|█████████▋| 17351/17834 [8:50:38<14:13,  1.77s/it] 97%|█████████▋| 17352/17834 [8:50:39<14:13,  1.77s/it] 97%|█████████▋| 17353/17834 [8:50:41<14:04,  1.76s/it] 97%|█████████▋| 17354/17834 [8:50:43<14:03,  1.76s/it] 97%|█████████▋| 17355/17834 [8:50:45<14:12,  1.78s/it] 97%|█████████▋| 17356/17834 [8:50:47<14:12,  1.78s/it] 97%|█████████▋| 17357/17834 [8:50:48<14:05,  1.77s/it] 97%|█████████▋| 17358/17834 [8:50:50<14:09,  1.79s/it] 97%|█████████▋| 17359/17834 [8:50:52<13:55,  1.76s/it] 97%|█████████▋| 17360/17834 [8:50:54<13:53,  1.76s/it] 97%|█████████▋| 17361/17834 [8:50:55<13:45,  1.75s/it] 97%|█████████▋| 17362/17834 [8:50:57<13:43,  1.75s/it] 97%|█████████▋| 17363/17834 [8:50:59<13:41,  1.74s/it] 97%|█████████▋| 17364/17834 [8:51:00<13:38,  1.74s/it] 97%|█████████▋| 17365/17834 [8:51:02<13:35,  1.74s/it] 97%|█████████▋| 17366/17834 [8:51:04<13:30,  1.73s/it] 97%|█████████▋| 17367/17834 [8:51:06<13:34,  1.74s/it] 97%|█████████▋| 17368/17834 [8:51:07<13:25,  1.73s/it] 97%|█████████▋| 17369/17834 [8:51:09<13:25,  1.73s/it] 97%|█████████▋| 17370/17834 [8:51:11<13:27,  1.74s/it] 97%|█████████▋| 17371/17834 [8:51:13<13:45,  1.78s/it] 97%|█████████▋| 17372/17834 [8:51:15<13:42,  1.78s/it] 97%|█████████▋| 17373/17834 [8:51:16<13:41,  1.78s/it] 97%|█████████▋| 17374/17834 [8:51:18<13:30,  1.76s/it] 97%|█████████▋| 17375/17834 [8:51:20<13:30,  1.77s/it] 97%|█████████▋| 17376/17834 [8:51:22<13:28,  1.76s/it] 97%|█████████▋| 17377/17834 [8:51:23<13:20,  1.75s/it] 97%|█████████▋| 17378/17834 [8:51:25<13:29,  1.78s/it] 97%|█████████▋| 17379/17834 [8:51:27<13:18,  1.76s/it] 97%|█████████▋| 17380/17834 [8:51:29<13:18,  1.76s/it] 97%|█████████▋| 17381/17834 [8:51:30<13:05,  1.73s/it] 97%|█████████▋| 17382/17834 [8:51:32<13:04,  1.74s/it] 97%|█████████▋| 17383/17834 [8:51:34<13:07,  1.75s/it] 97%|█████████▋| 17384/17834 [8:51:36<13:05,  1.74s/it] 97%|█████████▋| 17385/17834 [8:51:37<13:12,  1.76s/it] 97%|█████████▋| 17386/17834 [8:51:39<13:08,  1.76s/it] 97%|█████████▋| 17387/17834 [8:51:41<12:58,  1.74s/it] 97%|█████████▋| 17388/17834 [8:51:43<12:58,  1.75s/it] 98%|█████████▊| 17389/17834 [8:51:44<13:17,  1.79s/it] 98%|█████████▊| 17390/17834 [8:51:46<13:11,  1.78s/it] 98%|█████████▊| 17391/17834 [8:51:48<12:56,  1.75s/it] 98%|█████████▊| 17392/17834 [8:51:50<12:49,  1.74s/it] 98%|█████████▊| 17393/17834 [8:51:51<12:55,  1.76s/it] 98%|█████████▊| 17394/17834 [8:51:53<12:48,  1.75s/it] 98%|█████████▊| 17395/17834 [8:51:55<12:52,  1.76s/it] 98%|█████████▊| 17396/17834 [8:51:57<12:45,  1.75s/it] 98%|█████████▊| 17397/17834 [8:51:58<12:45,  1.75s/it] 98%|█████████▊| 17398/17834 [8:52:00<12:44,  1.75s/it] 98%|█████████▊| 17399/17834 [8:52:02<12:38,  1.74s/it]08/31/2024 04:06:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9912275671958923, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024481918662786484, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.141422748565674, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.157132148742676}
 98%|█████████▊| 17400/17834 [8:52:04<12:42,  1.76s/it] 98%|█████████▊| 17401/17834 [8:52:05<12:34,  1.74s/it] 98%|█████████▊| 17402/17834 [8:52:07<12:41,  1.76s/it] 98%|█████████▊| 17403/17834 [8:52:09<12:35,  1.75s/it] 98%|█████████▊| 17404/17834 [8:52:11<12:33,  1.75s/it] 98%|█████████▊| 17405/17834 [8:52:12<12:28,  1.74s/it] 98%|█████████▊| 17406/17834 [8:52:14<12:21,  1.73s/it] 98%|█████████▊| 17407/17834 [8:52:16<12:20,  1.73s/it] 98%|█████████▊| 17408/17834 [8:52:18<12:20,  1.74s/it] 98%|█████████▊| 17409/17834 [8:52:19<12:17,  1.73s/it] 98%|█████████▊| 17410/17834 [8:52:21<12:12,  1.73s/it] 98%|█████████▊| 17411/17834 [8:52:23<12:23,  1.76s/it] 98%|█████████▊| 17412/17834 [8:52:25<12:24,  1.76s/it] 98%|█████████▊| 17413/17834 [8:52:26<12:25,  1.77s/it] 98%|█████████▊| 17414/17834 [8:52:28<12:21,  1.77s/it] 98%|█████████▊| 17415/17834 [8:52:30<12:15,  1.76s/it] 98%|█████████▊| 17416/17834 [8:52:32<12:17,  1.77s/it] 98%|█████████▊| 17417/17834 [8:52:34<12:22,  1.78s/it] 98%|█████████▊| 17418/17834 [8:52:35<12:20,  1.78s/it] 98%|█████████▊| 17419/17834 [8:52:37<12:11,  1.76s/it] 98%|█████████▊| 17420/17834 [8:52:39<12:06,  1.76s/it] 98%|█████████▊| 17421/17834 [8:52:40<12:04,  1.75s/it] 98%|█████████▊| 17422/17834 [8:52:42<12:06,  1.76s/it] 98%|█████████▊| 17423/17834 [8:52:44<12:05,  1.76s/it] 98%|█████████▊| 17424/17834 [8:52:46<11:58,  1.75s/it] 98%|█████████▊| 17425/17834 [8:52:47<11:46,  1.73s/it] 98%|█████████▊| 17426/17834 [8:52:49<11:47,  1.73s/it] 98%|█████████▊| 17427/17834 [8:52:51<11:48,  1.74s/it] 98%|█████████▊| 17428/17834 [8:52:53<11:49,  1.75s/it] 98%|█████████▊| 17429/17834 [8:52:54<11:48,  1.75s/it] 98%|█████████▊| 17430/17834 [8:52:56<11:43,  1.74s/it] 98%|█████████▊| 17431/17834 [8:52:58<11:43,  1.74s/it] 98%|█████████▊| 17432/17834 [8:53:00<11:39,  1.74s/it] 98%|█████████▊| 17433/17834 [8:53:01<11:43,  1.76s/it] 98%|█████████▊| 17434/17834 [8:53:03<11:40,  1.75s/it] 98%|█████████▊| 17435/17834 [8:53:05<11:39,  1.75s/it] 98%|█████████▊| 17436/17834 [8:53:07<11:44,  1.77s/it] 98%|█████████▊| 17437/17834 [8:53:09<11:39,  1.76s/it] 98%|█████████▊| 17438/17834 [8:53:10<11:32,  1.75s/it] 98%|█████████▊| 17439/17834 [8:53:12<11:36,  1.76s/it] 98%|█████████▊| 17440/17834 [8:53:14<11:39,  1.78s/it] 98%|█████████▊| 17441/17834 [8:53:16<11:39,  1.78s/it] 98%|█████████▊| 17442/17834 [8:53:17<11:28,  1.76s/it] 98%|█████████▊| 17443/17834 [8:53:19<11:31,  1.77s/it] 98%|█████████▊| 17444/17834 [8:53:21<11:19,  1.74s/it] 98%|█████████▊| 17445/17834 [8:53:23<11:27,  1.77s/it] 98%|█████████▊| 17446/17834 [8:53:24<11:27,  1.77s/it] 98%|█████████▊| 17447/17834 [8:53:26<11:19,  1.76s/it] 98%|█████████▊| 17448/17834 [8:53:28<11:21,  1.77s/it] 98%|█████████▊| 17449/17834 [8:53:30<11:19,  1.76s/it]08/31/2024 04:07:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.085785984992981, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02480458840727806, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2177727222442627, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.3283634185791016}
 98%|█████████▊| 17450/17834 [8:53:31<11:11,  1.75s/it] 98%|█████████▊| 17451/17834 [8:53:33<11:20,  1.78s/it] 98%|█████████▊| 17452/17834 [8:53:35<11:12,  1.76s/it] 98%|█████████▊| 17453/17834 [8:53:37<11:03,  1.74s/it] 98%|█████████▊| 17454/17834 [8:53:38<11:04,  1.75s/it] 98%|█████████▊| 17455/17834 [8:53:40<11:14,  1.78s/it] 98%|█████████▊| 17456/17834 [8:53:42<11:10,  1.77s/it] 98%|█████████▊| 17457/17834 [8:53:44<11:01,  1.76s/it] 98%|█████████▊| 17458/17834 [8:53:45<10:53,  1.74s/it] 98%|█████████▊| 17459/17834 [8:53:47<11:00,  1.76s/it] 98%|█████████▊| 17460/17834 [8:53:49<10:52,  1.74s/it] 98%|█████████▊| 17461/17834 [8:53:51<10:50,  1.74s/it] 98%|█████████▊| 17462/17834 [8:53:52<10:48,  1.74s/it] 98%|█████████▊| 17463/17834 [8:53:54<10:48,  1.75s/it] 98%|█████████▊| 17464/17834 [8:53:56<10:50,  1.76s/it] 98%|█████████▊| 17465/17834 [8:53:58<10:45,  1.75s/it] 98%|█████████▊| 17466/17834 [8:53:59<10:40,  1.74s/it] 98%|█████████▊| 17467/17834 [8:54:01<10:39,  1.74s/it] 98%|█████████▊| 17468/17834 [8:54:03<10:44,  1.76s/it] 98%|█████████▊| 17469/17834 [8:54:05<10:43,  1.76s/it] 98%|█████████▊| 17470/17834 [8:54:06<10:35,  1.75s/it] 98%|█████████▊| 17471/17834 [8:54:08<10:33,  1.74s/it] 98%|█████████▊| 17472/17834 [8:54:10<10:31,  1.75s/it] 98%|█████████▊| 17473/17834 [8:54:12<10:26,  1.74s/it] 98%|█████████▊| 17474/17834 [8:54:13<10:25,  1.74s/it] 98%|█████████▊| 17475/17834 [8:54:15<10:29,  1.75s/it] 98%|█████████▊| 17476/17834 [8:54:17<10:29,  1.76s/it] 98%|█████████▊| 17477/17834 [8:54:19<10:24,  1.75s/it] 98%|█████████▊| 17478/17834 [8:54:20<10:22,  1.75s/it] 98%|█████████▊| 17479/17834 [8:54:22<10:25,  1.76s/it] 98%|█████████▊| 17480/17834 [8:54:24<10:14,  1.73s/it] 98%|█████████▊| 17481/17834 [8:54:26<10:09,  1.73s/it] 98%|█████████▊| 17482/17834 [8:54:28<10:30,  1.79s/it] 98%|█████████▊| 17483/17834 [8:54:29<10:23,  1.78s/it] 98%|█████████▊| 17484/17834 [8:54:31<10:22,  1.78s/it] 98%|█████████▊| 17485/17834 [8:54:33<10:16,  1.77s/it] 98%|█████████▊| 17486/17834 [8:54:35<10:13,  1.76s/it] 98%|█████████▊| 17487/17834 [8:54:36<10:14,  1.77s/it] 98%|█████████▊| 17488/17834 [8:54:38<10:06,  1.75s/it] 98%|█████████▊| 17489/17834 [8:54:40<09:55,  1.73s/it] 98%|█████████▊| 17490/17834 [8:54:41<09:51,  1.72s/it] 98%|█████████▊| 17491/17834 [8:54:43<09:52,  1.73s/it] 98%|█████████▊| 17492/17834 [8:54:45<10:01,  1.76s/it] 98%|█████████▊| 17493/17834 [8:54:47<09:51,  1.73s/it] 98%|█████████▊| 17494/17834 [8:54:48<09:50,  1.74s/it] 98%|█████████▊| 17495/17834 [8:54:50<09:47,  1.73s/it] 98%|█████████▊| 17496/17834 [8:54:52<09:49,  1.74s/it] 98%|█████████▊| 17497/17834 [8:54:54<09:49,  1.75s/it] 98%|█████████▊| 17498/17834 [8:54:55<09:47,  1.75s/it] 98%|█████████▊| 17499/17834 [8:54:57<09:57,  1.78s/it]08/31/2024 04:09:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.8411504030227661, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.021364636719226837, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.152979850769043, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0154948234558105}
 98%|█████████▊| 17500/17834 [8:54:59<09:50,  1.77s/it] 98%|█████████▊| 17501/17834 [8:55:01<09:45,  1.76s/it] 98%|█████████▊| 17502/17834 [8:55:02<09:38,  1.74s/it] 98%|█████████▊| 17503/17834 [8:55:04<09:36,  1.74s/it] 98%|█████████▊| 17504/17834 [8:55:06<09:32,  1.74s/it] 98%|█████████▊| 17505/17834 [8:55:08<09:36,  1.75s/it] 98%|█████████▊| 17506/17834 [8:55:10<09:39,  1.77s/it] 98%|█████████▊| 17507/17834 [8:55:11<09:33,  1.75s/it] 98%|█████████▊| 17508/17834 [8:55:13<09:24,  1.73s/it] 98%|█████████▊| 17509/17834 [8:55:15<09:20,  1.73s/it] 98%|█████████▊| 17510/17834 [8:55:16<09:29,  1.76s/it] 98%|█████████▊| 17511/17834 [8:55:18<09:37,  1.79s/it] 98%|█████████▊| 17512/17834 [8:55:20<09:31,  1.77s/it] 98%|█████████▊| 17513/17834 [8:55:22<09:28,  1.77s/it] 98%|█████████▊| 17514/17834 [8:55:24<09:25,  1.77s/it] 98%|█████████▊| 17515/17834 [8:55:25<09:17,  1.75s/it] 98%|█████████▊| 17516/17834 [8:55:27<09:18,  1.76s/it] 98%|█████████▊| 17517/17834 [8:55:29<09:18,  1.76s/it] 98%|█████████▊| 17518/17834 [8:55:31<09:08,  1.74s/it] 98%|█████████▊| 17519/17834 [8:55:32<09:19,  1.78s/it] 98%|█████████▊| 17520/17834 [8:55:34<09:12,  1.76s/it] 98%|█████████▊| 17521/17834 [8:55:36<09:16,  1.78s/it] 98%|█████████▊| 17522/17834 [8:55:38<09:19,  1.79s/it] 98%|█████████▊| 17523/17834 [8:55:39<09:09,  1.77s/it] 98%|█████████▊| 17524/17834 [8:55:41<09:12,  1.78s/it] 98%|█████████▊| 17525/17834 [8:55:43<09:07,  1.77s/it] 98%|█████████▊| 17526/17834 [8:55:45<08:58,  1.75s/it] 98%|█████████▊| 17527/17834 [8:55:46<08:51,  1.73s/it] 98%|█████████▊| 17528/17834 [8:55:48<08:45,  1.72s/it] 98%|█████████▊| 17529/17834 [8:55:50<08:50,  1.74s/it] 98%|█████████▊| 17530/17834 [8:55:52<08:48,  1.74s/it] 98%|█████████▊| 17531/17834 [8:55:53<08:46,  1.74s/it] 98%|█████████▊| 17532/17834 [8:55:55<08:41,  1.73s/it] 98%|█████████▊| 17533/17834 [8:55:57<08:41,  1.73s/it] 98%|█████████▊| 17534/17834 [8:55:59<08:41,  1.74s/it] 98%|█████████▊| 17535/17834 [8:56:00<08:39,  1.74s/it] 98%|█████████▊| 17536/17834 [8:56:02<08:47,  1.77s/it] 98%|█████████▊| 17537/17834 [8:56:04<08:37,  1.74s/it] 98%|█████████▊| 17538/17834 [8:56:06<08:35,  1.74s/it] 98%|█████████▊| 17539/17834 [8:56:07<08:29,  1.73s/it] 98%|█████████▊| 17540/17834 [8:56:09<08:27,  1.73s/it] 98%|█████████▊| 17541/17834 [8:56:11<08:33,  1.75s/it] 98%|█████████▊| 17542/17834 [8:56:13<08:32,  1.76s/it] 98%|█████████▊| 17543/17834 [8:56:14<08:34,  1.77s/it] 98%|█████████▊| 17544/17834 [8:56:16<08:33,  1.77s/it] 98%|█████████▊| 17545/17834 [8:56:18<08:25,  1.75s/it] 98%|█████████▊| 17546/17834 [8:56:20<08:20,  1.74s/it] 98%|█████████▊| 17547/17834 [8:56:21<08:22,  1.75s/it] 98%|█████████▊| 17548/17834 [8:56:23<08:22,  1.76s/it] 98%|█████████▊| 17549/17834 [8:56:25<08:25,  1.77s/it]08/31/2024 04:10:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9376158714294434, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029507622122764587, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1298465728759766, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.0969700813293457}
 98%|█████████▊| 17550/17834 [8:56:27<08:16,  1.75s/it] 98%|█████████▊| 17551/17834 [8:56:28<08:08,  1.73s/it] 98%|█████████▊| 17552/17834 [8:56:30<08:10,  1.74s/it] 98%|█████████▊| 17553/17834 [8:56:32<08:03,  1.72s/it] 98%|█████████▊| 17554/17834 [8:56:33<08:03,  1.73s/it] 98%|█████████▊| 17555/17834 [8:56:35<08:03,  1.73s/it] 98%|█████████▊| 17556/17834 [8:56:37<07:58,  1.72s/it] 98%|█████████▊| 17557/17834 [8:56:39<07:55,  1.72s/it] 98%|█████████▊| 17558/17834 [8:56:40<07:57,  1.73s/it] 98%|█████████▊| 17559/17834 [8:56:42<07:54,  1.72s/it] 98%|█████████▊| 17560/17834 [8:56:44<07:59,  1.75s/it] 98%|█████████▊| 17561/17834 [8:56:46<07:54,  1.74s/it] 98%|█████████▊| 17562/17834 [8:56:47<07:54,  1.75s/it] 98%|█████████▊| 17563/17834 [8:56:49<07:50,  1.74s/it] 98%|█████████▊| 17564/17834 [8:56:51<07:54,  1.76s/it] 98%|█████████▊| 17565/17834 [8:56:53<07:58,  1.78s/it] 98%|█████████▊| 17566/17834 [8:56:54<07:47,  1.74s/it] 99%|█████████▊| 17567/17834 [8:56:56<07:42,  1.73s/it] 99%|█████████▊| 17568/17834 [8:56:58<07:44,  1.74s/it] 99%|█████████▊| 17569/17834 [8:57:00<07:45,  1.76s/it] 99%|█████████▊| 17570/17834 [8:57:01<07:43,  1.76s/it] 99%|█████████▊| 17571/17834 [8:57:03<07:39,  1.75s/it] 99%|█████████▊| 17572/17834 [8:57:05<07:39,  1.76s/it] 99%|█████████▊| 17573/17834 [8:57:07<07:34,  1.74s/it] 99%|█████████▊| 17574/17834 [8:57:08<07:32,  1.74s/it] 99%|█████████▊| 17575/17834 [8:57:10<07:31,  1.74s/it] 99%|█████████▊| 17576/17834 [8:57:12<07:22,  1.72s/it] 99%|█████████▊| 17577/17834 [8:57:13<07:20,  1.71s/it] 99%|█████████▊| 17578/17834 [8:57:15<07:23,  1.73s/it] 99%|█████████▊| 17579/17834 [8:57:17<07:20,  1.73s/it] 99%|█████████▊| 17580/17834 [8:57:19<07:15,  1.72s/it] 99%|█████████▊| 17581/17834 [8:57:20<07:17,  1.73s/it] 99%|█████████▊| 17582/17834 [8:57:22<07:16,  1.73s/it] 99%|█████████▊| 17583/17834 [8:57:24<07:17,  1.74s/it] 99%|█████████▊| 17584/17834 [8:57:26<07:15,  1.74s/it] 99%|█████████▊| 17585/17834 [8:57:27<07:15,  1.75s/it] 99%|█████████▊| 17586/17834 [8:57:29<07:10,  1.74s/it] 99%|█████████▊| 17587/17834 [8:57:31<07:11,  1.75s/it] 99%|█████████▊| 17588/17834 [8:57:33<07:10,  1.75s/it] 99%|█████████▊| 17589/17834 [8:57:34<07:09,  1.75s/it] 99%|█████████▊| 17590/17834 [8:57:36<07:10,  1.76s/it] 99%|█████████▊| 17591/17834 [8:57:38<07:02,  1.74s/it] 99%|█████████▊| 17592/17834 [8:57:40<07:00,  1.74s/it] 99%|█████████▊| 17593/17834 [8:57:41<06:57,  1.73s/it] 99%|█████████▊| 17594/17834 [8:57:43<06:58,  1.74s/it] 99%|█████████▊| 17595/17834 [8:57:45<06:54,  1.73s/it] 99%|█████████▊| 17596/17834 [8:57:46<06:49,  1.72s/it] 99%|█████████▊| 17597/17834 [8:57:48<06:53,  1.75s/it] 99%|█████████▊| 17598/17834 [8:57:50<06:52,  1.75s/it] 99%|█████████▊| 17599/17834 [8:57:52<06:49,  1.74s/it]08/31/2024 04:12:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.4278061389923096, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.06224121153354645, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.42211651802063, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.912163734436035}
 99%|█████████▊| 17600/17834 [8:57:54<06:49,  1.75s/it] 99%|█████████▊| 17601/17834 [8:57:55<06:48,  1.75s/it] 99%|█████████▊| 17602/17834 [8:57:57<06:42,  1.73s/it] 99%|█████████▊| 17603/17834 [8:57:59<06:38,  1.72s/it] 99%|█████████▊| 17604/17834 [8:58:00<06:37,  1.73s/it] 99%|█████████▊| 17605/17834 [8:58:02<06:38,  1.74s/it] 99%|█████████▊| 17606/17834 [8:58:04<06:38,  1.75s/it] 99%|█████████▊| 17607/17834 [8:58:06<06:34,  1.74s/it] 99%|█████████▊| 17608/17834 [8:58:07<06:27,  1.71s/it] 99%|█████████▊| 17609/17834 [8:58:09<06:27,  1.72s/it] 99%|█████████▊| 17610/17834 [8:58:11<06:28,  1.73s/it] 99%|█████████▊| 17611/17834 [8:58:13<06:33,  1.76s/it] 99%|█████████▉| 17612/17834 [8:58:14<06:31,  1.76s/it] 99%|█████████▉| 17613/17834 [8:58:16<06:33,  1.78s/it] 99%|█████████▉| 17614/17834 [8:58:18<06:33,  1.79s/it] 99%|█████████▉| 17615/17834 [8:58:20<06:32,  1.79s/it] 99%|█████████▉| 17616/17834 [8:58:22<06:24,  1.76s/it] 99%|█████████▉| 17617/17834 [8:58:23<06:19,  1.75s/it] 99%|█████████▉| 17618/17834 [8:58:25<06:21,  1.77s/it] 99%|█████████▉| 17619/17834 [8:58:27<06:14,  1.74s/it] 99%|█████████▉| 17620/17834 [8:58:28<06:11,  1.74s/it] 99%|█████████▉| 17621/17834 [8:58:30<06:12,  1.75s/it] 99%|█████████▉| 17622/17834 [8:58:32<06:09,  1.74s/it] 99%|█████████▉| 17623/17834 [8:58:34<06:08,  1.74s/it] 99%|█████████▉| 17624/17834 [8:58:36<06:08,  1.75s/it] 99%|█████████▉| 17625/17834 [8:58:37<06:09,  1.77s/it] 99%|█████████▉| 17626/17834 [8:58:39<06:02,  1.74s/it] 99%|█████████▉| 17627/17834 [8:58:41<05:58,  1.73s/it] 99%|█████████▉| 17628/17834 [8:58:43<06:01,  1.76s/it] 99%|█████████▉| 17629/17834 [8:58:44<05:58,  1.75s/it] 99%|█████████▉| 17630/17834 [8:58:46<05:58,  1.76s/it] 99%|█████████▉| 17631/17834 [8:58:48<05:53,  1.74s/it] 99%|█████████▉| 17632/17834 [8:58:50<05:55,  1.76s/it] 99%|█████████▉| 17633/17834 [8:58:51<05:52,  1.76s/it] 99%|█████████▉| 17634/17834 [8:58:53<05:51,  1.76s/it] 99%|█████████▉| 17635/17834 [8:58:55<05:48,  1.75s/it] 99%|█████████▉| 17636/17834 [8:58:57<05:47,  1.75s/it] 99%|█████████▉| 17637/17834 [8:58:58<05:47,  1.76s/it] 99%|█████████▉| 17638/17834 [8:59:00<05:45,  1.77s/it] 99%|█████████▉| 17639/17834 [8:59:02<05:46,  1.78s/it] 99%|█████████▉| 17640/17834 [8:59:04<05:44,  1.78s/it] 99%|█████████▉| 17641/17834 [8:59:05<05:43,  1.78s/it] 99%|█████████▉| 17642/17834 [8:59:07<05:39,  1.77s/it] 99%|█████████▉| 17643/17834 [8:59:09<05:33,  1.75s/it] 99%|█████████▉| 17644/17834 [8:59:11<05:32,  1.75s/it] 99%|█████████▉| 17645/17834 [8:59:12<05:32,  1.76s/it] 99%|█████████▉| 17646/17834 [8:59:14<05:35,  1.78s/it] 99%|█████████▉| 17647/17834 [8:59:16<05:30,  1.77s/it] 99%|█████████▉| 17648/17834 [8:59:18<05:25,  1.75s/it] 99%|█████████▉| 17649/17834 [8:59:19<05:23,  1.75s/it]08/31/2024 04:13:39 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9069280624389648, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023803722113370895, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.173276901245117, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.104008674621582}
 99%|█████████▉| 17650/17834 [8:59:21<05:24,  1.77s/it] 99%|█████████▉| 17651/17834 [8:59:23<05:21,  1.76s/it] 99%|█████████▉| 17652/17834 [8:59:25<05:19,  1.75s/it] 99%|█████████▉| 17653/17834 [8:59:27<05:22,  1.78s/it] 99%|█████████▉| 17654/17834 [8:59:28<05:16,  1.76s/it] 99%|█████████▉| 17655/17834 [8:59:30<05:14,  1.76s/it] 99%|█████████▉| 17656/17834 [8:59:32<05:12,  1.76s/it] 99%|█████████▉| 17657/17834 [8:59:34<05:12,  1.77s/it] 99%|█████████▉| 17658/17834 [8:59:35<05:06,  1.74s/it] 99%|█████████▉| 17659/17834 [8:59:37<05:08,  1.76s/it] 99%|█████████▉| 17660/17834 [8:59:39<05:03,  1.74s/it] 99%|█████████▉| 17661/17834 [8:59:41<05:06,  1.77s/it] 99%|█████████▉| 17662/17834 [8:59:42<05:01,  1.75s/it] 99%|█████████▉| 17663/17834 [8:59:44<04:59,  1.75s/it] 99%|█████████▉| 17664/17834 [8:59:46<04:53,  1.73s/it] 99%|█████████▉| 17665/17834 [8:59:48<04:55,  1.75s/it] 99%|█████████▉| 17666/17834 [8:59:49<04:54,  1.75s/it] 99%|█████████▉| 17667/17834 [8:59:51<04:50,  1.74s/it] 99%|█████████▉| 17668/17834 [8:59:53<04:46,  1.73s/it] 99%|█████████▉| 17669/17834 [8:59:55<04:49,  1.75s/it] 99%|█████████▉| 17670/17834 [8:59:56<04:45,  1.74s/it] 99%|█████████▉| 17671/17834 [8:59:58<04:46,  1.76s/it] 99%|█████████▉| 17672/17834 [9:00:00<04:44,  1.76s/it] 99%|█████████▉| 17673/17834 [9:00:02<04:43,  1.76s/it] 99%|█████████▉| 17674/17834 [9:00:03<04:39,  1.75s/it] 99%|█████████▉| 17675/17834 [9:00:05<04:43,  1.78s/it] 99%|█████████▉| 17676/17834 [9:00:07<04:36,  1.75s/it] 99%|█████████▉| 17677/17834 [9:00:09<04:36,  1.76s/it] 99%|█████████▉| 17678/17834 [9:00:10<04:32,  1.74s/it] 99%|█████████▉| 17679/17834 [9:00:12<04:27,  1.73s/it] 99%|█████████▉| 17680/17834 [9:00:14<04:25,  1.72s/it] 99%|█████████▉| 17681/17834 [9:00:15<04:23,  1.72s/it] 99%|█████████▉| 17682/17834 [9:00:17<04:24,  1.74s/it] 99%|█████████▉| 17683/17834 [9:00:19<04:22,  1.74s/it] 99%|█████████▉| 17684/17834 [9:00:21<04:20,  1.74s/it] 99%|█████████▉| 17685/17834 [9:00:22<04:21,  1.75s/it] 99%|█████████▉| 17686/17834 [9:00:24<04:23,  1.78s/it] 99%|█████████▉| 17687/17834 [9:00:26<04:20,  1.77s/it] 99%|█████████▉| 17688/17834 [9:00:28<04:18,  1.77s/it] 99%|█████████▉| 17689/17834 [9:00:30<04:15,  1.76s/it] 99%|█████████▉| 17690/17834 [9:00:31<04:14,  1.77s/it] 99%|█████████▉| 17691/17834 [9:00:33<04:13,  1.77s/it] 99%|█████████▉| 17692/17834 [9:00:35<04:12,  1.78s/it] 99%|█████████▉| 17693/17834 [9:00:37<04:12,  1.79s/it] 99%|█████████▉| 17694/17834 [9:00:39<04:10,  1.79s/it] 99%|█████████▉| 17695/17834 [9:00:40<04:09,  1.79s/it] 99%|█████████▉| 17696/17834 [9:00:42<04:05,  1.78s/it] 99%|█████████▉| 17697/17834 [9:00:44<04:03,  1.78s/it] 99%|█████████▉| 17698/17834 [9:00:46<03:58,  1.76s/it] 99%|█████████▉| 17699/17834 [9:00:47<03:52,  1.73s/it]08/31/2024 04:15:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.9991509318351746, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03687998279929161, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.174818992614746, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.2108497619628906}
 99%|█████████▉| 17700/17834 [9:00:49<03:54,  1.75s/it] 99%|█████████▉| 17701/17834 [9:00:51<03:51,  1.74s/it] 99%|█████████▉| 17702/17834 [9:00:53<03:53,  1.77s/it] 99%|█████████▉| 17703/17834 [9:00:54<03:48,  1.75s/it] 99%|█████████▉| 17704/17834 [9:00:56<03:45,  1.74s/it] 99%|█████████▉| 17705/17834 [9:00:58<03:43,  1.73s/it] 99%|█████████▉| 17706/17834 [9:00:59<03:41,  1.73s/it] 99%|█████████▉| 17707/17834 [9:01:01<03:38,  1.72s/it] 99%|█████████▉| 17708/17834 [9:01:03<03:36,  1.72s/it] 99%|█████████▉| 17709/17834 [9:01:05<03:35,  1.72s/it] 99%|█████████▉| 17710/17834 [9:01:06<03:34,  1.73s/it] 99%|█████████▉| 17711/17834 [9:01:08<03:33,  1.73s/it] 99%|█████████▉| 17712/17834 [9:01:10<03:32,  1.74s/it] 99%|█████████▉| 17713/17834 [9:01:11<03:27,  1.71s/it] 99%|█████████▉| 17714/17834 [9:01:13<03:26,  1.72s/it] 99%|█████████▉| 17715/17834 [9:01:15<03:25,  1.73s/it] 99%|█████████▉| 17716/17834 [9:01:17<03:28,  1.77s/it] 99%|█████████▉| 17717/17834 [9:01:19<03:25,  1.76s/it] 99%|█████████▉| 17718/17834 [9:01:20<03:23,  1.75s/it] 99%|█████████▉| 17719/17834 [9:01:22<03:20,  1.74s/it] 99%|█████████▉| 17720/17834 [9:01:24<03:19,  1.75s/it] 99%|█████████▉| 17721/17834 [9:01:26<03:18,  1.75s/it] 99%|█████████▉| 17722/17834 [9:01:27<03:14,  1.73s/it] 99%|█████████▉| 17723/17834 [9:01:29<03:10,  1.72s/it] 99%|█████████▉| 17724/17834 [9:01:31<03:12,  1.75s/it] 99%|█████████▉| 17725/17834 [9:01:33<03:11,  1.76s/it] 99%|█████████▉| 17726/17834 [9:01:34<03:08,  1.74s/it] 99%|█████████▉| 17727/17834 [9:01:36<03:05,  1.74s/it] 99%|█████████▉| 17728/17834 [9:01:38<03:06,  1.76s/it] 99%|█████████▉| 17729/17834 [9:01:40<03:05,  1.77s/it] 99%|█████████▉| 17730/17834 [9:01:41<03:03,  1.77s/it] 99%|█████████▉| 17731/17834 [9:01:43<03:02,  1.77s/it] 99%|█████████▉| 17732/17834 [9:01:45<02:58,  1.75s/it] 99%|█████████▉| 17733/17834 [9:01:46<02:54,  1.73s/it] 99%|█████████▉| 17734/17834 [9:01:48<02:50,  1.71s/it] 99%|█████████▉| 17735/17834 [9:01:50<02:50,  1.72s/it] 99%|█████████▉| 17736/17834 [9:01:52<02:48,  1.72s/it] 99%|█████████▉| 17737/17834 [9:01:53<02:45,  1.71s/it] 99%|█████████▉| 17738/17834 [9:01:55<02:44,  1.71s/it] 99%|█████████▉| 17739/17834 [9:01:57<02:45,  1.74s/it] 99%|█████████▉| 17740/17834 [9:01:59<02:43,  1.73s/it] 99%|█████████▉| 17741/17834 [9:02:00<02:43,  1.75s/it] 99%|█████████▉| 17742/17834 [9:02:02<02:42,  1.76s/it] 99%|█████████▉| 17743/17834 [9:02:04<02:40,  1.76s/it] 99%|█████████▉| 17744/17834 [9:02:06<02:37,  1.75s/it]100%|█████████▉| 17745/17834 [9:02:07<02:36,  1.76s/it]100%|█████████▉| 17746/17834 [9:02:09<02:35,  1.76s/it]100%|█████████▉| 17747/17834 [9:02:11<02:33,  1.77s/it]100%|█████████▉| 17748/17834 [9:02:13<02:30,  1.75s/it]100%|█████████▉| 17749/17834 [9:02:14<02:28,  1.74s/it]08/31/2024 04:16:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.3548628091812134, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05076365917921066, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.257956027984619, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.6635823249816895}
100%|█████████▉| 17750/17834 [9:02:16<02:26,  1.75s/it]100%|█████████▉| 17751/17834 [9:02:18<02:27,  1.78s/it]100%|█████████▉| 17752/17834 [9:02:20<02:25,  1.77s/it]100%|█████████▉| 17753/17834 [9:02:21<02:23,  1.77s/it]100%|█████████▉| 17754/17834 [9:02:23<02:19,  1.74s/it]100%|█████████▉| 17755/17834 [9:02:25<02:16,  1.73s/it]100%|█████████▉| 17756/17834 [9:02:27<02:15,  1.73s/it]100%|█████████▉| 17757/17834 [9:02:28<02:13,  1.74s/it]100%|█████████▉| 17758/17834 [9:02:30<02:11,  1.73s/it]100%|█████████▉| 17759/17834 [9:02:32<02:11,  1.75s/it]100%|█████████▉| 17760/17834 [9:02:34<02:10,  1.76s/it]100%|█████████▉| 17761/17834 [9:02:36<02:10,  1.79s/it]100%|█████████▉| 17762/17834 [9:02:37<02:06,  1.76s/it]100%|█████████▉| 17763/17834 [9:02:39<02:04,  1.76s/it]100%|█████████▉| 17764/17834 [9:02:41<02:04,  1.77s/it]100%|█████████▉| 17765/17834 [9:02:43<02:01,  1.76s/it]100%|█████████▉| 17766/17834 [9:02:44<01:59,  1.75s/it]100%|█████████▉| 17767/17834 [9:02:46<01:57,  1.75s/it]100%|█████████▉| 17768/17834 [9:02:48<01:54,  1.74s/it]100%|█████████▉| 17769/17834 [9:02:50<01:54,  1.76s/it]100%|█████████▉| 17770/17834 [9:02:51<01:51,  1.75s/it]100%|█████████▉| 17771/17834 [9:02:53<01:50,  1.75s/it]100%|█████████▉| 17772/17834 [9:02:55<01:49,  1.76s/it]100%|█████████▉| 17773/17834 [9:02:57<01:47,  1.76s/it]100%|█████████▉| 17774/17834 [9:02:58<01:45,  1.76s/it]100%|█████████▉| 17775/17834 [9:03:00<01:44,  1.76s/it]100%|█████████▉| 17776/17834 [9:03:02<01:41,  1.75s/it]100%|█████████▉| 17777/17834 [9:03:04<01:39,  1.75s/it]100%|█████████▉| 17778/17834 [9:03:05<01:37,  1.75s/it]100%|█████████▉| 17779/17834 [9:03:07<01:35,  1.74s/it]100%|█████████▉| 17780/17834 [9:03:09<01:35,  1.76s/it]100%|█████████▉| 17781/17834 [9:03:11<01:32,  1.75s/it]100%|█████████▉| 17782/17834 [9:03:12<01:31,  1.75s/it]100%|█████████▉| 17783/17834 [9:03:14<01:28,  1.73s/it]100%|█████████▉| 17784/17834 [9:03:16<01:27,  1.75s/it]100%|█████████▉| 17785/17834 [9:03:18<01:26,  1.76s/it]100%|█████████▉| 17786/17834 [9:03:19<01:23,  1.74s/it]100%|█████████▉| 17787/17834 [9:03:21<01:21,  1.74s/it]100%|█████████▉| 17788/17834 [9:03:23<01:19,  1.72s/it]100%|█████████▉| 17789/17834 [9:03:25<01:18,  1.75s/it]100%|█████████▉| 17790/17834 [9:03:26<01:18,  1.78s/it]100%|█████████▉| 17791/17834 [9:03:28<01:16,  1.79s/it]100%|█████████▉| 17792/17834 [9:03:30<01:14,  1.78s/it]100%|█████████▉| 17793/17834 [9:03:32<01:11,  1.75s/it]100%|█████████▉| 17794/17834 [9:03:33<01:09,  1.74s/it]100%|█████████▉| 17795/17834 [9:03:35<01:08,  1.75s/it]100%|█████████▉| 17796/17834 [9:03:37<01:05,  1.73s/it]100%|█████████▉| 17797/17834 [9:03:39<01:04,  1.74s/it]100%|█████████▉| 17798/17834 [9:03:40<01:02,  1.74s/it]100%|█████████▉| 17799/17834 [9:03:42<01:00,  1.72s/it]08/31/2024 04:18:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 1.101781964302063, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038483649492263794, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2115695476531982, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 3.351835250854492}
100%|█████████▉| 17800/17834 [9:03:44<00:59,  1.74s/it]100%|█████████▉| 17801/17834 [9:03:45<00:57,  1.74s/it]100%|█████████▉| 17802/17834 [9:03:47<00:55,  1.74s/it]100%|█████████▉| 17803/17834 [9:03:49<00:54,  1.76s/it]100%|█████████▉| 17804/17834 [9:03:51<00:52,  1.74s/it]100%|█████████▉| 17805/17834 [9:03:53<00:51,  1.77s/it]100%|█████████▉| 17806/17834 [9:03:54<00:49,  1.77s/it]100%|█████████▉| 17807/17834 [9:03:56<00:47,  1.76s/it]100%|█████████▉| 17808/17834 [9:03:58<00:45,  1.74s/it]100%|█████████▉| 17809/17834 [9:04:00<00:43,  1.74s/it]100%|█████████▉| 17810/17834 [9:04:01<00:42,  1.75s/it]100%|█████████▉| 17811/17834 [9:04:03<00:40,  1.76s/it]100%|█████████▉| 17812/17834 [9:04:05<00:38,  1.76s/it]100%|█████████▉| 17813/17834 [9:04:07<00:36,  1.75s/it]100%|█████████▉| 17814/17834 [9:04:08<00:34,  1.74s/it]100%|█████████▉| 17815/17834 [9:04:10<00:32,  1.73s/it]100%|█████████▉| 17816/17834 [9:04:12<00:31,  1.76s/it]100%|█████████▉| 17817/17834 [9:04:14<00:29,  1.76s/it]100%|█████████▉| 17818/17834 [9:04:15<00:27,  1.75s/it]100%|█████████▉| 17819/17834 [9:04:17<00:26,  1.77s/it]08/31/2024 04:18:36 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
08/31/2024 04:18:36 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:51,  4.26it/s][A
  1%|▏         | 3/221 [00:00<01:05,  3.30it/s][A
  2%|▏         | 4/221 [00:01<00:54,  3.95it/s][A
  2%|▏         | 5/221 [00:01<00:57,  3.75it/s][A
  3%|▎         | 6/221 [00:01<00:51,  4.15it/s][A
  3%|▎         | 7/221 [00:01<00:58,  3.67it/s][A
  4%|▎         | 8/221 [00:02<00:58,  3.62it/s][A
  4%|▍         | 9/221 [00:02<01:12,  2.91it/s][A
  5%|▍         | 10/221 [00:03<01:15,  2.81it/s][A
  5%|▍         | 11/221 [00:03<01:04,  3.25it/s][A
  5%|▌         | 12/221 [00:03<00:55,  3.76it/s][A
  6%|▌         | 13/221 [00:03<00:48,  4.33it/s][A
  6%|▋         | 14/221 [00:03<00:47,  4.37it/s][A
  7%|▋         | 15/221 [00:03<00:43,  4.73it/s][A
  7%|▋         | 16/221 [00:04<00:47,  4.30it/s][A
  8%|▊         | 17/221 [00:04<00:47,  4.27it/s][A
  8%|▊         | 18/221 [00:04<00:42,  4.79it/s][A
  9%|▊         | 19/221 [00:04<00:41,  4.82it/s][A
  9%|▉         | 20/221 [00:04<00:39,  5.09it/s][A
 10%|▉         | 21/221 [00:05<00:42,  4.68it/s][A
 10%|▉         | 22/221 [00:05<00:46,  4.27it/s][A
 10%|█         | 23/221 [00:05<00:43,  4.53it/s][A
 11%|█         | 24/221 [00:05<00:37,  5.29it/s][A
 11%|█▏        | 25/221 [00:06<00:42,  4.64it/s][A
 12%|█▏        | 26/221 [00:06<00:39,  4.95it/s][A
 12%|█▏        | 27/221 [00:06<00:49,  3.93it/s][A
 13%|█▎        | 28/221 [00:07<00:58,  3.29it/s][A
 13%|█▎        | 29/221 [00:07<01:03,  3.02it/s][A
 14%|█▎        | 30/221 [00:07<01:09,  2.73it/s][A
 14%|█▍        | 31/221 [00:08<01:10,  2.70it/s][A
 14%|█▍        | 32/221 [00:08<01:07,  2.81it/s][A
 15%|█▍        | 33/221 [00:08<00:56,  3.30it/s][A
 15%|█▌        | 34/221 [00:09<00:53,  3.49it/s][A
 16%|█▌        | 35/221 [00:09<00:59,  3.13it/s][A
 16%|█▋        | 36/221 [00:09<01:00,  3.08it/s][A
 17%|█▋        | 37/221 [00:09<00:52,  3.52it/s][A
 17%|█▋        | 38/221 [00:10<00:51,  3.54it/s][A
 18%|█▊        | 39/221 [00:10<00:42,  4.30it/s][A
 18%|█▊        | 40/221 [00:10<00:47,  3.79it/s][A
 19%|█▊        | 41/221 [00:11<00:54,  3.28it/s][A
 19%|█▉        | 43/221 [00:11<00:43,  4.12it/s][A
 20%|█▉        | 44/221 [00:11<00:51,  3.46it/s][A
 20%|██        | 45/221 [00:12<00:55,  3.15it/s][A
 21%|██        | 46/221 [00:12<00:58,  3.01it/s][A
 21%|██▏       | 47/221 [00:12<00:55,  3.14it/s][A
 22%|██▏       | 48/221 [00:13<00:48,  3.55it/s][A
 22%|██▏       | 49/221 [00:13<00:45,  3.78it/s][A
 23%|██▎       | 50/221 [00:13<00:50,  3.36it/s][A
 23%|██▎       | 51/221 [00:14<00:52,  3.22it/s][A
 24%|██▎       | 52/221 [00:14<00:47,  3.53it/s][A
 24%|██▍       | 53/221 [00:14<00:44,  3.78it/s][A
 24%|██▍       | 54/221 [00:14<00:45,  3.71it/s][A
 25%|██▍       | 55/221 [00:15<00:47,  3.52it/s][A
 25%|██▌       | 56/221 [00:15<00:45,  3.64it/s][A
 26%|██▌       | 57/221 [00:15<00:54,  3.01it/s][A
 26%|██▌       | 58/221 [00:15<00:44,  3.67it/s][A
 27%|██▋       | 59/221 [00:16<00:37,  4.35it/s][A
 27%|██▋       | 60/221 [00:16<00:40,  4.02it/s][A
 28%|██▊       | 61/221 [00:16<00:42,  3.76it/s][A
 28%|██▊       | 62/221 [00:16<00:38,  4.17it/s][A
 29%|██▊       | 63/221 [00:17<00:41,  3.78it/s][A
 29%|██▉       | 64/221 [00:17<00:50,  3.08it/s][A
 29%|██▉       | 65/221 [00:18<00:56,  2.74it/s][A
 30%|██▉       | 66/221 [00:18<00:54,  2.86it/s][A
 30%|███       | 67/221 [00:18<00:55,  2.79it/s][A
 31%|███       | 68/221 [00:18<00:47,  3.24it/s][A
 31%|███       | 69/221 [00:19<00:54,  2.77it/s][A
 32%|███▏      | 70/221 [00:19<00:54,  2.75it/s][A
 32%|███▏      | 71/221 [00:20<00:47,  3.16it/s][A
 33%|███▎      | 72/221 [00:20<00:54,  2.74it/s][A
 33%|███▎      | 73/221 [00:20<00:53,  2.78it/s][A
 33%|███▎      | 74/221 [00:21<00:43,  3.40it/s][A
 34%|███▍      | 75/221 [00:21<00:40,  3.63it/s][A
 34%|███▍      | 76/221 [00:21<00:51,  2.82it/s][A
 35%|███▍      | 77/221 [00:22<00:59,  2.44it/s][A
 35%|███▌      | 78/221 [00:22<00:56,  2.52it/s][A
 36%|███▌      | 79/221 [00:23<01:05,  2.18it/s][A
 36%|███▌      | 80/221 [00:23<00:59,  2.36it/s][A
 37%|███▋      | 81/221 [00:23<00:47,  2.96it/s][A
 37%|███▋      | 82/221 [00:24<00:44,  3.12it/s][A
 38%|███▊      | 83/221 [00:24<00:44,  3.08it/s][A
 38%|███▊      | 84/221 [00:24<00:40,  3.34it/s][A
 38%|███▊      | 85/221 [00:24<00:40,  3.39it/s][A
 39%|███▉      | 86/221 [00:25<00:47,  2.85it/s][A
 39%|███▉      | 87/221 [00:25<00:51,  2.61it/s][A
 40%|███▉      | 88/221 [00:26<00:54,  2.42it/s][A
 40%|████      | 89/221 [00:26<00:50,  2.61it/s][A
 41%|████      | 90/221 [00:26<00:44,  2.94it/s][A
 41%|████      | 91/221 [00:27<00:37,  3.48it/s][A
 42%|████▏     | 92/221 [00:27<00:36,  3.50it/s][A
 42%|████▏     | 93/221 [00:27<00:46,  2.75it/s][A
 43%|████▎     | 94/221 [00:28<00:45,  2.79it/s][A
 43%|████▎     | 95/221 [00:28<00:39,  3.22it/s][A
 43%|████▎     | 96/221 [00:28<00:38,  3.21it/s][A
 44%|████▍     | 97/221 [00:29<00:39,  3.18it/s][A
 44%|████▍     | 98/221 [00:29<00:45,  2.69it/s][A
 45%|████▍     | 99/221 [00:29<00:43,  2.80it/s][A
 45%|████▌     | 100/221 [00:30<00:40,  2.96it/s][A
 46%|████▌     | 101/221 [00:30<00:35,  3.38it/s][A
 46%|████▌     | 102/221 [00:30<00:31,  3.79it/s][A
 47%|████▋     | 103/221 [00:30<00:34,  3.44it/s][A
 47%|████▋     | 104/221 [00:31<00:42,  2.73it/s][A
 48%|████▊     | 105/221 [00:31<00:43,  2.67it/s][A
 48%|████▊     | 106/221 [00:32<00:39,  2.91it/s][A
 48%|████▊     | 107/221 [00:32<00:39,  2.90it/s][A
 49%|████▉     | 108/221 [00:32<00:38,  2.97it/s][A
 49%|████▉     | 109/221 [00:32<00:32,  3.40it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.34it/s][A
 50%|█████     | 111/221 [00:33<00:32,  3.38it/s][A
 51%|█████     | 112/221 [00:33<00:31,  3.50it/s][A
 51%|█████     | 113/221 [00:34<00:28,  3.81it/s][A
 52%|█████▏    | 114/221 [00:34<00:25,  4.18it/s][A
 52%|█████▏    | 115/221 [00:34<00:32,  3.23it/s][A
 52%|█████▏    | 116/221 [00:34<00:31,  3.33it/s][A
 53%|█████▎    | 117/221 [00:35<00:28,  3.67it/s][A
 53%|█████▎    | 118/221 [00:35<00:26,  3.90it/s][A
 54%|█████▍    | 119/221 [00:35<00:25,  3.93it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.35it/s][A
 55%|█████▍    | 121/221 [00:36<00:27,  3.64it/s][A
 55%|█████▌    | 122/221 [00:36<00:28,  3.46it/s][A
 56%|█████▌    | 123/221 [00:36<00:26,  3.65it/s][A
 56%|█████▌    | 124/221 [00:37<00:27,  3.58it/s][A
 57%|█████▋    | 125/221 [00:37<00:29,  3.24it/s][A
 57%|█████▋    | 126/221 [00:37<00:25,  3.78it/s][A
 57%|█████▋    | 127/221 [00:37<00:25,  3.70it/s][A
 58%|█████▊    | 128/221 [00:38<00:24,  3.74it/s][A
 58%|█████▊    | 129/221 [00:38<00:21,  4.19it/s][A
 59%|█████▉    | 130/221 [00:38<00:20,  4.36it/s][A
 59%|█████▉    | 131/221 [00:38<00:23,  3.91it/s][A
 60%|█████▉    | 132/221 [00:39<00:26,  3.37it/s][A
 60%|██████    | 133/221 [00:39<00:32,  2.74it/s][A
 61%|██████    | 134/221 [00:40<00:31,  2.79it/s][A
 61%|██████    | 135/221 [00:40<00:30,  2.85it/s][A
 62%|██████▏   | 136/221 [00:40<00:27,  3.12it/s][A
 62%|██████▏   | 137/221 [00:40<00:24,  3.45it/s][A
 62%|██████▏   | 138/221 [00:41<00:23,  3.58it/s][A
 63%|██████▎   | 139/221 [00:41<00:21,  3.77it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.84it/s][A
 64%|██████▍   | 141/221 [00:42<00:23,  3.39it/s][A
 64%|██████▍   | 142/221 [00:42<00:21,  3.60it/s][A
 65%|██████▍   | 143/221 [00:42<00:29,  2.66it/s][A
 65%|██████▌   | 144/221 [00:43<00:30,  2.56it/s][A
 66%|██████▌   | 145/221 [00:43<00:28,  2.71it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.32it/s][A
 67%|██████▋   | 147/221 [00:44<00:23,  3.19it/s][A
 67%|██████▋   | 148/221 [00:44<00:21,  3.32it/s][A
 67%|██████▋   | 149/221 [00:44<00:24,  2.97it/s][A
 68%|██████▊   | 150/221 [00:44<00:19,  3.56it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:45<00:21,  3.23it/s][A
 69%|██████▉   | 153/221 [00:45<00:16,  4.03it/s][A
 70%|██████▉   | 154/221 [00:46<00:21,  3.18it/s][A
 70%|███████   | 155/221 [00:46<00:21,  3.04it/s][A
 71%|███████   | 156/221 [00:46<00:20,  3.16it/s][A
 71%|███████   | 157/221 [00:47<00:20,  3.13it/s][A
 71%|███████▏  | 158/221 [00:47<00:26,  2.42it/s][A
 72%|███████▏  | 159/221 [00:48<00:22,  2.72it/s][A
 72%|███████▏  | 160/221 [00:48<00:22,  2.71it/s][A
 73%|███████▎  | 161/221 [00:49<00:24,  2.41it/s][A
 73%|███████▎  | 162/221 [00:49<00:21,  2.72it/s][A
 74%|███████▍  | 163/221 [00:49<00:21,  2.71it/s][A
 74%|███████▍  | 164/221 [00:49<00:16,  3.36it/s][A
 75%|███████▍  | 165/221 [00:50<00:15,  3.59it/s][A
 75%|███████▌  | 166/221 [00:50<00:13,  4.03it/s][A
 76%|███████▌  | 167/221 [00:50<00:17,  3.01it/s][A
 76%|███████▌  | 168/221 [00:50<00:16,  3.17it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.27it/s][A
 77%|███████▋  | 170/221 [00:51<00:14,  3.45it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.37it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.33it/s][A
 78%|███████▊  | 173/221 [00:52<00:17,  2.74it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.20it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.29it/s][A
 80%|███████▉  | 176/221 [00:53<00:12,  3.67it/s][A
 80%|████████  | 177/221 [00:53<00:10,  4.13it/s][A
 81%|████████  | 178/221 [00:53<00:10,  4.25it/s][A
 81%|████████  | 179/221 [00:53<00:09,  4.43it/s][A
 81%|████████▏ | 180/221 [00:54<00:08,  4.66it/s][A
 82%|████████▏ | 181/221 [00:54<00:09,  4.42it/s][A
 82%|████████▏ | 182/221 [00:54<00:09,  4.29it/s][A
 83%|████████▎ | 183/221 [00:55<00:10,  3.60it/s][A
 83%|████████▎ | 184/221 [00:55<00:12,  3.04it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.26it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.41it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.26it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.42it/s][A
 86%|████████▌ | 189/221 [00:56<00:08,  3.95it/s][A
 86%|████████▌ | 190/221 [00:56<00:07,  4.17it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.09it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.42it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.46it/s][A
 88%|████████▊ | 194/221 [00:58<00:07,  3.66it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.46it/s][A
 89%|████████▊ | 196/221 [00:59<00:09,  2.77it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.11it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  3.11it/s][A
 90%|█████████ | 199/221 [00:59<00:06,  3.65it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.47it/s][A
 91%|█████████ | 201/221 [01:00<00:05,  3.82it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.61it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  3.85it/s][A
 92%|█████████▏| 204/221 [01:01<00:04,  3.94it/s][A
 93%|█████████▎| 205/221 [01:01<00:03,  4.29it/s][A
 93%|█████████▎| 206/221 [01:01<00:03,  4.81it/s][A
 94%|█████████▎| 207/221 [01:01<00:02,  4.73it/s][A
 94%|█████████▍| 208/221 [01:01<00:02,  5.35it/s][A
 95%|█████████▍| 209/221 [01:01<00:02,  5.80it/s][A
 95%|█████████▌| 210/221 [01:02<00:01,  5.89it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  4.16it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.41it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  3.56it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.21it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.66it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.62it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.35it/s][A
 99%|█████████▊| 218/221 [01:04<00:01,  2.86it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  2.85it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.06it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.32it/s][A100%|██████████| 221/221 [01:05<00:00,  3.37it/s]
08/31/2024 04:21:00 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 17819--===========

08/31/2024 04:21:00 - INFO - __main__ -   {'area_r1': 5.3, 'area_recall': '5.3/14.1/19.3', 'area_ravg': 12.9}
08/31/2024 04:21:00 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 17819--===========

08/31/2024 04:21:00 - INFO - __main__ -   {'area_r1': 39.8, 'area_recall': '39.8/72.3/82.4', 'area_ravg': 64.8}
08/31/2024 04:21:00 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 17819--===========

08/31/2024 04:21:00 - INFO - __main__ -   {'video_r1': 34.6, 'video_recall': '34.6/66.0/76.5', 'video_ravg': 59.0}
08/31/2024 04:21:00 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 3563=======

08/31/2024 04:21:00 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/67.2/77.6', 'video_ravg': 60.6}
08/31/2024 04:21:00 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 17819--===========

08/31/2024 04:21:00 - INFO - __main__ -   {'video_r1': 56.4, 'video_recall': '56.4/79.4/86.0', 'video_ravg': 73.9}
08/31/2024 04:21:00 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 7127=======

08/31/2024 04:21:00 - INFO - __main__ -   {'video_r1': 57.2, 'video_recall': '57.2/79.4/85.9', 'video_ravg': 74.2}
100%|█████████▉| 17820/17834 [9:07:03<11:54, 51.05s/it]100%|█████████▉| 17821/17834 [9:07:05<07:50, 36.22s/it]100%|█████████▉| 17822/17834 [9:07:06<05:10, 25.87s/it]100%|█████████▉| 17823/17834 [9:07:08<03:24, 18.63s/it]100%|█████████▉| 17824/17834 [9:07:10<02:15, 13.56s/it]100%|█████████▉| 17825/17834 [9:07:12<01:30, 10.02s/it]100%|█████████▉| 17826/17834 [9:07:13<01:00,  7.54s/it]100%|█████████▉| 17827/17834 [9:07:15<00:40,  5.80s/it]100%|█████████▉| 17828/17834 [9:07:17<00:27,  4.58s/it]100%|█████████▉| 17829/17834 [9:07:19<00:18,  3.72s/it]100%|█████████▉| 17830/17834 [9:07:20<00:12,  3.13s/it]100%|█████████▉| 17831/17834 [9:07:22<00:08,  2.72s/it]100%|█████████▉| 17832/17834 [9:07:24<00:04,  2.41s/it]100%|█████████▉| 17833/17834 [9:07:26<00:02,  2.24s/it]100%|██████████| 17834/17834 [9:07:27<00:00,  2.08s/it]100%|██████████| 17834/17834 [9:07:27<00:00,  1.84s/it]
wandb: 
wandb: Run history:
wandb:  loss_area ▅█▇▂▂▂▄▃▃▂▁▂▂▃▂▂▂▂▂▃▂▂▁▁▁▁▂▂▁▂▂▂▁▁▁▁▁▃▂▄
wandb:   loss_itc ▅█▇▅▄▄▅▅▅▃▂▂▃▃▂▂▂▂▄▄▄▂▂▁▂▃▂▂▂▂▂▂▂▂▁▁▁▃▂▄
wandb:   loss_itm ▄██▆▆▅▅▆▇▅▃▄▅▇▅▄▄▃▄▄▄▄▄▃▃▃▄▃▁▄▃▄▂▃▃▂▁▃▃▅
wandb: total_loss ▅█▇▄▄▃▅▅▄▃▂▂▃▃▂▂▂▂▃▄▃▂▂▁▁▂▂▂▁▂▂▂▂▁▁▁▁▃▂▄
wandb: 
wandb: Run summary:
wandb:  loss_area 2.19641
wandb:   loss_itc 1.2079
wandb:   loss_itm 0.03474
wandb: total_loss 3.43905
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/wandb/offline-run-20240830_191326-nvbgx8gz
wandb: Find logs at: ./wandb/offline-run-20240830_191326-nvbgx8gz/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
08/31/2024 04:21:49 - WARNING - urllib3.connectionpool -   Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x15421ea1f0a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
08/31/2024 04:21:49 - WARNING - urllib3.connectionpool -   Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x15421ea246a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
