NODELIST=lrdn3226
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
2130



DEVICE SET
DEVICE SET
DEVICE SET
DEVICE SET
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/10/2024 13:39:00 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/10/2024 13:39:00 - INFO - __main__ -   ==================model_configs==================

09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_model_type : vast
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_frozen_vision : False
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_frozen_audio : False
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_checkpointing : True
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_max_caption_len : 40
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_contra_dim : 512
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_vision_resolution : 224
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_audio_melbins : 64
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_audio_target_length : 1024
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_beam_size : 3
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_captioner_mode : False
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_generate_nums : 1
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_max_vision_sample_num : 2
09/10/2024 13:39:00 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
09/10/2024 13:39:00 - INFO - __main__ -   ==================run_configs==================

09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_checkpoint : 
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolumeAndvideoItm
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_optim : adamw
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_weight_decay : 0.01
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_grad_norm : 2.0
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_resume : False
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_seed : 50
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_fp16 : True
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_bf16 : False
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_zero_shot : False
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_new_lr : 0
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_new_params_name : []
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_valid_freq : 10
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_dataset_mix_type : random
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_first_eval : True
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_num_train_steps : 0
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_save_best : True
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_pin_mem : True
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_vision_resolution : 224
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_use_ddp : False
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_mode : training
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_log_steps : 100
09/10/2024 13:39:00 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
09/10/2024 13:39:00 - INFO - __main__ -   ==================data_configs==================

09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_type : annoindexed
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_training : True
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_name : finetune_area
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_txt : ../vast27m/annotations100k.json
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_vision : ../vast27m/videos/
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_audio : ../vast27m/audios
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_vision_transforms : crop_flip
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_vision_format : video_rawvideo
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_vision_sample_num : 2
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_audio_sample_num : 1
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_task : ret%tv%ta
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_epoch : 5
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_n_workers : 8
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_finetune_area_train_batch_size : 256
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 8
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tv
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
09/10/2024 13:39:00 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
09/10/2024 13:39:04 - INFO - __main__ -   finetune_area Using clip mean and std.
09/10/2024 13:39:04 - INFO - __main__ -   finetune_area transforms crop_flip
ci sono 99621 labelsci sono 99621 labels
ci sono 99621 labelsci sono 99621 labels


09/10/2024 13:39:45 - INFO - __main__ -   Create Dataset finetune_area Success
09/10/2024 13:39:45 - INFO - __main__ -    loader ret%tv%ta--finetune_area , ratio 1945 , bs_pergpu 64, n_workers 8
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x562f09a95240] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/10/2024 13:39:49 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/10/2024 13:39:49 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 884 labels
ci sono 884 labels
09/10/2024 13:39:49 - INFO - __main__ -   Create Dataset msrvtt_ret Success
ci sono 884 labels
ci sono 884 labels
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
09/10/2024 13:39:52 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
09/10/2024 13:39:52 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/10/2024 13:39:52 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/10/2024 13:39:52 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564c224cab80] mmco: unref short failure
[h264 @ 0x561aa57f5100] mmco: unref short failure
[h264 @ 0x561aa57f5100] mmco: unref short failure
[h264 @ 0x55cb76a2d4c0] mmco: unref short failure
[h264 @ 0x55cb76a2d4c0] mmco: unref short failure
[h264 @ 0x55cb76c3e900] mmco: unref short failure
[h264 @ 0x55cb76a2fa00] mmco: unref short failure
[h264 @ 0x55cb76a2fa00] mmco: unref short failure
[h264 @ 0x561aa599b580] mmco: unref short failure
[h264 @ 0x564c22e4df80] mmco: unref short failure
[h264 @ 0x55cb76d3b180] mmco: unref short failure
[h264 @ 0x55cb76d3b180] mmco: unref short failure
09/10/2024 13:40:58 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/10/2024 13:40:59 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/10/2024 13:41:03 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
[h264 @ 0x55cb781cc180] mmco: unref short failure
09/10/2024 13:41:12 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/10/2024 13:41:14 - INFO - root -   incompatible_keys.missing_keys: []
09/10/2024 13:41:14 - INFO - root -   incompatible_keys.missing_keys: []
09/10/2024 13:41:15 - INFO - root -   incompatible_keys.missing_keys: []
09/10/2024 13:41:16 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/10/2024 13:41:16 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/10/2024 13:41:17 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
[h264 @ 0x561aa5962ac0] mmco: unref short failure
[h264 @ 0x561aa5962ac0] mmco: unref short failure
[h264 @ 0x55cb76ae5ec0] mmco: unref short failure
[h264 @ 0x55cb76ae5ec0] mmco: unref short failure
09/10/2024 13:41:23 - INFO - root -   incompatible_keys.missing_keys: []
[h264 @ 0x561aa7afac40] mmco: unref short failure
[h264 @ 0x561aa7afac40] mmco: unref short failure
09/10/2024 13:41:24 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/10/2024 13:41:27 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/10/2024 13:41:29 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/10/2024 13:41:30 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
[h264 @ 0x562f0bf42e80] mmco: unref short failure
[h264 @ 0x562f0c957040] mmco: unref short failure
09/10/2024 13:41:36 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.value.bias', 'cls.predictions.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.value.weight', 'cls.predictions.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x562f0ce73180] mmco: unref short failure
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'cls.predictions.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x564c2d491440] mmco: unref short failure
[h264 @ 0x564c2d491440] mmco: unref short failure
[h264 @ 0x562f0ae51a40] mmco: unref short failure
[h264 @ 0x562f0ae51a40] mmco: unref short failure
[h264 @ 0x561aa75b6080] mmco: unref short failure
09/10/2024 13:42:05 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/ckpt/model_step_204994.pt
09/10/2024 13:42:05 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast
[h264 @ 0x564c2b35fcc0] mmco: unref short failure
09/10/2024 13:42:12 - INFO - __main__ -   Unexpected keys ['vision_encoder.text.logit_scale']
09/10/2024 13:42:12 - INFO - __main__ -   missing_keys  ['vision_encoder.logit_scale']
[h264 @ 0x564c23ae3300] mmco: unref short failure
[h264 @ 0x564c23ae3300] mmco: unref short failure
09/10/2024 13:42:20 - INFO - __main__ -   ==================learning_rate_settings==================

09/10/2024 13:42:20 - INFO - __main__ -     basic_lr : 2e-05
09/10/2024 13:42:20 - INFO - __main__ -     clip_lr_visual : 5e-07
09/10/2024 13:42:20 - INFO - __main__ -     clip_lr_visual_len : 245
09/10/2024 13:42:20 - INFO - __main__ -     new_lr : 0
09/10/2024 13:42:20 - INFO - __main__ -     new_params_name: []
09/10/2024 13:42:20 - INFO - __main__ -   evaluate on ret%tv--msrvtt_ret task
09/10/2024 13:42:20 - INFO - __main__ -   start running ret%tv validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x561aa8753540] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb7c8edf40] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564c2281c980] mmco: unref short failure
[h264 @ 0x564c2281c980] mmco: unref short failure
[h264 @ 0x564c259ba000] mmco: unref short failure
[h264 @ 0x564c259ba000] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[h264 @ 0x55cb80179440] mmco: unref short failure
[h264 @ 0x55cb80179440] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[h264 @ 0x564c28ac0400] mmco: unref short failure
[h264 @ 0x564c28ac0400] mmco: unref short failure
[h264 @ 0x562f0c259580] mmco: unref short failure
[h264 @ 0x562f0c259580] mmco: unref short failure
[h264 @ 0x55cb78bcacc0] mmco: unref short failure
[h264 @ 0x562f125d2e40] mmco: unref short failure
[h264 @ 0x561aad38a100] mmco: unref short failure
[h264 @ 0x561aad38a100] mmco: unref short failure
[h264 @ 0x562f0afe9780] mmco: unref short failure
[h264 @ 0x562f0afe9780] mmco: unref short failure
[h264 @ 0x562f11fdd280] mmco: unref short failure
[h264 @ 0x561aa4f363c0] mmco: unref short failure
[h264 @ 0x561aa4f363c0] mmco: unref short failure
[h264 @ 0x55cb8104e280] mmco: unref short failure
[h264 @ 0x564c228ef4c0] mmco: unref short failure
[h264 @ 0x564c2a03e380] mmco: unref short failure
[h264 @ 0x564c2a03e380] mmco: unref short failure
[h264 @ 0x564c2a03e380] mmco: unref short failure
[h264 @ 0x564c2a03e380] mmco: unref short failure
[h264 @ 0x561aa5a9e400] mmco: unref short failure
[h264 @ 0x55cb81701d40] mmco: unref short failure
[h264 @ 0x55cb81701d40] mmco: unref short failure
[h264 @ 0x564c2a848b40] mmco: unref short failure
[h264 @ 0x564c2a848b40] mmco: unref short failure
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<00:23,  9.28it/s]  1%|          | 2/221 [00:00<00:49,  4.47it/s]  1%|▏         | 3/221 [00:00<01:07,  3.24it/s]  2%|▏         | 4/221 [00:00<00:51,  4.22it/s]  2%|▏         | 5/221 [00:01<00:46,  4.69it/s]  3%|▎         | 7/221 [00:01<00:42,  5.02it/s][h264 @ 0x561aaca33b80] mmco: unref short failure
[h264 @ 0x561aaca33b80] mmco: unref short failure
  4%|▎         | 8/221 [00:01<00:49,  4.27it/s]  4%|▍         | 9/221 [00:01<00:43,  4.89it/s]  5%|▍         | 10/221 [00:02<00:50,  4.17it/s]  5%|▍         | 11/221 [00:02<00:51,  4.06it/s]  5%|▌         | 12/221 [00:02<01:02,  3.33it/s]  6%|▌         | 13/221 [00:03<01:09,  2.98it/s]  6%|▋         | 14/221 [00:03<01:04,  3.21it/s]  7%|▋         | 15/221 [00:03<00:59,  3.46it/s]  7%|▋         | 16/221 [00:04<01:00,  3.39it/s]  8%|▊         | 17/221 [00:04<01:14,  2.75it/s]  8%|▊         | 18/221 [00:04<01:06,  3.05it/s]  9%|▊         | 19/221 [00:05<01:04,  3.14it/s]  9%|▉         | 20/221 [00:05<00:54,  3.71it/s] 10%|▉         | 21/221 [00:05<00:53,  3.73it/s] 10%|▉         | 22/221 [00:06<01:06,  2.99it/s] 11%|█         | 24/221 [00:06<00:51,  3.86it/s] 11%|█▏        | 25/221 [00:06<00:51,  3.84it/s] 12%|█▏        | 26/221 [00:07<00:52,  3.73it/s] 12%|█▏        | 27/221 [00:07<00:47,  4.12it/s] 13%|█▎        | 28/221 [00:07<00:58,  3.29it/s] 13%|█▎        | 29/221 [00:08<00:59,  3.25it/s] 14%|█▎        | 30/221 [00:08<00:56,  3.40it/s] 14%|█▍        | 31/221 [00:08<00:53,  3.57it/s] 14%|█▍        | 32/221 [00:08<00:43,  4.35it/s] 15%|█▍        | 33/221 [00:08<00:40,  4.65it/s] 15%|█▌        | 34/221 [00:09<00:38,  4.83it/s] 16%|█▌        | 35/221 [00:09<00:43,  4.25it/s] 16%|█▋        | 36/221 [00:09<00:44,  4.13it/s] 17%|█▋        | 37/221 [00:09<00:43,  4.27it/s] 17%|█▋        | 38/221 [00:09<00:41,  4.38it/s] 18%|█▊        | 39/221 [00:10<00:42,  4.32it/s] 18%|█▊        | 40/221 [00:10<00:46,  3.90it/s] 19%|█▊        | 41/221 [00:10<00:47,  3.81it/s] 19%|█▉        | 42/221 [00:10<00:39,  4.51it/s] 19%|█▉        | 43/221 [00:11<00:36,  4.91it/s] 20%|█▉        | 44/221 [00:11<00:33,  5.29it/s] 20%|██        | 45/221 [00:11<00:42,  4.18it/s] 21%|██        | 46/221 [00:11<00:45,  3.87it/s] 21%|██▏       | 47/221 [00:12<00:53,  3.27it/s] 22%|██▏       | 48/221 [00:12<00:43,  3.95it/s] 22%|██▏       | 49/221 [00:12<00:40,  4.28it/s] 23%|██▎       | 50/221 [00:13<00:47,  3.57it/s] 23%|██▎       | 51/221 [00:13<00:41,  4.12it/s] 24%|██▎       | 52/221 [00:13<00:39,  4.27it/s] 24%|██▍       | 53/221 [00:13<00:39,  4.29it/s] 24%|██▍       | 54/221 [00:14<00:55,  2.99it/s] 25%|██▍       | 55/221 [00:14<00:55,  2.98it/s] 25%|██▌       | 56/221 [00:14<00:49,  3.33it/s] 26%|██▌       | 57/221 [00:14<00:44,  3.68it/s] 26%|██▌       | 58/221 [00:15<00:43,  3.79it/s] 27%|██▋       | 59/221 [00:15<00:37,  4.32it/s] 27%|██▋       | 60/221 [00:15<00:37,  4.35it/s] 28%|██▊       | 61/221 [00:15<00:36,  4.37it/s] 28%|██▊       | 62/221 [00:16<00:36,  4.36it/s] 29%|██▊       | 63/221 [00:16<00:34,  4.52it/s] 29%|██▉       | 64/221 [00:16<00:35,  4.37it/s] 29%|██▉       | 65/221 [00:16<00:33,  4.61it/s] 30%|██▉       | 66/221 [00:17<01:01,  2.52it/s] 30%|███       | 67/221 [00:17<00:58,  2.64it/s] 31%|███       | 68/221 [00:18<00:48,  3.17it/s] 31%|███       | 69/221 [00:18<01:00,  2.49it/s] 32%|███▏      | 70/221 [00:18<00:53,  2.80it/s] 32%|███▏      | 71/221 [00:19<00:45,  3.29it/s] 33%|███▎      | 72/221 [00:19<00:47,  3.16it/s] 33%|███▎      | 73/221 [00:19<00:41,  3.56it/s] 34%|███▍      | 75/221 [00:19<00:34,  4.29it/s] 34%|███▍      | 76/221 [00:20<00:31,  4.57it/s] 35%|███▍      | 77/221 [00:20<00:37,  3.88it/s] 35%|███▌      | 78/221 [00:20<00:34,  4.20it/s] 36%|███▌      | 79/221 [00:21<00:49,  2.84it/s] 36%|███▌      | 80/221 [00:21<00:48,  2.93it/s] 37%|███▋      | 81/221 [00:21<00:39,  3.55it/s] 37%|███▋      | 82/221 [00:22<00:43,  3.22it/s] 38%|███▊      | 83/221 [00:22<00:40,  3.43it/s] 38%|███▊      | 84/221 [00:22<00:36,  3.75it/s] 39%|███▉      | 86/221 [00:22<00:27,  4.90it/s] 39%|███▉      | 87/221 [00:23<00:26,  5.10it/s] 40%|███▉      | 88/221 [00:23<00:36,  3.65it/s] 40%|████      | 89/221 [00:23<00:41,  3.21it/s] 41%|████      | 90/221 [00:24<00:36,  3.57it/s] 41%|████      | 91/221 [00:24<00:31,  4.09it/s] 42%|████▏     | 92/221 [00:24<00:35,  3.64it/s] 42%|████▏     | 93/221 [00:25<00:45,  2.83it/s] 43%|████▎     | 94/221 [00:25<00:40,  3.11it/s] 43%|████▎     | 95/221 [00:25<00:37,  3.32it/s] 43%|████▎     | 96/221 [00:25<00:37,  3.32it/s] 44%|████▍     | 97/221 [00:26<00:34,  3.54it/s] 44%|████▍     | 98/221 [00:26<00:38,  3.17it/s] 45%|████▍     | 99/221 [00:26<00:38,  3.19it/s] 45%|████▌     | 100/221 [00:27<00:35,  3.41it/s] 46%|████▌     | 101/221 [00:27<00:32,  3.64it/s] 46%|████▌     | 102/221 [00:27<00:38,  3.10it/s] 47%|████▋     | 103/221 [00:28<00:32,  3.60it/s] 47%|████▋     | 104/221 [00:28<00:31,  3.69it/s] 48%|████▊     | 105/221 [00:28<00:34,  3.37it/s] 48%|████▊     | 106/221 [00:28<00:35,  3.27it/s] 48%|████▊     | 107/221 [00:29<00:34,  3.26it/s] 49%|████▉     | 108/221 [00:29<00:32,  3.49it/s] 49%|████▉     | 109/221 [00:29<00:27,  4.05it/s] 50%|████▉     | 110/221 [00:29<00:27,  3.99it/s] 50%|█████     | 111/221 [00:30<00:31,  3.44it/s] 51%|█████     | 112/221 [00:30<00:31,  3.49it/s] 51%|█████     | 113/221 [00:30<00:28,  3.84it/s] 52%|█████▏    | 114/221 [00:30<00:22,  4.70it/s] 52%|█████▏    | 115/221 [00:31<00:24,  4.27it/s] 52%|█████▏    | 116/221 [00:31<00:40,  2.57it/s] 53%|█████▎    | 117/221 [00:32<00:37,  2.74it/s] 53%|█████▎    | 118/221 [00:32<00:32,  3.15it/s] 54%|█████▍    | 119/221 [00:32<00:31,  3.25it/s] 54%|█████▍    | 120/221 [00:32<00:28,  3.50it/s] 55%|█████▍    | 121/221 [00:33<00:23,  4.33it/s] 55%|█████▌    | 122/221 [00:33<00:22,  4.45it/s] 56%|█████▌    | 123/221 [00:33<00:22,  4.40it/s] 56%|█████▌    | 124/221 [00:33<00:23,  4.19it/s] 57%|█████▋    | 125/221 [00:34<00:29,  3.29it/s] 57%|█████▋    | 126/221 [00:34<00:27,  3.43it/s] 57%|█████▋    | 127/221 [00:35<00:33,  2.78it/s] 58%|█████▊    | 128/221 [00:35<00:33,  2.81it/s] 58%|█████▊    | 129/221 [00:35<00:27,  3.37it/s] 59%|█████▉    | 130/221 [00:35<00:24,  3.72it/s] 60%|█████▉    | 132/221 [00:36<00:21,  4.15it/s] 60%|██████    | 133/221 [00:36<00:24,  3.55it/s] 61%|██████    | 134/221 [00:36<00:26,  3.23it/s] 61%|██████    | 135/221 [00:37<00:27,  3.16it/s] 62%|██████▏   | 136/221 [00:37<00:28,  2.99it/s] 62%|██████▏   | 137/221 [00:38<00:30,  2.76it/s] 62%|██████▏   | 138/221 [00:38<00:31,  2.67it/s] 63%|██████▎   | 139/221 [00:38<00:31,  2.60it/s] 63%|██████▎   | 140/221 [00:39<00:30,  2.66it/s] 64%|██████▍   | 141/221 [00:39<00:29,  2.74it/s] 64%|██████▍   | 142/221 [00:39<00:25,  3.14it/s] 65%|██████▍   | 143/221 [00:40<00:23,  3.26it/s] 65%|██████▌   | 144/221 [00:40<00:23,  3.31it/s] 66%|██████▌   | 145/221 [00:40<00:19,  3.86it/s] 66%|██████▌   | 146/221 [00:40<00:16,  4.43it/s] 67%|██████▋   | 147/221 [00:40<00:18,  4.02it/s] 67%|██████▋   | 148/221 [00:41<00:22,  3.24it/s] 67%|██████▋   | 149/221 [00:41<00:21,  3.41it/s] 68%|██████▊   | 150/221 [00:41<00:18,  3.83it/s] 68%|██████▊   | 151/221 [00:42<00:23,  3.02it/s] 69%|██████▉   | 152/221 [00:42<00:27,  2.55it/s] 69%|██████▉   | 153/221 [00:43<00:22,  2.99it/s] 70%|██████▉   | 154/221 [00:43<00:19,  3.49it/s] 70%|███████   | 155/221 [00:43<00:18,  3.50it/s] 71%|███████   | 156/221 [00:43<00:18,  3.45it/s] 71%|███████   | 157/221 [00:44<00:29,  2.18it/s] 71%|███████▏  | 158/221 [00:44<00:25,  2.46it/s] 72%|███████▏  | 159/221 [00:45<00:20,  3.04it/s] 72%|███████▏  | 160/221 [00:45<00:17,  3.56it/s] 73%|███████▎  | 161/221 [00:45<00:14,  4.08it/s] 73%|███████▎  | 162/221 [00:45<00:12,  4.54it/s] 74%|███████▍  | 163/221 [00:45<00:14,  4.08it/s] 74%|███████▍  | 164/221 [00:46<00:14,  3.86it/s] 75%|███████▍  | 165/221 [00:46<00:15,  3.52it/s] 75%|███████▌  | 166/221 [00:47<00:21,  2.61it/s] 76%|███████▌  | 167/221 [00:47<00:16,  3.26it/s] 76%|███████▌  | 168/221 [00:47<00:17,  3.08it/s] 76%|███████▋  | 169/221 [00:47<00:14,  3.67it/s] 77%|███████▋  | 170/221 [00:48<00:20,  2.52it/s] 77%|███████▋  | 171/221 [00:48<00:17,  2.89it/s] 78%|███████▊  | 172/221 [00:48<00:15,  3.22it/s] 78%|███████▊  | 173/221 [00:49<00:14,  3.20it/s] 79%|███████▊  | 174/221 [00:49<00:14,  3.23it/s] 79%|███████▉  | 175/221 [00:49<00:13,  3.29it/s] 80%|███████▉  | 176/221 [00:50<00:12,  3.52it/s] 80%|████████  | 177/221 [00:50<00:11,  3.83it/s] 81%|████████  | 178/221 [00:50<00:13,  3.09it/s] 81%|████████  | 179/221 [00:51<00:14,  2.86it/s] 81%|████████▏ | 180/221 [00:51<00:12,  3.37it/s] 82%|████████▏ | 181/221 [00:51<00:09,  4.06it/s] 82%|████████▏ | 182/221 [00:51<00:11,  3.46it/s] 83%|████████▎ | 183/221 [00:52<00:11,  3.44it/s] 83%|████████▎ | 184/221 [00:52<00:09,  3.85it/s] 84%|████████▎ | 185/221 [00:52<00:08,  4.32it/s] 84%|████████▍ | 186/221 [00:52<00:09,  3.84it/s] 85%|████████▍ | 187/221 [00:53<00:09,  3.57it/s] 85%|████████▌ | 188/221 [00:53<00:08,  3.67it/s] 86%|████████▌ | 189/221 [00:53<00:08,  3.85it/s] 86%|████████▌ | 190/221 [00:54<00:08,  3.57it/s] 86%|████████▋ | 191/221 [00:54<00:06,  4.36it/s] 87%|████████▋ | 192/221 [00:54<00:06,  4.31it/s] 88%|████████▊ | 194/221 [00:54<00:05,  5.14it/s] 88%|████████▊ | 195/221 [00:54<00:05,  4.94it/s] 89%|████████▊ | 196/221 [00:55<00:05,  4.77it/s] 89%|████████▉ | 197/221 [00:55<00:04,  4.84it/s] 90%|████████▉ | 198/221 [00:55<00:05,  4.12it/s] 90%|█████████ | 199/221 [00:55<00:05,  4.08it/s] 90%|█████████ | 200/221 [00:56<00:05,  3.53it/s] 91%|█████████ | 201/221 [00:56<00:05,  3.70it/s] 91%|█████████▏| 202/221 [00:56<00:05,  3.27it/s] 92%|█████████▏| 203/221 [00:57<00:04,  3.63it/s] 92%|█████████▏| 204/221 [00:57<00:04,  3.88it/s] 93%|█████████▎| 205/221 [00:57<00:03,  4.60it/s] 93%|█████████▎| 206/221 [00:57<00:04,  3.66it/s] 94%|█████████▎| 207/221 [00:58<00:03,  3.77it/s] 94%|█████████▍| 208/221 [00:58<00:03,  3.33it/s] 95%|█████████▍| 209/221 [00:58<00:03,  3.28it/s] 95%|█████████▌| 210/221 [00:58<00:02,  3.96it/s] 95%|█████████▌| 211/221 [00:59<00:02,  3.44it/s] 96%|█████████▌| 212/221 [00:59<00:02,  3.49it/s] 96%|█████████▋| 213/221 [00:59<00:02,  3.93it/s] 97%|█████████▋| 214/221 [01:00<00:02,  3.09it/s] 97%|█████████▋| 215/221 [01:00<00:01,  3.54it/s] 98%|█████████▊| 216/221 [01:00<00:01,  3.61it/s] 98%|█████████▊| 217/221 [01:01<00:01,  2.87it/s] 99%|█████████▊| 218/221 [01:01<00:00,  3.11it/s] 99%|█████████▉| 219/221 [01:01<00:00,  3.07it/s]100%|█████████▉| 220/221 [01:02<00:00,  2.18it/s]100%|██████████| 221/221 [01:02<00:00,  2.45it/s]100%|██████████| 221/221 [01:02<00:00,  3.52it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<00:58,  3.79it/s]  1%|          | 2/221 [00:00<00:57,  3.79it/s]  1%|▏         | 3/221 [00:00<00:57,  3.79it/s]  2%|▏         | 4/221 [00:01<00:57,  3.79it/s]  2%|▏         | 5/221 [00:01<00:57,  3.79it/s]  3%|▎         | 6/221 [00:01<00:56,  3.79it/s]  3%|▎         | 7/221 [00:01<00:56,  3.79it/s]  4%|▎         | 8/221 [00:02<00:56,  3.79it/s]  4%|▍         | 9/221 [00:02<00:55,  3.79it/s]  5%|▍         | 10/221 [00:02<00:55,  3.79it/s]  5%|▍         | 11/221 [00:02<00:55,  3.79it/s]  5%|▌         | 12/221 [00:03<00:55,  3.79it/s]  6%|▌         | 13/221 [00:03<00:54,  3.79it/s]  6%|▋         | 14/221 [00:03<00:54,  3.79it/s]  7%|▋         | 15/221 [00:03<00:54,  3.79it/s]  7%|▋         | 16/221 [00:04<00:54,  3.79it/s]  8%|▊         | 17/221 [00:04<00:53,  3.79it/s]  8%|▊         | 18/221 [00:04<00:53,  3.79it/s]  9%|▊         | 19/221 [00:05<00:53,  3.79it/s]  9%|▉         | 20/221 [00:05<00:53,  3.79it/s] 10%|▉         | 21/221 [00:05<00:52,  3.79it/s] 10%|▉         | 22/221 [00:05<00:52,  3.79it/s] 10%|█         | 23/221 [00:06<00:52,  3.79it/s] 11%|█         | 24/221 [00:06<00:51,  3.79it/s] 11%|█▏        | 25/221 [00:06<00:51,  3.79it/s] 12%|█▏        | 26/221 [00:06<00:51,  3.79it/s] 12%|█▏        | 27/221 [00:07<00:51,  3.79it/s] 13%|█▎        | 28/221 [00:07<00:50,  3.79it/s] 13%|█▎        | 29/221 [00:07<00:50,  3.79it/s] 14%|█▎        | 30/221 [00:07<00:50,  3.79it/s] 14%|█▍        | 31/221 [00:08<00:50,  3.79it/s] 14%|█▍        | 32/221 [00:08<00:49,  3.79it/s] 15%|█▍        | 33/221 [00:08<00:49,  3.79it/s] 15%|█▌        | 34/221 [00:08<00:49,  3.79it/s] 16%|█▌        | 35/221 [00:09<00:49,  3.79it/s] 16%|█▋        | 36/221 [00:09<00:48,  3.79it/s] 17%|█▋        | 37/221 [00:09<00:48,  3.79it/s] 17%|█▋        | 38/221 [00:10<00:48,  3.79it/s] 18%|█▊        | 39/221 [00:10<00:48,  3.79it/s] 18%|█▊        | 40/221 [00:10<00:47,  3.79it/s] 19%|█▊        | 41/221 [00:10<00:47,  3.79it/s] 19%|█▉        | 42/221 [00:11<00:47,  3.79it/s] 19%|█▉        | 43/221 [00:11<00:46,  3.79it/s] 20%|█▉        | 44/221 [00:11<00:46,  3.79it/s] 20%|██        | 45/221 [00:11<00:46,  3.79it/s] 21%|██        | 46/221 [00:12<00:46,  3.79it/s] 21%|██▏       | 47/221 [00:12<00:45,  3.79it/s] 22%|██▏       | 48/221 [00:12<00:45,  3.79it/s] 22%|██▏       | 49/221 [00:12<00:45,  3.79it/s] 23%|██▎       | 50/221 [00:13<00:45,  3.79it/s] 23%|██▎       | 51/221 [00:13<00:44,  3.79it/s] 24%|██▎       | 52/221 [00:13<00:44,  3.79it/s] 24%|██▍       | 53/221 [00:13<00:44,  3.79it/s] 24%|██▍       | 54/221 [00:14<00:44,  3.79it/s] 25%|██▍       | 55/221 [00:14<00:43,  3.79it/s] 25%|██▌       | 56/221 [00:14<00:43,  3.79it/s] 26%|██▌       | 57/221 [00:15<00:43,  3.79it/s] 26%|██▌       | 58/221 [00:15<00:42,  3.79it/s] 27%|██▋       | 59/221 [00:15<00:42,  3.79it/s] 27%|██▋       | 60/221 [00:15<00:42,  3.79it/s] 28%|██▊       | 61/221 [00:16<00:42,  3.79it/s] 28%|██▊       | 62/221 [00:16<00:41,  3.79it/s] 29%|██▊       | 63/221 [00:16<00:41,  3.79it/s] 29%|██▉       | 64/221 [00:16<00:41,  3.79it/s] 29%|██▉       | 65/221 [00:17<00:41,  3.79it/s] 30%|██▉       | 66/221 [00:17<00:40,  3.79it/s] 30%|███       | 67/221 [00:17<00:40,  3.79it/s] 31%|███       | 68/221 [00:17<00:40,  3.79it/s] 31%|███       | 69/221 [00:18<00:40,  3.79it/s] 32%|███▏      | 70/221 [00:18<00:39,  3.79it/s] 32%|███▏      | 71/221 [00:18<00:39,  3.79it/s] 33%|███▎      | 72/221 [00:18<00:39,  3.79it/s] 33%|███▎      | 73/221 [00:19<00:39,  3.79it/s] 33%|███▎      | 74/221 [00:19<00:38,  3.79it/s] 34%|███▍      | 75/221 [00:19<00:38,  3.79it/s] 34%|███▍      | 76/221 [00:20<00:38,  3.79it/s] 35%|███▍      | 77/221 [00:20<00:37,  3.79it/s] 35%|███▌      | 78/221 [00:20<00:37,  3.79it/s] 36%|███▌      | 79/221 [00:20<00:37,  3.79it/s] 36%|███▌      | 80/221 [00:21<00:37,  3.79it/s] 37%|███▋      | 81/221 [00:21<00:36,  3.79it/s] 37%|███▋      | 82/221 [00:21<00:36,  3.79it/s] 38%|███▊      | 83/221 [00:21<00:36,  3.79it/s] 38%|███▊      | 84/221 [00:22<00:36,  3.79it/s] 38%|███▊      | 85/221 [00:22<00:35,  3.79it/s] 39%|███▉      | 86/221 [00:22<00:35,  3.79it/s] 39%|███▉      | 87/221 [00:22<00:35,  3.79it/s] 40%|███▉      | 88/221 [00:23<00:35,  3.79it/s] 40%|████      | 89/221 [00:23<00:34,  3.79it/s] 41%|████      | 90/221 [00:23<00:34,  3.79it/s] 41%|████      | 91/221 [00:24<00:34,  3.79it/s] 42%|████▏     | 92/221 [00:24<00:34,  3.79it/s] 42%|████▏     | 93/221 [00:24<00:33,  3.79it/s] 43%|████▎     | 94/221 [00:24<00:33,  3.79it/s] 43%|████▎     | 95/221 [00:25<00:33,  3.79it/s] 43%|████▎     | 96/221 [00:25<00:32,  3.79it/s] 44%|████▍     | 97/221 [00:25<00:32,  3.79it/s] 44%|████▍     | 98/221 [00:25<00:32,  3.79it/s] 45%|████▍     | 99/221 [00:26<00:32,  3.79it/s] 45%|████▌     | 100/221 [00:26<00:31,  3.79it/s] 46%|████▌     | 101/221 [00:26<00:31,  3.79it/s] 46%|████▌     | 102/221 [00:26<00:31,  3.79it/s] 47%|████▋     | 103/221 [00:27<00:31,  3.79it/s] 47%|████▋     | 104/221 [00:27<00:30,  3.79it/s] 48%|████▊     | 105/221 [00:27<00:30,  3.79it/s] 48%|████▊     | 106/221 [00:27<00:30,  3.79it/s] 48%|████▊     | 107/221 [00:28<00:30,  3.79it/s] 49%|████▉     | 108/221 [00:28<00:29,  3.79it/s] 49%|████▉     | 109/221 [00:28<00:29,  3.79it/s] 50%|████▉     | 110/221 [00:29<00:29,  3.79it/s] 50%|█████     | 111/221 [00:29<00:29,  3.79it/s] 51%|█████     | 112/221 [00:29<00:28,  3.79it/s] 51%|█████     | 113/221 [00:29<00:28,  3.79it/s] 52%|█████▏    | 114/221 [00:30<00:28,  3.79it/s] 52%|█████▏    | 115/221 [00:30<00:27,  3.79it/s] 52%|█████▏    | 116/221 [00:30<00:27,  3.79it/s] 53%|█████▎    | 117/221 [00:30<00:27,  3.79it/s] 53%|█████▎    | 118/221 [00:31<00:27,  3.79it/s] 54%|█████▍    | 119/221 [00:31<00:26,  3.79it/s] 54%|█████▍    | 120/221 [00:31<00:26,  3.79it/s] 55%|█████▍    | 121/221 [00:31<00:26,  3.79it/s] 55%|█████▌    | 122/221 [00:32<00:26,  3.79it/s] 56%|█████▌    | 123/221 [00:32<00:25,  3.79it/s] 56%|█████▌    | 124/221 [00:32<00:25,  3.79it/s] 57%|█████▋    | 125/221 [00:32<00:25,  3.79it/s] 57%|█████▋    | 126/221 [00:33<00:25,  3.79it/s] 57%|█████▋    | 127/221 [00:33<00:24,  3.79it/s] 58%|█████▊    | 128/221 [00:33<00:24,  3.79it/s] 58%|█████▊    | 129/221 [00:34<00:24,  3.79it/s] 59%|█████▉    | 130/221 [00:34<00:24,  3.79it/s] 59%|█████▉    | 131/221 [00:34<00:23,  3.79it/s] 60%|█████▉    | 132/221 [00:34<00:23,  3.79it/s] 60%|██████    | 133/221 [00:35<00:23,  3.79it/s] 61%|██████    | 134/221 [00:35<00:22,  3.79it/s] 61%|██████    | 135/221 [00:35<00:22,  3.79it/s] 62%|██████▏   | 136/221 [00:35<00:22,  3.79it/s] 62%|██████▏   | 137/221 [00:36<00:22,  3.79it/s] 62%|██████▏   | 138/221 [00:36<00:21,  3.79it/s] 63%|██████▎   | 139/221 [00:36<00:21,  3.79it/s] 63%|██████▎   | 140/221 [00:36<00:21,  3.79it/s] 64%|██████▍   | 141/221 [00:37<00:21,  3.79it/s] 64%|██████▍   | 142/221 [00:37<00:20,  3.79it/s] 65%|██████▍   | 143/221 [00:37<00:20,  3.79it/s] 65%|██████▌   | 144/221 [00:37<00:20,  3.79it/s] 66%|██████▌   | 145/221 [00:38<00:20,  3.79it/s] 66%|██████▌   | 146/221 [00:38<00:19,  3.79it/s] 67%|██████▋   | 147/221 [00:38<00:19,  3.79it/s] 67%|██████▋   | 148/221 [00:39<00:19,  3.79it/s] 67%|██████▋   | 149/221 [00:39<00:18,  3.79it/s] 68%|██████▊   | 150/221 [00:39<00:18,  3.79it/s] 68%|██████▊   | 151/221 [00:39<00:18,  3.79it/s] 69%|██████▉   | 152/221 [00:40<00:18,  3.79it/s] 69%|██████▉   | 153/221 [00:40<00:17,  3.79it/s] 70%|██████▉   | 154/221 [00:40<00:17,  3.79it/s] 70%|███████   | 155/221 [00:40<00:17,  3.79it/s] 71%|███████   | 156/221 [00:41<00:17,  3.79it/s] 71%|███████   | 157/221 [00:41<00:16,  3.79it/s] 71%|███████▏  | 158/221 [00:41<00:16,  3.79it/s] 72%|███████▏  | 159/221 [00:41<00:16,  3.79it/s] 72%|███████▏  | 160/221 [00:42<00:16,  3.79it/s] 73%|███████▎  | 161/221 [00:42<00:15,  3.79it/s] 73%|███████▎  | 162/221 [00:42<00:15,  3.79it/s] 74%|███████▍  | 163/221 [00:42<00:15,  3.79it/s] 74%|███████▍  | 164/221 [00:43<00:15,  3.79it/s] 75%|███████▍  | 165/221 [00:43<00:14,  3.79it/s] 75%|███████▌  | 166/221 [00:43<00:14,  3.79it/s] 76%|███████▌  | 167/221 [00:44<00:14,  3.79it/s] 76%|███████▌  | 168/221 [00:44<00:13,  3.79it/s] 76%|███████▋  | 169/221 [00:44<00:13,  3.79it/s] 77%|███████▋  | 170/221 [00:44<00:13,  3.79it/s] 77%|███████▋  | 171/221 [00:45<00:13,  3.79it/s] 78%|███████▊  | 172/221 [00:45<00:12,  3.79it/s] 78%|███████▊  | 173/221 [00:45<00:12,  3.79it/s] 79%|███████▊  | 174/221 [00:45<00:12,  3.79it/s] 79%|███████▉  | 175/221 [00:46<00:12,  3.79it/s] 80%|███████▉  | 176/221 [00:46<00:11,  3.79it/s] 80%|████████  | 177/221 [00:46<00:11,  3.79it/s] 81%|████████  | 178/221 [00:46<00:11,  3.79it/s] 81%|████████  | 179/221 [00:47<00:11,  3.79it/s] 81%|████████▏ | 180/221 [00:47<00:10,  3.79it/s] 82%|████████▏ | 181/221 [00:47<00:10,  3.79it/s] 82%|████████▏ | 182/221 [00:48<00:10,  3.79it/s] 83%|████████▎ | 183/221 [00:48<00:10,  3.79it/s] 83%|████████▎ | 184/221 [00:48<00:09,  3.79it/s] 84%|████████▎ | 185/221 [00:48<00:09,  3.79it/s] 84%|████████▍ | 186/221 [00:49<00:09,  3.79it/s] 85%|████████▍ | 187/221 [00:49<00:08,  3.79it/s] 85%|████████▌ | 188/221 [00:49<00:08,  3.79it/s] 86%|████████▌ | 189/221 [00:49<00:08,  3.79it/s] 86%|████████▌ | 190/221 [00:50<00:08,  3.79it/s] 86%|████████▋ | 191/221 [00:50<00:07,  3.79it/s] 87%|████████▋ | 192/221 [00:50<00:07,  3.79it/s] 87%|████████▋ | 193/221 [00:50<00:07,  3.79it/s] 88%|████████▊ | 194/221 [00:51<00:07,  3.79it/s] 88%|████████▊ | 195/221 [00:51<00:06,  3.79it/s] 89%|████████▊ | 196/221 [00:51<00:06,  3.79it/s] 89%|████████▉ | 197/221 [00:51<00:06,  3.79it/s] 90%|████████▉ | 198/221 [00:52<00:06,  3.79it/s] 90%|█████████ | 199/221 [00:52<00:05,  3.79it/s] 90%|█████████ | 200/221 [00:52<00:05,  3.79it/s] 91%|█████████ | 201/221 [00:53<00:05,  3.79it/s] 91%|█████████▏| 202/221 [00:53<00:05,  3.79it/s] 92%|█████████▏| 203/221 [00:53<00:04,  3.79it/s] 92%|█████████▏| 204/221 [00:53<00:04,  3.79it/s] 93%|█████████▎| 205/221 [00:54<00:04,  3.79it/s] 93%|█████████▎| 206/221 [00:54<00:03,  3.79it/s] 94%|█████████▎| 207/221 [00:54<00:03,  3.79it/s] 94%|█████████▍| 208/221 [00:54<00:03,  3.79it/s] 95%|█████████▍| 209/221 [00:55<00:03,  3.79it/s] 95%|█████████▌| 210/221 [00:55<00:02,  3.79it/s] 95%|█████████▌| 211/221 [00:55<00:02,  3.79it/s] 96%|█████████▌| 212/221 [00:55<00:02,  3.79it/s] 96%|█████████▋| 213/221 [00:56<00:02,  3.79it/s] 97%|█████████▋| 214/221 [00:56<00:01,  3.79it/s] 97%|█████████▋| 215/221 [00:56<00:01,  3.79it/s] 98%|█████████▊| 216/221 [00:56<00:01,  3.79it/s] 98%|█████████▊| 217/221 [00:57<00:01,  3.79it/s] 99%|█████████▊| 218/221 [00:57<00:00,  3.79it/s] 99%|█████████▉| 219/221 [00:57<00:00,  3.79it/s]100%|█████████▉| 220/221 [00:58<00:00,  3.79it/s]100%|██████████| 221/221 [00:58<00:00,  3.79it/s]100%|██████████| 221/221 [00:58<00:00,  3.79it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  1%|          | 2/221 [00:00<00:37,  5.77it/s]  1%|▏         | 3/221 [00:00<00:45,  4.78it/s]  2%|▏         | 4/221 [00:00<00:37,  5.81it/s]  2%|▏         | 5/221 [00:00<00:36,  5.99it/s]  3%|▎         | 7/221 [00:01<00:37,  5.72it/s]  4%|▎         | 8/221 [00:01<00:45,  4.66it/s]  4%|▍         | 9/221 [00:01<00:40,  5.18it/s]  5%|▍         | 10/221 [00:02<00:54,  3.90it/s]  5%|▍         | 11/221 [00:02<00:55,  3.75it/s]  5%|▌         | 12/221 [00:02<00:52,  4.01it/s]  6%|▌         | 13/221 [00:03<01:08,  3.04it/s]  6%|▋         | 14/221 [00:03<01:03,  3.27it/s]  7%|▋         | 15/221 [00:03<00:59,  3.47it/s]  7%|▋         | 16/221 [00:03<01:03,  3.22it/s]  8%|▊         | 17/221 [00:04<01:13,  2.78it/s]  8%|▊         | 18/221 [00:04<01:05,  3.12it/s]  9%|▊         | 19/221 [00:04<01:02,  3.21it/s]  9%|▉         | 20/221 [00:05<00:53,  3.73it/s] 10%|▉         | 21/221 [00:05<00:53,  3.75it/s] 10%|▉         | 22/221 [00:05<00:54,  3.68it/s] 11%|█         | 24/221 [00:06<00:45,  4.33it/s] 11%|█▏        | 25/221 [00:06<00:49,  3.96it/s] 12%|█▏        | 26/221 [00:06<00:51,  3.78it/s] 12%|█▏        | 27/221 [00:06<00:47,  4.11it/s] 13%|█▎        | 28/221 [00:07<01:00,  3.21it/s] 13%|█▎        | 29/221 [00:07<00:58,  3.29it/s] 14%|█▎        | 30/221 [00:07<00:59,  3.22it/s] 14%|█▍        | 31/221 [00:08<00:53,  3.57it/s] 14%|█▍        | 32/221 [00:08<00:45,  4.11it/s] 15%|█▍        | 33/221 [00:08<00:43,  4.30it/s] 15%|█▌        | 34/221 [00:08<00:43,  4.33it/s] 16%|█▌        | 35/221 [00:09<00:49,  3.78it/s] 16%|█▋        | 36/221 [00:09<00:50,  3.67it/s] 17%|█▋        | 37/221 [00:09<00:46,  3.94it/s] 17%|█▋        | 38/221 [00:09<00:45,  4.03it/s] 18%|█▊        | 39/221 [00:10<00:43,  4.14it/s] 18%|█▊        | 40/221 [00:10<00:51,  3.53it/s] 19%|█▊        | 41/221 [00:10<00:53,  3.35it/s] 19%|█▉        | 42/221 [00:10<00:44,  4.07it/s] 19%|█▉        | 43/221 [00:11<00:41,  4.32it/s] 20%|█▉        | 44/221 [00:11<00:39,  4.48it/s] 20%|██        | 45/221 [00:11<00:42,  4.15it/s] 21%|██        | 46/221 [00:11<00:43,  4.03it/s] 21%|██▏       | 47/221 [00:12<00:40,  4.34it/s] 22%|██▏       | 48/221 [00:12<00:34,  4.96it/s] 22%|██▏       | 49/221 [00:12<00:35,  4.90it/s] 23%|██▎       | 50/221 [00:12<00:51,  3.30it/s] 23%|██▎       | 51/221 [00:13<00:45,  3.73it/s] 24%|██▎       | 52/221 [00:13<00:46,  3.64it/s] 24%|██▍       | 53/221 [00:13<00:44,  3.79it/s] 24%|██▍       | 54/221 [00:13<00:46,  3.58it/s] 25%|██▍       | 55/221 [00:14<00:43,  3.78it/s] 25%|██▌       | 56/221 [00:14<00:42,  3.89it/s] 26%|██▌       | 57/221 [00:14<00:40,  4.07it/s] 26%|██▌       | 58/221 [00:14<00:43,  3.75it/s] 27%|██▋       | 59/221 [00:15<00:38,  4.19it/s] 27%|██▋       | 60/221 [00:15<00:35,  4.57it/s] 28%|██▊       | 61/221 [00:15<00:35,  4.48it/s] 28%|██▊       | 62/221 [00:15<00:36,  4.39it/s] 29%|██▊       | 63/221 [00:16<00:35,  4.40it/s] 29%|██▉       | 64/221 [00:16<00:39,  3.97it/s] 29%|██▉       | 65/221 [00:16<00:39,  4.00it/s] 30%|██▉       | 66/221 [00:17<00:49,  3.11it/s] 30%|███       | 67/221 [00:17<00:48,  3.15it/s] 31%|███       | 68/221 [00:17<00:43,  3.55it/s] 31%|███       | 69/221 [00:18<01:00,  2.50it/s] 32%|███▏      | 70/221 [00:18<00:55,  2.73it/s] 32%|███▏      | 71/221 [00:18<00:47,  3.19it/s] 33%|███▎      | 72/221 [00:19<00:48,  3.05it/s] 33%|███▎      | 73/221 [00:19<00:42,  3.49it/s] 33%|███▎      | 74/221 [00:19<00:34,  4.24it/s] 34%|███▍      | 75/221 [00:19<00:35,  4.12it/s] 34%|███▍      | 76/221 [00:19<00:32,  4.47it/s] 35%|███▍      | 77/221 [00:20<00:40,  3.54it/s] 35%|███▌      | 78/221 [00:20<00:37,  3.79it/s] 36%|███▌      | 79/221 [00:21<00:51,  2.74it/s] 36%|███▌      | 80/221 [00:21<00:50,  2.78it/s] 37%|███▋      | 81/221 [00:21<00:40,  3.44it/s] 37%|███▋      | 82/221 [00:21<00:42,  3.30it/s] 38%|███▊      | 83/221 [00:22<00:41,  3.32it/s] 38%|███▊      | 84/221 [00:22<00:38,  3.54it/s] 38%|███▊      | 85/221 [00:22<00:33,  4.02it/s] 39%|███▉      | 86/221 [00:22<00:30,  4.46it/s] 39%|███▉      | 87/221 [00:22<00:28,  4.70it/s] 40%|███▉      | 88/221 [00:23<00:39,  3.33it/s] 40%|████      | 89/221 [00:23<00:42,  3.12it/s] 41%|████      | 90/221 [00:24<00:38,  3.45it/s] 41%|████      | 91/221 [00:24<00:33,  3.86it/s] 42%|████▏     | 92/221 [00:24<00:38,  3.35it/s] 42%|████▏     | 93/221 [00:25<00:45,  2.80it/s] 43%|████▎     | 94/221 [00:25<00:43,  2.94it/s] 43%|████▎     | 95/221 [00:25<00:41,  3.07it/s] 43%|████▎     | 96/221 [00:25<00:36,  3.45it/s] 44%|████▍     | 97/221 [00:26<00:36,  3.44it/s] 44%|████▍     | 98/221 [00:26<00:41,  2.99it/s] 45%|████▍     | 99/221 [00:26<00:41,  2.97it/s] 45%|████▌     | 100/221 [00:27<00:40,  3.02it/s] 46%|████▌     | 101/221 [00:27<00:38,  3.12it/s] 46%|████▌     | 102/221 [00:28<00:42,  2.83it/s] 47%|████▋     | 103/221 [00:28<00:36,  3.26it/s] 47%|████▋     | 104/221 [00:28<00:35,  3.33it/s] 48%|████▊     | 105/221 [00:28<00:40,  2.88it/s] 48%|████▊     | 106/221 [00:29<00:35,  3.22it/s] 48%|████▊     | 107/221 [00:29<00:36,  3.14it/s] 49%|████▉     | 108/221 [00:29<00:35,  3.19it/s] 49%|████▉     | 109/221 [00:29<00:30,  3.68it/s] 50%|████▉     | 110/221 [00:30<00:31,  3.47it/s] 50%|█████     | 111/221 [00:30<00:34,  3.23it/s] 51%|█████     | 112/221 [00:30<00:34,  3.18it/s] 51%|█████     | 113/221 [00:31<00:30,  3.51it/s] 52%|█████▏    | 114/221 [00:31<00:25,  4.23it/s] 52%|█████▏    | 115/221 [00:31<00:28,  3.76it/s] 52%|█████▏    | 116/221 [00:31<00:29,  3.62it/s] 53%|█████▎    | 117/221 [00:32<00:28,  3.69it/s] 53%|█████▎    | 118/221 [00:32<00:27,  3.79it/s] 54%|█████▍    | 119/221 [00:32<00:28,  3.64it/s] 54%|█████▍    | 120/221 [00:33<00:26,  3.75it/s] 55%|█████▍    | 121/221 [00:33<00:23,  4.33it/s] 55%|█████▌    | 122/221 [00:33<00:22,  4.39it/s] 56%|█████▌    | 123/221 [00:33<00:23,  4.12it/s] 56%|█████▌    | 124/221 [00:33<00:24,  4.02it/s] 57%|█████▋    | 125/221 [00:34<00:28,  3.38it/s] 57%|█████▋    | 126/221 [00:34<00:25,  3.71it/s] 57%|█████▋    | 127/221 [00:34<00:30,  3.10it/s] 58%|█████▊    | 128/221 [00:35<00:28,  3.24it/s] 58%|█████▊    | 129/221 [00:35<00:23,  3.95it/s] 59%|█████▉    | 130/221 [00:35<00:23,  3.95it/s] 60%|█████▉    | 132/221 [00:36<00:20,  4.41it/s] 60%|██████    | 133/221 [00:36<00:23,  3.79it/s] 61%|██████    | 134/221 [00:36<00:24,  3.60it/s] 61%|██████    | 135/221 [00:36<00:22,  3.89it/s] 62%|██████▏   | 136/221 [00:37<00:22,  3.71it/s] 62%|██████▏   | 137/221 [00:37<00:23,  3.63it/s] 62%|██████▏   | 138/221 [00:37<00:24,  3.35it/s] 63%|██████▎   | 139/221 [00:38<00:25,  3.21it/s] 63%|██████▎   | 140/221 [00:38<00:24,  3.27it/s] 64%|██████▍   | 141/221 [00:38<00:23,  3.34it/s] 64%|██████▍   | 142/221 [00:39<00:21,  3.70it/s] 65%|██████▍   | 143/221 [00:39<00:21,  3.58it/s] 65%|██████▌   | 144/221 [00:39<00:22,  3.38it/s] 66%|██████▌   | 145/221 [00:39<00:20,  3.75it/s] 66%|██████▌   | 146/221 [00:39<00:17,  4.32it/s] 67%|██████▋   | 147/221 [00:40<00:19,  3.75it/s] 67%|██████▋   | 148/221 [00:40<00:21,  3.33it/s] 67%|██████▋   | 149/221 [00:41<00:21,  3.30it/s] 68%|██████▊   | 150/221 [00:41<00:21,  3.34it/s] 68%|██████▊   | 151/221 [00:41<00:24,  2.91it/s] 69%|██████▉   | 152/221 [00:42<00:26,  2.58it/s] 69%|██████▉   | 153/221 [00:42<00:22,  3.02it/s] 70%|██████▉   | 154/221 [00:42<00:19,  3.47it/s] 70%|███████   | 155/221 [00:42<00:19,  3.43it/s] 71%|███████   | 156/221 [00:43<00:19,  3.27it/s] 71%|███████   | 157/221 [00:43<00:21,  3.04it/s] 71%|███████▏  | 158/221 [00:43<00:19,  3.17it/s] 72%|███████▏  | 159/221 [00:44<00:16,  3.85it/s] 72%|███████▏  | 160/221 [00:44<00:14,  4.27it/s] 73%|███████▎  | 161/221 [00:44<00:13,  4.42it/s] 73%|███████▎  | 162/221 [00:44<00:11,  5.16it/s] 74%|███████▍  | 163/221 [00:44<00:13,  4.46it/s] 74%|███████▍  | 164/221 [00:45<00:12,  4.55it/s] 75%|███████▍  | 165/221 [00:45<00:15,  3.73it/s] 75%|███████▌  | 166/221 [00:45<00:14,  3.88it/s] 76%|███████▌  | 167/221 [00:45<00:12,  4.41it/s] 76%|███████▌  | 168/221 [00:46<00:11,  4.45it/s] 76%|███████▋  | 169/221 [00:46<00:10,  4.91it/s] 77%|███████▋  | 170/221 [00:46<00:16,  3.10it/s] 77%|███████▋  | 171/221 [00:47<00:14,  3.50it/s] 78%|███████▊  | 172/221 [00:47<00:13,  3.69it/s] 78%|███████▊  | 173/221 [00:47<00:14,  3.40it/s] 79%|███████▊  | 174/221 [00:47<00:14,  3.16it/s] 79%|███████▉  | 175/221 [00:48<00:14,  3.19it/s] 80%|███████▉  | 176/221 [00:48<00:13,  3.45it/s] 80%|████████  | 177/221 [00:48<00:11,  3.79it/s] 81%|████████  | 178/221 [00:49<00:13,  3.22it/s] 81%|████████  | 179/221 [00:49<00:12,  3.28it/s] 81%|████████▏ | 180/221 [00:49<00:11,  3.71it/s] 82%|████████▏ | 181/221 [00:49<00:09,  4.19it/s] 82%|████████▏ | 182/221 [00:50<00:11,  3.25it/s] 83%|████████▎ | 183/221 [00:50<00:11,  3.25it/s] 83%|████████▎ | 184/221 [00:50<00:09,  3.71it/s] 84%|████████▎ | 185/221 [00:50<00:09,  3.97it/s] 84%|████████▍ | 186/221 [00:51<00:10,  3.44it/s] 85%|████████▍ | 187/221 [00:51<00:10,  3.24it/s] 85%|████████▌ | 188/221 [00:51<00:10,  3.28it/s] 86%|████████▌ | 189/221 [00:52<00:08,  3.63it/s] 86%|████████▌ | 190/221 [00:52<00:09,  3.41it/s] 86%|████████▋ | 191/221 [00:52<00:07,  3.91it/s] 87%|████████▋ | 192/221 [00:52<00:07,  3.88it/s] 87%|████████▋ | 193/221 [00:53<00:06,  4.40it/s] 88%|████████▊ | 194/221 [00:53<00:05,  4.50it/s] 88%|████████▊ | 195/221 [00:53<00:06,  4.19it/s] 89%|████████▊ | 196/221 [00:53<00:06,  3.89it/s] 89%|████████▉ | 197/221 [00:54<00:05,  4.07it/s] 90%|████████▉ | 198/221 [00:54<00:06,  3.57it/s] 90%|█████████ | 199/221 [00:54<00:06,  3.51it/s] 90%|█████████ | 200/221 [00:55<00:06,  3.06it/s] 91%|█████████ | 201/221 [00:55<00:06,  3.19it/s] 91%|█████████▏| 202/221 [00:55<00:06,  2.88it/s] 92%|█████████▏| 203/221 [00:56<00:05,  3.29it/s] 92%|█████████▏| 204/221 [00:56<00:04,  3.55it/s] 93%|█████████▎| 205/221 [00:56<00:03,  4.10it/s] 93%|█████████▎| 206/221 [00:56<00:04,  3.51it/s] 94%|█████████▎| 207/221 [00:57<00:03,  3.64it/s] 94%|█████████▍| 208/221 [00:57<00:03,  3.28it/s] 95%|█████████▍| 209/221 [00:57<00:03,  3.29it/s] 95%|█████████▌| 210/221 [00:58<00:02,  3.68it/s] 95%|█████████▌| 211/221 [00:58<00:03,  3.28it/s] 96%|█████████▌| 212/221 [00:58<00:02,  3.45it/s] 96%|█████████▋| 213/221 [00:58<00:02,  3.97it/s] 97%|█████████▋| 214/221 [00:59<00:02,  3.42it/s] 97%|█████████▋| 215/221 [00:59<00:01,  3.69it/s] 98%|█████████▊| 216/221 [00:59<00:01,  3.51it/s] 98%|█████████▊| 217/221 [01:00<00:01,  3.25it/s] 99%|█████████▊| 218/221 [01:00<00:00,  3.25it/s] 99%|█████████▉| 219/221 [01:00<00:00,  3.08it/s]100%|█████████▉| 220/221 [01:01<00:00,  3.10it/s]100%|██████████| 221/221 [01:01<00:00,  2.98it/s]100%|██████████| 221/221 [01:01<00:00,  3.60it/s]
09/10/2024 13:48:00 - INFO - __main__ -   ==== evaluation--ret%tv--msrvtt_ret_ret_area_forward========

09/10/2024 13:48:00 - INFO - __main__ -   {'area_r1': 24.1, 'area_recall': '24.1/43.1/50.6', 'area_ravg': 39.3}
09/10/2024 13:48:00 - INFO - __main__ -   ==== evaluation--ret%tv--msrvtt_ret_ret_area_backard========

09/10/2024 13:48:00 - INFO - __main__ -   {'forward_r1': 33.3, 'forward_recall': '33.3/64.7/75.1', 'forward_ravg': 57.7}
09/10/2024 13:48:00 - INFO - __main__ -   ==== evaluation--ret%tv--msrvtt_ret_ret_area_back_with_video========

09/10/2024 13:48:00 - INFO - __main__ -   {'area_video_r1': 33.3, 'area_video_recall': '33.3/64.4/75.9', 'area_video_ravg': 57.8}
09/10/2024 13:48:00 - INFO - __main__ -   ==== evaluation--ret%tv--msrvtt_ret_ret_itm_area========

09/10/2024 13:48:00 - INFO - __main__ -   {'area_video_r1': 50.3, 'area_video_recall': '50.3/72.9/80.5', 'area_video_ravg': 67.9, 'area_video_back_r1': 43.2, 'area_video_back_recall': '43.2/67.1/77.0', 'area_video_back_ravg': 62.4}
09/10/2024 13:48:00 - INFO - __main__ -   ==== evaluation--ret%tv--msrvtt_ret_ret_itc_tv========

09/10/2024 13:48:00 - INFO - __main__ -   {'video_r1': 44.9, 'video_recall': '44.9/72.4/82.5', 'video_ravg': 66.6}
09/10/2024 13:48:00 - INFO - __main__ -   ==== evaluation--ret%tv--msrvtt_ret_ret_itm_tv========

09/10/2024 13:48:00 - INFO - __main__ -   {'video_r1': 50.3, 'video_recall': '50.3/72.4/80.4', 'video_ravg': 67.7}
  0%|          | 0/1945 [00:00<?, ?it/s][h264 @ 0x561aa5149900] mmco: unref short failure
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/1945 [00:09<4:58:26,  9.21s/it][h264 @ 0x55cb76d8bdc0] mmco: unref short failure
[h264 @ 0x55cb76d8bdc0] mmco: unref short failure
[h264 @ 0x55cb76d8bdc0] mmco: unref short failure
[h264 @ 0x55cb76d8bdc0] mmco: unref short failure
  0%|          | 2/1945 [00:13<3:15:18,  6.03s/it][h264 @ 0x561aaa8b72c0] mmco: unref short failure
[h264 @ 0x564c229b4600] mmco: unref short failure
[h264 @ 0x564c229b4600] mmco: unref short failure
[h264 @ 0x564c229b4600] mmco: unref short failure
[h264 @ 0x564c229b4600] mmco: unref short failure
[h264 @ 0x564c229b4600] mmco: unref short failure
[h264 @ 0x564c229b4600] mmco: unref short failure
  0%|          | 3/1945 [00:17<2:48:57,  5.22s/it][h264 @ 0x564c2294fa80] mmco: unref short failure
[h264 @ 0x564c2294fa80] mmco: unref short failure
[h264 @ 0x562f0c57a640] mmco: unref short failure
  0%|          | 4/1945 [00:21<2:39:34,  4.93s/it][h264 @ 0x55cb76e89bc0] mmco: unref short failure
[h264 @ 0x55cb76e89bc0] mmco: unref short failure
  0%|          | 5/1945 [00:27<2:43:04,  5.04s/it][h264 @ 0x55cb79bf9840] mmco: unref short failure
[h264 @ 0x55cb79bf9840] mmco: unref short failure
[h264 @ 0x55cb79bf9840] mmco: unref short failure
[h264 @ 0x564c2905da80] mmco: unref short failure
[h264 @ 0x564c2905da80] mmco: unref short failure
  0%|          | 6/1945 [00:32<2:48:38,  5.22s/it][h264 @ 0x562f16cbedc0] mmco: unref short failure
[h264 @ 0x562f16cbedc0] mmco: unref short failure
[h264 @ 0x55cb7e7dc1c0] mmco: unref short failure
[h264 @ 0x55cb7e7dc1c0] mmco: unref short failure
  0%|          | 7/1945 [00:39<3:07:30,  5.80s/it]  0%|          | 8/1945 [00:45<3:11:27,  5.93s/it][h264 @ 0x564c26ed83c0] mmco: unref short failure
  0%|          | 9/1945 [00:52<3:14:28,  6.03s/it]  1%|          | 10/1945 [00:57<3:13:42,  6.01s/it][h264 @ 0x55cb81051900] mmco: unref short failure
[h264 @ 0x55cb81051900] mmco: unref short failure
not have audios 8-qwaveiHMM.3
  1%|          | 11/1945 [01:03<3:12:41,  5.98s/it][h264 @ 0x564c26d73a80] mmco: unref short failure
[h264 @ 0x564c26d73a80] mmco: unref short failure
  1%|          | 12/1945 [01:09<3:11:36,  5.95s/it]  1%|          | 13/1945 [01:15<3:12:38,  5.98s/it]  1%|          | 14/1945 [01:21<3:10:41,  5.92s/it][h264 @ 0x562f0fd00400] mmco: unref short failure
[h264 @ 0x562f0fd00400] mmco: unref short failure
[h264 @ 0x561aab054040] mmco: unref short failure
  1%|          | 15/1945 [01:27<3:08:59,  5.88s/it][h264 @ 0x562f18db8ec0] mmco: unref short failure
[h264 @ 0x562f18db8ec0] mmco: unref short failure
[h264 @ 0x55cb8160ae40] mmco: unref short failure
[h264 @ 0x55cb8160ae40] mmco: unref short failure
  1%|          | 16/1945 [01:49<5:42:36, 10.66s/it][h264 @ 0x561ab0e10f40] mmco: unref short failure
[h264 @ 0x561ab0e10f40] mmco: unref short failure
[h264 @ 0x561ab0e10f40] mmco: unref short failure
[h264 @ 0x561ab0e10f40] mmco: unref short failure
  1%|          | 17/1945 [01:56<5:07:10,  9.56s/it][h264 @ 0x564c2a85f640] mmco: unref short failure
[h264 @ 0x564c2a85f640] mmco: unref short failure
[h264 @ 0x564c2a85f640] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x564c266f9100] mmco: unref short failure
[h264 @ 0x562f10093140] mmco: unref short failure
[h264 @ 0x55cb7721dcc0] mmco: unref short failure
[h264 @ 0x55cb7721dcc0] mmco: unref short failure
[h264 @ 0x561ab0723200] mmco: unref short failure
[h264 @ 0x561ab0723200] mmco: unref short failure
[h264 @ 0x562f17ddfa40] mmco: unref short failure
[h264 @ 0x562f17ddfa40] mmco: unref short failure
[h264 @ 0x564c2bc58f80] mmco: unref short failure
[h264 @ 0x564c2c86c800] mmco: unref short failure
[h264 @ 0x561aafbabf00] mmco: unref short failure
[h264 @ 0x561aafbabf00] mmco: unref short failure
[h264 @ 0x561ab0196f80] mmco: unref short failure
[h264 @ 0x561ab0196f80] mmco: unref short failure
[h264 @ 0x561ab0196f80] mmco: unref short failure
[h264 @ 0x561ab0196f80] mmco: unref short failure
  1%|          | 18/1945 [02:13<6:20:11, 11.84s/it][h264 @ 0x564c279ffe00] mmco: unref short failure
[h264 @ 0x561ab0527000] mmco: unref short failure
[h264 @ 0x561ab0527000] mmco: unref short failure
  1%|          | 19/1945 [02:18<5:19:57,  9.97s/it][h264 @ 0x55cb7eee1440] mmco: unref short failure
[h264 @ 0x55cb7eee1440] mmco: unref short failure
[h264 @ 0x562f150cfd80] mmco: unref short failure
[h264 @ 0x562f182c5d00] mmco: unref short failure
[h264 @ 0x562f182c5d00] mmco: unref short failure
[h264 @ 0x562f182c5d00] mmco: unref short failure
[h264 @ 0x562f182c5d00] mmco: unref short failure
[h264 @ 0x561aafac2480] mmco: unref short failure
[h264 @ 0x562f18a10240] mmco: unref short failure
  1%|          | 20/1945 [02:35<6:27:36, 12.08s/it]  1%|          | 21/1945 [02:41<5:28:38, 10.25s/it][h264 @ 0x564c336f0500] mmco: unref short failure
[h264 @ 0x564c336f0500] mmco: unref short failure
[h264 @ 0x564c336f0500] mmco: unref short failure
[h264 @ 0x564c336f0500] mmco: unref short failure
[h264 @ 0x55cb77d54f40] mmco: unref short failure
  1%|          | 22/1945 [02:47<4:41:32,  8.78s/it][h264 @ 0x55cb7ca8ca00] mmco: unref short failure
  1%|          | 23/1945 [02:53<4:14:35,  7.95s/it][h264 @ 0x562f0c63b440] mmco: unref short failure
[h264 @ 0x561aaa324780] mmco: unref short failure
[h264 @ 0x561aaa324780] mmco: unref short failure
[h264 @ 0x564c2a8216c0] mmco: unref short failure
[h264 @ 0x564c2a8216c0] mmco: unref short failure
[h264 @ 0x564c30077380] mmco: unref short failure
[h264 @ 0x564c30077380] mmco: unref short failure
[h264 @ 0x564c30077380] mmco: unref short failure
[h264 @ 0x564c30077380] mmco: unref short failure
[h264 @ 0x561aa8044b00] mmco: unref short failure
[h264 @ 0x561aa8044b00] mmco: unref short failure
[h264 @ 0x562f0ce2e6c0] mmco: unref short failure
[h264 @ 0x562f0ce2e6c0] mmco: unref short failure
[h264 @ 0x55cb797fb000] mmco: unref short failure
[h264 @ 0x55cb797fb000] mmco: unref short failure
[h264 @ 0x564c29bc8a80] mmco: unref short failure
[h264 @ 0x564c29bc8a80] mmco: unref short failure
[h264 @ 0x55cb76509e80] mmco: unref short failure
[h264 @ 0x55cb76509e80] mmco: unref short failure
[h264 @ 0x55cb7cfe7980] mmco: unref short failure
[h264 @ 0x561ab3d78380] mmco: unref short failure
  1%|          | 24/1945 [03:47<11:43:10, 21.96s/it][h264 @ 0x561aa98be280] mmco: unref short failure
  1%|▏         | 25/1945 [03:53<9:08:26, 17.14s/it] [h264 @ 0x564c28dd6200] mmco: unref short failure
[h264 @ 0x561ab407d4c0] mmco: unref short failure
[h264 @ 0x561ab407d4c0] mmco: unref short failure
[h264 @ 0x562f1b2d50c0] mmco: unref short failure
[h264 @ 0x562f1b2d50c0] mmco: unref short failure
[h264 @ 0x561aa4ffc880] mmco: unref short failure
[h264 @ 0x55cb76eaaf00] mmco: unref short failure
[h264 @ 0x55cb76eaaf00] mmco: unref short failure
[h264 @ 0x564c250bac80] mmco: unref short failure
[h264 @ 0x564c250bac80] mmco: unref short failure
[h264 @ 0x564c250bac80] mmco: unref short failure
[h264 @ 0x564c250bac80] mmco: unref short failure
[h264 @ 0x564c2d569fc0] mmco: unref short failure
[h264 @ 0x561aa5d4b400] mmco: unref short failure
[h264 @ 0x561aa5d4b400] mmco: unref short failure
  1%|▏         | 26/1945 [04:21<10:48:32, 20.28s/it]  1%|▏         | 27/1945 [04:27<8:35:55, 16.14s/it]   1%|▏         | 28/1945 [04:33<6:56:30, 13.04s/it]  1%|▏         | 29/1945 [04:39<5:48:31, 10.91s/it][h264 @ 0x561aa9c832c0] mmco: unref short failure
  2%|▏         | 30/1945 [04:45<5:00:50,  9.43s/it][h264 @ 0x55cb806e2800] mmco: unref short failure
[h264 @ 0x561aa7882180] mmco: unref short failure
[h264 @ 0x561aaa4f0c40] mmco: unref short failure
  2%|▏         | 31/1945 [04:52<4:37:12,  8.69s/it][h264 @ 0x562f0c5c6180] mmco: unref short failure
[h264 @ 0x55cb8a266dc0] mmco: unref short failure
[h264 @ 0x55cb8a266dc0] mmco: unref short failure
[h264 @ 0x564c2c76a000] mmco: unref short failure
[h264 @ 0x564c2c76a000] mmco: unref short failure
[h264 @ 0x55cb79dbe580] mmco: unref short failure
[h264 @ 0x55cb79dbe580] mmco: unref short failure
[h264 @ 0x562f0b520d80] mmco: unref short failure
[h264 @ 0x562f1b2d5300] mmco: unref short failure
[h264 @ 0x562f1b2d5300] mmco: unref short failure
[h264 @ 0x562f1b2d5300] mmco: unref short failure
[h264 @ 0x562f1b2d5300] mmco: unref short failure
[h264 @ 0x564c35cd8300] mmco: unref short failure
[h264 @ 0x564c35cd8300] mmco: unref short failure
[h264 @ 0x55cb8138ad80] mmco: unref short failure
[h264 @ 0x561aa5092dc0] mmco: unref short failure
[h264 @ 0x561ab68e1c40] mmco: unref short failure
  2%|▏         | 32/1945 [05:50<12:30:00, 23.52s/it]  2%|▏         | 33/1945 [05:56<9:44:53, 18.35s/it] [h264 @ 0x562f0af2af40] mmco: unref short failure
[h264 @ 0x562f0af2af40] mmco: unref short failure
[h264 @ 0x561aaf418540] mmco: unref short failure
[h264 @ 0x561aa5189580] mmco: unref short failure
[h264 @ 0x561aa5189580] mmco: unref short failure
[h264 @ 0x561aa6067540] mmco: unref short failure
[h264 @ 0x561aa8d451c0] mmco: unref short failure
[h264 @ 0x564c31239400] mmco: unref short failure
[h264 @ 0x55cb7f438e00] mmco: unref short failure
[h264 @ 0x55cb824bd7c0] mmco: unref short failure
[h264 @ 0x55cb824bd7c0] mmco: unref short failure
[h264 @ 0x562f1aee4a80] mmco: unref short failure
[h264 @ 0x562f1aee4a80] mmco: unref short failure
  2%|▏         | 34/1945 [06:30<12:08:05, 22.86s/it]  2%|▏         | 35/1945 [06:36<9:29:40, 17.90s/it] [h264 @ 0x564c2f610ec0] mmco: unref short failure
[h264 @ 0x564c2f610ec0] mmco: unref short failure
[h264 @ 0x562f18b26c80] mmco: unref short failure
[h264 @ 0x562f18b26c80] mmco: unref short failure
[h264 @ 0x561ab665e340] mmco: unref short failure
  2%|▏         | 36/1945 [06:45<8:02:35, 15.17s/it][h264 @ 0x561ab2100a80] mmco: unref short failure
[h264 @ 0x561ab2100a80] mmco: unref short failure
  2%|▏         | 37/1945 [06:55<7:14:47, 13.67s/it]  2%|▏         | 38/1945 [07:01<5:59:37, 11.31s/it][h264 @ 0x561aa814b940] mmco: unref short failure
[h264 @ 0x561aa814b940] mmco: unref short failure
[h264 @ 0x561aa570e980] mmco: unref short failure
[h264 @ 0x561aa570e980] mmco: unref short failure
  2%|▏         | 39/1945 [07:07<5:12:02,  9.82s/it][h264 @ 0x564c27184340] mmco: unref short failure
[h264 @ 0x562f1141fd80] mmco: unref short failure
[h264 @ 0x55cb79573080] mmco: unref short failure
[h264 @ 0x55cb79573080] mmco: unref short failure
[h264 @ 0x561aa6b96b40] mmco: unref short failure
[h264 @ 0x564c318eff80] mmco: unref short failure
[h264 @ 0x564c318eff80] mmco: unref short failure
[h264 @ 0x562f0c505740] mmco: unref short failure
[h264 @ 0x562f0c505740] mmco: unref short failure
[h264 @ 0x564c23d3f3c0] mmco: unref short failure
[h264 @ 0x561aa81037c0] mmco: unref short failure
[h264 @ 0x561ab10c01c0] mmco: unref short failure
[h264 @ 0x561ab10c01c0] mmco: unref short failure
[h264 @ 0x55cb7ac2cdc0] mmco: unref short failure
[h264 @ 0x55cb7ac2cdc0] mmco: unref short failure
[h264 @ 0x564c310c6900] mmco: unref short failure
[h264 @ 0x564c310c6900] mmco: unref short failure
[h264 @ 0x562f0e467f40] mmco: unref short failure
[h264 @ 0x562f0e467f40] mmco: unref short failure
[h264 @ 0x55cb82aef780] mmco: unref short failure
[h264 @ 0x55cb82aef780] mmco: unref short failure
[h264 @ 0x562f10d87300] mmco: unref short failure
[h264 @ 0x562f10d87300] mmco: unref short failure
[h264 @ 0x562f10d87300] mmco: unref short failure
[h264 @ 0x562f10d87300] mmco: unref short failure
  2%|▏         | 40/1945 [07:53<10:51:36, 20.52s/it][h264 @ 0x564c231dfec0] mmco: unref short failure
  2%|▏         | 41/1945 [07:58<8:29:52, 16.07s/it] [h264 @ 0x564c2a6b7080] mmco: unref short failure
[h264 @ 0x564c2a6b7080] mmco: unref short failure
[h264 @ 0x561ab5250fc0] mmco: unref short failure
[h264 @ 0x561ab37710c0] mmco: unref short failure
[h264 @ 0x561aaaefd340] mmco: unref short failure
[h264 @ 0x561aae1a9140] mmco: unref short failure
[h264 @ 0x562f0ff85500] mmco: unref short failure
[h264 @ 0x55cb7a4838c0] mmco: unref short failure
[h264 @ 0x564c2587a6c0] mmco: unref short failure
  2%|▏         | 42/1945 [08:34<11:33:26, 21.86s/it][h264 @ 0x564c26bde800] mmco: unref short failure
[h264 @ 0x564c2a86d180] mmco: unref short failure
[h264 @ 0x564c2a86d180] mmco: unref short failure
  2%|▏         | 43/1945 [08:40<9:04:58, 17.19s/it]   2%|▏         | 44/1945 [08:46<7:15:06, 13.73s/it][h264 @ 0x562f1f569000] mmco: unref short failure
[h264 @ 0x562f1f569000] mmco: unref short failure
  2%|▏         | 45/1945 [08:52<5:58:59, 11.34s/it][h264 @ 0x55cb84286580] mmco: unref short failure
  2%|▏         | 46/1945 [08:57<5:04:31,  9.62s/it][h264 @ 0x561ab85163c0] mmco: unref short failure
[h264 @ 0x561aa6173cc0] mmco: unref short failure
[h264 @ 0x564c32157ac0] mmco: unref short failure
[h264 @ 0x564c32157ac0] mmco: unref short failure
[h264 @ 0x564c32157ac0] mmco: unref short failure
[h264 @ 0x564c32157ac0] mmco: unref short failure
  2%|▏         | 47/1945 [09:03<4:30:42,  8.56s/it][h264 @ 0x562f19ebc580] mmco: unref short failure
[h264 @ 0x564c297eaf40] mmco: unref short failure
[h264 @ 0x564c297eaf40] mmco: unref short failure
[h264 @ 0x564c297eaf40] mmco: unref short failure
[h264 @ 0x564c231cde00] mmco: unref short failure
[h264 @ 0x564c231cde00] mmco: unref short failure
[h264 @ 0x55cb8a6707c0] mmco: unref short failure
[h264 @ 0x55cb899433c0] mmco: unref short failure
[h264 @ 0x55cb899433c0] mmco: unref short failure
[h264 @ 0x564c25e138c0] mmco: unref short failure
[h264 @ 0x564c25e138c0] mmco: unref short failure
[h264 @ 0x562f0c27f680] mmco: unref short failure
[h264 @ 0x562f0c27f680] mmco: unref short failure
  2%|▏         | 48/1945 [09:41<9:10:04, 17.40s/it][h264 @ 0x564c318ee940] mmco: unref short failure
[h264 @ 0x564c318ee940] mmco: unref short failure
[h264 @ 0x562f1f978d00] mmco: unref short failure
  3%|▎         | 49/1945 [09:54<8:26:29, 16.03s/it]09/10/2024 13:57:54 - INFO - __main__ -   evaluate on ret%tv--msrvtt_ret task
09/10/2024 13:57:54 - INFO - __main__ -   start running ret%tv validation...
[h264 @ 0x561ab78b0740] mmco: unref short failure
[h264 @ 0x561ab78b0740] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x562f11c210c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x562f21992380] mmco: unref short failure
[h264 @ 0x562f21992380] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564c32fbe800] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x562f16644b00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564c37115e80] mmco: unref short failure
[h264 @ 0x561aabdd7a80] mmco: unref short failure
[h264 @ 0x55cb7835fb80] mmco: unref short failure
[h264 @ 0x562f13686840] mmco: unref short failure
[h264 @ 0x564c289e26c0] mmco: unref short failure
[h264 @ 0x564c289e26c0] mmco: unref short failure
[h264 @ 0x564c289e26c0] mmco: unref short failure
[h264 @ 0x564c289e26c0] mmco: unref short failure
[h264 @ 0x561ab7d5dd80] mmco: unref short failure
[h264 @ 0x561ab7d5dd80] mmco: unref short failure
[h264 @ 0x55cb7cd3f280] mmco: unref short failure
[h264 @ 0x55cb7cd3f280] mmco: unref short failure
[h264 @ 0x562f2173db80] mmco: unref short failure
[h264 @ 0x55cb8352fb40] mmco: unref short failure
[h264 @ 0x55cb8352fb40] mmco: unref short failure
[h264 @ 0x561ab15e0a40] mmco: unref short failure
[h264 @ 0x561ab15e0a40] mmco: unref short failure
[h264 @ 0x55cb824bd340] mmco: unref short failure
[h264 @ 0x562f0c5ab900] mmco: unref short failure
[h264 @ 0x562f0c5ab900] mmco: unref short failure
[h264 @ 0x55cb8060ff80] mmco: unref short failure
[h264 @ 0x55cb8060ff80] mmco: unref short failure
[h264 @ 0x561aab2fcd40] mmco: unref short failure
[h264 @ 0x561aab2fcd40] mmco: unref short failure
[h264 @ 0x564c31beda00] mmco: unref short failure
[h264 @ 0x564c31beda00] mmco: unref short failure
[h264 @ 0x55cb81f16b00] mmco: unref short failure
[h264 @ 0x55cb81f16b00] mmco: unref short failure
[h264 @ 0x562f19324540] mmco: unref short failure
[h264 @ 0x564c331a69c0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<01:03,  3.46it/s][A
  1%|▏         | 3/221 [00:00<01:13,  2.95it/s][A
  2%|▏         | 4/221 [00:01<01:01,  3.54it/s][A
  2%|▏         | 5/221 [00:01<01:03,  3.41it/s][A
  3%|▎         | 6/221 [00:01<01:01,  3.52it/s][A
  3%|▎         | 7/221 [00:02<01:15,  2.85it/s][A
  4%|▎         | 8/221 [00:02<01:13,  2.88it/s][A
[h264 @ 0x564c27d4dd80] mmco: unref short failure
[h264 @ 0x564c27d4dd80] mmco: unref short failure
  4%|▍         | 9/221 [00:02<01:05,  3.23it/s][A
  5%|▍         | 10/221 [00:03<01:14,  2.84it/s][A
  5%|▍         | 11/221 [00:03<01:08,  3.07it/s][A
  5%|▌         | 12/221 [00:03<01:06,  3.15it/s][A
  6%|▌         | 13/221 [00:04<01:15,  2.77it/s][A
  6%|▋         | 14/221 [00:04<01:09,  2.99it/s][A
  7%|▋         | 15/221 [00:04<01:06,  3.09it/s][A
  7%|▋         | 16/221 [00:05<01:11,  2.88it/s][A
  8%|▊         | 17/221 [00:05<01:28,  2.31it/s][A
  8%|▊         | 18/221 [00:06<01:25,  2.38it/s][A
  9%|▊         | 19/221 [00:06<01:21,  2.48it/s][A
  9%|▉         | 20/221 [00:06<01:13,  2.73it/s][A
 10%|▉         | 21/221 [00:07<01:11,  2.78it/s][A
 10%|▉         | 22/221 [00:07<01:19,  2.51it/s][A
 10%|█         | 23/221 [00:07<01:01,  3.21it/s][A
 11%|█         | 24/221 [00:08<00:59,  3.33it/s][A
 11%|█▏        | 25/221 [00:08<00:58,  3.33it/s][A
 12%|█▏        | 26/221 [00:09<01:26,  2.26it/s][A
 12%|█▏        | 27/221 [00:09<01:09,  2.79it/s][A
 13%|█▎        | 28/221 [00:09<01:17,  2.47it/s][A
 13%|█▎        | 29/221 [00:10<01:08,  2.81it/s][A[h264 @ 0x55cb80152380] mmco: unref short failure

 14%|█▎        | 30/221 [00:10<01:14,  2.57it/s][A
 14%|█▍        | 31/221 [00:10<01:11,  2.65it/s][A
 14%|█▍        | 32/221 [00:11<00:59,  3.20it/s][A
 15%|█▍        | 33/221 [00:11<00:55,  3.40it/s][A
 15%|█▌        | 34/221 [00:11<00:47,  3.93it/s][A
 16%|█▌        | 35/221 [00:11<00:51,  3.63it/s][A
 16%|█▋        | 36/221 [00:12<01:01,  3.03it/s][A
 17%|█▋        | 37/221 [00:12<01:02,  2.96it/s][A
 17%|█▋        | 38/221 [00:12<01:00,  3.01it/s][A
 18%|█▊        | 39/221 [00:13<00:54,  3.36it/s][A
 18%|█▊        | 40/221 [00:13<00:56,  3.20it/s][A
 19%|█▊        | 41/221 [00:13<00:56,  3.17it/s][A
 19%|█▉        | 42/221 [00:14<00:55,  3.20it/s][A[h264 @ 0x562f18407780] mmco: unref short failure

 19%|█▉        | 43/221 [00:14<00:57,  3.11it/s][A
 20%|█▉        | 44/221 [00:14<00:51,  3.44it/s][A
 20%|██        | 45/221 [00:15<01:04,  2.73it/s][A
 21%|██        | 46/221 [00:15<01:06,  2.65it/s][A[h264 @ 0x561aaeceb740] mmco: unref short failure
[h264 @ 0x561aaeceb740] mmco: unref short failure

 21%|██▏       | 47/221 [00:16<01:20,  2.17it/s][A
 22%|██▏       | 48/221 [00:16<01:09,  2.48it/s][A
 22%|██▏       | 49/221 [00:16<01:02,  2.77it/s][A[h264 @ 0x564c24840cc0] mmco: unref short failure
[h264 @ 0x564c24840cc0] mmco: unref short failure

 23%|██▎       | 50/221 [00:17<01:07,  2.55it/s][A
 23%|██▎       | 51/221 [00:17<00:57,  2.96it/s][A
 24%|██▎       | 52/221 [00:17<00:53,  3.17it/s][A
 24%|██▍       | 53/221 [00:18<00:53,  3.12it/s][A
 24%|██▍       | 54/221 [00:18<01:10,  2.36it/s][A
 25%|██▍       | 55/221 [00:19<01:09,  2.38it/s][A
 25%|██▌       | 56/221 [00:19<00:57,  2.86it/s][A
 26%|██▌       | 57/221 [00:19<00:54,  3.03it/s][A
 26%|██▌       | 58/221 [00:20<00:59,  2.72it/s][A
 27%|██▋       | 59/221 [00:20<00:59,  2.72it/s][A
 27%|██▋       | 60/221 [00:20<00:55,  2.89it/s][A
 28%|██▊       | 61/221 [00:21<00:53,  2.96it/s][A
 28%|██▊       | 62/221 [00:21<01:08,  2.32it/s][A
 29%|██▊       | 63/221 [00:22<01:04,  2.46it/s][A
 29%|██▉       | 64/221 [00:22<01:06,  2.35it/s][A
 29%|██▉       | 65/221 [00:23<01:06,  2.36it/s][A
 30%|██▉       | 66/221 [00:23<01:27,  1.77it/s][A
 30%|███       | 67/221 [00:24<01:18,  1.96it/s][A
 31%|███       | 68/221 [00:24<01:04,  2.39it/s][A
 31%|███       | 69/221 [00:25<01:15,  2.00it/s][A
 32%|███▏      | 70/221 [00:25<01:06,  2.28it/s][A
 32%|███▏      | 71/221 [00:25<00:55,  2.72it/s][A[h264 @ 0x564c231dbf00] mmco: unref short failure

 33%|███▎      | 72/221 [00:26<01:00,  2.48it/s][A
 33%|███▎      | 73/221 [00:26<00:57,  2.60it/s][A
 33%|███▎      | 74/221 [00:26<00:49,  2.99it/s][A
 34%|███▍      | 75/221 [00:27<00:50,  2.92it/s][A[h264 @ 0x55cb864f40c0] mmco: unref short failure
[h264 @ 0x55cb864f40c0] mmco: unref short failure

 34%|███▍      | 76/221 [00:27<00:46,  3.10it/s][A
 35%|███▍      | 77/221 [00:27<00:49,  2.88it/s][A
 35%|███▌      | 78/221 [00:28<00:46,  3.10it/s][A
 36%|███▌      | 79/221 [00:28<00:59,  2.39it/s][A[h264 @ 0x562f12cd8580] mmco: unref short failure

 36%|███▌      | 80/221 [00:29<00:54,  2.57it/s][A
 37%|███▋      | 81/221 [00:29<00:50,  2.80it/s][A
 37%|███▋      | 82/221 [00:29<00:52,  2.67it/s][A
 38%|███▊      | 83/221 [00:30<00:50,  2.71it/s][A
 38%|███▊      | 84/221 [00:30<00:48,  2.82it/s][A
 38%|███▊      | 85/221 [00:30<00:40,  3.33it/s][A
 39%|███▉      | 86/221 [00:30<00:37,  3.59it/s][A
 39%|███▉      | 87/221 [00:31<00:44,  3.04it/s][A[h264 @ 0x562f0cb374c0] mmco: unref short failure
[h264 @ 0x562f0cb374c0] mmco: unref short failure

 40%|███▉      | 88/221 [00:31<00:53,  2.49it/s][A
 40%|████      | 89/221 [00:32<00:50,  2.63it/s][A
 41%|████      | 90/221 [00:32<00:48,  2.70it/s][A
 41%|████      | 91/221 [00:32<00:42,  3.08it/s][A
 42%|████▏     | 92/221 [00:33<00:42,  3.06it/s][A
 42%|████▏     | 93/221 [00:33<00:51,  2.49it/s][A
 43%|████▎     | 94/221 [00:33<00:49,  2.58it/s][A
 43%|████▎     | 95/221 [00:34<00:46,  2.72it/s][A
 43%|████▎     | 96/221 [00:34<00:50,  2.49it/s][A
 44%|████▍     | 97/221 [00:35<00:48,  2.57it/s][A
 44%|████▍     | 98/221 [00:35<00:53,  2.31it/s][A
 45%|████▍     | 99/221 [00:35<00:48,  2.54it/s][A[h264 @ 0x561aa52f4e00] mmco: unref short failure

 45%|████▌     | 100/221 [00:36<00:50,  2.41it/s][A
 46%|████▌     | 101/221 [00:36<00:50,  2.36it/s][A
 46%|████▌     | 102/221 [00:37<00:57,  2.08it/s][A
 47%|████▋     | 103/221 [00:37<00:47,  2.48it/s][A
 47%|████▋     | 104/221 [00:37<00:41,  2.81it/s][A
 48%|████▊     | 105/221 [00:38<00:43,  2.64it/s][A
 48%|████▊     | 106/221 [00:38<00:46,  2.50it/s][A
 48%|████▊     | 107/221 [00:39<00:41,  2.72it/s][A
 49%|████▉     | 108/221 [00:39<00:40,  2.82it/s][A
 49%|████▉     | 109/221 [00:39<00:37,  2.99it/s][A
 50%|████▉     | 110/221 [00:40<00:35,  3.15it/s][A
 50%|█████     | 111/221 [00:40<00:38,  2.86it/s][A
 51%|█████     | 112/221 [00:40<00:37,  2.92it/s][A
 51%|█████     | 113/221 [00:41<00:34,  3.13it/s][A
 52%|█████▏    | 114/221 [00:41<00:32,  3.33it/s][A
 52%|█████▏    | 115/221 [00:41<00:30,  3.42it/s][A
 52%|█████▏    | 116/221 [00:42<00:37,  2.83it/s][A
 53%|█████▎    | 117/221 [00:42<00:36,  2.87it/s][A
 53%|█████▎    | 118/221 [00:42<00:36,  2.82it/s][A[h264 @ 0x564c31beda00] mmco: unref short failure
[h264 @ 0x564c31beda00] mmco: unref short failure

 54%|█████▍    | 119/221 [00:43<00:38,  2.68it/s][A
 54%|█████▍    | 120/221 [00:43<00:33,  3.05it/s][A
 55%|█████▍    | 121/221 [00:43<00:26,  3.79it/s][A
 55%|█████▌    | 122/221 [00:43<00:26,  3.70it/s][A
 56%|█████▌    | 123/221 [00:44<00:29,  3.36it/s][A
 56%|█████▌    | 124/221 [00:44<00:29,  3.32it/s][A[h264 @ 0x561ab5d46c80] mmco: unref short failure
[h264 @ 0x561ab5d46c80] mmco: unref short failure

 57%|█████▋    | 125/221 [00:44<00:32,  2.98it/s][A
 57%|█████▋    | 126/221 [00:45<00:30,  3.11it/s][A
 57%|█████▋    | 127/221 [00:45<00:36,  2.56it/s][A
 58%|█████▊    | 128/221 [00:46<00:38,  2.43it/s][A
 58%|█████▊    | 129/221 [00:46<00:31,  2.97it/s][A
 59%|█████▉    | 130/221 [00:46<00:28,  3.23it/s][A
 59%|█████▉    | 131/221 [00:46<00:25,  3.52it/s][A
 60%|█████▉    | 132/221 [00:47<00:37,  2.38it/s][A
 60%|██████    | 133/221 [00:48<00:39,  2.21it/s][A
 61%|██████    | 134/221 [00:48<00:43,  2.02it/s][A[h264 @ 0x561ab178a600] mmco: unref short failure

 61%|██████    | 135/221 [00:49<00:44,  1.95it/s][A[h264 @ 0x562f18ded780] mmco: unref short failure
[h264 @ 0x561ab178a600] mmco: unref short failure

 62%|██████▏   | 136/221 [00:49<00:41,  2.05it/s][A
 62%|██████▏   | 137/221 [00:50<00:40,  2.06it/s][A
 62%|██████▏   | 138/221 [00:50<00:41,  2.02it/s][A
 63%|██████▎   | 139/221 [00:51<00:40,  2.02it/s][A
 63%|██████▎   | 140/221 [00:51<00:38,  2.09it/s][A
 64%|██████▍   | 141/221 [00:52<00:38,  2.10it/s][A
 64%|██████▍   | 142/221 [00:52<00:34,  2.30it/s][A
 65%|██████▍   | 143/221 [00:52<00:31,  2.44it/s][A
 65%|██████▌   | 144/221 [00:53<00:30,  2.51it/s][A
 66%|██████▌   | 145/221 [00:53<00:26,  2.91it/s][A
 66%|██████▌   | 146/221 [00:53<00:20,  3.58it/s][A
 67%|██████▋   | 147/221 [00:53<00:22,  3.36it/s][A
 67%|██████▋   | 148/221 [00:54<00:29,  2.47it/s][A
 67%|██████▋   | 149/221 [00:54<00:30,  2.36it/s][A
 68%|██████▊   | 150/221 [00:55<00:28,  2.53it/s][A
 68%|██████▊   | 151/221 [00:55<00:28,  2.46it/s][A
 69%|██████▉   | 152/221 [00:56<00:30,  2.26it/s][A
 69%|██████▉   | 153/221 [00:56<00:24,  2.73it/s][A
 70%|██████▉   | 154/221 [00:56<00:20,  3.23it/s][A
 70%|███████   | 155/221 [00:56<00:19,  3.35it/s][A
 71%|███████   | 156/221 [00:57<00:20,  3.17it/s][A[h264 @ 0x564c32815800] mmco: unref short failure
[h264 @ 0x564c32815800] mmco: unref short failure
[h264 @ 0x564c32815800] mmco: unref short failure
[h264 @ 0x564c32815800] mmco: unref short failure

 71%|███████   | 157/221 [00:58<00:30,  2.10it/s][A
 71%|███████▏  | 158/221 [00:58<00:26,  2.37it/s][A
 72%|███████▏  | 159/221 [00:58<00:21,  2.94it/s][A
 72%|███████▏  | 160/221 [00:58<00:17,  3.40it/s][A
 73%|███████▎  | 161/221 [00:58<00:15,  3.77it/s][A
 73%|███████▎  | 162/221 [00:59<00:14,  4.19it/s][A
 74%|███████▍  | 163/221 [00:59<00:15,  3.81it/s][A[h264 @ 0x55cb7bf53900] mmco: unref short failure

 74%|███████▍  | 164/221 [00:59<00:16,  3.52it/s][A
 75%|███████▍  | 165/221 [01:00<00:17,  3.25it/s][A
 75%|███████▌  | 166/221 [01:00<00:19,  2.89it/s][A
 76%|███████▌  | 167/221 [01:00<00:17,  3.17it/s][A
 76%|███████▌  | 168/221 [01:01<00:18,  2.82it/s][A
 76%|███████▋  | 169/221 [01:01<00:15,  3.46it/s][A
 77%|███████▋  | 170/221 [01:01<00:16,  3.10it/s][A
 77%|███████▋  | 171/221 [01:02<00:17,  2.81it/s][A
 78%|███████▊  | 172/221 [01:02<00:17,  2.84it/s][A
 78%|███████▊  | 173/221 [01:02<00:18,  2.64it/s][A
 79%|███████▊  | 174/221 [01:03<00:16,  2.92it/s][A
 79%|███████▉  | 175/221 [01:03<00:14,  3.18it/s][A
 80%|███████▉  | 176/221 [01:03<00:12,  3.70it/s][A
 80%|████████  | 177/221 [01:03<00:11,  3.82it/s][A[h264 @ 0x562f0b58ef40] mmco: unref short failure

 81%|████████  | 178/221 [01:04<00:14,  2.94it/s][A
 81%|████████  | 179/221 [01:04<00:15,  2.76it/s][A
 81%|████████▏ | 180/221 [01:04<00:12,  3.29it/s][A
 82%|████████▏ | 181/221 [01:05<00:10,  3.78it/s][A
 82%|████████▏ | 182/221 [01:05<00:10,  3.65it/s][A
 83%|████████▎ | 183/221 [01:05<00:11,  3.39it/s][A
 83%|████████▎ | 184/221 [01:06<00:10,  3.42it/s][A
 84%|████████▎ | 185/221 [01:06<00:09,  3.77it/s][A
 84%|████████▍ | 186/221 [01:06<00:12,  2.81it/s][A
 85%|████████▍ | 187/221 [01:07<00:11,  3.05it/s][A
 85%|████████▌ | 188/221 [01:07<00:10,  3.27it/s][A[h264 @ 0x561abfa13680] mmco: unref short failure
[h264 @ 0x561abfa13680] mmco: unref short failure
[h264 @ 0x561abfa13680] mmco: unref short failure
[h264 @ 0x561abfa13680] mmco: unref short failure

 86%|████████▌ | 189/221 [01:07<00:09,  3.27it/s][A
 86%|████████▌ | 190/221 [01:07<00:09,  3.41it/s][A
 86%|████████▋ | 191/221 [01:08<00:07,  4.21it/s][A
 87%|████████▋ | 192/221 [01:08<00:06,  4.35it/s][A
 87%|████████▋ | 193/221 [01:08<00:05,  5.03it/s][A
 88%|████████▊ | 194/221 [01:08<00:06,  4.45it/s][A
 88%|████████▊ | 195/221 [01:08<00:05,  4.39it/s][A
 89%|████████▊ | 196/221 [01:09<00:05,  4.23it/s][A
 89%|████████▉ | 197/221 [01:09<00:05,  4.55it/s][A
 90%|████████▉ | 198/221 [01:09<00:05,  3.86it/s][A
 90%|█████████ | 199/221 [01:10<00:05,  3.68it/s][A
 90%|█████████ | 200/221 [01:10<00:05,  3.70it/s][A
 91%|█████████ | 201/221 [01:10<00:04,  4.05it/s][A
 91%|█████████▏| 202/221 [01:10<00:05,  3.49it/s][A
 92%|█████████▏| 203/221 [01:11<00:04,  3.75it/s][A
 92%|█████████▏| 204/221 [01:11<00:04,  3.97it/s][A
 93%|█████████▎| 205/221 [01:11<00:03,  4.44it/s][A
 93%|█████████▎| 206/221 [01:11<00:04,  3.66it/s][A
 94%|█████████▎| 207/221 [01:12<00:03,  3.85it/s][A
 94%|█████████▍| 208/221 [01:12<00:03,  3.53it/s][A
 95%|█████████▍| 209/221 [01:12<00:03,  3.52it/s][A
 95%|█████████▌| 210/221 [01:12<00:02,  4.34it/s][A
 95%|█████████▌| 211/221 [01:13<00:02,  3.44it/s][A
 96%|█████████▌| 212/221 [01:13<00:02,  3.19it/s][A
 96%|█████████▋| 213/221 [01:13<00:02,  3.57it/s][A
 97%|█████████▋| 214/221 [01:14<00:02,  3.00it/s][A
 97%|█████████▋| 215/221 [01:14<00:01,  3.57it/s][A
 98%|█████████▊| 216/221 [01:14<00:01,  3.46it/s][A
 98%|█████████▊| 217/221 [01:15<00:01,  2.30it/s][A
 99%|█████████▊| 218/221 [01:15<00:01,  2.58it/s][A
 99%|█████████▉| 219/221 [01:16<00:00,  2.85it/s][A
100%|█████████▉| 220/221 [01:16<00:00,  2.65it/s][A
100%|██████████| 221/221 [01:16<00:00,  2.93it/s][A100%|██████████| 221/221 [01:16<00:00,  2.88it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:02,  3.51it/s][A
  1%|          | 2/221 [00:00<00:59,  3.66it/s][A
  1%|▏         | 3/221 [00:00<00:58,  3.71it/s][A
  2%|▏         | 4/221 [00:01<00:58,  3.73it/s][A
  2%|▏         | 5/221 [00:01<00:57,  3.74it/s][A
  3%|▎         | 6/221 [00:01<00:57,  3.75it/s][A
  3%|▎         | 7/221 [00:01<00:56,  3.76it/s][A
  4%|▎         | 8/221 [00:02<00:56,  3.76it/s][A
  4%|▍         | 9/221 [00:02<00:56,  3.77it/s][A
  5%|▍         | 10/221 [00:02<00:56,  3.77it/s][A
  5%|▍         | 11/221 [00:02<00:55,  3.77it/s][A
  5%|▌         | 12/221 [00:03<00:55,  3.77it/s][A
  6%|▌         | 13/221 [00:03<00:55,  3.77it/s][A
  6%|▋         | 14/221 [00:03<00:54,  3.77it/s][A
  7%|▋         | 15/221 [00:03<00:54,  3.77it/s][A
  7%|▋         | 16/221 [00:04<00:54,  3.77it/s][A
  8%|▊         | 17/221 [00:04<00:54,  3.77it/s][A
  8%|▊         | 18/221 [00:04<00:53,  3.77it/s][A
  9%|▊         | 19/221 [00:05<00:53,  3.77it/s][A
  9%|▉         | 20/221 [00:05<00:53,  3.77it/s][A
 10%|▉         | 21/221 [00:05<00:52,  3.78it/s][A
 10%|▉         | 22/221 [00:05<00:52,  3.77it/s][A
 10%|█         | 23/221 [00:06<00:52,  3.78it/s][A
 11%|█         | 24/221 [00:06<00:52,  3.77it/s][A
 11%|█▏        | 25/221 [00:06<00:51,  3.77it/s][A
 12%|█▏        | 26/221 [00:06<00:53,  3.62it/s][A
 12%|█▏        | 27/221 [00:07<00:52,  3.67it/s][A
 13%|█▎        | 28/221 [00:07<00:52,  3.70it/s][A
 13%|█▎        | 29/221 [00:07<00:51,  3.73it/s][A
 14%|█▎        | 30/221 [00:08<00:51,  3.74it/s][A
 14%|█▍        | 31/221 [00:08<00:50,  3.76it/s][A
 14%|█▍        | 32/221 [00:08<00:50,  3.76it/s][A
 15%|█▍        | 33/221 [00:08<00:49,  3.77it/s][A
 15%|█▌        | 34/221 [00:09<00:49,  3.78it/s][A
 16%|█▌        | 35/221 [00:09<00:49,  3.78it/s][A
 16%|█▋        | 36/221 [00:09<00:48,  3.78it/s][A
 17%|█▋        | 37/221 [00:09<00:48,  3.78it/s][A
 17%|█▋        | 38/221 [00:10<00:48,  3.78it/s][A
 18%|█▊        | 39/221 [00:10<00:48,  3.79it/s][A
 18%|█▊        | 40/221 [00:10<00:47,  3.78it/s][A
 19%|█▊        | 41/221 [00:10<00:47,  3.79it/s][A
 19%|█▉        | 42/221 [00:11<00:47,  3.79it/s][A
 19%|█▉        | 43/221 [00:11<00:47,  3.79it/s][A
 20%|█▉        | 44/221 [00:11<00:46,  3.79it/s][A
 20%|██        | 45/221 [00:11<00:46,  3.78it/s][A
 21%|██        | 46/221 [00:12<00:46,  3.79it/s][A
 21%|██▏       | 47/221 [00:12<00:45,  3.79it/s][A
 22%|██▏       | 48/221 [00:12<00:45,  3.79it/s][A
 22%|██▏       | 49/221 [00:13<00:45,  3.79it/s][A
 23%|██▎       | 50/221 [00:13<00:45,  3.79it/s][A
 23%|██▎       | 51/221 [00:13<00:44,  3.79it/s][A
 24%|██▎       | 52/221 [00:13<00:44,  3.79it/s][A
 24%|██▍       | 53/221 [00:14<00:44,  3.79it/s][A
 24%|██▍       | 54/221 [00:14<00:44,  3.79it/s][A
 25%|██▍       | 55/221 [00:14<00:43,  3.79it/s][A
 25%|██▌       | 56/221 [00:14<00:43,  3.79it/s][A
 26%|██▌       | 57/221 [00:15<00:43,  3.79it/s][A
 26%|██▌       | 58/221 [00:15<00:43,  3.79it/s][A
 27%|██▋       | 59/221 [00:15<00:42,  3.79it/s][A
 27%|██▋       | 60/221 [00:15<00:42,  3.79it/s][A
 28%|██▊       | 61/221 [00:16<00:42,  3.79it/s][A
 28%|██▊       | 62/221 [00:16<00:41,  3.79it/s][A
 29%|██▊       | 63/221 [00:16<00:41,  3.79it/s][A
 29%|██▉       | 64/221 [00:16<00:41,  3.79it/s][A
 29%|██▉       | 65/221 [00:17<00:41,  3.79it/s][A
 30%|██▉       | 66/221 [00:17<00:40,  3.79it/s][A
 30%|███       | 67/221 [00:17<00:40,  3.79it/s][A
 31%|███       | 68/221 [00:18<00:40,  3.79it/s][A
 31%|███       | 69/221 [00:18<00:40,  3.79it/s][A
 32%|███▏      | 70/221 [00:18<00:39,  3.78it/s][A
 32%|███▏      | 71/221 [00:18<00:39,  3.78it/s][A
 33%|███▎      | 72/221 [00:19<00:39,  3.78it/s][A
 33%|███▎      | 73/221 [00:19<00:39,  3.78it/s][A
 33%|███▎      | 74/221 [00:19<00:38,  3.78it/s][A
 34%|███▍      | 75/221 [00:19<00:38,  3.78it/s][A
 34%|███▍      | 76/221 [00:20<00:38,  3.78it/s][A
 35%|███▍      | 77/221 [00:20<00:38,  3.79it/s][A
 35%|███▌      | 78/221 [00:20<00:37,  3.79it/s][A
 36%|███▌      | 79/221 [00:20<00:37,  3.79it/s][A
 36%|███▌      | 80/221 [00:21<00:37,  3.79it/s][A
 37%|███▋      | 81/221 [00:21<00:36,  3.79it/s][A
 37%|███▋      | 82/221 [00:21<00:36,  3.79it/s][A
 38%|███▊      | 83/221 [00:22<00:36,  3.79it/s][A
 38%|███▊      | 84/221 [00:22<00:36,  3.79it/s][A
 38%|███▊      | 85/221 [00:22<00:35,  3.79it/s][A
 39%|███▉      | 86/221 [00:22<00:35,  3.79it/s][A
 39%|███▉      | 87/221 [00:23<00:35,  3.79it/s][A
 40%|███▉      | 88/221 [00:23<00:35,  3.79it/s][A
 40%|████      | 89/221 [00:23<00:34,  3.78it/s][A
 41%|████      | 90/221 [00:23<00:34,  3.78it/s][A
 41%|████      | 91/221 [00:24<00:34,  3.78it/s][A
 42%|████▏     | 92/221 [00:24<00:34,  3.78it/s][A
 42%|████▏     | 93/221 [00:24<00:33,  3.79it/s][A
 43%|████▎     | 94/221 [00:24<00:33,  3.79it/s][A
 43%|████▎     | 95/221 [00:25<00:33,  3.79it/s][A
 43%|████▎     | 96/221 [00:25<00:33,  3.79it/s][A
 44%|████▍     | 97/221 [00:25<00:32,  3.79it/s][A
 44%|████▍     | 98/221 [00:25<00:32,  3.79it/s][A
 45%|████▍     | 99/221 [00:26<00:32,  3.79it/s][A
 45%|████▌     | 100/221 [00:26<00:31,  3.79it/s][A
 46%|████▌     | 101/221 [00:26<00:31,  3.79it/s][A
 46%|████▌     | 102/221 [00:27<00:31,  3.79it/s][A
 47%|████▋     | 103/221 [00:27<00:31,  3.79it/s][A
 47%|████▋     | 104/221 [00:27<00:30,  3.79it/s][A
 48%|████▊     | 105/221 [00:27<00:30,  3.79it/s][A
 48%|████▊     | 106/221 [00:28<00:30,  3.79it/s][A
 48%|████▊     | 107/221 [00:28<00:30,  3.79it/s][A
 49%|████▉     | 108/221 [00:28<00:29,  3.79it/s][A
 49%|████▉     | 109/221 [00:28<00:29,  3.79it/s][A
 50%|████▉     | 110/221 [00:29<00:29,  3.79it/s][A
 50%|█████     | 111/221 [00:29<00:29,  3.79it/s][A
 51%|█████     | 112/221 [00:29<00:28,  3.79it/s][A
 51%|█████     | 113/221 [00:29<00:28,  3.79it/s][A
 52%|█████▏    | 114/221 [00:30<00:28,  3.79it/s][A
 52%|█████▏    | 115/221 [00:30<00:27,  3.79it/s][A
 52%|█████▏    | 116/221 [00:30<00:27,  3.79it/s][A
 53%|█████▎    | 117/221 [00:30<00:27,  3.79it/s][A
 53%|█████▎    | 118/221 [00:31<00:27,  3.79it/s][A
 54%|█████▍    | 119/221 [00:31<00:26,  3.79it/s][A
 54%|█████▍    | 120/221 [00:31<00:26,  3.78it/s][A
 55%|█████▍    | 121/221 [00:32<00:26,  3.78it/s][A
 55%|█████▌    | 122/221 [00:32<00:26,  3.78it/s][A
 56%|█████▌    | 123/221 [00:32<00:25,  3.78it/s][A
 56%|█████▌    | 124/221 [00:32<00:25,  3.79it/s][A
 57%|█████▋    | 125/221 [00:33<00:25,  3.79it/s][A
 57%|█████▋    | 126/221 [00:33<00:25,  3.79it/s][A
 57%|█████▋    | 127/221 [00:33<00:24,  3.79it/s][A
 58%|█████▊    | 128/221 [00:33<00:24,  3.79it/s][A
 58%|█████▊    | 129/221 [00:34<00:24,  3.79it/s][A
 59%|█████▉    | 130/221 [00:34<00:24,  3.79it/s][A
 59%|█████▉    | 131/221 [00:34<00:23,  3.79it/s][A
 60%|█████▉    | 132/221 [00:34<00:23,  3.79it/s][A
 60%|██████    | 133/221 [00:35<00:23,  3.79it/s][A
 61%|██████    | 134/221 [00:35<00:22,  3.79it/s][A
 61%|██████    | 135/221 [00:35<00:22,  3.79it/s][A
 62%|██████▏   | 136/221 [00:36<00:22,  3.79it/s][A
 62%|██████▏   | 137/221 [00:36<00:22,  3.79it/s][A
 62%|██████▏   | 138/221 [00:36<00:21,  3.79it/s][A
 63%|██████▎   | 139/221 [00:36<00:21,  3.79it/s][A
 63%|██████▎   | 140/221 [00:37<00:21,  3.79it/s][A
 64%|██████▍   | 141/221 [00:37<00:21,  3.79it/s][A
 64%|██████▍   | 142/221 [00:37<00:20,  3.79it/s][A
 65%|██████▍   | 143/221 [00:37<00:20,  3.79it/s][A
 65%|██████▌   | 144/221 [00:38<00:20,  3.79it/s][A
 66%|██████▌   | 145/221 [00:38<00:20,  3.79it/s][A
 66%|██████▌   | 146/221 [00:38<00:19,  3.78it/s][A
 67%|██████▋   | 147/221 [00:38<00:19,  3.78it/s][A
 67%|██████▋   | 148/221 [00:39<00:19,  3.78it/s][A
 67%|██████▋   | 149/221 [00:39<00:19,  3.78it/s][A
 68%|██████▊   | 150/221 [00:39<00:18,  3.78it/s][A
 68%|██████▊   | 151/221 [00:39<00:18,  3.78it/s][A
 69%|██████▉   | 152/221 [00:40<00:18,  3.78it/s][A
 69%|██████▉   | 153/221 [00:40<00:17,  3.78it/s][A
 70%|██████▉   | 154/221 [00:40<00:17,  3.78it/s][A
 70%|███████   | 155/221 [00:41<00:17,  3.78it/s][A
 71%|███████   | 156/221 [00:41<00:17,  3.79it/s][A
 71%|███████   | 157/221 [00:41<00:16,  3.79it/s][A
 71%|███████▏  | 158/221 [00:41<00:16,  3.79it/s][A
 72%|███████▏  | 159/221 [00:42<00:16,  3.79it/s][A
 72%|███████▏  | 160/221 [00:42<00:16,  3.79it/s][A
 73%|███████▎  | 161/221 [00:42<00:15,  3.78it/s][A
 73%|███████▎  | 162/221 [00:42<00:15,  3.78it/s][A
 74%|███████▍  | 163/221 [00:43<00:15,  3.78it/s][A
 74%|███████▍  | 164/221 [00:43<00:15,  3.78it/s][A
 75%|███████▍  | 165/221 [00:43<00:14,  3.78it/s][A
 75%|███████▌  | 166/221 [00:43<00:14,  3.78it/s][A
 76%|███████▌  | 167/221 [00:44<00:14,  3.79it/s][A
 76%|███████▌  | 168/221 [00:44<00:13,  3.79it/s][A
 76%|███████▋  | 169/221 [00:44<00:13,  3.79it/s][A
 77%|███████▋  | 170/221 [00:44<00:13,  3.79it/s][A
 77%|███████▋  | 171/221 [00:45<00:13,  3.79it/s][A
 78%|███████▊  | 172/221 [00:45<00:12,  3.79it/s][A
 78%|███████▊  | 173/221 [00:45<00:12,  3.79it/s][A
 79%|███████▊  | 174/221 [00:46<00:12,  3.79it/s][A
 79%|███████▉  | 175/221 [00:46<00:12,  3.79it/s][A
 80%|███████▉  | 176/221 [00:46<00:11,  3.79it/s][A
 80%|████████  | 177/221 [00:46<00:11,  3.79it/s][A
 81%|████████  | 178/221 [00:47<00:11,  3.79it/s][A
 81%|████████  | 179/221 [00:47<00:11,  3.79it/s][A
 81%|████████▏ | 180/221 [00:47<00:10,  3.79it/s][A
 82%|████████▏ | 181/221 [00:47<00:10,  3.78it/s][A
 82%|████████▏ | 182/221 [00:48<00:10,  3.78it/s][A
 83%|████████▎ | 183/221 [00:48<00:10,  3.79it/s][A
 83%|████████▎ | 184/221 [00:48<00:09,  3.79it/s][A
 84%|████████▎ | 185/221 [00:48<00:09,  3.79it/s][A
 84%|████████▍ | 186/221 [00:49<00:09,  3.79it/s][A
 85%|████████▍ | 187/221 [00:49<00:08,  3.79it/s][A
 85%|████████▌ | 188/221 [00:49<00:08,  3.79it/s][A
 86%|████████▌ | 189/221 [00:50<00:08,  3.79it/s][A
 86%|████████▌ | 190/221 [00:50<00:08,  3.78it/s][A
 86%|████████▋ | 191/221 [00:50<00:07,  3.79it/s][A
 87%|████████▋ | 192/221 [00:50<00:07,  3.79it/s][A
 87%|████████▋ | 193/221 [00:51<00:07,  3.79it/s][A
 88%|████████▊ | 194/221 [00:51<00:07,  3.78it/s][A
 88%|████████▊ | 195/221 [00:51<00:06,  3.78it/s][A
 89%|████████▊ | 196/221 [00:51<00:06,  3.78it/s][A
 89%|████████▉ | 197/221 [00:52<00:06,  3.78it/s][A
 90%|████████▉ | 198/221 [00:52<00:06,  3.78it/s][A
 90%|█████████ | 199/221 [00:52<00:05,  3.78it/s][A
 90%|█████████ | 200/221 [00:52<00:05,  3.79it/s][A
 91%|█████████ | 201/221 [00:53<00:05,  3.79it/s][A
 91%|█████████▏| 202/221 [00:53<00:05,  3.79it/s][A
 92%|█████████▏| 203/221 [00:53<00:04,  3.79it/s][A
 92%|█████████▏| 204/221 [00:53<00:04,  3.79it/s][A
 93%|█████████▎| 205/221 [00:54<00:04,  3.79it/s][A
 93%|█████████▎| 206/221 [00:54<00:03,  3.79it/s][A
 94%|█████████▎| 207/221 [00:54<00:03,  3.79it/s][A
 94%|█████████▍| 208/221 [00:55<00:03,  3.79it/s][A
 95%|█████████▍| 209/221 [00:55<00:03,  3.79it/s][A
 95%|█████████▌| 210/221 [00:55<00:02,  3.79it/s][A
 95%|█████████▌| 211/221 [00:55<00:02,  3.79it/s][A
 96%|█████████▌| 212/221 [00:56<00:02,  3.79it/s][A
 96%|█████████▋| 213/221 [00:56<00:02,  3.79it/s][A
 97%|█████████▋| 214/221 [00:56<00:01,  3.79it/s][A
 97%|█████████▋| 215/221 [00:56<00:01,  3.79it/s][A
 98%|█████████▊| 216/221 [00:57<00:01,  3.79it/s][A
 98%|█████████▊| 217/221 [00:57<00:01,  3.79it/s][A
 99%|█████████▊| 218/221 [00:57<00:00,  3.78it/s][A
 99%|█████████▉| 219/221 [00:57<00:00,  3.78it/s][A
100%|█████████▉| 220/221 [00:58<00:00,  3.78it/s][A
100%|██████████| 221/221 [00:58<00:00,  3.78it/s][A100%|██████████| 221/221 [00:58<00:00,  3.78it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:48,  4.49it/s][A
  1%|▏         | 3/221 [00:00<00:54,  4.03it/s][A
  2%|▏         | 4/221 [00:00<00:44,  4.85it/s][A
  2%|▏         | 5/221 [00:01<00:44,  4.87it/s][A
  3%|▎         | 7/221 [00:01<00:42,  5.08it/s][A
  4%|▎         | 8/221 [00:01<00:47,  4.51it/s][A
  4%|▍         | 9/221 [00:01<00:44,  4.80it/s][A
  5%|▍         | 10/221 [00:02<01:03,  3.32it/s][A
  5%|▍         | 11/221 [00:02<01:00,  3.50it/s][A
  5%|▌         | 12/221 [00:02<00:53,  3.88it/s][A
  6%|▌         | 13/221 [00:03<01:03,  3.26it/s][A
  6%|▋         | 14/221 [00:03<00:58,  3.52it/s][A
  7%|▋         | 15/221 [00:03<00:55,  3.73it/s][A
  7%|▋         | 16/221 [00:04<01:01,  3.33it/s][A
  8%|▊         | 17/221 [00:04<01:17,  2.62it/s][A
  8%|▊         | 18/221 [00:04<01:06,  3.05it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.27it/s][A
  9%|▉         | 20/221 [00:05<00:53,  3.75it/s][A
 10%|▉         | 21/221 [00:05<00:51,  3.90it/s][A
 10%|▉         | 22/221 [00:05<00:50,  3.91it/s][A
 11%|█         | 24/221 [00:06<00:40,  4.88it/s][A
 11%|█▏        | 25/221 [00:06<00:40,  4.86it/s][A
 12%|█▏        | 26/221 [00:06<00:45,  4.30it/s][A
 12%|█▏        | 27/221 [00:06<00:39,  4.93it/s][A
 13%|█▎        | 28/221 [00:07<00:53,  3.64it/s][A
 13%|█▎        | 29/221 [00:07<00:49,  3.90it/s][A
 14%|█▎        | 30/221 [00:07<00:57,  3.31it/s][A
 14%|█▍        | 31/221 [00:08<00:53,  3.55it/s][A
 14%|█▍        | 32/221 [00:08<00:44,  4.26it/s][A
 15%|█▍        | 33/221 [00:08<00:42,  4.41it/s][A
 15%|█▌        | 34/221 [00:08<00:37,  5.05it/s][A
 16%|█▌        | 35/221 [00:08<00:44,  4.20it/s][A
 16%|█▋        | 36/221 [00:09<00:48,  3.78it/s][A
 17%|█▋        | 37/221 [00:09<00:43,  4.20it/s][A
 17%|█▋        | 38/221 [00:09<00:43,  4.22it/s][A
 18%|█▊        | 39/221 [00:09<00:40,  4.51it/s][A
 18%|█▊        | 40/221 [00:10<00:47,  3.80it/s][A
 19%|█▊        | 41/221 [00:10<00:48,  3.74it/s][A
 19%|█▉        | 42/221 [00:10<00:41,  4.28it/s][A
 19%|█▉        | 43/221 [00:10<00:39,  4.48it/s][A
 20%|█▉        | 44/221 [00:10<00:37,  4.72it/s][A
 20%|██        | 45/221 [00:11<00:44,  3.92it/s][A
 21%|██        | 46/221 [00:11<00:44,  3.92it/s][A
 21%|██▏       | 47/221 [00:11<00:43,  4.00it/s][A
 22%|██▏       | 48/221 [00:11<00:36,  4.75it/s][A
 22%|██▏       | 49/221 [00:12<00:35,  4.80it/s][A
 23%|██▎       | 50/221 [00:12<00:49,  3.42it/s][A
 23%|██▎       | 51/221 [00:12<00:41,  4.14it/s][A
 24%|██▎       | 52/221 [00:13<00:44,  3.82it/s][A
 24%|██▍       | 53/221 [00:13<00:41,  4.09it/s][A
 24%|██▍       | 54/221 [00:13<00:44,  3.77it/s][A
 25%|██▍       | 55/221 [00:13<00:45,  3.66it/s][A
 25%|██▌       | 56/221 [00:14<00:39,  4.19it/s][A
 26%|██▌       | 57/221 [00:14<00:37,  4.32it/s][A
 26%|██▌       | 58/221 [00:14<00:46,  3.52it/s][A
 27%|██▋       | 59/221 [00:14<00:40,  3.96it/s][A
 27%|██▋       | 60/221 [00:14<00:35,  4.48it/s][A
 28%|██▊       | 61/221 [00:15<00:35,  4.52it/s][A
 28%|██▊       | 62/221 [00:15<00:50,  3.12it/s][A
 29%|██▊       | 63/221 [00:16<00:46,  3.38it/s][A
 29%|██▉       | 64/221 [00:16<00:49,  3.17it/s][A
 29%|██▉       | 65/221 [00:16<00:45,  3.44it/s][A
 30%|██▉       | 66/221 [00:17<00:52,  2.97it/s][A
 30%|███       | 67/221 [00:17<00:48,  3.18it/s][A
 31%|███       | 68/221 [00:17<00:43,  3.55it/s][A
 31%|███       | 69/221 [00:18<00:59,  2.57it/s][A
 32%|███▏      | 70/221 [00:18<00:53,  2.80it/s][A
 32%|███▏      | 71/221 [00:18<00:45,  3.33it/s][A
 33%|███▎      | 72/221 [00:19<00:51,  2.88it/s][A
 33%|███▎      | 73/221 [00:19<00:48,  3.05it/s][A
 34%|███▍      | 75/221 [00:19<00:37,  3.88it/s][A
 34%|███▍      | 76/221 [00:19<00:35,  4.07it/s][A
 35%|███▍      | 77/221 [00:20<00:42,  3.36it/s][A
 35%|███▌      | 78/221 [00:20<00:37,  3.78it/s][A
 36%|███▌      | 79/221 [00:21<00:47,  2.98it/s][A
 36%|███▌      | 80/221 [00:21<00:45,  3.07it/s][A
 37%|███▋      | 81/221 [00:21<00:39,  3.50it/s][A
 37%|███▋      | 82/221 [00:21<00:42,  3.30it/s][A
 38%|███▊      | 83/221 [00:22<00:41,  3.30it/s][A
 38%|███▊      | 84/221 [00:22<00:40,  3.36it/s][A
 38%|███▊      | 85/221 [00:22<00:33,  4.06it/s][A
 39%|███▉      | 86/221 [00:22<00:30,  4.41it/s][A
 39%|███▉      | 87/221 [00:23<00:35,  3.82it/s][A
 40%|███▉      | 88/221 [00:23<00:43,  3.03it/s][A
 40%|████      | 89/221 [00:23<00:42,  3.09it/s][A
 41%|████      | 90/221 [00:24<00:38,  3.38it/s][A
 41%|████      | 91/221 [00:24<00:34,  3.80it/s][A
 42%|████▏     | 92/221 [00:24<00:37,  3.46it/s][A
 42%|████▏     | 93/221 [00:25<00:42,  2.98it/s][A
 43%|████▎     | 94/221 [00:25<00:44,  2.85it/s][A
 43%|████▎     | 95/221 [00:25<00:42,  2.95it/s][A
 43%|████▎     | 96/221 [00:26<00:42,  2.95it/s][A
 44%|████▍     | 97/221 [00:26<00:39,  3.16it/s][A
 44%|████▍     | 98/221 [00:27<00:50,  2.46it/s][A
 45%|████▍     | 99/221 [00:27<00:43,  2.84it/s][A
 45%|████▌     | 100/221 [00:27<00:41,  2.90it/s][A
 46%|████▌     | 101/221 [00:27<00:38,  3.15it/s][A
 46%|████▌     | 102/221 [00:28<00:44,  2.68it/s][A
 47%|████▋     | 103/221 [00:28<00:36,  3.22it/s][A
 47%|████▋     | 104/221 [00:28<00:32,  3.55it/s][A
 48%|████▊     | 105/221 [00:29<00:36,  3.17it/s][A
 48%|████▊     | 106/221 [00:29<00:35,  3.27it/s][A
 48%|████▊     | 107/221 [00:29<00:32,  3.53it/s][A
 49%|████▉     | 108/221 [00:29<00:31,  3.55it/s][A
 49%|████▉     | 109/221 [00:30<00:27,  4.07it/s][A
 50%|████▉     | 110/221 [00:30<00:28,  3.89it/s][A
 50%|█████     | 111/221 [00:30<00:31,  3.49it/s][A
 51%|█████     | 112/221 [00:30<00:31,  3.49it/s][A
 51%|█████     | 113/221 [00:31<00:27,  3.90it/s][A
 52%|█████▏    | 114/221 [00:31<00:22,  4.71it/s][A
 52%|█████▏    | 115/221 [00:31<00:23,  4.54it/s][A
 52%|█████▏    | 116/221 [00:31<00:24,  4.28it/s][A
 53%|█████▎    | 117/221 [00:32<00:25,  4.12it/s][A
 53%|█████▎    | 118/221 [00:32<00:24,  4.20it/s][A
 54%|█████▍    | 119/221 [00:32<00:28,  3.58it/s][A
 54%|█████▍    | 120/221 [00:32<00:25,  3.90it/s][A
 55%|█████▍    | 121/221 [00:32<00:21,  4.71it/s][A
 55%|█████▌    | 122/221 [00:33<00:23,  4.28it/s][A
 56%|█████▌    | 123/221 [00:33<00:23,  4.21it/s][A
 56%|█████▌    | 124/221 [00:33<00:25,  3.78it/s][A
 57%|█████▋    | 125/221 [00:34<00:28,  3.40it/s][A
 57%|█████▋    | 126/221 [00:34<00:25,  3.77it/s][A
 57%|█████▋    | 127/221 [00:34<00:28,  3.25it/s][A
 58%|█████▊    | 128/221 [00:35<00:28,  3.30it/s][A
 58%|█████▊    | 129/221 [00:35<00:22,  4.04it/s][A
 59%|█████▉    | 130/221 [00:35<00:22,  4.04it/s][A
 59%|█████▉    | 131/221 [00:35<00:18,  4.79it/s][A
 60%|█████▉    | 132/221 [00:36<00:28,  3.17it/s][A
 60%|██████    | 133/221 [00:36<00:27,  3.19it/s][A
 61%|██████    | 134/221 [00:36<00:31,  2.79it/s][A
 61%|██████    | 135/221 [00:37<00:31,  2.77it/s][A
 62%|██████▏   | 136/221 [00:37<00:28,  2.96it/s][A
 62%|██████▏   | 137/221 [00:37<00:26,  3.22it/s][A
 62%|██████▏   | 138/221 [00:38<00:26,  3.07it/s][A
 63%|██████▎   | 139/221 [00:38<00:26,  3.06it/s][A
 63%|██████▎   | 140/221 [00:38<00:27,  2.95it/s][A
 64%|██████▍   | 141/221 [00:39<00:26,  3.00it/s][A
 64%|██████▍   | 142/221 [00:39<00:23,  3.36it/s][A
 65%|██████▍   | 143/221 [00:39<00:24,  3.22it/s][A
 65%|██████▌   | 144/221 [00:40<00:24,  3.11it/s][A
 66%|██████▌   | 145/221 [00:40<00:21,  3.55it/s][A
 66%|██████▌   | 146/221 [00:40<00:17,  4.38it/s][A
 67%|██████▋   | 147/221 [00:40<00:19,  3.78it/s][A
 67%|██████▋   | 148/221 [00:41<00:24,  3.03it/s][A
 67%|██████▋   | 149/221 [00:41<00:26,  2.67it/s][A
 68%|██████▊   | 150/221 [00:41<00:24,  2.86it/s][A
 68%|██████▊   | 151/221 [00:42<00:25,  2.80it/s][A
 69%|██████▉   | 152/221 [00:42<00:24,  2.79it/s][A
 69%|██████▉   | 153/221 [00:42<00:20,  3.35it/s][A
 70%|██████▉   | 154/221 [00:43<00:17,  3.81it/s][A
 70%|███████   | 155/221 [00:43<00:17,  3.88it/s][A
 71%|███████   | 156/221 [00:43<00:18,  3.59it/s][A
 71%|███████   | 157/221 [00:43<00:19,  3.33it/s][A
 71%|███████▏  | 158/221 [00:44<00:18,  3.41it/s][A
 72%|███████▏  | 159/221 [00:44<00:15,  4.13it/s][A
 72%|███████▏  | 160/221 [00:44<00:13,  4.48it/s][A
 73%|███████▎  | 161/221 [00:44<00:13,  4.56it/s][A
 74%|███████▍  | 163/221 [00:45<00:11,  4.89it/s][A
 74%|███████▍  | 164/221 [00:45<00:12,  4.58it/s][A
 75%|███████▍  | 165/221 [00:45<00:14,  3.82it/s][A
 75%|███████▌  | 166/221 [00:46<00:14,  3.88it/s][A
 76%|███████▌  | 167/221 [00:46<00:11,  4.53it/s][A
 76%|███████▌  | 168/221 [00:46<00:11,  4.53it/s][A
 76%|███████▋  | 169/221 [00:46<00:09,  5.20it/s][A
 77%|███████▋  | 170/221 [00:46<00:12,  4.17it/s][A
 77%|███████▋  | 171/221 [00:47<00:12,  3.90it/s][A
 78%|███████▊  | 172/221 [00:47<00:13,  3.65it/s][A
 78%|███████▊  | 173/221 [00:47<00:15,  3.14it/s][A
 79%|███████▊  | 174/221 [00:48<00:15,  3.11it/s][A
 79%|███████▉  | 175/221 [00:48<00:13,  3.30it/s][A
 80%|███████▉  | 176/221 [00:48<00:11,  3.96it/s][A
 80%|████████  | 177/221 [00:48<00:10,  4.02it/s][A
 81%|████████  | 178/221 [00:49<00:13,  3.22it/s][A
 81%|████████  | 179/221 [00:49<00:13,  3.20it/s][A
 81%|████████▏ | 180/221 [00:49<00:11,  3.63it/s][A
 82%|████████▏ | 181/221 [00:50<00:09,  4.03it/s][A
 82%|████████▏ | 182/221 [00:50<00:11,  3.44it/s][A
 83%|████████▎ | 183/221 [00:50<00:11,  3.26it/s][A
 83%|████████▎ | 184/221 [00:50<00:10,  3.49it/s][A
 84%|████████▎ | 185/221 [00:51<00:09,  3.93it/s][A
 84%|████████▍ | 186/221 [00:51<00:12,  2.73it/s][A
 85%|████████▍ | 187/221 [00:52<00:11,  2.87it/s][A
 85%|████████▌ | 188/221 [00:52<00:10,  3.02it/s][A
 86%|████████▌ | 189/221 [00:52<00:09,  3.21it/s][A
 86%|████████▌ | 190/221 [00:52<00:09,  3.32it/s][A
 86%|████████▋ | 191/221 [00:53<00:07,  3.92it/s][A
 87%|████████▋ | 192/221 [00:53<00:07,  3.90it/s][A
 87%|████████▋ | 193/221 [00:53<00:06,  4.54it/s][A
 88%|████████▊ | 194/221 [00:53<00:06,  4.31it/s][A
 88%|████████▊ | 195/221 [00:53<00:06,  4.07it/s][A
 89%|████████▊ | 196/221 [00:54<00:06,  3.67it/s][A
 89%|████████▉ | 197/221 [00:54<00:06,  3.97it/s][A
 90%|████████▉ | 198/221 [00:54<00:06,  3.44it/s][A
 90%|█████████ | 199/221 [00:55<00:06,  3.41it/s][A
 90%|█████████ | 200/221 [00:55<00:06,  3.44it/s][A
 91%|█████████ | 201/221 [00:55<00:05,  3.78it/s][A
 91%|█████████▏| 202/221 [00:56<00:05,  3.26it/s][A
 92%|█████████▏| 203/221 [00:56<00:05,  3.56it/s][A
 92%|█████████▏| 204/221 [00:56<00:04,  3.83it/s][A
 93%|█████████▎| 205/221 [00:56<00:03,  4.50it/s][A
 93%|█████████▎| 206/221 [00:57<00:03,  3.80it/s][A
 94%|█████████▎| 207/221 [00:57<00:03,  3.80it/s][A
 94%|█████████▍| 208/221 [00:57<00:03,  3.42it/s][A
 95%|█████████▍| 209/221 [00:57<00:03,  3.42it/s][A
 95%|█████████▌| 210/221 [00:58<00:02,  4.11it/s][A
 95%|█████████▌| 211/221 [00:58<00:02,  3.42it/s][A
 96%|█████████▌| 212/221 [00:58<00:02,  3.54it/s][A
 96%|█████████▋| 213/221 [00:58<00:01,  4.01it/s][A
 97%|█████████▋| 214/221 [00:59<00:02,  3.42it/s][A
 97%|█████████▋| 215/221 [00:59<00:01,  3.89it/s][A
 98%|█████████▊| 216/221 [00:59<00:01,  3.46it/s][A
 98%|█████████▊| 217/221 [01:00<00:01,  2.98it/s][A
 99%|█████████▊| 218/221 [01:00<00:01,  2.97it/s][A
 99%|█████████▉| 219/221 [01:00<00:00,  3.08it/s][A
100%|█████████▉| 220/221 [01:01<00:00,  3.32it/s][A
100%|██████████| 221/221 [01:01<00:00,  3.27it/s][A100%|██████████| 221/221 [01:01<00:00,  3.59it/s]
09/10/2024 14:03:35 - INFO - __main__ -   ====-evaluation--ret%tv--msrvtt_ret_ret_area_forward=====step 49--===========

09/10/2024 14:03:35 - INFO - __main__ -   {'area_r1': 36.4, 'area_recall': '36.4/58.6/65.5', 'area_ravg': 53.5}
09/10/2024 14:03:35 - INFO - __main__ -   ====-evaluation--ret%tv--msrvtt_ret_ret_area_backard=====step 49--===========

09/10/2024 14:03:35 - INFO - __main__ -   {'forward_r1': 37.9, 'forward_recall': '37.9/67.1/77.7', 'forward_ravg': 60.9}
09/10/2024 14:03:35 - INFO - __main__ -   ====-evaluation--ret%tv--msrvtt_ret_ret_area_back_with_video=====step 49--===========

09/10/2024 14:03:35 - INFO - __main__ -   {'area_video_r1': 37.1, 'area_video_recall': '37.1/68.3/77.8', 'area_video_ravg': 61.1}
09/10/2024 14:03:35 - INFO - __main__ -   ======evaluation--ret%tv--msrvtt_ret_ret_area_back_with_video====history best step: 49=======

09/10/2024 14:03:35 - INFO - __main__ -   {'area_video_r1': 37.1, 'area_video_recall': '37.1/68.3/77.8', 'area_video_ravg': 61.1}
09/10/2024 14:03:35 - INFO - __main__ -   ====-evaluation--ret%tv--msrvtt_ret_ret_itm_area=====step 49--===========

09/10/2024 14:03:35 - INFO - __main__ -   {'area_video_r1': 49.7, 'area_video_recall': '49.7/73.9/81.0', 'area_video_ravg': 68.2, 'area_video_back_r1': 46.6, 'area_video_back_recall': '46.6/71.0/79.0', 'area_video_back_ravg': 65.5}
09/10/2024 14:03:35 - INFO - __main__ -   ======evaluation--ret%tv--msrvtt_ret_ret_itm_area====history best step: 49=======

09/10/2024 14:03:35 - INFO - __main__ -   {'area_video_r1': 49.7, 'area_video_recall': '49.7/73.9/81.0', 'area_video_ravg': 68.2, 'area_video_back_r1': 46.6, 'area_video_back_recall': '46.6/71.0/79.0', 'area_video_back_ravg': 65.5}
09/10/2024 14:03:35 - INFO - __main__ -   ====-evaluation--ret%tv--msrvtt_ret_ret_itc_tv=====step 49--===========

09/10/2024 14:03:35 - INFO - __main__ -   {'video_r1': 44.8, 'video_recall': '44.8/72.4/82.5', 'video_ravg': 66.6}
09/10/2024 14:03:35 - INFO - __main__ -   ======evaluation--ret%tv--msrvtt_ret_ret_itc_tv====history best step: 49=======

09/10/2024 14:03:35 - INFO - __main__ -   {'video_r1': 44.8, 'video_recall': '44.8/72.4/82.5', 'video_ravg': 66.6}
09/10/2024 14:03:35 - INFO - __main__ -   ====-evaluation--ret%tv--msrvtt_ret_ret_itm_tv=====step 49--===========

09/10/2024 14:03:35 - INFO - __main__ -   {'video_r1': 49.9, 'video_recall': '49.9/73.9/81.3', 'video_ravg': 68.4}
09/10/2024 14:03:35 - INFO - __main__ -   ======evaluation--ret%tv--msrvtt_ret_ret_itm_tv====history best step: 49=======

09/10/2024 14:03:35 - INFO - __main__ -   {'video_r1': 49.9, 'video_recall': '49.9/73.9/81.3', 'video_ravg': 68.4}
Traceback (most recent call last):
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 668, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/100: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/./run.py", line 81, in <module>
    main()
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/./run.py", line 64, in main
    train(model, optimizer, train_loader, val_loaders, args.run_cfg, start_step = start_step, verbose_time=False)
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/pipeline.py", line 143, in train
    model_saver.save(model, global_step, optimizer,best_indicator, run_cfg.save_best)
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/save.py", line 26, in save
    torch.save(state_dict, output_model_file)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 442, in save
    return
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 291, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 191360 vs 191248
Traceback (most recent call last):
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 441, in save
    _save(obj, opened_zipfile, pickle_module, pickle_protocol)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 668, in _save
    zip_file.write_record(name, storage.data_ptr(), num_bytes)
RuntimeError: [enforce fail at inline_container.cc:471] . PytorchStreamWriter failed writing file data/100: file write failed

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/./run.py", line 81, in <module>
    main()
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/./run.py", line 64, in main
    train(model, optimizer, train_loader, val_loaders, args.run_cfg, start_step = start_step, verbose_time=False)
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/pipeline.py", line 143, in train
    model_saver.save(model, global_step, optimizer,best_indicator, run_cfg.save_best)
  File "/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/save.py", line 26, in save
    torch.save(state_dict, output_model_file)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 442, in save
    return
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/serialization.py", line 291, in __exit__
    self.file_like.write_end_of_file()
RuntimeError: [enforce fail at inline_container.cc:337] . unexpected pos 191360 vs 191248
wandb: 
wandb: Run history:
wandb:  loss_area ██▇▇█▇██▇█▇▇▇█▇▇▇▇▆▆▆▆▆▆▆▆▄▅▅▅▄▄▃▄▄▃▂▃▂▁
wandb:   loss_itc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   loss_itm ▇▇▆▄▄▅█▆▄▅▆▃▄▆▃▄▃▃▂▂▂▄▂▂▆▄▂▃▃▂▃▃▁▃▃▆▃▃▃▃
wandb: total_loss ██▇▇█▇██▇█▇▇▇█▇▇▇▇▆▆▆▆▆▆▆▆▄▅▅▅▄▄▃▄▄▃▂▃▂▁
wandb: 
wandb: Run summary:
wandb:  loss_area 3.80171
wandb:   loss_itc 0.0
wandb:   loss_itm 0.00858
wandb: total_loss 3.81029
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/wandb/offline-run-20240910_133901-4es7wxpf
wandb: Find logs at: ./wandb/offline-run-20240910_133901-4es7wxpf/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
09/10/2024 14:03:40 - WARNING - urllib3.connectionpool -   Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x152c1965a130>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
09/10/2024 14:03:40 - WARNING - urllib3.connectionpool -   Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x152c1965bbb0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 457467 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 457468 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 457469 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 457466) of binary: /leonardo/home/userexternal/gcicchet/.conda/envs/vast/bin/python3
Traceback (most recent call last):
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py", line 196, in <module>
    main()
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py", line 192, in main
    launch(args)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py", line 177, in launch
    run(args)
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/run.py", line 785, in run
    elastic_launch(
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 134, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 250, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
./run.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2024-09-10_14:03:46
  host      : lrdn3226-net9-3.leonardo.local
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 457466)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
srun: error: lrdn3226: task 0: Exited with exit code 1
