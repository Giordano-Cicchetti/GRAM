scaNODELIST=lrdn0057
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
3012



DEVICE SET
DEVICE SET
DEVICE SET
DEVICE SET
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/11/2024 15:01:47 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/11/2024 15:01:47 - INFO - __main__ -   ==================model_configs==================

09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_model_type : vast
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_frozen_vision : False
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_frozen_audio : False
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_checkpointing : True
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_max_caption_len : 40
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_contra_dim : 512
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_vision_resolution : 224
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_audio_melbins : 64
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_audio_target_length : 1024
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_beam_size : 3
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_captioner_mode : False
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_generate_nums : 1
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_max_vision_sample_num : 2
09/11/2024 15:01:47 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
09/11/2024 15:01:47 - INFO - __main__ -   ==================run_configs==================

09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_checkpoint : 
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume3Mod120k
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_optim : adamw
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_weight_decay : 0.01
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_grad_norm : 2.0
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_resume : False
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_seed : 50
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_fp16 : True
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_bf16 : False
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_zero_shot : False
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_new_lr : 0
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_new_params_name : []
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_valid_freq : 10
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_dataset_mix_type : random
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_first_eval : True
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_num_train_steps : 0
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_save_best : True
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_pin_mem : True
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_vision_resolution : 224
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_use_ddp : False
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_mode : training
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_log_steps : 100
09/11/2024 15:01:47 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
09/11/2024 15:01:47 - INFO - __main__ -   ==================data_configs==================

09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_type : annoindexed
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_training : True
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_name : finetune_area
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_txt : ../vast27m/annotations120k.json
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_vision : ../vast27m/videos/
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_audio : ../vast27m/audios
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_vision_transforms : crop_flip
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_vision_format : video_rawvideo
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_vision_sample_num : 2
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_audio_sample_num : 1
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_task : ret%tv%ta
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_epoch : 5
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_n_workers : 8
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_finetune_area_train_batch_size : 256
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 8
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tva
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
09/11/2024 15:01:47 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
09/11/2024 15:01:51 - INFO - __main__ -   finetune_area Using clip mean and std.
09/11/2024 15:01:51 - INFO - __main__ -   finetune_area transforms crop_flip
ci sono 118544 labels
ci sono 118544 labels
ci sono 118544 labelsci sono 118544 labels

09/11/2024 15:02:40 - INFO - __main__ -   Create Dataset finetune_area Success
09/11/2024 15:02:40 - INFO - __main__ -    loader ret%tv%ta--finetune_area , ratio 2315 , bs_pergpu 64, n_workers 8
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564005978600] mmco: unref short failure
[h264 @ 0x564005978600] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/11/2024 15:02:43 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/11/2024 15:02:43 - INFO - __main__ -   msrvtt_ret transforms crop_flip
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
ci sono 884 labels
ci sono 884 labels
09/11/2024 15:02:44 - INFO - __main__ -   Create Dataset msrvtt_ret Success
ci sono 884 labels
ci sono 884 labels
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'Please 'pip install xformers'
Please 'pip install xformers'

Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
09/11/2024 15:02:46 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/11/2024 15:02:46 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/11/2024 15:02:46 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/11/2024 15:02:46 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x557819ad4540] mmco: unref short failure
[h264 @ 0x557819ad4540] mmco: unref short failure
[h264 @ 0x557819ad4540] mmco: unref short failure
[h264 @ 0x557819ad4540] mmco: unref short failure
[h264 @ 0x557819ad4540] mmco: unref short failure
[h264 @ 0x557819ad4540] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55e4bc226a40] mmco: unref short failure
[h264 @ 0x564007c904c0] mmco: unref short failure
[h264 @ 0x564007c904c0] mmco: unref short failure
[h264 @ 0x564007c904c0] mmco: unref short failure
[h264 @ 0x564007c904c0] mmco: unref short failure
[h264 @ 0x564007d79040] mmco: unref short failure
[h264 @ 0x55e4bc468340] mmco: unref short failure
[h264 @ 0x55e4bc50ce00] mmco: unref short failure
[h264 @ 0x55e4bc50ce00] mmco: unref short failure
[h264 @ 0x55e4bc50ce00] mmco: unref short failure
[h264 @ 0x55e4bc50ce00] mmco: unref short failure
[h264 @ 0x557819b4d040] mmco: unref short failure
[h264 @ 0x557819b4d040] mmco: unref short failure
[h264 @ 0x56154261bbc0] mmco: unref short failure
[h264 @ 0x56154261bbc0] mmco: unref short failure
[h264 @ 0x5640078e0700] mmco: unref short failure
[h264 @ 0x55781a95f5c0] mmco: unref short failure
[h264 @ 0x55781a95f5c0] mmco: unref short failure
[h264 @ 0x5615426156c0] mmco: unref short failure
[h264 @ 0x5615426156c0] mmco: unref short failure
[h264 @ 0x55e4bc2faf00] mmco: unref short failure
[h264 @ 0x55e4bc2faf00] mmco: unref short failure
09/11/2024 15:03:52 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/11/2024 15:03:53 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
[h264 @ 0x557819f9eb00] mmco: unref short failure
[h264 @ 0x557819f9eb00] mmco: unref short failure
[h264 @ 0x561542aec800] mmco: unref short failure
09/11/2024 15:03:56 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
[h264 @ 0x55781b747980] mmco: unref short failure
09/11/2024 15:04:03 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/11/2024 15:04:05 - INFO - root -   incompatible_keys.missing_keys: []
09/11/2024 15:04:05 - INFO - root -   incompatible_keys.missing_keys: []
09/11/2024 15:04:07 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/11/2024 15:04:08 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/11/2024 15:04:08 - INFO - root -   incompatible_keys.missing_keys: []
[h264 @ 0x55781a8ce300] mmco: unref short failure
[h264 @ 0x55781a8ce300] mmco: unref short failure
09/11/2024 15:04:10 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
[h264 @ 0x56400801ca80] mmco: unref short failure
[h264 @ 0x56400801ca80] mmco: unref short failure
[h264 @ 0x5615423f5c40] mmco: unref short failure
[h264 @ 0x5615423f5c40] mmco: unref short failure
[h264 @ 0x55781a81ee80] mmco: unref short failure
09/11/2024 15:04:14 - INFO - root -   incompatible_keys.missing_keys: []
[h264 @ 0x5615459c6640] mmco: unref short failure
09/11/2024 15:04:16 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/11/2024 15:04:20 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/11/2024 15:04:20 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/11/2024 15:04:21 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
[h264 @ 0x561548b53380] mmco: unref short failure
[h264 @ 0x561548b53380] mmco: unref short failure
[h264 @ 0x55e4bc5f7c40] mmco: unref short failure
[h264 @ 0x55e4bc5f7c40] mmco: unref short failure
[h264 @ 0x5615433a2380] mmco: unref short failure
[h264 @ 0x5615433a2380] mmco: unref short failure
[h264 @ 0x5615425faf80] mmco: unref short failure
[h264 @ 0x5615425faf80] mmco: unref short failure
[h264 @ 0x5615425faf80] mmco: unref short failure
09/11/2024 15:04:27 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.weight', 'cls.predictions.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.bias', 'cls.predictions.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.key.bias', 'cls.predictions.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x564008415b80] mmco: unref short failure
[h264 @ 0x564008415b80] mmco: unref short failure
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'cls.predictions.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.value.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x55e4bcbb3480] mmco: unref short failure
[h264 @ 0x55781c6dba80] mmco: unref short failure
[h264 @ 0x55781c6dba80] mmco: unref short failure
[h264 @ 0x55781c6dba80] mmco: unref short failure
[h264 @ 0x55781c6dba80] mmco: unref short failure
[h264 @ 0x557819fe94c0] mmco: unref short failure
[h264 @ 0x557819fe94c0] mmco: unref short failure
[h264 @ 0x55781969ad00] mmco: unref short failure
09/11/2024 15:04:52 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/ckpt/model_step_204994.pt
09/11/2024 15:04:52 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast
[h264 @ 0x56400831dd80] mmco: unref short failure
[h264 @ 0x561546bc24c0] mmco: unref short failure
[h264 @ 0x55e4c4e38680] mmco: unref short failure
09/11/2024 15:04:58 - INFO - __main__ -   Unexpected keys ['vision_encoder.text.logit_scale']
09/11/2024 15:04:58 - INFO - __main__ -   missing_keys  ['vision_encoder.logit_scale']
[h264 @ 0x557819685d00] mmco: unref short failure
[h264 @ 0x557819685d00] mmco: unref short failure
09/11/2024 15:05:05 - INFO - __main__ -   ==================learning_rate_settings==================

09/11/2024 15:05:05 - INFO - __main__ -     basic_lr : 2e-05
09/11/2024 15:05:05 - INFO - __main__ -     clip_lr_visual : 5e-07
09/11/2024 15:05:05 - INFO - __main__ -     clip_lr_visual_len : 245
09/11/2024 15:05:05 - INFO - __main__ -     new_lr : 0
09/11/2024 15:05:05 - INFO - __main__ -     new_params_name: []
09/11/2024 15:05:05 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
09/11/2024 15:05:05 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564008bf6580] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55781f212300] mmco: unref short failure
[h264 @ 0x55781f212300] mmco: unref short failure
[h264 @ 0x55e4bc039a40] mmco: unref short failure
[h264 @ 0x56154293a0c0] mmco: unref short failure
[h264 @ 0x56154293a0c0] mmco: unref short failure
[h264 @ 0x561542560040] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x561544aca940] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x561548021880] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55e4bfcda6c0] mmco: unref short failure
[h264 @ 0x55e4bfcda6c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x561547f620c0] mmco: unref short failure
[h264 @ 0x561547f620c0] mmco: unref short failure
[h264 @ 0x561547f620c0] mmco: unref short failure
[h264 @ 0x561547f620c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x564007903100] mmco: unref short failure
[h264 @ 0x564007903100] mmco: unref short failure
[h264 @ 0x55e4bc6db4c0] mmco: unref short failure
[h264 @ 0x55e4c4b1b300] mmco: unref short failure
[h264 @ 0x55e4c4b1b300] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[h264 @ 0x55e4bc7a4dc0] mmco: unref short failure
[h264 @ 0x55e4bc364700] mmco: unref short failure
[h264 @ 0x56154148e600] mmco: unref short failure
[h264 @ 0x56154148e600] mmco: unref short failure
[h264 @ 0x56154a938440] mmco: unref short failure
[h264 @ 0x561547355ac0] mmco: unref short failure
[h264 @ 0x561547355ac0] mmco: unref short failure
[h264 @ 0x55e4bd8b5440] mmco: unref short failure
[h264 @ 0x55e4bc642c00] mmco: unref short failure
[h264 @ 0x55e4bc642c00] mmco: unref short failure
[h264 @ 0x55e4c0bb7780] mmco: unref short failure
[h264 @ 0x561543a92a40] mmco: unref short failure
[h264 @ 0x5578195b3ac0] mmco: unref short failure
[h264 @ 0x56154a1b0180] mmco: unref short failure
[h264 @ 0x56154a1b0180] mmco: unref short failure
[h264 @ 0x56154a1b0180] mmco: unref short failure
[h264 @ 0x56154a1b0180] mmco: unref short failure
[h264 @ 0x55e4bf44dcc0] mmco: unref short failure
[h264 @ 0x55e4bf44dcc0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s]
  0%|          | 1/221 [00:00<00:33,  6.64it/s]
  1%|          | 2/221 [00:00<00:39,  5.56it/s]
  1%|▏         | 3/221 [00:00<00:52,  4.18it/s]
  2%|▏         | 4/221 [00:00<00:42,  5.13it/s]
  2%|▏         | 5/221 [00:00<00:36,  5.89it/s]
  3%|▎         | 7/221 [00:01<00:30,  7.04it/s]
  4%|▎         | 8/221 [00:01<00:29,  7.21it/s]
  4%|▍         | 9/221 [00:01<00:27,  7.62it/s]
  5%|▍         | 10/221 [00:01<00:26,  7.98it/s]
  5%|▍         | 11/221 [00:01<00:25,  8.27it/s]
  5%|▌         | 12/221 [00:04<02:53,  1.20it/s]
  6%|▌         | 13/221 [00:04<02:13,  1.56it/s]
  7%|▋         | 15/221 [00:04<01:21,  2.52it/s]
  8%|▊         | 17/221 [00:04<01:06,  3.08it/s]
  8%|▊         | 18/221 [00:05<00:58,  3.50it/s]
  9%|▊         | 19/221 [00:05<00:53,  3.80it/s]
  9%|▉         | 20/221 [00:05<00:44,  4.47it/s]
 10%|▉         | 21/221 [00:05<00:39,  5.12it/s]
 10%|▉         | 22/221 [00:08<03:14,  1.03it/s]
 11%|█         | 24/221 [00:08<01:55,  1.71it/s]
 11%|█▏        | 25/221 [00:08<01:38,  1.99it/s]
 12%|█▏        | 26/221 [00:09<01:20,  2.44it/s]
 13%|█▎        | 28/221 [00:09<00:54,  3.51it/s]
 13%|█▎        | 29/221 [00:09<00:46,  4.13it/s]
 14%|█▍        | 31/221 [00:09<00:36,  5.27it/s]
 15%|█▍        | 33/221 [00:09<00:30,  6.22it/s]
 16%|█▌        | 35/221 [00:10<00:25,  7.19it/s]
 17%|█▋        | 37/221 [00:10<00:24,  7.42it/s]
 17%|█▋        | 38/221 [00:10<00:23,  7.75it/s]
 18%|█▊        | 40/221 [00:10<00:22,  7.91it/s]
 19%|█▉        | 42/221 [00:10<00:20,  8.63it/s]
 20%|█▉        | 44/221 [00:11<00:19,  8.92it/s][h264 @ 0x55781f869780] mmco: unref short failure
[h264 @ 0x561540d9d3c0] mmco: unref short failure
[h264 @ 0x55781f869780] mmco: unref short failure

 20%|██        | 45/221 [00:11<00:37,  4.69it/s]
 21%|██        | 46/221 [00:12<00:39,  4.44it/s][h264 @ 0x561541eb9a40] mmco: unref short failure
[h264 @ 0x561541eb9a40] mmco: unref short failure
[h264 @ 0x561540e66080] mmco: unref short failure
[h264 @ 0x561540e66080] mmco: unref short failure
[h264 @ 0x561548d3a540] mmco: unref short failure
[h264 @ 0x561548d3a540] mmco: unref short failure
[h264 @ 0x561543bb5bc0] mmco: unref short failure
[h264 @ 0x561543bb5bc0] mmco: unref short failure
[h264 @ 0x561543bb5bc0] mmco: unref short failure
[h264 @ 0x561543bb5bc0] mmco: unref short failure

 21%|██▏       | 47/221 [00:16<03:47,  1.31s/it]
 22%|██▏       | 49/221 [00:17<02:20,  1.23it/s]
 23%|██▎       | 51/221 [00:17<01:32,  1.84it/s]
 24%|██▍       | 53/221 [00:17<01:05,  2.57it/s]
 24%|██▍       | 54/221 [00:21<03:09,  1.13s/it]
 25%|██▍       | 55/221 [00:24<04:19,  1.56s/it]
 26%|██▌       | 58/221 [00:24<02:14,  1.21it/s]
 27%|██▋       | 60/221 [00:24<01:35,  1.69it/s]
 29%|██▊       | 63/221 [00:25<00:58,  2.71it/s]
 30%|██▉       | 66/221 [00:30<02:09,  1.19it/s]
 31%|███       | 68/221 [00:30<01:38,  1.56it/s]
 32%|███▏      | 70/221 [00:30<01:14,  2.03it/s]
 33%|███▎      | 72/221 [00:30<00:57,  2.61it/s]
 34%|███▍      | 75/221 [00:30<00:38,  3.83it/s]
 35%|███▍      | 77/221 [00:30<00:29,  4.81it/s]
 36%|███▌      | 79/221 [00:31<00:29,  4.88it/s]
 37%|███▋      | 82/221 [00:31<00:23,  5.93it/s]
 38%|███▊      | 85/221 [00:31<00:16,  8.02it/s]
 40%|███▉      | 88/221 [00:31<00:14,  8.87it/s]
 41%|████      | 90/221 [00:32<00:16,  8.03it/s]
 42%|████▏     | 93/221 [00:32<00:15,  8.31it/s]
 43%|████▎     | 95/221 [00:32<00:13,  9.45it/s]
 44%|████▍     | 97/221 [00:33<00:15,  8.15it/s]
 45%|████▌     | 100/221 [00:33<00:11, 10.67it/s]
 46%|████▌     | 102/221 [00:33<00:10, 11.81it/s]
 48%|████▊     | 105/221 [00:33<00:08, 14.21it/s]
 48%|████▊     | 107/221 [00:33<00:09, 11.72it/s]
 50%|████▉     | 110/221 [00:33<00:07, 14.58it/s]
 51%|█████     | 112/221 [00:34<00:10, 10.55it/s]
 52%|█████▏    | 115/221 [00:34<00:07, 13.25it/s]
 53%|█████▎    | 117/221 [00:39<01:15,  1.38it/s]
 54%|█████▍    | 119/221 [00:39<00:56,  1.81it/s]
 55%|█████▌    | 122/221 [00:39<00:36,  2.69it/s]
 56%|█████▌    | 124/221 [00:40<00:28,  3.42it/s]
 57%|█████▋    | 126/221 [00:40<00:28,  3.29it/s]
 58%|█████▊    | 128/221 [00:41<00:27,  3.34it/s]
 59%|█████▉    | 130/221 [00:41<00:20,  4.37it/s]
 60%|█████▉    | 132/221 [00:41<00:16,  5.51it/s]
 61%|██████    | 134/221 [00:41<00:17,  5.01it/s]
 62%|██████▏   | 136/221 [00:42<00:21,  3.91it/s]
 62%|██████▏   | 137/221 [00:43<00:25,  3.34it/s]
 62%|██████▏   | 138/221 [00:43<00:25,  3.30it/s]
 63%|██████▎   | 139/221 [00:43<00:25,  3.23it/s]
 63%|██████▎   | 140/221 [00:44<00:25,  3.21it/s]
 64%|██████▍   | 141/221 [00:44<00:22,  3.62it/s]
 65%|██████▍   | 143/221 [00:44<00:15,  5.01it/s]
 67%|██████▋   | 147/221 [00:44<00:08,  8.93it/s]
 67%|██████▋   | 149/221 [00:44<00:07,  9.27it/s]
 68%|██████▊   | 151/221 [00:45<00:08,  7.78it/s]
 69%|██████▉   | 152/221 [00:45<00:09,  7.44it/s]
 70%|██████▉   | 154/221 [00:45<00:07,  9.12it/s]
 71%|███████   | 157/221 [00:50<00:49,  1.30it/s]
 71%|███████▏  | 158/221 [00:50<00:41,  1.50it/s]
 72%|███████▏  | 160/221 [00:50<00:28,  2.13it/s]
 74%|███████▍  | 163/221 [00:51<00:18,  3.20it/s]
 75%|███████▍  | 165/221 [00:51<00:14,  3.84it/s]
 76%|███████▌  | 167/221 [00:56<00:49,  1.10it/s]
 76%|███████▌  | 168/221 [01:00<01:12,  1.36s/it]
 77%|███████▋  | 170/221 [01:00<00:49,  1.03it/s]
 77%|███████▋  | 171/221 [01:00<00:40,  1.23it/s]
 79%|███████▊  | 174/221 [01:00<00:21,  2.15it/s]
 80%|███████▉  | 176/221 [01:00<00:15,  2.88it/s]
 81%|████████  | 178/221 [01:00<00:11,  3.81it/s]
 81%|████████▏ | 180/221 [01:01<00:11,  3.42it/s]
 83%|████████▎ | 183/221 [01:01<00:07,  4.89it/s]
 84%|████████▎ | 185/221 [01:02<00:05,  6.02it/s]
 85%|████████▌ | 188/221 [01:02<00:04,  8.16it/s]
 86%|████████▌ | 190/221 [01:02<00:03,  8.71it/s]
 88%|████████▊ | 194/221 [01:02<00:02, 12.72it/s]
 89%|████████▊ | 196/221 [01:02<00:01, 13.62it/s]
 90%|█████████ | 199/221 [01:02<00:01, 15.82it/s]
 91%|█████████▏| 202/221 [01:02<00:01, 16.30it/s]
 93%|█████████▎| 205/221 [01:02<00:00, 18.69it/s]
 94%|█████████▍| 208/221 [01:03<00:00, 15.40it/s]
 95%|█████████▌| 210/221 [01:03<00:00, 16.21it/s]
 96%|█████████▌| 212/221 [01:03<00:00, 15.01it/s]
 97%|█████████▋| 214/221 [01:03<00:00, 11.24it/s]
 98%|█████████▊| 217/221 [01:05<00:01,  3.22it/s]
 99%|█████████▉| 219/221 [01:06<00:00,  4.09it/s]
100%|██████████| 221/221 [01:11<00:00,  1.17it/s]
100%|██████████| 221/221 [01:11<00:00,  3.11it/s]

  0%|          | 0/221 [00:00<?, ?it/s]
  0%|          | 1/221 [00:00<01:04,  3.42it/s]
  1%|          | 2/221 [00:00<01:04,  3.42it/s]
  1%|▏         | 3/221 [00:00<01:03,  3.42it/s]
  2%|▏         | 4/221 [00:01<01:03,  3.42it/s]
  2%|▏         | 5/221 [00:01<01:03,  3.42it/s]
  3%|▎         | 6/221 [00:01<01:02,  3.42it/s]
  3%|▎         | 7/221 [00:02<01:02,  3.42it/s]
  4%|▎         | 8/221 [00:02<01:02,  3.42it/s]
  4%|▍         | 9/221 [00:02<01:01,  3.42it/s]
  5%|▍         | 10/221 [00:02<01:01,  3.42it/s]
  5%|▍         | 11/221 [00:03<01:01,  3.42it/s]
  5%|▌         | 12/221 [00:03<01:01,  3.42it/s]
  6%|▌         | 13/221 [00:03<01:00,  3.42it/s]
  6%|▋         | 14/221 [00:04<01:00,  3.42it/s]
  7%|▋         | 15/221 [00:04<01:00,  3.42it/s]
  7%|▋         | 16/221 [00:04<00:59,  3.42it/s]
  8%|▊         | 17/221 [00:04<00:59,  3.42it/s]
  8%|▊         | 18/221 [00:05<00:59,  3.42it/s]
  9%|▊         | 19/221 [00:05<00:59,  3.42it/s]
  9%|▉         | 20/221 [00:05<00:58,  3.42it/s]
 10%|▉         | 21/221 [00:06<00:58,  3.42it/s]
 10%|▉         | 22/221 [00:06<00:58,  3.42it/s]
 10%|█         | 23/221 [00:06<00:57,  3.42it/s]
 11%|█         | 24/221 [00:07<00:57,  3.42it/s]
 11%|█▏        | 25/221 [00:07<00:57,  3.42it/s]
 12%|█▏        | 26/221 [00:07<00:57,  3.42it/s]
 12%|█▏        | 27/221 [00:07<00:56,  3.42it/s]
 13%|█▎        | 28/221 [00:08<00:56,  3.42it/s]
 13%|█▎        | 29/221 [00:08<00:56,  3.42it/s]
 14%|█▎        | 30/221 [00:08<00:55,  3.42it/s]
 14%|█▍        | 31/221 [00:09<00:55,  3.42it/s]
 14%|█▍        | 32/221 [00:09<00:55,  3.42it/s]
 15%|█▍        | 33/221 [00:09<00:54,  3.42it/s]
 15%|█▌        | 34/221 [00:09<00:54,  3.42it/s]
 16%|█▌        | 35/221 [00:10<00:54,  3.42it/s]
 16%|█▋        | 36/221 [00:10<00:54,  3.42it/s]
 17%|█▋        | 37/221 [00:10<00:53,  3.42it/s]
 17%|█▋        | 38/221 [00:11<00:53,  3.42it/s]
 18%|█▊        | 39/221 [00:11<00:53,  3.42it/s]
 18%|█▊        | 40/221 [00:11<00:52,  3.42it/s]
 19%|█▊        | 41/221 [00:11<00:52,  3.42it/s]
 19%|█▉        | 42/221 [00:12<00:52,  3.42it/s]
 19%|█▉        | 43/221 [00:12<00:52,  3.42it/s]
 20%|█▉        | 44/221 [00:12<00:51,  3.42it/s]
 20%|██        | 45/221 [00:13<00:51,  3.42it/s]
 21%|██        | 46/221 [00:13<00:51,  3.42it/s]
 21%|██▏       | 47/221 [00:13<00:50,  3.42it/s]
 22%|██▏       | 48/221 [00:14<00:50,  3.42it/s]
 22%|██▏       | 49/221 [00:14<00:50,  3.42it/s]
 23%|██▎       | 50/221 [00:14<00:50,  3.42it/s]
 23%|██▎       | 51/221 [00:14<00:49,  3.42it/s]
 24%|██▎       | 52/221 [00:15<00:49,  3.42it/s]
 24%|██▍       | 53/221 [00:15<00:49,  3.42it/s]
 24%|██▍       | 54/221 [00:15<00:48,  3.42it/s]
 25%|██▍       | 55/221 [00:16<00:48,  3.42it/s]
 25%|██▌       | 56/221 [00:16<00:48,  3.42it/s]
 26%|██▌       | 57/221 [00:16<00:47,  3.42it/s]
 26%|██▌       | 58/221 [00:16<00:47,  3.42it/s]
 27%|██▋       | 59/221 [00:17<00:47,  3.42it/s]
 27%|██▋       | 60/221 [00:17<00:47,  3.42it/s]
 28%|██▊       | 61/221 [00:17<00:46,  3.42it/s]
 28%|██▊       | 62/221 [00:18<00:46,  3.42it/s]
 29%|██▊       | 63/221 [00:18<00:46,  3.42it/s]
 29%|██▉       | 64/221 [00:18<00:45,  3.42it/s]
 29%|██▉       | 65/221 [00:19<00:45,  3.42it/s]
 30%|██▉       | 66/221 [00:19<00:45,  3.42it/s]
 30%|███       | 67/221 [00:19<00:45,  3.42it/s]
 31%|███       | 68/221 [00:19<00:44,  3.42it/s]
 31%|███       | 69/221 [00:20<00:44,  3.42it/s]
 32%|███▏      | 70/221 [00:20<00:44,  3.42it/s]
 32%|███▏      | 71/221 [00:20<00:43,  3.42it/s]
 33%|███▎      | 72/221 [00:21<00:43,  3.42it/s]
 33%|███▎      | 73/221 [00:21<00:43,  3.42it/s]
 33%|███▎      | 74/221 [00:21<00:42,  3.42it/s]
 34%|███▍      | 75/221 [00:21<00:42,  3.42it/s]
 34%|███▍      | 76/221 [00:22<00:42,  3.42it/s]
 35%|███▍      | 77/221 [00:22<00:42,  3.42it/s]
 35%|███▌      | 78/221 [00:22<00:41,  3.42it/s]
 36%|███▌      | 79/221 [00:23<00:41,  3.42it/s]
 36%|███▌      | 80/221 [00:23<00:41,  3.42it/s]
 37%|███▋      | 81/221 [00:23<00:40,  3.42it/s]
 37%|███▋      | 82/221 [00:23<00:40,  3.42it/s]
 38%|███▊      | 83/221 [00:24<00:40,  3.42it/s]
 38%|███▊      | 84/221 [00:24<00:40,  3.42it/s]
 38%|███▊      | 85/221 [00:24<00:39,  3.42it/s]
 39%|███▉      | 86/221 [00:25<00:39,  3.42it/s]
 39%|███▉      | 87/221 [00:25<00:39,  3.42it/s]
 40%|███▉      | 88/221 [00:25<00:38,  3.42it/s]
 40%|████      | 89/221 [00:26<00:38,  3.42it/s]
 41%|████      | 90/221 [00:26<00:38,  3.42it/s]
 41%|████      | 91/221 [00:26<00:38,  3.42it/s]
 42%|████▏     | 92/221 [00:26<00:37,  3.42it/s]
 42%|████▏     | 93/221 [00:27<00:37,  3.42it/s]
 43%|████▎     | 94/221 [00:27<00:37,  3.42it/s]
 43%|████▎     | 95/221 [00:27<00:36,  3.42it/s]
 43%|████▎     | 96/221 [00:28<00:36,  3.42it/s]
 44%|████▍     | 97/221 [00:28<00:36,  3.42it/s]
 44%|████▍     | 98/221 [00:28<00:35,  3.42it/s]
 45%|████▍     | 99/221 [00:28<00:35,  3.42it/s]
 45%|████▌     | 100/221 [00:29<00:35,  3.42it/s]
 46%|████▌     | 101/221 [00:29<00:35,  3.42it/s]
 46%|████▌     | 102/221 [00:29<00:34,  3.42it/s]
 47%|████▋     | 103/221 [00:30<00:34,  3.42it/s]
 47%|████▋     | 104/221 [00:30<00:34,  3.42it/s]
 48%|████▊     | 105/221 [00:30<00:33,  3.42it/s]
 48%|████▊     | 106/221 [00:30<00:33,  3.42it/s]
 48%|████▊     | 107/221 [00:31<00:33,  3.42it/s]
 49%|████▉     | 108/221 [00:31<00:33,  3.42it/s]
 49%|████▉     | 109/221 [00:31<00:32,  3.42it/s]
 50%|████▉     | 110/221 [00:32<00:32,  3.42it/s]
 50%|█████     | 111/221 [00:32<00:32,  3.42it/s]
 51%|█████     | 112/221 [00:32<00:31,  3.42it/s]
 51%|█████     | 113/221 [00:33<00:31,  3.42it/s]
 52%|█████▏    | 114/221 [00:33<00:31,  3.42it/s]
 52%|█████▏    | 115/221 [00:33<00:30,  3.42it/s]
 52%|█████▏    | 116/221 [00:33<00:30,  3.42it/s]
 53%|█████▎    | 117/221 [00:34<00:30,  3.42it/s]
 53%|█████▎    | 118/221 [00:34<00:30,  3.42it/s]
 54%|█████▍    | 119/221 [00:34<00:29,  3.42it/s]
 54%|█████▍    | 120/221 [00:35<00:29,  3.42it/s]
 55%|█████▍    | 121/221 [00:35<00:29,  3.42it/s]
 55%|█████▌    | 122/221 [00:35<00:28,  3.42it/s]
 56%|█████▌    | 123/221 [00:35<00:28,  3.42it/s]
 56%|█████▌    | 124/221 [00:36<00:28,  3.42it/s]
 57%|█████▋    | 125/221 [00:36<00:28,  3.42it/s]
 57%|█████▋    | 126/221 [00:36<00:27,  3.42it/s]
 57%|█████▋    | 127/221 [00:37<00:27,  3.42it/s]
 58%|█████▊    | 128/221 [00:37<00:27,  3.42it/s]
 58%|█████▊    | 129/221 [00:37<00:26,  3.42it/s]
 59%|█████▉    | 130/221 [00:38<00:26,  3.42it/s]
 59%|█████▉    | 131/221 [00:38<00:26,  3.42it/s]
 60%|█████▉    | 132/221 [00:38<00:26,  3.42it/s]
 60%|██████    | 133/221 [00:38<00:25,  3.42it/s]
 61%|██████    | 134/221 [00:39<00:25,  3.42it/s]
 61%|██████    | 135/221 [00:39<00:25,  3.42it/s]
 62%|██████▏   | 136/221 [00:39<00:24,  3.42it/s]
 62%|██████▏   | 137/221 [00:40<00:24,  3.42it/s]
 62%|██████▏   | 138/221 [00:40<00:24,  3.42it/s]
 63%|██████▎   | 139/221 [00:40<00:23,  3.42it/s]
 63%|██████▎   | 140/221 [00:40<00:23,  3.42it/s]
 64%|██████▍   | 141/221 [00:41<00:23,  3.42it/s]
 64%|██████▍   | 142/221 [00:41<00:23,  3.42it/s]
 65%|██████▍   | 143/221 [00:41<00:22,  3.42it/s]
 65%|██████▌   | 144/221 [00:42<00:22,  3.42it/s]
 66%|██████▌   | 145/221 [00:42<00:22,  3.42it/s]
 66%|██████▌   | 146/221 [00:42<00:21,  3.42it/s]
 67%|██████▋   | 147/221 [00:42<00:21,  3.42it/s]
 67%|██████▋   | 148/221 [00:43<00:21,  3.42it/s]
 67%|██████▋   | 149/221 [00:43<00:21,  3.42it/s]
 68%|██████▊   | 150/221 [00:43<00:20,  3.42it/s]
 68%|██████▊   | 151/221 [00:44<00:20,  3.42it/s]
 69%|██████▉   | 152/221 [00:44<00:20,  3.42it/s]
 69%|██████▉   | 153/221 [00:44<00:19,  3.42it/s]
 70%|██████▉   | 154/221 [00:45<00:19,  3.42it/s]
 70%|███████   | 155/221 [00:45<00:19,  3.42it/s]
 71%|███████   | 156/221 [00:45<00:19,  3.42it/s]
 71%|███████   | 157/221 [00:45<00:18,  3.42it/s]
 71%|███████▏  | 158/221 [00:46<00:18,  3.42it/s]
 72%|███████▏  | 159/221 [00:46<00:18,  3.42it/s]
 72%|███████▏  | 160/221 [00:46<00:17,  3.42it/s]
 73%|███████▎  | 161/221 [00:47<00:17,  3.42it/s]
 73%|███████▎  | 162/221 [00:47<00:17,  3.42it/s]
 74%|███████▍  | 163/221 [00:47<00:16,  3.42it/s]
 74%|███████▍  | 164/221 [00:47<00:16,  3.42it/s]
 75%|███████▍  | 165/221 [00:48<00:16,  3.42it/s]
 75%|███████▌  | 166/221 [00:48<00:16,  3.42it/s]
 76%|███████▌  | 167/221 [00:48<00:15,  3.42it/s]
 76%|███████▌  | 168/221 [00:49<00:15,  3.42it/s]
 76%|███████▋  | 169/221 [00:49<00:15,  3.42it/s]
 77%|███████▋  | 170/221 [00:49<00:14,  3.42it/s]
 77%|███████▋  | 171/221 [00:50<00:14,  3.42it/s]
 78%|███████▊  | 172/221 [00:50<00:14,  3.42it/s]
 78%|███████▊  | 173/221 [00:50<00:14,  3.42it/s]
 79%|███████▊  | 174/221 [00:50<00:13,  3.42it/s]
 79%|███████▉  | 175/221 [00:51<00:13,  3.42it/s]
 80%|███████▉  | 176/221 [00:51<00:13,  3.42it/s]
 80%|████████  | 177/221 [00:51<00:12,  3.42it/s]
 81%|████████  | 178/221 [00:52<00:12,  3.42it/s]
 81%|████████  | 179/221 [00:52<00:12,  3.42it/s]
 81%|████████▏ | 180/221 [00:52<00:11,  3.42it/s]
 82%|████████▏ | 181/221 [00:52<00:11,  3.42it/s]
 82%|████████▏ | 182/221 [00:53<00:11,  3.42it/s]
 83%|████████▎ | 183/221 [00:53<00:11,  3.42it/s]
 83%|████████▎ | 184/221 [00:53<00:10,  3.42it/s]
 84%|████████▎ | 185/221 [00:54<00:10,  3.42it/s]
 84%|████████▍ | 186/221 [00:54<00:10,  3.42it/s]
 85%|████████▍ | 187/221 [00:54<00:09,  3.42it/s]
 85%|████████▌ | 188/221 [00:54<00:09,  3.42it/s]
 86%|████████▌ | 189/221 [00:55<00:09,  3.42it/s]
 86%|████████▌ | 190/221 [00:55<00:09,  3.42it/s]
 86%|████████▋ | 191/221 [00:55<00:08,  3.42it/s]
 87%|████████▋ | 192/221 [00:56<00:08,  3.42it/s]
 87%|████████▋ | 193/221 [00:56<00:08,  3.42it/s]
 88%|████████▊ | 194/221 [00:56<00:07,  3.42it/s]
 88%|████████▊ | 195/221 [00:57<00:07,  3.42it/s]
 89%|████████▊ | 196/221 [00:57<00:07,  3.42it/s]
 89%|████████▉ | 197/221 [00:57<00:07,  3.42it/s]
 90%|████████▉ | 198/221 [00:57<00:06,  3.42it/s]
 90%|█████████ | 199/221 [00:58<00:06,  3.42it/s]
 90%|█████████ | 200/221 [00:58<00:06,  3.42it/s]
 91%|█████████ | 201/221 [00:58<00:05,  3.42it/s]
 91%|█████████▏| 202/221 [00:59<00:05,  3.42it/s]
 92%|█████████▏| 203/221 [00:59<00:05,  3.42it/s]
 92%|█████████▏| 204/221 [00:59<00:04,  3.42it/s]
 93%|█████████▎| 205/221 [00:59<00:04,  3.42it/s]
 93%|█████████▎| 206/221 [01:00<00:04,  3.42it/s]
 94%|█████████▎| 207/221 [01:00<00:04,  3.42it/s]
 94%|█████████▍| 208/221 [01:00<00:03,  3.42it/s]
 95%|█████████▍| 209/221 [01:01<00:03,  3.42it/s]
 95%|█████████▌| 210/221 [01:01<00:03,  3.42it/s]
 95%|█████████▌| 211/221 [01:01<00:02,  3.42it/s]
 96%|█████████▌| 212/221 [01:01<00:02,  3.42it/s]
 96%|█████████▋| 213/221 [01:02<00:02,  3.42it/s]
 97%|█████████▋| 214/221 [01:02<00:02,  3.42it/s]
 97%|█████████▋| 215/221 [01:02<00:01,  3.42it/s]
 98%|█████████▊| 216/221 [01:03<00:01,  3.42it/s]
 98%|█████████▊| 217/221 [01:03<00:01,  3.42it/s]
 99%|█████████▊| 218/221 [01:03<00:00,  3.42it/s]
 99%|█████████▉| 219/221 [01:04<00:00,  3.42it/s]
100%|█████████▉| 220/221 [01:04<00:00,  3.42it/s]
100%|██████████| 221/221 [01:04<00:00,  3.42it/s]
100%|██████████| 221/221 [01:04<00:00,  3.42it/s]

  0%|          | 0/221 [00:00<?, ?it/s]
  0%|          | 1/221 [00:00<00:42,  5.14it/s]
  1%|          | 2/221 [00:00<01:05,  3.34it/s]
  1%|▏         | 3/221 [00:01<01:18,  2.76it/s]
  2%|▏         | 4/221 [00:01<01:14,  2.91it/s]
  2%|▏         | 5/221 [00:01<01:02,  3.45it/s]
  3%|▎         | 6/221 [00:01<00:51,  4.17it/s]
  3%|▎         | 7/221 [00:01<00:47,  4.51it/s]
  4%|▎         | 8/221 [00:02<00:48,  4.41it/s]
  4%|▍         | 9/221 [00:02<01:20,  2.64it/s]
  5%|▍         | 10/221 [00:03<01:18,  2.68it/s]
  5%|▍         | 11/221 [00:03<01:30,  2.32it/s]
  5%|▌         | 12/221 [00:04<01:23,  2.51it/s]
  6%|▌         | 13/221 [00:04<01:39,  2.09it/s]
  6%|▋         | 14/221 [00:04<01:23,  2.48it/s]
  7%|▋         | 15/221 [00:05<01:15,  2.72it/s]
  7%|▋         | 16/221 [00:05<01:06,  3.10it/s]
  8%|▊         | 17/221 [00:05<01:18,  2.60it/s]
  8%|▊         | 18/221 [00:06<01:20,  2.53it/s]
  9%|▊         | 19/221 [00:06<01:07,  3.01it/s]
  9%|▉         | 20/221 [00:06<00:58,  3.43it/s]
 10%|▉         | 22/221 [00:07<00:48,  4.07it/s]
 10%|█         | 23/221 [00:07<00:45,  4.37it/s]
 11%|█         | 24/221 [00:07<00:47,  4.16it/s]
 11%|█▏        | 25/221 [00:07<00:45,  4.26it/s]
 12%|█▏        | 26/221 [00:07<00:42,  4.60it/s]
 12%|█▏        | 27/221 [00:08<00:48,  3.99it/s]
 13%|█▎        | 28/221 [00:08<00:52,  3.67it/s]
 13%|█▎        | 29/221 [00:09<01:07,  2.84it/s]
 14%|█▎        | 30/221 [00:09<00:56,  3.37it/s]
 14%|█▍        | 31/221 [00:09<00:50,  3.80it/s]
 14%|█▍        | 32/221 [00:09<00:55,  3.38it/s]
 15%|█▍        | 33/221 [00:10<00:49,  3.79it/s]
 15%|█▌        | 34/221 [00:10<00:50,  3.73it/s]
 16%|█▌        | 35/221 [00:10<00:44,  4.17it/s]
 16%|█▋        | 36/221 [00:10<00:49,  3.74it/s]
 17%|█▋        | 37/221 [00:11<00:50,  3.68it/s]
 17%|█▋        | 38/221 [00:11<00:46,  3.95it/s]
 18%|█▊        | 39/221 [00:11<00:41,  4.33it/s]
 18%|█▊        | 40/221 [00:11<00:38,  4.70it/s]
 19%|█▊        | 41/221 [00:11<00:34,  5.24it/s]
 19%|█▉        | 42/221 [00:12<00:31,  5.75it/s]
 19%|█▉        | 43/221 [00:12<00:40,  4.43it/s]
 20%|█▉        | 44/221 [00:12<00:41,  4.24it/s]
 20%|██        | 45/221 [00:13<01:01,  2.87it/s]
 21%|██        | 46/221 [00:13<01:00,  2.91it/s]
 21%|██▏       | 47/221 [00:13<00:48,  3.56it/s]
 22%|██▏       | 48/221 [00:13<00:41,  4.19it/s]
 23%|██▎       | 50/221 [00:14<00:39,  4.36it/s]
 23%|██▎       | 51/221 [00:14<00:37,  4.56it/s]
 24%|██▎       | 52/221 [00:14<00:33,  5.10it/s]
 24%|██▍       | 53/221 [00:15<00:47,  3.55it/s]
 24%|██▍       | 54/221 [00:15<00:45,  3.67it/s]
 25%|██▍       | 55/221 [00:15<00:43,  3.84it/s]
 25%|██▌       | 56/221 [00:15<00:43,  3.79it/s]
 26%|██▌       | 57/221 [00:16<00:45,  3.58it/s]
 26%|██▌       | 58/221 [00:16<00:37,  4.35it/s]
 27%|██▋       | 59/221 [00:16<00:37,  4.29it/s]
 27%|██▋       | 60/221 [00:16<00:35,  4.58it/s]
 28%|██▊       | 61/221 [00:17<00:42,  3.73it/s]
 28%|██▊       | 62/221 [00:17<00:38,  4.09it/s]
 29%|██▊       | 63/221 [00:17<00:38,  4.11it/s]
 29%|██▉       | 64/221 [00:17<00:33,  4.69it/s]
 29%|██▉       | 65/221 [00:17<00:32,  4.78it/s]
 30%|██▉       | 66/221 [00:18<00:37,  4.14it/s]
 30%|███       | 67/221 [00:18<00:41,  3.70it/s]
 31%|███       | 68/221 [00:18<00:43,  3.53it/s]
 31%|███       | 69/221 [00:19<01:07,  2.26it/s]
 32%|███▏      | 70/221 [00:19<00:57,  2.63it/s]
 32%|███▏      | 71/221 [00:20<01:00,  2.50it/s]
 33%|███▎      | 72/221 [00:20<00:53,  2.76it/s]
 33%|███▎      | 73/221 [00:20<00:47,  3.13it/s]
 33%|███▎      | 74/221 [00:20<00:40,  3.67it/s]
 34%|███▍      | 75/221 [00:21<00:42,  3.42it/s]
 34%|███▍      | 76/221 [00:21<00:42,  3.38it/s]
 35%|███▍      | 77/221 [00:21<00:36,  4.00it/s]
 35%|███▌      | 78/221 [00:22<00:39,  3.58it/s]
 36%|███▌      | 79/221 [00:22<00:33,  4.21it/s]
 36%|███▌      | 80/221 [00:22<00:33,  4.25it/s]
 37%|███▋      | 81/221 [00:22<00:30,  4.52it/s]
 37%|███▋      | 82/221 [00:23<00:36,  3.84it/s]
 38%|███▊      | 83/221 [00:23<00:35,  3.87it/s]
 38%|███▊      | 84/221 [00:23<00:40,  3.39it/s]
 38%|███▊      | 85/221 [00:24<00:44,  3.08it/s]
 39%|███▉      | 86/221 [00:24<00:36,  3.68it/s]
 39%|███▉      | 87/221 [00:24<00:41,  3.23it/s]
 40%|███▉      | 88/221 [00:24<00:37,  3.58it/s]
 40%|████      | 89/221 [00:25<00:46,  2.86it/s]
 41%|████      | 90/221 [00:25<00:44,  2.93it/s]
 41%|████      | 91/221 [00:25<00:35,  3.67it/s]
 42%|████▏     | 92/221 [00:25<00:30,  4.25it/s]
 42%|████▏     | 93/221 [00:26<00:38,  3.34it/s]
 43%|████▎     | 94/221 [00:26<00:34,  3.63it/s]
 43%|████▎     | 95/221 [00:27<00:57,  2.19it/s]
 43%|████▎     | 96/221 [00:27<00:50,  2.48it/s]
 44%|████▍     | 97/221 [00:28<00:52,  2.36it/s]
 44%|████▍     | 98/221 [00:28<00:46,  2.63it/s]
 45%|████▍     | 99/221 [00:28<00:42,  2.86it/s]
 45%|████▌     | 100/221 [00:29<00:46,  2.61it/s]
 46%|████▌     | 101/221 [00:29<00:40,  2.98it/s]
 46%|████▌     | 102/221 [00:29<00:38,  3.10it/s]
 47%|████▋     | 103/221 [00:29<00:33,  3.54it/s]
 48%|████▊     | 105/221 [00:30<00:25,  4.64it/s]
 48%|████▊     | 106/221 [00:30<00:29,  3.86it/s]
 49%|████▉     | 108/221 [00:30<00:19,  5.71it/s]
 49%|████▉     | 109/221 [00:30<00:20,  5.57it/s]
 50%|████▉     | 110/221 [00:31<00:22,  4.88it/s]
 50%|█████     | 111/221 [00:31<00:24,  4.47it/s]
 51%|█████     | 112/221 [00:31<00:30,  3.63it/s]
 51%|█████     | 113/221 [00:32<00:29,  3.60it/s]
 52%|█████▏    | 114/221 [00:32<00:24,  4.30it/s]
 52%|█████▏    | 116/221 [00:32<00:21,  4.85it/s]
 53%|█████▎    | 117/221 [00:32<00:22,  4.59it/s]
 53%|█████▎    | 118/221 [00:33<00:23,  4.31it/s]
 54%|█████▍    | 119/221 [00:33<00:26,  3.86it/s]
 54%|█████▍    | 120/221 [00:33<00:30,  3.34it/s]
 55%|█████▍    | 121/221 [00:34<00:27,  3.57it/s]
 55%|█████▌    | 122/221 [00:34<00:34,  2.85it/s]
 56%|█████▌    | 123/221 [00:34<00:29,  3.29it/s]
 56%|█████▌    | 124/221 [00:35<00:38,  2.55it/s]
 57%|█████▋    | 125/221 [00:36<00:43,  2.22it/s]
 57%|█████▋    | 126/221 [00:36<00:37,  2.55it/s]
 57%|█████▋    | 127/221 [00:36<00:34,  2.71it/s]
 58%|█████▊    | 128/221 [00:36<00:31,  2.99it/s]
 58%|█████▊    | 129/221 [00:36<00:24,  3.72it/s]
 59%|█████▉    | 130/221 [00:37<00:23,  3.85it/s]
 59%|█████▉    | 131/221 [00:37<00:20,  4.45it/s]
 60%|█████▉    | 132/221 [00:37<00:26,  3.40it/s]
 60%|██████    | 133/221 [00:38<00:32,  2.75it/s]
 61%|██████    | 134/221 [00:38<00:29,  3.00it/s]
 61%|██████    | 135/221 [00:38<00:25,  3.39it/s]
 62%|██████▏   | 136/221 [00:39<00:27,  3.07it/s]
 62%|██████▏   | 137/221 [00:39<00:26,  3.19it/s]
 62%|██████▏   | 138/221 [00:39<00:24,  3.35it/s]
 63%|██████▎   | 139/221 [00:40<00:30,  2.71it/s]
 63%|██████▎   | 140/221 [00:40<00:25,  3.22it/s]
 64%|██████▍   | 141/221 [00:40<00:25,  3.17it/s]
 64%|██████▍   | 142/221 [00:40<00:22,  3.53it/s]
 65%|██████▍   | 143/221 [00:41<00:23,  3.35it/s]
 65%|██████▌   | 144/221 [00:41<00:22,  3.40it/s]
 66%|██████▌   | 145/221 [00:42<00:26,  2.86it/s]
 66%|██████▌   | 146/221 [00:42<00:25,  2.91it/s]
 67%|██████▋   | 147/221 [00:42<00:20,  3.55it/s]
 67%|██████▋   | 148/221 [00:43<00:36,  2.01it/s]
 67%|██████▋   | 149/221 [00:43<00:28,  2.51it/s]
 68%|██████▊   | 150/221 [00:43<00:23,  3.03it/s]
 68%|██████▊   | 151/221 [00:44<00:37,  1.88it/s]
 69%|██████▉   | 152/221 [00:45<00:41,  1.68it/s]
 69%|██████▉   | 153/221 [00:45<00:34,  1.95it/s]
 70%|██████▉   | 154/221 [00:46<00:28,  2.38it/s]
 70%|███████   | 155/221 [00:46<00:25,  2.57it/s]
 71%|███████   | 156/221 [00:46<00:23,  2.75it/s]
 71%|███████   | 157/221 [00:47<00:21,  2.97it/s]
 71%|███████▏  | 158/221 [00:47<00:21,  2.87it/s]
 72%|███████▏  | 159/221 [00:47<00:19,  3.21it/s]
 72%|███████▏  | 160/221 [00:48<00:20,  3.03it/s]
 73%|███████▎  | 161/221 [00:48<00:16,  3.71it/s]
 74%|███████▍  | 163/221 [00:48<00:12,  4.72it/s]
 74%|███████▍  | 164/221 [00:48<00:16,  3.48it/s]
 75%|███████▍  | 165/221 [00:49<00:15,  3.66it/s]
 75%|███████▌  | 166/221 [00:49<00:18,  3.01it/s]
 76%|███████▌  | 167/221 [00:49<00:16,  3.36it/s]
 76%|███████▌  | 168/221 [00:50<00:15,  3.41it/s]
 77%|███████▋  | 170/221 [00:50<00:14,  3.41it/s]
 77%|███████▋  | 171/221 [00:51<00:16,  3.12it/s]
 78%|███████▊  | 172/221 [00:51<00:14,  3.44it/s]
 78%|███████▊  | 173/221 [00:51<00:13,  3.63it/s]
 79%|███████▊  | 174/221 [00:51<00:13,  3.45it/s]
 79%|███████▉  | 175/221 [00:52<00:13,  3.36it/s]
 80%|███████▉  | 176/221 [00:52<00:11,  3.99it/s]
 80%|████████  | 177/221 [00:52<00:09,  4.59it/s]
 81%|████████  | 178/221 [00:53<00:13,  3.17it/s]
 81%|████████  | 179/221 [00:53<00:13,  3.14it/s]
 81%|████████▏ | 180/221 [00:53<00:10,  3.77it/s]
 82%|████████▏ | 181/221 [00:53<00:12,  3.33it/s]
 82%|████████▏ | 182/221 [00:54<00:09,  4.01it/s]
 83%|████████▎ | 183/221 [00:54<00:09,  3.96it/s]
 83%|████████▎ | 184/221 [00:54<00:10,  3.49it/s]
 84%|████████▎ | 185/221 [00:54<00:10,  3.54it/s]
 84%|████████▍ | 186/221 [00:55<00:12,  2.81it/s]
 85%|████████▌ | 188/221 [00:55<00:08,  3.75it/s]
 86%|████████▌ | 189/221 [00:55<00:07,  4.02it/s]
 86%|████████▌ | 190/221 [00:56<00:08,  3.79it/s]
 87%|████████▋ | 192/221 [00:56<00:06,  4.46it/s]
 87%|████████▋ | 193/221 [00:56<00:07,  3.99it/s]
 88%|████████▊ | 194/221 [00:57<00:06,  3.87it/s]
 88%|████████▊ | 195/221 [00:57<00:06,  3.98it/s]
 89%|████████▊ | 196/221 [00:58<00:08,  3.09it/s]
 89%|████████▉ | 197/221 [00:58<00:06,  3.50it/s]
 90%|████████▉ | 198/221 [00:58<00:06,  3.63it/s]
 90%|█████████ | 199/221 [00:58<00:05,  4.22it/s]
 90%|█████████ | 200/221 [00:58<00:04,  4.52it/s]
 91%|█████████ | 201/221 [00:58<00:03,  5.34it/s]
 91%|█████████▏| 202/221 [00:59<00:03,  5.24it/s]
 92%|█████████▏| 203/221 [00:59<00:03,  5.30it/s]
 92%|█████████▏| 204/221 [00:59<00:03,  4.91it/s]
 93%|█████████▎| 205/221 [00:59<00:03,  4.94it/s]
 93%|█████████▎| 206/221 [01:00<00:03,  4.17it/s]
 94%|█████████▎| 207/221 [01:00<00:04,  3.18it/s]
 94%|█████████▍| 208/221 [01:00<00:03,  3.94it/s]
 95%|█████████▍| 209/221 [01:00<00:03,  3.94it/s]
 95%|█████████▌| 210/221 [01:01<00:03,  3.42it/s]
 95%|█████████▌| 211/221 [01:01<00:02,  3.37it/s]
 96%|█████████▌| 212/221 [01:01<00:02,  3.21it/s]
 96%|█████████▋| 213/221 [01:02<00:02,  3.29it/s]
 97%|█████████▋| 214/221 [01:02<00:02,  2.65it/s]
 97%|█████████▋| 215/221 [01:03<00:02,  2.86it/s]
 98%|█████████▊| 216/221 [01:03<00:01,  2.75it/s]
 98%|█████████▊| 217/221 [01:03<00:01,  2.81it/s]
 99%|█████████▊| 218/221 [01:04<00:01,  2.38it/s]
 99%|█████████▉| 219/221 [01:04<00:00,  2.46it/s]
100%|█████████▉| 220/221 [01:04<00:00,  2.78it/s]
100%|██████████| 221/221 [01:05<00:00,  2.74it/s]
100%|██████████| 221/221 [01:05<00:00,  3.38it/s]
09/11/2024 15:11:23 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_area_forward========

09/11/2024 15:11:23 - INFO - __main__ -   {'area_r1': 24.1, 'area_recall': '24.1/42.6/51.4', 'area_ravg': 39.4}
09/11/2024 15:11:23 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_area_backard========

09/11/2024 15:11:23 - INFO - __main__ -   {'forward_r1': 32.8, 'forward_recall': '32.8/64.7/75.2', 'forward_ravg': 57.6}
09/11/2024 15:11:23 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video========

09/11/2024 15:11:23 - INFO - __main__ -   {'area_video_r1': 33.4, 'area_video_recall': '33.4/64.3/75.8', 'area_video_ravg': 57.8}
09/11/2024 15:11:23 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_itm_area========

09/11/2024 15:11:23 - INFO - __main__ -   {'area_video_r1': 47.3, 'area_video_recall': '47.3/67.1/73.2', 'area_video_ravg': 62.5, 'area_video_back_r1': 43.6, 'area_video_back_recall': '43.6/68.4/78.2', 'area_video_back_ravg': 63.4}
09/11/2024 15:11:23 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_itc_tva========

09/11/2024 15:11:23 - INFO - __main__ -   {'video_r1': 35.7, 'video_recall': '35.7/63.6/72.5', 'video_ravg': 57.3}
09/11/2024 15:11:23 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_itm_tva========

09/11/2024 15:11:23 - INFO - __main__ -   {'video_r1': 49.3, 'video_recall': '49.3/70.8/80.0', 'video_ravg': 66.7}

  0%|          | 0/2315 [00:00<?, ?it/s][h264 @ 0x56400dc77dc0] mmco: unref short failure
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)

  0%|          | 1/2315 [00:09<6:02:41,  9.40s/it][h264 @ 0x56400d49d900] mmco: unref short failure
[h264 @ 0x561544209dc0] mmco: unref short failure
[h264 @ 0x55781afa90c0] mmco: unref short failure
[h264 @ 0x55781afa90c0] mmco: unref short failure

  0%|          | 2/2315 [00:13<3:57:13,  6.15s/it]
  0%|          | 3/2315 [00:18<3:47:09,  5.90s/it][h264 @ 0x55e4bfbb6500] mmco: unref short failure
[h264 @ 0x56400810a700] mmco: unref short failure

  0%|          | 4/2315 [00:23<3:22:46,  5.26s/it][h264 @ 0x55781afa89c0] mmco: unref short failure
[h264 @ 0x55781afa89c0] mmco: unref short failure
[h264 @ 0x55781a52cc80] mmco: unref short failure

  0%|          | 5/2315 [00:27<3:04:45,  4.80s/it][h264 @ 0x557821501040] mmco: unref short failure
[h264 @ 0x557821501040] mmco: unref short failure
[h264 @ 0x55e4c00b4180] mmco: unref short failure
[h264 @ 0x55e4c00b4180] mmco: unref short failure

  0%|          | 6/2315 [00:31<3:04:32,  4.80s/it][h264 @ 0x561548bf8dc0] mmco: unref short failure
[h264 @ 0x561548bf8dc0] mmco: unref short failure

  0%|          | 7/2315 [00:36<3:04:37,  4.80s/it][h264 @ 0x5578198bc3c0] mmco: unref short failure

  0%|          | 8/2315 [00:41<3:09:15,  4.92s/it]
  0%|          | 9/2315 [00:46<3:06:14,  4.85s/it][h264 @ 0x56400b8d2280] mmco: unref short failure
[h264 @ 0x56400b8d2280] mmco: unref short failure

  0%|          | 10/2315 [00:51<3:01:05,  4.71s/it][h264 @ 0x55e4c1ff4d80] mmco: unref short failure
[h264 @ 0x55e4c1ff4d80] mmco: unref short failure

  0%|          | 11/2315 [00:56<3:04:46,  4.81s/it][h264 @ 0x561541482b80] mmco: unref short failure
[h264 @ 0x561541482b80] mmco: unref short failure
[h264 @ 0x561541482b80] mmco: unref short failure
[h264 @ 0x561541482b80] mmco: unref short failure

  1%|          | 12/2315 [01:00<3:00:44,  4.71s/it][h264 @ 0x55e4c31f6740] mmco: unref short failure

  1%|          | 13/2315 [01:05<3:04:37,  4.81s/it]
  1%|          | 14/2315 [01:11<3:13:50,  5.05s/it][h264 @ 0x5615437f7980] mmco: unref short failure
[h264 @ 0x5615437f7980] mmco: unref short failure

  1%|          | 15/2315 [01:16<3:14:01,  5.06s/it][h264 @ 0x55e4c382d480] mmco: unref short failure
[h264 @ 0x55e4c382d480] mmco: unref short failure
[h264 @ 0x55782100f140] mmco: unref short failure
[h264 @ 0x56400db268c0] mmco: unref short failure
[h264 @ 0x56400db268c0] mmco: unref short failure
[h264 @ 0x5615412c4ec0] mmco: unref short failure
[h264 @ 0x5615412c4ec0] mmco: unref short failure

  1%|          | 16/2315 [01:42<7:20:19, 11.49s/it][h264 @ 0x5640154f9e80] mmco: unref short failure
[h264 @ 0x55e4c839e000] mmco: unref short failure
[h264 @ 0x55e4bebe2c40] mmco: unref short failure
[h264 @ 0x55e4bebe2c40] mmco: unref short failure

  1%|          | 17/2315 [01:50<6:35:54, 10.34s/it][h264 @ 0x557822c461c0] mmco: unref short failure
[h264 @ 0x56154e41c440] mmco: unref short failure
[h264 @ 0x56154e41c440] mmco: unref short failure
[h264 @ 0x56154aaa4640] mmco: unref short failure
[h264 @ 0x55e4c32cf940] mmco: unref short failure
[h264 @ 0x55e4c32cf940] mmco: unref short failure
[h264 @ 0x55e4c32cf940] mmco: unref short failure
[h264 @ 0x55e4bc01e200] mmco: unref short failure
[h264 @ 0x55e4bc01e200] mmco: unref short failure
[h264 @ 0x55e4c32cf940] mmco: unref short failure
[h264 @ 0x55e4c32cf940] mmco: unref short failure
[h264 @ 0x55e4c32cf940] mmco: unref short failure

  1%|          | 18/2315 [02:16<9:42:31, 15.22s/it][h264 @ 0x56400b46dd00] mmco: unref short failure
[h264 @ 0x56400b46dd00] mmco: unref short failure
[h264 @ 0x564010a01100] mmco: unref short failure

  1%|          | 19/2315 [02:23<8:07:39, 12.74s/it][h264 @ 0x55e4c1fee840] mmco: unref short failure
[h264 @ 0x55e4c1fee840] mmco: unref short failure
[h264 @ 0x564011344280] mmco: unref short failure
[h264 @ 0x564010cf5940] mmco: unref short failure
[h264 @ 0x564010cf5940] mmco: unref short failure
[h264 @ 0x55e4c1fd95c0] mmco: unref short failure
[h264 @ 0x55e4c1fd95c0] mmco: unref short failure
[h264 @ 0x564010cf5940] mmco: unref short failure
[h264 @ 0x564010cf5940] mmco: unref short failure

  1%|          | 20/2315 [02:28<6:33:47, 10.30s/it][h264 @ 0x55781c7d4f80] mmco: unref short failure
[h264 @ 0x5640119afa40] mmco: unref short failure
[h264 @ 0x5640119afa40] mmco: unref short failure
[h264 @ 0x564010d4f8c0] mmco: unref short failure
[h264 @ 0x564010d4f8c0] mmco: unref short failure
[h264 @ 0x564008b37140] mmco: unref short failure
[h264 @ 0x564008b37140] mmco: unref short failure
[h264 @ 0x561543852d80] mmco: unref short failure
[h264 @ 0x56154204f880] mmco: unref short failure
[h264 @ 0x56154204f880] mmco: unref short failure

  1%|          | 21/2315 [02:42<7:21:33, 11.55s/it]
  1%|          | 22/2315 [02:47<6:02:15,  9.48s/it]
  1%|          | 23/2315 [02:52<5:07:28,  8.05s/it][h264 @ 0x5640095f6080] mmco: unref short failure
[h264 @ 0x5640095f6080] mmco: unref short failure
[h264 @ 0x5640095f6080] mmco: unref short failure
[h264 @ 0x5640095f6080] mmco: unref short failure
[h264 @ 0x55781b0ff980] mmco: unref short failure
[h264 @ 0x55781b0ff980] mmco: unref short failure
[h264 @ 0x55e4bc352640] mmco: unref short failure
[h264 @ 0x56400946ff40] mmco: unref short failure
[h264 @ 0x56400946ff40] mmco: unref short failure
[h264 @ 0x55782c8867c0] mmco: unref short failure
[h264 @ 0x55781ef7d440] mmco: unref short failure
[h264 @ 0x5640172a5740] mmco: unref short failure
[h264 @ 0x5640172a5740] mmco: unref short failure
[h264 @ 0x55781bc2c300] mmco: unref short failure
[h264 @ 0x5578217e2800] mmco: unref short failure
[h264 @ 0x561551b0e1c0] mmco: unref short failure
[h264 @ 0x561551b0e1c0] mmco: unref short failure
[h264 @ 0x55e4c3654680] mmco: unref short failure

  1%|          | 24/2315 [03:56<15:47:03, 24.80s/it][h264 @ 0x557823321480] mmco: unref short failure
[h264 @ 0x557823321480] mmco: unref short failure

  1%|          | 25/2315 [04:01<11:57:50, 18.81s/it][h264 @ 0x55e4c1f27680] mmco: unref short failure
[h264 @ 0x557819e58bc0] mmco: unref short failure
[h264 @ 0x557819e58bc0] mmco: unref short failure
[h264 @ 0x561546e354c0] mmco: unref short failure
[h264 @ 0x55e4c73b3700] mmco: unref short failure
[h264 @ 0x561546d497c0] mmco: unref short failure

  1%|          | 26/2315 [04:22<12:32:40, 19.73s/it]
  1%|          | 27/2315 [04:34<11:03:32, 17.40s/it][h264 @ 0x55e4c6a5dd80] mmco: unref short failure
[h264 @ 0x55e4c6a5dd80] mmco: unref short failure
[h264 @ 0x55e4c6a5dd80] mmco: unref short failure
[h264 @ 0x55e4c6a5dd80] mmco: unref short failure
[h264 @ 0x55e4c6a5dd80] mmco: unref short failure
[h264 @ 0x55e4c6a5dd80] mmco: unref short failure
[h264 @ 0x55e4bd4c1580] mmco: unref short failure
[h264 @ 0x55e4bd4c1580] mmco: unref short failure
[h264 @ 0x55e4bd4c1580] mmco: unref short failure
[h264 @ 0x55e4bd4c1580] mmco: unref short failure
[h264 @ 0x55e4bd4c1580] mmco: unref short failure
[h264 @ 0x55e4bd4c1580] mmco: unref short failure
[h264 @ 0x55782a947800] mmco: unref short failure
[h264 @ 0x55e4c05c8540] mmco: unref short failure
[h264 @ 0x55e4c05c8540] mmco: unref short failure

  1%|          | 28/2315 [04:39<8:39:45, 13.64s/it] [h264 @ 0x55e4c2b02540] mmco: unref short failure
[h264 @ 0x55e4c2b02540] mmco: unref short failure

  1%|▏         | 29/2315 [04:54<8:56:28, 14.08s/it][h264 @ 0x561549d94900] mmco: unref short failure

  1%|▏         | 30/2315 [05:00<7:15:41, 11.44s/it]
  1%|▏         | 31/2315 [05:05<6:03:48,  9.56s/it][h264 @ 0x5615428be280] mmco: unref short failure
[h264 @ 0x55781febc8c0] mmco: unref short failure
[h264 @ 0x55781febc8c0] mmco: unref short failure
[h264 @ 0x55e4bc01dd80] mmco: unref short failure
[h264 @ 0x55e4bc01dd80] mmco: unref short failure
[h264 @ 0x557827382140] mmco: unref short failure
[h264 @ 0x55e4bb4297c0] mmco: unref short failure
[h264 @ 0x55e4bb4297c0] mmco: unref short failure
[h264 @ 0x55781bff83c0] mmco: unref short failure
[h264 @ 0x55e4c69c8d00] mmco: unref short failure
[h264 @ 0x56154e830100] mmco: unref short failure
[h264 @ 0x55781a5bef80] mmco: unref short failure
[h264 @ 0x55781a5bef80] mmco: unref short failure
[h264 @ 0x55e4bda4aa40] mmco: unref short failure
[h264 @ 0x55e4bda4aa40] mmco: unref short failure
[h264 @ 0x56401636c640] mmco: unref short failure
[h264 @ 0x56401636c640] mmco: unref short failure

  1%|▏         | 32/2315 [06:03<15:20:36, 24.19s/it][h264 @ 0x5578254569c0] mmco: unref short failure
[h264 @ 0x5578254569c0] mmco: unref short failure

  1%|▏         | 33/2315 [06:08<11:38:30, 18.37s/it][h264 @ 0x5615467b8d80] mmco: unref short failure
[h264 @ 0x5615467b8d80] mmco: unref short failure
[h264 @ 0x56154bc13cc0] mmco: unref short failure
[h264 @ 0x56154bc13cc0] mmco: unref short failure
[h264 @ 0x5615467b8d80] mmco: unref short failure
[h264 @ 0x5615467b8d80] mmco: unref short failure
[h264 @ 0x55782bfb7cc0] mmco: unref short failure

  1%|▏         | 34/2315 [06:34<13:04:37, 20.64s/it][h264 @ 0x557828c19880] mmco: unref short failure
[h264 @ 0x557828c19880] mmco: unref short failure

  2%|▏         | 35/2315 [06:39<10:07:05, 15.98s/it][h264 @ 0x55781f222540] mmco: unref short failure

  2%|▏         | 36/2315 [06:44<7:59:19, 12.62s/it] [h264 @ 0x5640172a5dc0] mmco: unref short failure
[h264 @ 0x56154f169b80] mmco: unref short failure

  2%|▏         | 37/2315 [06:59<8:28:16, 13.39s/it]
  2%|▏         | 38/2315 [07:04<6:50:17, 10.81s/it][h264 @ 0x55781fb60c40] mmco: unref short failure
[h264 @ 0x55781fb60c40] mmco: unref short failure
[h264 @ 0x564019498480] mmco: unref short failure
[h264 @ 0x564019498480] mmco: unref short failure
[h264 @ 0x564019498480] mmco: unref short failure

  2%|▏         | 39/2315 [07:09<5:48:37,  9.19s/it][h264 @ 0x55e4d0524bc0] mmco: unref short failure
[h264 @ 0x564017a72740] mmco: unref short failure
[h264 @ 0x564017a72740] mmco: unref short failure
[h264 @ 0x55e4bd2f94c0] mmco: unref short failure
[h264 @ 0x55e4bd2f94c0] mmco: unref short failure
[h264 @ 0x55e4c07c2d00] mmco: unref short failure
[h264 @ 0x55e4c07c2d00] mmco: unref short failure
[h264 @ 0x557823041f00] mmco: unref short failure
[h264 @ 0x557823041f00] mmco: unref short failure
[h264 @ 0x56154d838ec0] mmco: unref short failure
[h264 @ 0x56154d838ec0] mmco: unref short failure
[h264 @ 0x56154c017440] mmco: unref short failure
[h264 @ 0x564014361a00] mmco: unref short failure
[h264 @ 0x564014361a00] mmco: unref short failure
[h264 @ 0x55e4bec82300] mmco: unref short failure
[h264 @ 0x55e4bec82300] mmco: unref short failure
[h264 @ 0x55e4bec82300] mmco: unref short failure

  2%|▏         | 40/2315 [08:13<16:11:56, 25.63s/it][h264 @ 0x5640115d4f40] mmco: unref short failure
[h264 @ 0x561543eedf80] mmco: unref short failure
[h264 @ 0x561543eedf80] mmco: unref short failure
[h264 @ 0x5615434d5500] mmco: unref short failure
[h264 @ 0x5615434d5500] mmco: unref short failure

  2%|▏         | 41/2315 [08:18<12:20:34, 19.54s/it][h264 @ 0x5640077cb8c0] mmco: unref short failure
[h264 @ 0x5640077cb8c0] mmco: unref short failure
[h264 @ 0x5640077cb8c0] mmco: unref short failure
[h264 @ 0x5640077cb8c0] mmco: unref short failure
[h264 @ 0x5615469a4f00] mmco: unref short failure
[h264 @ 0x55e4ce5177c0] mmco: unref short failure
[h264 @ 0x55e4ce5177c0] mmco: unref short failure
[h264 @ 0x5640077f14c0] mmco: unref short failure
[h264 @ 0x5640077f14c0] mmco: unref short failure

  2%|▏         | 42/2315 [08:39<12:32:21, 19.86s/it][h264 @ 0x55e4c314a080] mmco: unref short failure
[h264 @ 0x55e4c314a080] mmco: unref short failure

  2%|▏         | 43/2315 [08:48<10:26:21, 16.54s/it]
  2%|▏         | 44/2315 [08:56<8:47:53, 13.95s/it] [h264 @ 0x564008843ac0] mmco: unref short failure
[h264 @ 0x564008843ac0] mmco: unref short failure
[h264 @ 0x557828fb1c80] mmco: unref short failure
[h264 @ 0x55e4d0269980] mmco: unref short failure
[h264 @ 0x55e4d0269980] mmco: unref short failure
[h264 @ 0x5615437f7c00] mmco: unref short failure

  2%|▏         | 45/2315 [09:13<9:29:18, 15.05s/it][h264 @ 0x557824283b00] mmco: unref short failure
[h264 @ 0x557824283b00] mmco: unref short failure

  2%|▏         | 46/2315 [09:19<7:38:47, 12.13s/it][h264 @ 0x561546a1f840] mmco: unref short failure
[h264 @ 0x561546a1f840] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x564010e5fb40] mmco: unref short failure
[h264 @ 0x5640192c6c00] mmco: unref short failure

  2%|▏         | 47/2315 [09:27<6:55:01, 10.98s/it][h264 @ 0x56154e78d400] mmco: unref short failure
[h264 @ 0x56154e78d400] mmco: unref short failure
[h264 @ 0x55e4ca7ea480] mmco: unref short failure
[h264 @ 0x56401630aa00] mmco: unref short failure
[h264 @ 0x56155a5f7140] mmco: unref short failure
[h264 @ 0x56155a5f7140] mmco: unref short failure
[h264 @ 0x56155a5f7140] mmco: unref short failure
[h264 @ 0x56155a5f7140] mmco: unref short failure
[h264 @ 0x55e4bedc51c0] mmco: unref short failure
[h264 @ 0x55e4bedc51c0] mmco: unref short failure
[h264 @ 0x55e4bc911d80] mmco: unref short failure
[h264 @ 0x55e4bc911d80] mmco: unref short failure
[h264 @ 0x56154b6a5c80] mmco: unref short failure
[h264 @ 0x56154b6a5c80] mmco: unref short failure
[h264 @ 0x561546d35280] mmco: unref short failure
[h264 @ 0x561546d35280] mmco: unref short failure
[h264 @ 0x55781dcdfdc0] mmco: unref short failure
[h264 @ 0x55781dcdfdc0] mmco: unref short failure
[h264 @ 0x55781dcdfdc0] mmco: unref short failure
[h264 @ 0x55781dcdfdc0] mmco: unref short failure

  2%|▏         | 48/2315 [10:16<14:09:03, 22.47s/it]
  2%|▏         | 49/2315 [10:21<10:49:38, 17.20s/it]09/11/2024 15:21:44 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
09/11/2024 15:21:44 - INFO - __main__ -   start running ret%tva validation...
[h264 @ 0x55782ef66c00] mmco: unref short failure
[h264 @ 0x55782ef66c00] mmco: unref short failure
[h264 @ 0x561543b6e140] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5615526a6740] mmco: unref short failure
[h264 @ 0x5615526a6740] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5615526a6740] mmco: unref short failure
[h264 @ 0x5615526a6740] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x557831284200] mmco: unref short failure
not have audios 8-qwaveiHMM.3
[h264 @ 0x55e4bf01dcc0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x56401ae40c80] mmco: unref short failure
[h264 @ 0x56401ae40c80] mmco: unref short failure
[h264 @ 0x561550d175c0] mmco: unref short failure
[h264 @ 0x561550d175c0] mmco: unref short failure
[h264 @ 0x5640083ef1c0] mmco: unref short failure
[h264 @ 0x5640083ef1c0] mmco: unref short failure
[h264 @ 0x5640083ef1c0] mmco: unref short failure
[h264 @ 0x5640083ef1c0] mmco: unref short failure
[h264 @ 0x55e4c943d200] mmco: unref short failure
[h264 @ 0x55e4c943d200] mmco: unref short failure
[h264 @ 0x56401d7cc340] mmco: unref short failure
[h264 @ 0x55e4c2786d00] mmco: unref short failure
[h264 @ 0x55e4c2786d00] mmco: unref short failure
[h264 @ 0x55781e45dec0] mmco: unref short failure
[h264 @ 0x55781e45dec0] mmco: unref short failure
[h264 @ 0x55782f5848c0] mmco: unref short failure
[h264 @ 0x55e4d08d6380] mmco: unref short failure
[h264 @ 0x55e4d08d6380] mmco: unref short failure
[h264 @ 0x5578225e1f40] mmco: unref short failure
[h264 @ 0x55e4c5dd3bc0] mmco: unref short failure
[h264 @ 0x55e4c5dd3bc0] mmco: unref short failure
[h264 @ 0x55e4d033bc40] mmco: unref short failure
[h264 @ 0x55e4d033bc40] mmco: unref short failure
[h264 @ 0x5615474a7540] mmco: unref short failure
[h264 @ 0x5615474a7540] mmco: unref short failure
[h264 @ 0x55782a2b4440] mmco: unref short failure
[h264 @ 0x55782a2b4440] mmco: unref short failure


  0%|          | 0/221 [00:00<?, ?it/s][A

  0%|          | 1/221 [00:00<00:47,  4.62it/s][A

  1%|          | 2/221 [00:00<00:45,  4.79it/s][A

  1%|▏         | 3/221 [00:00<00:51,  4.21it/s][A

  2%|▏         | 4/221 [00:00<00:42,  5.10it/s][A

  2%|▏         | 5/221 [00:00<00:35,  6.06it/s][A

  3%|▎         | 7/221 [00:01<00:29,  7.33it/s][A

  4%|▎         | 8/221 [00:01<00:28,  7.46it/s][A

  5%|▍         | 10/221 [00:01<00:25,  8.35it/s][A

  5%|▍         | 11/221 [00:01<00:30,  6.89it/s][A

  5%|▌         | 12/221 [00:03<01:38,  2.13it/s][A[h264 @ 0x557829c21840] mmco: unref short failure


  6%|▌         | 13/221 [00:03<01:18,  2.67it/s][A

  6%|▋         | 14/221 [00:03<01:12,  2.87it/s][A

  7%|▋         | 15/221 [00:03<00:59,  3.45it/s][A

  7%|▋         | 16/221 [00:03<00:54,  3.74it/s][A

  8%|▊         | 17/221 [00:04<01:00,  3.38it/s][A

  8%|▊         | 18/221 [00:04<00:51,  3.94it/s][A

  9%|▊         | 19/221 [00:04<00:43,  4.61it/s][A

  9%|▉         | 20/221 [00:04<00:37,  5.35it/s][A

 10%|▉         | 22/221 [00:05<01:11,  2.77it/s][A

 11%|█         | 24/221 [00:06<00:55,  3.52it/s][A

 11%|█▏        | 25/221 [00:06<00:48,  4.03it/s][A

 12%|█▏        | 26/221 [00:06<00:47,  4.09it/s][A

 12%|█▏        | 27/221 [00:06<00:40,  4.81it/s][A

 13%|█▎        | 28/221 [00:06<00:40,  4.74it/s][A

 13%|█▎        | 29/221 [00:06<00:36,  5.26it/s][A

 14%|█▎        | 30/221 [00:07<00:35,  5.33it/s][A

 14%|█▍        | 31/221 [00:07<00:33,  5.59it/s][A

 15%|█▍        | 33/221 [00:07<00:29,  6.41it/s][A

 15%|█▌        | 34/221 [00:07<00:27,  6.78it/s][A

 16%|█▌        | 35/221 [00:07<00:26,  7.04it/s][A

 16%|█▋        | 36/221 [00:07<00:24,  7.41it/s][A

 17%|█▋        | 37/221 [00:08<00:30,  6.00it/s][A

 17%|█▋        | 38/221 [00:08<00:29,  6.17it/s][A

 18%|█▊        | 40/221 [00:08<00:31,  5.68it/s][A

 19%|█▊        | 41/221 [00:08<00:33,  5.40it/s][A

 19%|█▉        | 42/221 [00:09<00:32,  5.56it/s][A

 19%|█▉        | 43/221 [00:09<00:31,  5.66it/s][A[h264 @ 0x5578225e1f40] mmco: unref short failure
[h264 @ 0x5578225e1f40] mmco: unref short failure


 20%|██        | 45/221 [00:09<00:43,  4.04it/s][A

 21%|██        | 46/221 [00:10<00:42,  4.13it/s][A

 21%|██▏       | 47/221 [00:15<04:12,  1.45s/it][A

 22%|██▏       | 48/221 [00:15<03:09,  1.09s/it][A

 22%|██▏       | 49/221 [00:15<02:22,  1.21it/s][A

 23%|██▎       | 50/221 [00:15<01:47,  1.59it/s][A

 23%|██▎       | 51/221 [00:15<01:24,  2.01it/s][A

 24%|██▎       | 52/221 [00:15<01:05,  2.57it/s][A

 24%|██▍       | 53/221 [00:15<00:51,  3.27it/s][A

 24%|██▍       | 54/221 [00:19<03:37,  1.30s/it][A

 25%|██▍       | 55/221 [00:20<03:07,  1.13s/it][A

 25%|██▌       | 56/221 [00:20<02:16,  1.21it/s][A[h264 @ 0x55e4c6ab41c0] mmco: unref short failure


 26%|██▌       | 57/221 [00:20<01:40,  1.63it/s][A

 26%|██▌       | 58/221 [00:20<01:19,  2.05it/s][A

 27%|██▋       | 59/221 [00:20<01:01,  2.64it/s][A

 27%|██▋       | 60/221 [00:21<00:57,  2.78it/s][A

 28%|██▊       | 61/221 [00:21<00:47,  3.39it/s][A

 28%|██▊       | 62/221 [00:21<00:38,  4.13it/s][A

 29%|██▊       | 63/221 [00:21<00:32,  4.91it/s][A

 29%|██▉       | 65/221 [00:21<00:24,  6.43it/s][A

 30%|██▉       | 66/221 [00:25<02:54,  1.12s/it][A

 30%|███       | 67/221 [00:25<02:15,  1.14it/s][A

 31%|███       | 68/221 [00:26<01:43,  1.47it/s][A

 31%|███       | 69/221 [00:26<01:32,  1.64it/s][A

 32%|███▏      | 70/221 [00:26<01:19,  1.91it/s][A

 32%|███▏      | 71/221 [00:27<01:02,  2.41it/s][A

 33%|███▎      | 72/221 [00:27<00:52,  2.81it/s][A

 33%|███▎      | 73/221 [00:27<00:50,  2.95it/s][A

 33%|███▎      | 74/221 [00:27<00:41,  3.52it/s][A

 34%|███▍      | 75/221 [00:27<00:40,  3.64it/s][A[h264 @ 0x55781fa11080] mmco: unref short failure


 34%|███▍      | 76/221 [00:28<00:34,  4.21it/s][A

 35%|███▍      | 77/221 [00:28<00:29,  4.88it/s][A

 35%|███▌      | 78/221 [00:28<00:27,  5.28it/s][A[h264 @ 0x557826eb2240] mmco: unref short failure
[h264 @ 0x557826eb2240] mmco: unref short failure
[h264 @ 0x557826eb2240] mmco: unref short failure


 36%|███▌      | 79/221 [00:28<00:36,  3.94it/s][A[h264 @ 0x557826eb2240] mmco: unref short failure


 36%|███▌      | 80/221 [00:29<00:34,  4.13it/s][A

 37%|███▋      | 81/221 [00:29<00:34,  4.10it/s][A[h264 @ 0x56400b017600] mmco: unref short failure
[h264 @ 0x56400b017600] mmco: unref short failure
[h264 @ 0x56400b017600] mmco: unref short failure
[h264 @ 0x56400b017600] mmco: unref short failure


 37%|███▋      | 82/221 [00:29<00:37,  3.72it/s][A

 38%|███▊      | 83/221 [00:29<00:33,  4.13it/s][A

 38%|███▊      | 84/221 [00:29<00:29,  4.60it/s][A

 39%|███▉      | 86/221 [00:30<00:22,  5.89it/s][A

 39%|███▉      | 87/221 [00:30<00:21,  6.38it/s][A

 40%|███▉      | 88/221 [00:30<00:27,  4.89it/s][A

 40%|████      | 89/221 [00:30<00:28,  4.67it/s][A

 41%|████      | 90/221 [00:31<00:27,  4.79it/s][A

 41%|████      | 91/221 [00:31<00:27,  4.81it/s][A[h264 @ 0x55e4bbf4ef80] mmco: unref short failure
[h264 @ 0x55e4bbf4ef80] mmco: unref short failure


 42%|████▏     | 92/221 [00:31<00:24,  5.26it/s][A

 42%|████▏     | 93/221 [00:31<00:26,  4.77it/s][A

 43%|████▎     | 94/221 [00:31<00:22,  5.62it/s][A

 43%|████▎     | 95/221 [00:31<00:22,  5.61it/s][A

 43%|████▎     | 96/221 [00:32<00:28,  4.46it/s][A

 44%|████▍     | 97/221 [00:32<00:23,  5.20it/s][A

 44%|████▍     | 98/221 [00:32<00:21,  5.85it/s][A[h264 @ 0x55e4ca593d40] mmco: unref short failure
[h264 @ 0x55e4ca593d40] mmco: unref short failure


 45%|████▍     | 99/221 [00:32<00:23,  5.23it/s][A

 45%|████▌     | 100/221 [00:32<00:21,  5.55it/s][A

 46%|████▌     | 101/221 [00:33<00:22,  5.23it/s][A

 46%|████▌     | 102/221 [00:33<00:26,  4.57it/s][A

 47%|████▋     | 103/221 [00:33<00:22,  5.33it/s][A

 47%|████▋     | 104/221 [00:33<00:19,  6.00it/s][A

 48%|████▊     | 105/221 [00:33<00:19,  6.10it/s][A

 48%|████▊     | 106/221 [00:34<00:25,  4.42it/s][A

 48%|████▊     | 107/221 [00:34<00:23,  4.94it/s][A

 49%|████▉     | 108/221 [00:34<00:22,  5.08it/s][A

 49%|████▉     | 109/221 [00:34<00:22,  4.92it/s][A

 50%|████▉     | 110/221 [00:34<00:23,  4.77it/s][A[h264 @ 0x5640123d4980] mmco: unref short failure


 50%|█████     | 111/221 [00:35<00:30,  3.61it/s][A

 51%|█████     | 112/221 [00:35<00:27,  3.93it/s][A

 51%|█████     | 113/221 [00:35<00:23,  4.54it/s][A

 52%|█████▏    | 114/221 [00:35<00:20,  5.34it/s][A[h264 @ 0x56154279d080] mmco: unref short failure
[h264 @ 0x56154279d080] mmco: unref short failure


 52%|█████▏    | 116/221 [00:41<02:18,  1.32s/it][A

 53%|█████▎    | 117/221 [00:41<01:55,  1.11s/it][A[h264 @ 0x55e4cce38cc0] mmco: unref short failure


 53%|█████▎    | 118/221 [00:41<01:29,  1.15it/s][A

 54%|█████▍    | 119/221 [00:41<01:11,  1.43it/s][A

 54%|█████▍    | 120/221 [00:42<00:54,  1.85it/s][A

 55%|█████▍    | 121/221 [00:42<00:42,  2.36it/s][A

 55%|█████▌    | 122/221 [00:42<00:34,  2.90it/s][A

 56%|█████▌    | 123/221 [00:42<00:27,  3.59it/s][A

 56%|█████▌    | 124/221 [00:42<00:22,  4.31it/s][A

 57%|█████▋    | 125/221 [00:43<00:30,  3.16it/s][A

 57%|█████▋    | 126/221 [00:43<00:35,  2.68it/s][A

 57%|█████▋    | 127/221 [00:44<00:53,  1.75it/s][A

 58%|█████▊    | 128/221 [00:45<00:52,  1.78it/s][A

 58%|█████▊    | 129/221 [00:45<00:42,  2.16it/s][A

 59%|█████▉    | 130/221 [00:45<00:33,  2.68it/s][A

 59%|█████▉    | 131/221 [00:45<00:26,  3.38it/s][A

 60%|█████▉    | 132/221 [00:46<00:29,  3.03it/s][A

 60%|██████    | 133/221 [00:46<00:28,  3.13it/s][A

 61%|██████    | 134/221 [00:47<00:33,  2.56it/s][A

 61%|██████    | 135/221 [00:47<00:33,  2.56it/s][A

 62%|██████▏   | 136/221 [00:48<00:41,  2.03it/s][A

 62%|██████▏   | 137/221 [00:52<02:19,  1.66s/it][A

 62%|██████▏   | 138/221 [00:53<02:05,  1.51s/it][A[h264 @ 0x5615520f8800] mmco: unref short failure
[h264 @ 0x5615520f8800] mmco: unref short failure


 63%|██████▎   | 139/221 [00:54<01:41,  1.24s/it][A

 63%|██████▎   | 140/221 [00:54<01:17,  1.04it/s][A

 64%|██████▍   | 141/221 [00:55<01:06,  1.21it/s][A

 64%|██████▍   | 142/221 [00:55<00:51,  1.52it/s][A

 65%|██████▍   | 143/221 [00:55<00:40,  1.94it/s][A

 65%|██████▌   | 144/221 [00:55<00:30,  2.49it/s][A[h264 @ 0x55e4cf1f5c80] mmco: unref short failure


 66%|██████▌   | 145/221 [00:55<00:24,  3.15it/s][A

 66%|██████▌   | 146/221 [00:55<00:19,  3.81it/s][A

 67%|██████▋   | 147/221 [00:56<00:17,  4.23it/s][A

 67%|██████▋   | 148/221 [00:56<00:21,  3.35it/s][A

 67%|██████▋   | 149/221 [00:56<00:17,  4.16it/s][A

 68%|██████▊   | 151/221 [00:57<00:16,  4.33it/s][A

 69%|██████▉   | 152/221 [00:57<00:16,  4.12it/s][A

 69%|██████▉   | 153/221 [00:57<00:14,  4.71it/s][A

 70%|███████   | 155/221 [00:57<00:10,  6.04it/s][A

 71%|███████   | 156/221 [00:57<00:09,  6.54it/s][A[h264 @ 0x561554deccc0] mmco: unref short failure
[h264 @ 0x564019b492c0] mmco: unref short failure
[h264 @ 0x564019b492c0] mmco: unref short failure


 71%|███████   | 157/221 [01:02<01:29,  1.39s/it][A

 71%|███████▏  | 158/221 [01:03<01:08,  1.08s/it][A

 72%|███████▏  | 159/221 [01:03<00:52,  1.19it/s][A

 72%|███████▏  | 160/221 [01:03<00:39,  1.54it/s][A

 73%|███████▎  | 161/221 [01:03<00:31,  1.90it/s][A

 73%|███████▎  | 162/221 [01:03<00:24,  2.37it/s][A

 74%|███████▍  | 163/221 [01:04<00:21,  2.69it/s][A

 74%|███████▍  | 164/221 [01:04<00:17,  3.21it/s][A

 75%|███████▍  | 165/221 [01:04<00:14,  3.98it/s][A

 75%|███████▌  | 166/221 [01:09<01:31,  1.67s/it][A[h264 @ 0x55781fa11540] mmco: unref short failure
[h264 @ 0x55781fa11540] mmco: unref short failure
[h264 @ 0x55781fa11540] mmco: unref short failure
[h264 @ 0x56402219bac0] mmco: unref short failure
[h264 @ 0x56402219bac0] mmco: unref short failure
[h264 @ 0x55e4c1384d80] mmco: unref short failure
[h264 @ 0x55e4c1384d80] mmco: unref short failure


 76%|███████▌  | 168/221 [01:13<01:42,  1.94s/it][A

 76%|███████▋  | 169/221 [01:14<01:17,  1.49s/it][A

 77%|███████▋  | 170/221 [01:14<01:00,  1.19s/it][A

 77%|███████▋  | 171/221 [01:14<00:47,  1.06it/s][A

 78%|███████▊  | 173/221 [01:14<00:27,  1.73it/s][A

 79%|███████▉  | 175/221 [01:15<00:18,  2.50it/s][A

 80%|███████▉  | 176/221 [01:15<00:16,  2.81it/s][A

 80%|████████  | 177/221 [01:15<00:13,  3.23it/s][A

 81%|████████  | 178/221 [01:15<00:12,  3.48it/s][A

 81%|████████  | 179/221 [01:19<00:51,  1.22s/it][A

 81%|████████▏ | 180/221 [01:19<00:38,  1.05it/s][A

 82%|████████▏ | 181/221 [01:19<00:28,  1.40it/s][A

 82%|████████▏ | 182/221 [01:20<00:21,  1.77it/s][A

 83%|████████▎ | 183/221 [01:20<00:17,  2.19it/s][A

 83%|████████▎ | 184/221 [01:20<00:13,  2.75it/s][A

 84%|████████▍ | 186/221 [01:20<00:08,  4.26it/s][A

 85%|████████▍ | 187/221 [01:20<00:08,  4.20it/s][A

 85%|████████▌ | 188/221 [01:20<00:06,  4.79it/s][A[h264 @ 0x55e4c69f2900] mmco: unref short failure
[h264 @ 0x55e4c69f2900] mmco: unref short failure


 86%|████████▌ | 189/221 [01:21<00:06,  4.64it/s][A

 86%|████████▌ | 190/221 [01:21<00:06,  5.11it/s][A

 87%|████████▋ | 192/221 [01:21<00:03,  7.56it/s][A

 88%|████████▊ | 194/221 [01:21<00:03,  8.89it/s][A

 89%|████████▊ | 196/221 [01:21<00:03,  8.25it/s][A

 90%|████████▉ | 198/221 [01:22<00:02,  9.49it/s][A

 90%|█████████ | 200/221 [01:22<00:02,  9.47it/s][A

 91%|█████████▏| 202/221 [01:22<00:02,  9.25it/s][A

 92%|█████████▏| 203/221 [01:22<00:01,  9.07it/s][A

 93%|█████████▎| 205/221 [01:22<00:01,  8.82it/s][A

 93%|█████████▎| 206/221 [01:23<00:02,  7.00it/s][A

 94%|█████████▍| 208/221 [01:23<00:01,  8.27it/s][A

 95%|█████████▌| 210/221 [01:23<00:01,  9.32it/s][A

 95%|█████████▌| 211/221 [01:23<00:01,  6.82it/s][A

 96%|█████████▌| 212/221 [01:23<00:01,  7.10it/s][A

 96%|█████████▋| 213/221 [01:23<00:01,  7.43it/s][A

 97%|█████████▋| 214/221 [01:24<00:01,  6.89it/s][A

 98%|█████████▊| 216/221 [01:24<00:00,  8.30it/s][A[h264 @ 0x561546346b40] mmco: unref short failure
[h264 @ 0x561546346b40] mmco: unref short failure


 98%|█████████▊| 217/221 [01:29<00:04,  1.24s/it][A[h264 @ 0x56400b017a80] mmco: unref short failure


 99%|█████████▉| 219/221 [01:29<00:01,  1.28it/s][A

100%|█████████▉| 220/221 [01:34<00:01,  1.72s/it][A
100%|██████████| 221/221 [01:34<00:00,  2.34it/s]


  0%|          | 0/221 [00:00<?, ?it/s][A

  0%|          | 1/221 [00:00<01:05,  3.37it/s][A

  1%|          | 2/221 [00:00<01:04,  3.39it/s][A[h264 @ 0x56400c27bd40] mmco: unref short failure


  1%|▏         | 3/221 [00:00<01:04,  3.37it/s][A

  2%|▏         | 4/221 [00:01<01:04,  3.38it/s][A

  2%|▏         | 5/221 [00:01<01:04,  3.35it/s][A

  3%|▎         | 6/221 [00:01<01:05,  3.29it/s][A

  3%|▎         | 7/221 [00:02<01:04,  3.33it/s][A

  4%|▎         | 8/221 [00:02<01:05,  3.27it/s][A

  4%|▍         | 9/221 [00:02<01:04,  3.29it/s][A

  5%|▍         | 10/221 [00:03<01:04,  3.30it/s][A

  5%|▍         | 11/221 [00:03<01:03,  3.33it/s][A

  5%|▌         | 12/221 [00:03<01:02,  3.35it/s][A

  6%|▌         | 13/221 [00:03<01:02,  3.32it/s][A

  6%|▋         | 14/221 [00:04<01:02,  3.34it/s][A

  7%|▋         | 15/221 [00:04<01:01,  3.36it/s][A

  7%|▋         | 16/221 [00:04<01:00,  3.37it/s][A

  8%|▊         | 17/221 [00:05<01:00,  3.38it/s][A

  8%|▊         | 18/221 [00:05<01:00,  3.38it/s][A

  9%|▊         | 19/221 [00:05<01:00,  3.35it/s][A

  9%|▉         | 20/221 [00:05<00:59,  3.36it/s][A

 10%|▉         | 21/221 [00:06<00:59,  3.37it/s][A

 10%|▉         | 22/221 [00:06<00:58,  3.38it/s][A

 10%|█         | 23/221 [00:06<00:58,  3.38it/s][A

 11%|█         | 24/221 [00:07<01:01,  3.22it/s][A

 11%|█▏        | 25/221 [00:07<01:00,  3.23it/s][A

 12%|█▏        | 26/221 [00:07<00:59,  3.28it/s][A

 12%|█▏        | 27/221 [00:08<00:58,  3.31it/s][A

 13%|█▎        | 28/221 [00:08<00:57,  3.34it/s][A

 13%|█▎        | 29/221 [00:08<00:57,  3.36it/s][A

 14%|█▎        | 30/221 [00:08<00:56,  3.37it/s][A

 14%|█▍        | 31/221 [00:09<00:56,  3.38it/s][A

 14%|█▍        | 32/221 [00:09<00:55,  3.39it/s][A

 15%|█▍        | 33/221 [00:09<00:55,  3.39it/s][A

 15%|█▌        | 34/221 [00:10<00:55,  3.40it/s][A

 16%|█▌        | 35/221 [00:10<00:54,  3.40it/s][A

 16%|█▋        | 36/221 [00:10<00:54,  3.40it/s][A

 17%|█▋        | 37/221 [00:11<00:54,  3.40it/s][A

 17%|█▋        | 38/221 [00:11<00:53,  3.41it/s][A

 18%|█▊        | 39/221 [00:11<00:53,  3.41it/s][A

 18%|█▊        | 40/221 [00:11<00:53,  3.41it/s][A

 19%|█▊        | 41/221 [00:12<00:52,  3.41it/s][A

 19%|█▉        | 42/221 [00:12<00:52,  3.41it/s][A

 19%|█▉        | 43/221 [00:12<00:52,  3.41it/s][A

 20%|█▉        | 44/221 [00:13<00:51,  3.41it/s][A

 20%|██        | 45/221 [00:13<00:51,  3.42it/s][A

 21%|██        | 46/221 [00:13<00:51,  3.42it/s][A

 21%|██▏       | 47/221 [00:13<00:50,  3.42it/s][A

 22%|██▏       | 48/221 [00:14<00:50,  3.42it/s][A

 22%|██▏       | 49/221 [00:14<00:50,  3.42it/s][A

 23%|██▎       | 50/221 [00:14<00:50,  3.42it/s][A

 23%|██▎       | 51/221 [00:15<00:49,  3.42it/s][A

 24%|██▎       | 52/221 [00:15<00:49,  3.42it/s][A

 24%|██▍       | 53/221 [00:15<00:49,  3.42it/s][A

 24%|██▍       | 54/221 [00:16<00:48,  3.42it/s][A

 25%|██▍       | 55/221 [00:16<00:48,  3.42it/s][A

 25%|██▌       | 56/221 [00:16<00:48,  3.42it/s][A

 26%|██▌       | 57/221 [00:16<00:47,  3.42it/s][A

 26%|██▌       | 58/221 [00:17<00:47,  3.42it/s][A

 27%|██▋       | 59/221 [00:17<00:47,  3.42it/s][A

 27%|██▋       | 60/221 [00:17<00:47,  3.42it/s][A

 28%|██▊       | 61/221 [00:18<00:46,  3.42it/s][A

 28%|██▊       | 62/221 [00:18<00:46,  3.42it/s][A

 29%|██▊       | 63/221 [00:18<00:46,  3.42it/s][A

 29%|██▉       | 64/221 [00:18<00:45,  3.42it/s][A

 29%|██▉       | 65/221 [00:19<00:45,  3.42it/s][A

 30%|██▉       | 66/221 [00:19<00:45,  3.42it/s][A

 30%|███       | 67/221 [00:19<00:45,  3.42it/s][A

 31%|███       | 68/221 [00:20<00:44,  3.42it/s][A

 31%|███       | 69/221 [00:20<00:44,  3.42it/s][A

 32%|███▏      | 70/221 [00:20<00:44,  3.42it/s][A

 32%|███▏      | 71/221 [00:20<00:43,  3.42it/s][A

 33%|███▎      | 72/221 [00:21<00:43,  3.42it/s][A

 33%|███▎      | 73/221 [00:21<00:43,  3.42it/s][A

 33%|███▎      | 74/221 [00:21<00:43,  3.42it/s][A

 34%|███▍      | 75/221 [00:22<00:42,  3.42it/s][A

 34%|███▍      | 76/221 [00:22<00:42,  3.42it/s][A

 35%|███▍      | 77/221 [00:22<00:42,  3.42it/s][A

 35%|███▌      | 78/221 [00:23<00:41,  3.42it/s][A

 36%|███▌      | 79/221 [00:23<00:41,  3.42it/s][A

 36%|███▌      | 80/221 [00:23<00:41,  3.42it/s][A

 37%|███▋      | 81/221 [00:23<00:40,  3.42it/s][A

 37%|███▋      | 82/221 [00:24<00:40,  3.42it/s][A

 38%|███▊      | 83/221 [00:24<00:40,  3.42it/s][A

 38%|███▊      | 84/221 [00:24<00:40,  3.42it/s][A

 38%|███▊      | 85/221 [00:25<00:39,  3.42it/s][A

 39%|███▉      | 86/221 [00:25<00:39,  3.42it/s][A

 39%|███▉      | 87/221 [00:25<00:39,  3.42it/s][A

 40%|███▉      | 88/221 [00:25<00:38,  3.42it/s][A

 40%|████      | 89/221 [00:26<00:38,  3.42it/s][A

 41%|████      | 90/221 [00:26<00:38,  3.42it/s][A

 41%|████      | 91/221 [00:26<00:38,  3.42it/s][A

 42%|████▏     | 92/221 [00:27<00:37,  3.42it/s][A

 42%|████▏     | 93/221 [00:27<00:37,  3.42it/s][A

 43%|████▎     | 94/221 [00:27<00:37,  3.42it/s][A

 43%|████▎     | 95/221 [00:28<00:36,  3.42it/s][A

 43%|████▎     | 96/221 [00:28<00:36,  3.42it/s][A

 44%|████▍     | 97/221 [00:28<00:36,  3.42it/s][A

 44%|████▍     | 98/221 [00:28<00:35,  3.42it/s][A

 45%|████▍     | 99/221 [00:29<00:35,  3.42it/s][A

 45%|████▌     | 100/221 [00:29<00:35,  3.42it/s][A

 46%|████▌     | 101/221 [00:29<00:35,  3.42it/s][A

 46%|████▌     | 102/221 [00:30<00:34,  3.42it/s][A

 47%|████▋     | 103/221 [00:30<00:34,  3.42it/s][A

 47%|████▋     | 104/221 [00:30<00:34,  3.42it/s][A

 48%|████▊     | 105/221 [00:30<00:33,  3.42it/s][A

 48%|████▊     | 106/221 [00:31<00:33,  3.42it/s][A

 48%|████▊     | 107/221 [00:31<00:33,  3.42it/s][A

 49%|████▉     | 108/221 [00:31<00:33,  3.42it/s][A

 49%|████▉     | 109/221 [00:32<00:32,  3.42it/s][A

 50%|████▉     | 110/221 [00:32<00:32,  3.42it/s][A

 50%|█████     | 111/221 [00:32<00:32,  3.42it/s][A

 51%|█████     | 112/221 [00:32<00:31,  3.42it/s][A

 51%|█████     | 113/221 [00:33<00:31,  3.42it/s][A

 52%|█████▏    | 114/221 [00:33<00:31,  3.42it/s][A

 52%|█████▏    | 115/221 [00:33<00:31,  3.42it/s][A

 52%|█████▏    | 116/221 [00:34<00:30,  3.42it/s][A

 53%|█████▎    | 117/221 [00:34<00:30,  3.42it/s][A

 53%|█████▎    | 118/221 [00:34<00:30,  3.42it/s][A

 54%|█████▍    | 119/221 [00:35<00:29,  3.42it/s][A

 54%|█████▍    | 120/221 [00:35<00:29,  3.42it/s][A

 55%|█████▍    | 121/221 [00:35<00:29,  3.42it/s][A

 55%|█████▌    | 122/221 [00:35<00:28,  3.42it/s][A

 56%|█████▌    | 123/221 [00:36<00:28,  3.42it/s][A

 56%|█████▌    | 124/221 [00:36<00:28,  3.42it/s][A

 57%|█████▋    | 125/221 [00:36<00:28,  3.42it/s][A

 57%|█████▋    | 126/221 [00:37<00:27,  3.42it/s][A

 57%|█████▋    | 127/221 [00:37<00:27,  3.42it/s][A

 58%|█████▊    | 128/221 [00:37<00:27,  3.42it/s][A

 58%|█████▊    | 129/221 [00:37<00:26,  3.42it/s][A

 59%|█████▉    | 130/221 [00:38<00:26,  3.42it/s][A

 59%|█████▉    | 131/221 [00:38<00:26,  3.42it/s][A

 60%|█████▉    | 132/221 [00:38<00:26,  3.42it/s][A

 60%|██████    | 133/221 [00:39<00:25,  3.42it/s][A

 61%|██████    | 134/221 [00:39<00:25,  3.42it/s][A

 61%|██████    | 135/221 [00:39<00:25,  3.42it/s][A

 62%|██████▏   | 136/221 [00:40<00:24,  3.42it/s][A

 62%|██████▏   | 137/221 [00:40<00:24,  3.42it/s][A

 62%|██████▏   | 138/221 [00:40<00:24,  3.42it/s][A

 63%|██████▎   | 139/221 [00:40<00:23,  3.42it/s][A

 63%|██████▎   | 140/221 [00:41<00:23,  3.42it/s][A

 64%|██████▍   | 141/221 [00:41<00:23,  3.42it/s][A

 64%|██████▍   | 142/221 [00:41<00:23,  3.42it/s][A

 65%|██████▍   | 143/221 [00:42<00:22,  3.42it/s][A

 65%|██████▌   | 144/221 [00:42<00:22,  3.42it/s][A

 66%|██████▌   | 145/221 [00:42<00:22,  3.42it/s][A

 66%|██████▌   | 146/221 [00:42<00:21,  3.42it/s][A

 67%|██████▋   | 147/221 [00:43<00:21,  3.42it/s][A

 67%|██████▋   | 148/221 [00:43<00:21,  3.42it/s][A

 67%|██████▋   | 149/221 [00:43<00:21,  3.42it/s][A

 68%|██████▊   | 150/221 [00:44<00:20,  3.42it/s][A

 68%|██████▊   | 151/221 [00:44<00:20,  3.42it/s][A

 69%|██████▉   | 152/221 [00:44<00:20,  3.42it/s][A

 69%|██████▉   | 153/221 [00:44<00:19,  3.42it/s][A

 70%|██████▉   | 154/221 [00:45<00:19,  3.42it/s][A

 70%|███████   | 155/221 [00:45<00:19,  3.42it/s][A

 71%|███████   | 156/221 [00:45<00:19,  3.42it/s][A

 71%|███████   | 157/221 [00:46<00:18,  3.42it/s][A

 71%|███████▏  | 158/221 [00:46<00:18,  3.42it/s][A

 72%|███████▏  | 159/221 [00:46<00:18,  3.42it/s][A

 72%|███████▏  | 160/221 [00:47<00:17,  3.42it/s][A

 73%|███████▎  | 161/221 [00:47<00:17,  3.42it/s][A

 73%|███████▎  | 162/221 [00:47<00:17,  3.42it/s][A

 74%|███████▍  | 163/221 [00:47<00:16,  3.42it/s][A

 74%|███████▍  | 164/221 [00:48<00:16,  3.42it/s][A

 75%|███████▍  | 165/221 [00:48<00:16,  3.42it/s][A

 75%|███████▌  | 166/221 [00:48<00:16,  3.42it/s][A

 76%|███████▌  | 167/221 [00:49<00:15,  3.42it/s][A

 76%|███████▌  | 168/221 [00:49<00:15,  3.42it/s][A

 76%|███████▋  | 169/221 [00:49<00:15,  3.42it/s][A

 77%|███████▋  | 170/221 [00:49<00:14,  3.42it/s][A

 77%|███████▋  | 171/221 [00:50<00:14,  3.42it/s][A

 78%|███████▊  | 172/221 [00:50<00:14,  3.42it/s][A

 78%|███████▊  | 173/221 [00:50<00:14,  3.42it/s][A

 79%|███████▊  | 174/221 [00:51<00:13,  3.42it/s][A

 79%|███████▉  | 175/221 [00:51<00:13,  3.42it/s][A

 80%|███████▉  | 176/221 [00:51<00:13,  3.42it/s][A

 80%|████████  | 177/221 [00:52<00:12,  3.42it/s][A

 81%|████████  | 178/221 [00:52<00:12,  3.42it/s][A

 81%|████████  | 179/221 [00:52<00:12,  3.42it/s][A

 81%|████████▏ | 180/221 [00:52<00:11,  3.42it/s][A

 82%|████████▏ | 181/221 [00:53<00:11,  3.42it/s][A

 82%|████████▏ | 182/221 [00:53<00:11,  3.42it/s][A

 83%|████████▎ | 183/221 [00:53<00:11,  3.42it/s][A

 83%|████████▎ | 184/221 [00:54<00:10,  3.42it/s][A

 84%|████████▎ | 185/221 [00:54<00:10,  3.42it/s][A

 84%|████████▍ | 186/221 [00:54<00:10,  3.42it/s][A

 85%|████████▍ | 187/221 [00:54<00:09,  3.42it/s][A

 85%|████████▌ | 188/221 [00:55<00:09,  3.42it/s][A

 86%|████████▌ | 189/221 [00:55<00:09,  3.42it/s][A

 86%|████████▌ | 190/221 [00:55<00:09,  3.42it/s][A

 86%|████████▋ | 191/221 [00:56<00:08,  3.42it/s][A

 87%|████████▋ | 192/221 [00:56<00:08,  3.42it/s][A

 87%|████████▋ | 193/221 [00:56<00:08,  3.42it/s][A

 88%|████████▊ | 194/221 [00:56<00:07,  3.42it/s][A

 88%|████████▊ | 195/221 [00:57<00:07,  3.42it/s][A

 89%|████████▊ | 196/221 [00:57<00:07,  3.42it/s][A

 89%|████████▉ | 197/221 [00:57<00:07,  3.42it/s][A

 90%|████████▉ | 198/221 [00:58<00:06,  3.42it/s][A

 90%|█████████ | 199/221 [00:58<00:06,  3.42it/s][A

 90%|█████████ | 200/221 [00:58<00:06,  3.42it/s][A

 91%|█████████ | 201/221 [00:59<00:05,  3.42it/s][A

 91%|█████████▏| 202/221 [00:59<00:05,  3.42it/s][A

 92%|█████████▏| 203/221 [00:59<00:05,  3.42it/s][A

 92%|█████████▏| 204/221 [00:59<00:04,  3.42it/s][A

 93%|█████████▎| 205/221 [01:00<00:04,  3.42it/s][A

 93%|█████████▎| 206/221 [01:00<00:04,  3.42it/s][A

 94%|█████████▎| 207/221 [01:00<00:04,  3.42it/s][A

 94%|█████████▍| 208/221 [01:01<00:03,  3.42it/s][A

 95%|█████████▍| 209/221 [01:01<00:03,  3.42it/s][A

 95%|█████████▌| 210/221 [01:01<00:03,  3.42it/s][A

 95%|█████████▌| 211/221 [01:01<00:02,  3.42it/s][A

 96%|█████████▌| 212/221 [01:02<00:02,  3.42it/s][A

 96%|█████████▋| 213/221 [01:02<00:02,  3.42it/s][A

 97%|█████████▋| 214/221 [01:02<00:02,  3.42it/s][A

 97%|█████████▋| 215/221 [01:03<00:01,  3.42it/s][A

 98%|█████████▊| 216/221 [01:03<00:01,  3.42it/s][A

 98%|█████████▊| 217/221 [01:03<00:01,  3.42it/s][A

 99%|█████████▊| 218/221 [01:03<00:00,  3.42it/s][A

 99%|█████████▉| 219/221 [01:04<00:00,  3.42it/s][A

100%|█████████▉| 220/221 [01:04<00:00,  3.42it/s][A

100%|██████████| 221/221 [01:04<00:00,  3.42it/s][A
100%|██████████| 221/221 [01:04<00:00,  3.41it/s]


  0%|          | 0/221 [00:00<?, ?it/s][A

  0%|          | 1/221 [00:00<00:30,  7.16it/s][A

  1%|          | 2/221 [00:00<01:13,  2.99it/s][A

  1%|▏         | 3/221 [00:01<01:29,  2.42it/s][A

  2%|▏         | 4/221 [00:01<01:16,  2.84it/s][A

  2%|▏         | 5/221 [00:01<01:06,  3.25it/s][A

  3%|▎         | 6/221 [00:01<00:54,  3.97it/s][A

  3%|▎         | 7/221 [00:01<00:44,  4.80it/s][A

  4%|▎         | 8/221 [00:02<00:47,  4.48it/s][A

  4%|▍         | 9/221 [00:02<01:19,  2.68it/s][A

  5%|▍         | 10/221 [00:03<01:19,  2.66it/s][A

  5%|▍         | 11/221 [00:03<01:21,  2.57it/s][A

  5%|▌         | 12/221 [00:03<01:16,  2.73it/s][A

  6%|▌         | 13/221 [00:04<01:32,  2.26it/s][A

  6%|▋         | 14/221 [00:04<01:19,  2.60it/s][A

  7%|▋         | 15/221 [00:05<01:17,  2.65it/s][A

  7%|▋         | 16/221 [00:05<01:11,  2.88it/s][A

  8%|▊         | 17/221 [00:05<01:21,  2.50it/s][A

  8%|▊         | 18/221 [00:06<01:18,  2.58it/s][A

  9%|▊         | 19/221 [00:06<01:11,  2.83it/s][A

  9%|▉         | 20/221 [00:06<01:01,  3.27it/s][A

 10%|▉         | 21/221 [00:06<00:49,  4.07it/s][A

 10%|▉         | 22/221 [00:07<00:50,  3.91it/s][A

 10%|█         | 23/221 [00:07<00:47,  4.16it/s][A

 11%|█         | 24/221 [00:07<00:44,  4.47it/s][A

 11%|█▏        | 25/221 [00:07<00:42,  4.59it/s][A

 12%|█▏        | 26/221 [00:07<00:40,  4.78it/s][A

 12%|█▏        | 27/221 [00:08<00:45,  4.27it/s][A

 13%|█▎        | 28/221 [00:08<00:51,  3.75it/s][A

 13%|█▎        | 29/221 [00:09<01:07,  2.86it/s][A

 14%|█▎        | 30/221 [00:09<01:00,  3.18it/s][A

 14%|█▍        | 31/221 [00:09<00:54,  3.50it/s][A

 14%|█▍        | 32/221 [00:09<01:00,  3.12it/s][A

 15%|█▍        | 33/221 [00:10<00:52,  3.56it/s][A

 15%|█▌        | 34/221 [00:10<00:49,  3.76it/s][A

 16%|█▌        | 35/221 [00:10<00:42,  4.37it/s][A

 16%|█▋        | 36/221 [00:10<00:53,  3.44it/s][A

 17%|█▋        | 37/221 [00:11<00:52,  3.48it/s][A

 17%|█▋        | 38/221 [00:11<00:51,  3.58it/s][A

 18%|█▊        | 39/221 [00:11<00:48,  3.74it/s][A

 18%|█▊        | 40/221 [00:11<00:44,  4.05it/s][A

 19%|█▊        | 41/221 [00:12<00:42,  4.27it/s][A

 19%|█▉        | 42/221 [00:12<00:38,  4.70it/s][A

 19%|█▉        | 43/221 [00:12<00:46,  3.83it/s][A

 20%|█▉        | 44/221 [00:12<00:46,  3.84it/s][A

 20%|██        | 45/221 [00:13<01:03,  2.76it/s][A

 21%|██        | 46/221 [00:13<01:04,  2.69it/s][A

 21%|██▏       | 47/221 [00:14<00:53,  3.24it/s][A

 22%|██▏       | 48/221 [00:14<00:43,  3.98it/s][A

 23%|██▎       | 50/221 [00:14<00:44,  3.87it/s][A

 23%|██▎       | 51/221 [00:14<00:41,  4.08it/s][A

 24%|██▎       | 52/221 [00:15<00:36,  4.58it/s][A

 24%|██▍       | 53/221 [00:15<00:44,  3.81it/s][A

 24%|██▍       | 54/221 [00:15<00:47,  3.54it/s][A

 25%|██▍       | 55/221 [00:15<00:40,  4.09it/s][A

 25%|██▌       | 56/221 [00:16<00:42,  3.87it/s][A

 26%|██▌       | 57/221 [00:16<00:45,  3.57it/s][A

 26%|██▌       | 58/221 [00:16<00:39,  4.14it/s][A

 27%|██▋       | 59/221 [00:16<00:38,  4.18it/s][A

 27%|██▋       | 60/221 [00:17<00:35,  4.52it/s][A

 28%|██▊       | 61/221 [00:17<00:42,  3.80it/s][A

 28%|██▊       | 62/221 [00:17<00:41,  3.84it/s][A

 29%|██▊       | 63/221 [00:18<00:39,  3.98it/s][A

 29%|██▉       | 64/221 [00:18<00:34,  4.53it/s][A

 29%|██▉       | 65/221 [00:18<00:34,  4.51it/s][A

 30%|██▉       | 66/221 [00:18<00:43,  3.57it/s][A

 30%|███       | 67/221 [00:19<00:47,  3.25it/s][A

 31%|███       | 68/221 [00:19<00:51,  2.99it/s][A

 31%|███       | 69/221 [00:20<01:11,  2.11it/s][A

 32%|███▏      | 70/221 [00:20<01:01,  2.44it/s][A

 32%|███▏      | 71/221 [00:20<00:57,  2.59it/s][A

 33%|███▎      | 72/221 [00:21<00:54,  2.74it/s][A

 33%|███▎      | 73/221 [00:21<00:53,  2.77it/s][A

 33%|███▎      | 74/221 [00:21<00:44,  3.29it/s][A

 34%|███▍      | 75/221 [00:22<00:46,  3.11it/s][A

 34%|███▍      | 76/221 [00:22<00:47,  3.05it/s][A

 35%|███▍      | 77/221 [00:22<00:39,  3.67it/s][A

 35%|███▌      | 78/221 [00:23<00:42,  3.34it/s][A

 36%|███▌      | 79/221 [00:23<00:36,  3.93it/s][A

 36%|███▌      | 80/221 [00:23<00:35,  4.00it/s][A

 37%|███▋      | 81/221 [00:23<00:32,  4.35it/s][A

 37%|███▋      | 82/221 [00:23<00:35,  3.92it/s][A

 38%|███▊      | 83/221 [00:24<00:32,  4.22it/s][A

 38%|███▊      | 84/221 [00:24<00:43,  3.18it/s][A

 38%|███▊      | 85/221 [00:24<00:42,  3.18it/s][A

 39%|███▉      | 86/221 [00:25<00:34,  3.93it/s][A

 39%|███▉      | 87/221 [00:25<00:43,  3.07it/s][A

 40%|███▉      | 88/221 [00:25<00:40,  3.27it/s][A

 40%|████      | 89/221 [00:26<00:46,  2.84it/s][A

 41%|████      | 90/221 [00:26<00:48,  2.72it/s][A

 41%|████      | 91/221 [00:26<00:38,  3.36it/s][A

 42%|████▏     | 92/221 [00:27<00:36,  3.53it/s][A

 42%|████▏     | 93/221 [00:27<00:39,  3.24it/s][A

 43%|████▎     | 94/221 [00:27<00:41,  3.04it/s][A

 43%|████▎     | 95/221 [00:28<00:58,  2.17it/s][A

 43%|████▎     | 96/221 [00:28<00:52,  2.36it/s][A

 44%|████▍     | 97/221 [00:29<00:48,  2.55it/s][A

 44%|████▍     | 98/221 [00:29<00:52,  2.35it/s][A

 45%|████▍     | 99/221 [00:29<00:46,  2.61it/s][A

 45%|████▌     | 100/221 [00:30<00:47,  2.53it/s][A

 46%|████▌     | 101/221 [00:30<00:41,  2.87it/s][A

 46%|████▌     | 102/221 [00:30<00:41,  2.88it/s][A

 47%|████▋     | 103/221 [00:31<00:36,  3.27it/s][A

 48%|████▊     | 105/221 [00:31<00:27,  4.26it/s][A

 48%|████▊     | 106/221 [00:31<00:30,  3.72it/s][A

 48%|████▊     | 107/221 [00:31<00:25,  4.41it/s][A

 49%|████▉     | 108/221 [00:32<00:21,  5.16it/s][A

 49%|████▉     | 109/221 [00:32<00:21,  5.12it/s][A

 50%|████▉     | 110/221 [00:32<00:22,  4.87it/s][A

 50%|█████     | 111/221 [00:32<00:26,  4.22it/s][A

 51%|█████     | 112/221 [00:33<00:29,  3.70it/s][A

 51%|█████     | 113/221 [00:33<00:28,  3.76it/s][A

 52%|█████▏    | 114/221 [00:33<00:23,  4.58it/s][A

 52%|█████▏    | 116/221 [00:33<00:19,  5.28it/s][A

 53%|█████▎    | 117/221 [00:34<00:20,  5.15it/s][A

 53%|█████▎    | 118/221 [00:34<00:23,  4.44it/s][A

 54%|█████▍    | 119/221 [00:34<00:28,  3.56it/s][A

 54%|█████▍    | 120/221 [00:35<00:30,  3.30it/s][A

 55%|█████▍    | 121/221 [00:35<00:30,  3.26it/s][A

 55%|█████▌    | 122/221 [00:35<00:30,  3.24it/s][A

 56%|█████▌    | 123/221 [00:35<00:26,  3.63it/s][A

 56%|█████▌    | 124/221 [00:36<00:33,  2.88it/s][A

 57%|█████▋    | 125/221 [00:36<00:36,  2.65it/s][A

 57%|█████▋    | 126/221 [00:37<00:33,  2.87it/s][A

 57%|█████▋    | 127/221 [00:37<00:32,  2.89it/s][A

 58%|█████▊    | 128/221 [00:37<00:29,  3.15it/s][A

 58%|█████▊    | 129/221 [00:37<00:24,  3.79it/s][A

 59%|█████▉    | 130/221 [00:38<00:22,  3.99it/s][A

 59%|█████▉    | 131/221 [00:38<00:21,  4.25it/s][A

 60%|█████▉    | 132/221 [00:38<00:27,  3.22it/s][A

 60%|██████    | 133/221 [00:39<00:29,  2.99it/s][A

 61%|██████    | 134/221 [00:39<00:30,  2.82it/s][A

 61%|██████    | 135/221 [00:39<00:28,  3.07it/s][A

 62%|██████▏   | 136/221 [00:40<00:28,  3.02it/s][A

 62%|██████▏   | 137/221 [00:40<00:26,  3.19it/s][A

 62%|██████▏   | 138/221 [00:40<00:23,  3.46it/s][A

 63%|██████▎   | 139/221 [00:41<00:28,  2.93it/s][A

 63%|██████▎   | 140/221 [00:41<00:24,  3.28it/s][A

 64%|██████▍   | 141/221 [00:41<00:23,  3.44it/s][A

 64%|██████▍   | 142/221 [00:41<00:20,  3.81it/s][A

 65%|██████▍   | 143/221 [00:42<00:23,  3.32it/s][A

 65%|██████▌   | 144/221 [00:42<00:21,  3.57it/s][A

 66%|██████▌   | 145/221 [00:43<00:27,  2.80it/s][A

 66%|██████▌   | 146/221 [00:43<00:24,  3.00it/s][A

 67%|██████▋   | 147/221 [00:43<00:20,  3.68it/s][A

 67%|██████▋   | 148/221 [00:44<00:29,  2.48it/s][A

 67%|██████▋   | 149/221 [00:44<00:25,  2.88it/s][A

 68%|██████▊   | 150/221 [00:44<00:22,  3.17it/s][A

 68%|██████▊   | 151/221 [00:45<00:34,  2.05it/s][A

 69%|██████▉   | 152/221 [00:46<00:34,  1.98it/s][A

 69%|██████▉   | 153/221 [00:46<00:29,  2.28it/s][A

 70%|██████▉   | 154/221 [00:46<00:24,  2.75it/s][A

 70%|███████   | 155/221 [00:46<00:21,  3.06it/s][A

 71%|███████   | 156/221 [00:47<00:19,  3.34it/s][A

 71%|███████   | 157/221 [00:47<00:18,  3.42it/s][A

 71%|███████▏  | 158/221 [00:47<00:17,  3.53it/s][A

 72%|███████▏  | 159/221 [00:47<00:15,  3.94it/s][A

 72%|███████▏  | 160/221 [00:48<00:17,  3.44it/s][A

 73%|███████▎  | 162/221 [00:48<00:11,  5.04it/s][A

 74%|███████▍  | 163/221 [00:48<00:11,  5.00it/s][A

 74%|███████▍  | 164/221 [00:48<00:12,  4.57it/s][A

 75%|███████▍  | 165/221 [00:49<00:13,  4.05it/s][A

 75%|███████▌  | 166/221 [00:49<00:15,  3.44it/s][A

 76%|███████▌  | 167/221 [00:49<00:14,  3.66it/s][A

 76%|███████▌  | 168/221 [00:49<00:14,  3.74it/s][A

 77%|███████▋  | 170/221 [00:50<00:12,  3.93it/s][A

 77%|███████▋  | 171/221 [00:50<00:15,  3.14it/s][A

 78%|███████▊  | 172/221 [00:51<00:14,  3.33it/s][A

 78%|███████▊  | 173/221 [00:51<00:14,  3.29it/s][A

 79%|███████▊  | 174/221 [00:51<00:14,  3.32it/s][A

 79%|███████▉  | 175/221 [00:52<00:14,  3.14it/s][A

 80%|███████▉  | 176/221 [00:52<00:11,  3.83it/s][A

 80%|████████  | 177/221 [00:52<00:10,  4.14it/s][A

 81%|████████  | 178/221 [00:53<00:15,  2.74it/s][A

 81%|████████  | 179/221 [00:53<00:14,  2.82it/s][A

 81%|████████▏ | 180/221 [00:53<00:11,  3.52it/s][A

 82%|████████▏ | 181/221 [00:53<00:12,  3.24it/s][A

 82%|████████▏ | 182/221 [00:54<00:10,  3.74it/s][A

 83%|████████▎ | 183/221 [00:54<00:10,  3.63it/s][A

 83%|████████▎ | 184/221 [00:54<00:11,  3.32it/s][A

 84%|████████▎ | 185/221 [00:55<00:09,  3.61it/s][A

 84%|████████▍ | 186/221 [00:55<00:12,  2.86it/s][A

 85%|████████▌ | 188/221 [00:55<00:08,  3.89it/s][A

 86%|████████▌ | 189/221 [00:56<00:07,  4.14it/s][A

 86%|████████▌ | 190/221 [00:56<00:08,  3.81it/s][A

 86%|████████▋ | 191/221 [00:56<00:06,  4.49it/s][A

 87%|████████▋ | 192/221 [00:56<00:06,  4.45it/s][A

 87%|████████▋ | 193/221 [00:57<00:07,  3.74it/s][A

 88%|████████▊ | 194/221 [00:57<00:07,  3.46it/s][A

 88%|████████▊ | 195/221 [00:57<00:07,  3.45it/s][A

 89%|████████▊ | 196/221 [00:58<00:08,  3.03it/s][A

 89%|████████▉ | 197/221 [00:58<00:07,  3.40it/s][A

 90%|████████▉ | 198/221 [00:58<00:07,  3.12it/s][A

 90%|█████████ | 199/221 [00:58<00:06,  3.50it/s][A

 90%|█████████ | 200/221 [00:59<00:05,  3.87it/s][A

 91%|█████████ | 201/221 [00:59<00:04,  4.26it/s][A

 91%|█████████▏| 202/221 [00:59<00:04,  4.55it/s][A

 92%|█████████▏| 203/221 [00:59<00:04,  4.35it/s][A

 92%|█████████▏| 204/221 [00:59<00:03,  4.50it/s][A

 93%|█████████▎| 205/221 [01:00<00:03,  4.49it/s][A

 93%|█████████▎| 206/221 [01:00<00:03,  3.76it/s][A

 94%|█████████▎| 207/221 [01:00<00:04,  3.20it/s][A

 94%|█████████▍| 208/221 [01:01<00:03,  3.44it/s][A

 95%|█████████▍| 209/221 [01:01<00:03,  3.35it/s][A

 95%|█████████▌| 210/221 [01:01<00:03,  3.42it/s][A

 95%|█████████▌| 211/221 [01:02<00:02,  3.58it/s][A

 96%|█████████▌| 212/221 [01:02<00:02,  3.57it/s][A

 96%|█████████▋| 213/221 [01:02<00:02,  3.44it/s][A

 97%|█████████▋| 214/221 [01:03<00:02,  2.72it/s][A

 97%|█████████▋| 215/221 [01:03<00:02,  2.93it/s][A

 98%|█████████▊| 216/221 [01:03<00:01,  2.87it/s][A

 98%|█████████▊| 217/221 [01:04<00:01,  2.87it/s][A

 99%|█████████▊| 218/221 [01:04<00:01,  2.74it/s][A

 99%|█████████▉| 219/221 [01:04<00:00,  2.71it/s][A

100%|█████████▉| 220/221 [01:05<00:00,  3.14it/s][A

100%|██████████| 221/221 [01:05<00:00,  2.85it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.37it/s]
09/11/2024 15:27:44 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 49--===========

09/11/2024 15:27:44 - INFO - __main__ -   {'area_r1': 34.8, 'area_recall': '34.8/56.2/63.3', 'area_ravg': 51.5}
09/11/2024 15:27:44 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 49--===========

09/11/2024 15:27:44 - INFO - __main__ -   {'forward_r1': 37.2, 'forward_recall': '37.2/66.9/78.3', 'forward_ravg': 60.8}
09/11/2024 15:27:44 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video=====step 49--===========

09/11/2024 15:27:44 - INFO - __main__ -   {'area_video_r1': 37.3, 'area_video_recall': '37.3/67.5/77.6', 'area_video_ravg': 60.8}
09/11/2024 15:27:44 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video====history best step: 49=======

09/11/2024 15:27:44 - INFO - __main__ -   {'area_video_r1': 37.3, 'area_video_recall': '37.3/67.5/77.6', 'area_video_ravg': 60.8}
09/11/2024 15:27:44 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_area=====step 49--===========

09/11/2024 15:27:44 - INFO - __main__ -   {'area_video_r1': 50.0, 'area_video_recall': '50.0/71.0/78.3', 'area_video_ravg': 66.4, 'area_video_back_r1': 46.4, 'area_video_back_recall': '46.4/70.8/80.0', 'area_video_back_ravg': 65.7}
09/11/2024 15:27:44 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_area====history best step: 49=======

09/11/2024 15:27:44 - INFO - __main__ -   {'area_video_r1': 50.0, 'area_video_recall': '50.0/71.0/78.3', 'area_video_ravg': 66.4, 'area_video_back_r1': 46.4, 'area_video_back_recall': '46.4/70.8/80.0', 'area_video_back_ravg': 65.7}
09/11/2024 15:27:44 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 49--===========

09/11/2024 15:27:44 - INFO - __main__ -   {'video_r1': 36.8, 'video_recall': '36.8/65.6/73.9', 'video_ravg': 58.7}
09/11/2024 15:27:44 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 49=======

09/11/2024 15:27:44 - INFO - __main__ -   {'video_r1': 36.8, 'video_recall': '36.8/65.6/73.9', 'video_ravg': 58.7}
09/11/2024 15:27:44 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 49--===========

09/11/2024 15:27:44 - INFO - __main__ -   {'video_r1': 50.8, 'video_recall': '50.8/71.8/79.0', 'video_ravg': 67.2}
09/11/2024 15:27:44 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 49=======

09/11/2024 15:27:44 - INFO - __main__ -   {'video_r1': 50.8, 'video_recall': '50.8/71.8/79.0', 'video_ravg': 67.2}
09/11/2024 15:28:21 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.00728287547826767, 'loss_ret%tv%ta--finetune_area/loss_area': 4.072731018066406, 'loss_ret%tv%ta--finetune_area/total_loss': 4.080013751983643}
[h264 @ 0x557828fb1540] mmco: unref short failure

  2%|▏         | 50/2315 [17:00<82:50:36, 131.67s/it]
  2%|▏         | 51/2315 [17:03<58:37:48, 93.23s/it] 
  2%|▏         | 52/2315 [17:07<41:41:16, 66.32s/it]
  2%|▏         | 53/2315 [17:11<29:58:31, 47.71s/it]
  2%|▏         | 54/2315 [17:15<21:42:58, 34.58s/it][h264 @ 0x55e4c39a8140] mmco: unref short failure
[h264 @ 0x55e4c39a8140] mmco: unref short failure

  2%|▏         | 55/2315 [17:20<16:02:41, 25.56s/it]
  2%|▏         | 56/2315 [17:25<12:09:28, 19.38s/it][h264 @ 0x55e4cd4cd280] mmco: unref short failure
[h264 @ 0x55e4cd4cd280] mmco: unref short failure
[h264 @ 0x55e4c77fd700] mmco: unref short failure
[h264 @ 0x55e4c77fd700] mmco: unref short failure
[h264 @ 0x55e4c77fd700] mmco: unref short failure
[h264 @ 0x55e4c77fd700] mmco: unref short failure

  2%|▏         | 57/2315 [17:30<9:25:36, 15.03s/it] 
  3%|▎         | 58/2315 [17:35<7:38:28, 12.19s/it][h264 @ 0x564020c0a3c0] mmco: unref short failure
[h264 @ 0x564020c0a3c0] mmco: unref short failure

  3%|▎         | 59/2315 [17:40<6:14:47,  9.97s/it][h264 @ 0x557829aaa700] mmco: unref short failure

  3%|▎         | 60/2315 [17:45<5:15:10,  8.39s/it][h264 @ 0x5578240ccbc0] mmco: unref short failure
[h264 @ 0x5578240ccbc0] mmco: unref short failure

  3%|▎         | 61/2315 [17:50<4:36:30,  7.36s/it][h264 @ 0x561554098cc0] mmco: unref short failure

  3%|▎         | 62/2315 [17:55<4:09:42,  6.65s/it][h264 @ 0x55782bd507c0] mmco: unref short failure
[h264 @ 0x55782bd507c0] mmco: unref short failure

  3%|▎         | 63/2315 [17:59<3:49:34,  6.12s/it][h264 @ 0x55e4d115ed80] mmco: unref short failure
[h264 @ 0x55e4d115ed80] mmco: unref short failure

  3%|▎         | 64/2315 [18:04<3:32:20,  5.66s/it][h264 @ 0x56154706c840] mmco: unref short failure
[h264 @ 0x56154706c840] mmco: unref short failure
[h264 @ 0x55782bc9f300] mmco: unref short failure

  3%|▎         | 65/2315 [18:10<3:32:23,  5.66s/it][h264 @ 0x55e4be31a700] mmco: unref short failure
[h264 @ 0x55e4be31a700] mmco: unref short failure
[h264 @ 0x55781ef7d440] mmco: unref short failure
[h264 @ 0x55781dcaf0c0] mmco: unref short failure
[h264 @ 0x5578223c5980] mmco: unref short failure
[h264 @ 0x55781ef30880] mmco: unref short failure
[h264 @ 0x55781ef30880] mmco: unref short failure
[h264 @ 0x55781ef30880] mmco: unref short failure
[h264 @ 0x55781ef30880] mmco: unref short failure
[h264 @ 0x561556919140] mmco: unref short failure
[h264 @ 0x561556919140] mmco: unref short failure

  3%|▎         | 66/2315 [18:40<8:12:15, 13.13s/it][h264 @ 0x55e4bedc7580] mmco: unref short failure
[h264 @ 0x55e4bedc7580] mmco: unref short failure
[h264 @ 0x56401b0b1f00] mmco: unref short failure
[h264 @ 0x55e4d05ade00] mmco: unref short failure
[h264 @ 0x55782bf9e0c0] mmco: unref short failure
[h264 @ 0x55782bf9e0c0] mmco: unref short failure
[h264 @ 0x56400e1b70c0] mmco: unref short failure

  3%|▎         | 67/2315 [19:02<9:51:05, 15.78s/it][h264 @ 0x55782b008340] mmco: unref short failure
[h264 @ 0x56401e5a5840] mmco: unref short failure
[h264 @ 0x56401e5a5840] mmco: unref short failure
[h264 @ 0x56401a0103c0] mmco: unref short failure
[h264 @ 0x56401a0103c0] mmco: unref short failure

  3%|▎         | 68/2315 [19:13<8:50:53, 14.18s/it]
  3%|▎         | 69/2315 [19:21<7:45:04, 12.42s/it][h264 @ 0x55e4c5a0f6c0] mmco: unref short failure
[h264 @ 0x55e4c5a0f6c0] mmco: unref short failure
[h264 @ 0x56154e3b84c0] mmco: unref short failure
[h264 @ 0x56154e3b84c0] mmco: unref short failure
[h264 @ 0x55e4c9934c00] mmco: unref short failure

  3%|▎         | 70/2315 [19:31<7:16:21, 11.66s/it][h264 @ 0x56154fa44240] mmco: unref short failure
[h264 @ 0x56154fa44240] mmco: unref short failure
[h264 @ 0x56154fa44240] mmco: unref short failure
[h264 @ 0x56154fa44240] mmco: unref short failure

  3%|▎         | 71/2315 [19:36<6:01:55,  9.68s/it][h264 @ 0x55781e0d5100] mmco: unref short failure
[h264 @ 0x55781e0d5100] mmco: unref short failure
[h264 @ 0x55e4c0eeb940] mmco: unref short failure
[h264 @ 0x55e4c0eeb940] mmco: unref short failure

  3%|▎         | 72/2315 [19:42<5:20:26,  8.57s/it][h264 @ 0x5615579399c0] mmco: unref short failure
[h264 @ 0x5615579399c0] mmco: unref short failure
[h264 @ 0x55782abbf100] mmco: unref short failure
[h264 @ 0x55782abbf100] mmco: unref short failure

  3%|▎         | 73/2315 [19:49<5:05:54,  8.19s/it][h264 @ 0x5578260d8880] mmco: unref short failure
[h264 @ 0x5578260d8880] mmco: unref short failure
[h264 @ 0x5578260d8880] mmco: unref short failure
[h264 @ 0x55e4cf299900] mmco: unref short failure
[h264 @ 0x55e4cf299900] mmco: unref short failure
[h264 @ 0x56154d5b0380] mmco: unref short failure
[h264 @ 0x56154d5b0380] mmco: unref short failure
[h264 @ 0x56400f1f8180] mmco: unref short failure
[h264 @ 0x564009658140] mmco: unref short failure
[h264 @ 0x564009658140] mmco: unref short failure
[h264 @ 0x5640214d8b80] mmco: unref short failure
[h264 @ 0x5640120f1ac0] mmco: unref short failure
[h264 @ 0x55782f459d80] mmco: unref short failure
[h264 @ 0x56400f1f2a00] mmco: unref short failure
[h264 @ 0x557831e6d700] mmco: unref short failure
[h264 @ 0x557831e6d700] mmco: unref short failure
[h264 @ 0x557825c11940] mmco: unref short failure
[h264 @ 0x557825c11940] mmco: unref short failure

  3%|▎         | 74/2315 [20:50<14:58:58, 24.07s/it][h264 @ 0x56154c6b68c0] mmco: unref short failure
[h264 @ 0x5640095727c0] mmco: unref short failure
[h264 @ 0x5640095727c0] mmco: unref short failure

  3%|▎         | 75/2315 [21:11<14:19:42, 23.03s/it][h264 @ 0x561555d4d100] mmco: unref short failure
[h264 @ 0x561555d4d100] mmco: unref short failure
[h264 @ 0x561545b4ff00] mmco: unref short failure
[h264 @ 0x561545b4ff00] mmco: unref short failure

  3%|▎         | 76/2315 [21:20<11:38:26, 18.72s/it]
  3%|▎         | 77/2315 [21:24<9:00:57, 14.50s/it] [h264 @ 0x55e4c8d50540] mmco: unref short failure
[h264 @ 0x55e4c8d50540] mmco: unref short failure
[h264 @ 0x55e4c8d50540] mmco: unref short failure
[h264 @ 0x55e4c8d50540] mmco: unref short failure
[h264 @ 0x5640128f10c0] mmco: unref short failure
[h264 @ 0x5640128f10c0] mmco: unref short failure

  3%|▎         | 78/2315 [21:44<9:58:43, 16.06s/it][h264 @ 0x55e4d2613540] mmco: unref short failure
[h264 @ 0x55e4d2613540] mmco: unref short failure
[h264 @ 0x55e4d2613540] mmco: unref short failure
[h264 @ 0x55e4d2613540] mmco: unref short failure

  3%|▎         | 79/2315 [21:49<7:54:23, 12.73s/it][h264 @ 0x564015626c80] mmco: unref short failure
[h264 @ 0x564015626c80] mmco: unref short failure
[h264 @ 0x564015626c80] mmco: unref short failure
[h264 @ 0x564015626c80] mmco: unref short failure

  3%|▎         | 80/2315 [21:55<6:37:19, 10.67s/it]
  3%|▎         | 81/2315 [22:00<5:34:08,  8.97s/it][h264 @ 0x557825c11700] mmco: unref short failure
[h264 @ 0x557825c11700] mmco: unref short failure
[h264 @ 0x55e4cf4689c0] mmco: unref short failure
[h264 @ 0x55781b81bd00] mmco: unref short failure
[h264 @ 0x55781b81bd00] mmco: unref short failure
[h264 @ 0x56154e1cef40] mmco: unref short failure
[h264 @ 0x56154e1cef40] mmco: unref short failure
[h264 @ 0x56154268ad00] mmco: unref short failure
[h264 @ 0x56154268ad00] mmco: unref short failure
[h264 @ 0x56155006a480] mmco: unref short failure
[h264 @ 0x55782c754bc0] mmco: unref short failure
[h264 @ 0x55782c754bc0] mmco: unref short failure
[h264 @ 0x5615563b4180] mmco: unref short failure
[h264 @ 0x56401c366340] mmco: unref short failure
[h264 @ 0x56401c366340] mmco: unref short failure
[h264 @ 0x55e4bed20440] mmco: unref short failure
[h264 @ 0x55e4bed20440] mmco: unref short failure
[h264 @ 0x55e4be8ff4c0] mmco: unref short failure
[h264 @ 0x55e4be8ff4c0] mmco: unref short failure

  4%|▎         | 82/2315 [23:03<15:37:44, 25.20s/it][h264 @ 0x55781dcaf0c0] mmco: unref short failure
[h264 @ 0x564010a94a00] mmco: unref short failure
[h264 @ 0x564010a94a00] mmco: unref short failure
[h264 @ 0x5578207aaa00] mmco: unref short failure
[h264 @ 0x56155bc45b00] mmco: unref short failure
[h264 @ 0x5578316ae780] mmco: unref short failure
[h264 @ 0x5578316ae780] mmco: unref short failure
[h264 @ 0x55e4d23244c0] mmco: unref short failure
[h264 @ 0x56401b81d400] mmco: unref short failure
[h264 @ 0x56401b81d400] mmco: unref short failure
[h264 @ 0x56401b81d400] mmco: unref short failure
[h264 @ 0x56401b81d400] mmco: unref short failure
[h264 @ 0x56155513dec0] mmco: unref short failure
[h264 @ 0x55e4d267be40] mmco: unref short failure
[h264 @ 0x5615473f7640] mmco: unref short failure
[h264 @ 0x5615473f7640] mmco: unref short failure
[h264 @ 0x5615473f7640] mmco: unref short failure
[h264 @ 0x5615473f7640] mmco: unref short failure
[h264 @ 0x557834fa8bc0] mmco: unref short failure
[h264 @ 0x557834fa8bc0] mmco: unref short failure
[h264 @ 0x564010c70480] mmco: unref short failure
[h264 @ 0x55e4d73ff900] mmco: unref short failure
[h264 @ 0x55e4d5e11780] mmco: unref short failure
[h264 @ 0x55e4d5e11780] mmco: unref short failure
[h264 @ 0x56400d527380] mmco: unref short failure
[h264 @ 0x5615472ebd40] mmco: unref short failure
[h264 @ 0x5615472ebd40] mmco: unref short failure
[h264 @ 0x5578316264c0] mmco: unref short failure
[h264 @ 0x5578316264c0] mmco: unref short failure
[h264 @ 0x56154f9e6380] mmco: unref short failure
[h264 @ 0x56154f9e6380] mmco: unref short failure
[h264 @ 0x55e4bc8bdd80] mmco: unref short failure
[h264 @ 0x55e4bc8bdd80] mmco: unref short failure
[h264 @ 0x55781b7fd480] mmco: unref short failure
[h264 @ 0x56401a52db80] mmco: unref short failure
[h264 @ 0x5640157e0e00] mmco: unref short failure
[h264 @ 0x557828ea2840] mmco: unref short failure
[h264 @ 0x5578302b8740] mmco: unref short failure
[h264 @ 0x5578302b8740] mmco: unref short failure
[h264 @ 0x5640096583c0] mmco: unref short failure
[h264 @ 0x564008f301c0] mmco: unref short failure
[h264 @ 0x564008f301c0] mmco: unref short failure
[h264 @ 0x55e4bc7df5c0] mmco: unref short failure
[h264 @ 0x55e4bc7df5c0] mmco: unref short failure
[h264 @ 0x561551dca0c0] mmco: unref short failure
[h264 @ 0x55e4c8862140] mmco: unref short failure
[h264 @ 0x564016a218c0] mmco: unref short failure
[h264 @ 0x557829b00680] mmco: unref short failure
[h264 @ 0x55783868f040] mmco: unref short failure
[h264 @ 0x55783868f040] mmco: unref short failure
[h264 @ 0x564009e08640] mmco: unref short failure
[h264 @ 0x564009e08640] mmco: unref short failure
[h264 @ 0x561546c313c0] mmco: unref short failure
[h264 @ 0x561546c313c0] mmco: unref short failure
[h264 @ 0x5578315c51c0] mmco: unref short failure
[h264 @ 0x561559e44800] mmco: unref short failure
[h264 @ 0x564012711300] mmco: unref short failure
[h264 @ 0x564012711300] mmco: unref short failure
[h264 @ 0x564012711300] mmco: unref short failure
[h264 @ 0x564012711300] mmco: unref short failure
[h264 @ 0x55e4d5f05680] mmco: unref short failure
slurmstepd: error: *** STEP 7478895.0 ON lrdn0057 CANCELLED AT 2024-09-11T15:43:12 ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 771014 closing signal SIGTERM
slurmstepd: error: *** JOB 7478895 ON lrdn0057 CANCELLED AT 2024-09-11T15:43:12 ***
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 771015 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 771016 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 771017 closing signal SIGTERM
