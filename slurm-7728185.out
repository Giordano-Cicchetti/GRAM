NODELIST=lrdn2614
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
1
032


DEVICE SET
DEVICE SET
DEVICE SET
DEVICE SET
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/16/2024 16:02:49 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/16/2024 16:02:49 - INFO - __main__ -   ==================model_configs==================

09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_model_type : vast
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_frozen_vision : False
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_frozen_audio : False
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_checkpointing : True
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_max_caption_len : 40
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_contra_dim : 512
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_vision_resolution : 224
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_audio_melbins : 64
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_audio_target_length : 1024
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_beam_size : 3
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_captioner_mode : False
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_generate_nums : 1
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_max_vision_sample_num : 2
09/16/2024 16:02:49 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
09/16/2024 16:02:49 - INFO - __main__ -   ==================run_configs==================

09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_checkpoint : 
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod150k
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_optim : adamw
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_weight_decay : 0.01
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_grad_norm : 2.0
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_resume : False
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_seed : 50
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_fp16 : True
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_bf16 : False
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_zero_shot : False
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_new_lr : 0
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_new_params_name : []
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_valid_freq : 10
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_dataset_mix_type : random
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_first_eval : True
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_num_train_steps : 0
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_save_best : True
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_pin_mem : True
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_vision_resolution : 224
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_use_ddp : False
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_mode : training
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_log_steps : 100
09/16/2024 16:02:49 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
09/16/2024 16:02:49 - INFO - __main__ -   ==================data_configs==================

09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_type : annoindexed
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_training : True
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_name : finetune_area
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_txt : ../vast27m/annotations150k.json
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_vision : ../vast27m/videos/
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_audio : ../vast27m/audios
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_vision_transforms : crop_flip
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_vision_format : video_rawvideo
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_vision_sample_num : 2
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_audio_sample_num : 1
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_task : ret%tv%ta
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_epoch : 5
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_n_workers : 8
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_finetune_area_train_batch_size : 256
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 8
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tva
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
09/16/2024 16:02:49 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
09/16/2024 16:02:54 - INFO - __main__ -   finetune_area Using clip mean and std.
09/16/2024 16:02:54 - INFO - __main__ -   finetune_area transforms crop_flip
ci sono 141221 labelsci sono 141221 labels
ci sono 141221 labels

ci sono 141221 labels
09/16/2024 16:03:58 - INFO - __main__ -   Create Dataset finetune_area Success
09/16/2024 16:03:58 - INFO - __main__ -    loader ret%tv%ta--finetune_area , ratio 2755 , bs_pergpu 64, n_workers 8
[h264 @ 0x55f3d001d740] mmco: unref short failure
[h264 @ 0x55f3d001d740] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/16/2024 16:04:01 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/16/2024 16:04:01 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 884 labelsci sono 884 labels
09/16/2024 16:04:01 - INFO - __main__ -   Create Dataset msrvtt_ret Success

ci sono 884 labels
ci sono 884 labels
[h264 @ 0x557ad9623f00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'

Please 'pip install xformers'Please 'pip install xformers'

Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'Please 'pip install xformers'

Please 'pip install xformers'
Please 'pip install xformers'
09/16/2024 16:04:03 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/16/2024 16:04:03 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/16/2024 16:04:03 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/16/2024 16:04:03 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
[h264 @ 0x557adbb3e2c0] mmco: unref short failure
[h264 @ 0x557adbb3e2c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55f3d1910580] mmco: unref short failure
[h264 @ 0x55f3d1910580] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55f3d186dfc0] mmco: unref short failure
[h264 @ 0x55f3d186dfc0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x561765787140] mmco: unref short failure
[h264 @ 0x561765787140] mmco: unref short failure
09/16/2024 16:04:40 - INFO - __main__ -   current idx EPDKsPkI96E.8 from finetune_area returns wrong image/video, use 135879 instead.
[h264 @ 0x561765d57040] mmco: unref short failure
[h264 @ 0x561765d57040] mmco: unref short failure
09/16/2024 16:04:52 - INFO - __main__ -   current idx H8mHStlahoo.66 from finetune_area returns wrong image/video, use 7366 instead.
[h264 @ 0x5615d4055980] mmco: unref short failure
[h264 @ 0x557adc3fcf40] mmco: unref short failure
[h264 @ 0x557adc3fcf40] mmco: unref short failure
09/16/2024 16:05:06 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/16/2024 16:05:08 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/16/2024 16:05:09 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
[h264 @ 0x5615d3613fc0] mmco: unref short failure
[h264 @ 0x5615d3613fc0] mmco: unref short failure
09/16/2024 16:05:14 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/16/2024 16:05:19 - INFO - root -   incompatible_keys.missing_keys: []
09/16/2024 16:05:19 - INFO - root -   incompatible_keys.missing_keys: []
09/16/2024 16:05:19 - INFO - root -   incompatible_keys.missing_keys: []
09/16/2024 16:05:21 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/16/2024 16:05:21 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/16/2024 16:05:21 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/16/2024 16:05:24 - INFO - root -   incompatible_keys.missing_keys: []
09/16/2024 16:05:26 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
[h264 @ 0x55f3d387c000] mmco: unref short failure
09/16/2024 16:05:32 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/16/2024 16:05:32 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/16/2024 16:05:32 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/16/2024 16:05:37 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.weight', 'cls.predictions.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.query.bias', 'cls.predictions.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.query.bias', 'cls.predictions.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x55f3d1529d00] mmco: unref short failure
[h264 @ 0x55f3d1529d00] mmco: unref short failure
[h264 @ 0x55f3d1529d00] mmco: unref short failure
[h264 @ 0x55f3d1529d00] mmco: unref short failure
[h264 @ 0x557adbf9bdc0] mmco: unref short failure
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.weight', 'cls.predictions.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x5615d35ae8c0] mmco: unref short failure
[h264 @ 0x5615d35ae8c0] mmco: unref short failure
[h264 @ 0x5615d2782c40] mmco: unref short failure
[h264 @ 0x5615d2782c40] mmco: unref short failure
[h264 @ 0x557adf457140] mmco: unref short failure
[h264 @ 0x5615d2792000] mmco: unref short failure
09/16/2024 16:06:01 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/ckpt/model_step_204994.pt
09/16/2024 16:06:01 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast
09/16/2024 16:06:01 - INFO - __main__ -   current idx ZJ3l8xjgJQw.15 from finetune_area returns wrong image/video, use 77162 instead.
[h264 @ 0x55f3d3bbe440] mmco: unref short failure
[h264 @ 0x55f3d3bbe440] mmco: unref short failure
[h264 @ 0x55f3d3bbe440] mmco: unref short failure
[h264 @ 0x55f3d3bbe440] mmco: unref short failure
09/16/2024 16:06:06 - INFO - __main__ -   Unexpected keys ['vision_encoder.text.logit_scale']
09/16/2024 16:06:06 - INFO - __main__ -   missing_keys  ['vision_encoder.logit_scale']
[h264 @ 0x557adc24ef80] mmco: unref short failure
[h264 @ 0x557adc24ef80] mmco: unref short failure
09/16/2024 16:06:12 - INFO - __main__ -   ==================learning_rate_settings==================

09/16/2024 16:06:12 - INFO - __main__ -     basic_lr : 2e-05
09/16/2024 16:06:12 - INFO - __main__ -     clip_lr_visual : 5e-07
09/16/2024 16:06:12 - INFO - __main__ -     clip_lr_visual_len : 245
09/16/2024 16:06:12 - INFO - __main__ -     new_lr : 0
09/16/2024 16:06:12 - INFO - __main__ -     new_params_name: []
09/16/2024 16:06:12 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
09/16/2024 16:06:12 - INFO - __main__ -   start running ret%tva validation...
[h264 @ 0x5617654f0300] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x557adcfad9c0] mmco: unref short failure
[h264 @ 0x5615d8de9180] mmco: unref short failure
[h264 @ 0x561765f712c0] mmco: unref short failure
[h264 @ 0x561765f712c0] mmco: unref short failure
[h264 @ 0x561765f712c0] mmco: unref short failure
[h264 @ 0x561765f712c0] mmco: unref short failure
[h264 @ 0x5615d275f100] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55f3d8a086c0] mmco: unref short failure
[h264 @ 0x55f3d8a086c0] mmco: unref short failure
[h264 @ 0x55f3d5557f40] mmco: unref short failure
[h264 @ 0x55f3d5557f40] mmco: unref short failure
[h264 @ 0x55f3d60734c0] mmco: unref short failure
[h264 @ 0x55f3d60734c0] mmco: unref short failure
[h264 @ 0x56176616d6c0] mmco: unref short failure
[h264 @ 0x56176616d6c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[h264 @ 0x56176abcbe40] mmco: unref short failure
[h264 @ 0x55f3d3737000] mmco: unref short failure
[h264 @ 0x56176817d780] mmco: unref short failure
[h264 @ 0x5617677dc4c0] mmco: unref short failure
[h264 @ 0x557ae414aa00] mmco: unref short failure
[h264 @ 0x5617655a90c0] mmco: unref short failure
[h264 @ 0x5617655a90c0] mmco: unref short failure
[h264 @ 0x5617655a90c0] mmco: unref short failure
[h264 @ 0x5617655a90c0] mmco: unref short failure
[h264 @ 0x56176cb83180] mmco: unref short failure
[h264 @ 0x56176cb83180] mmco: unref short failure
[h264 @ 0x557ae6dd9140] mmco: unref short failure
[h264 @ 0x55f3d2f80180] mmco: unref short failure
[h264 @ 0x5615da645b40] mmco: unref short failure
[h264 @ 0x5615da645b40] mmco: unref short failure
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<00:23,  9.49it/s]  1%|          | 2/221 [00:00<00:30,  7.28it/s]  1%|▏         | 3/221 [00:00<00:51,  4.21it/s]  2%|▏         | 4/221 [00:00<00:39,  5.43it/s]  2%|▏         | 5/221 [00:00<00:34,  6.35it/s]  3%|▎         | 7/221 [00:01<00:29,  7.26it/s]  4%|▎         | 8/221 [00:01<00:27,  7.82it/s]  5%|▍         | 10/221 [00:01<00:23,  9.14it/s]  5%|▍         | 11/221 [00:01<00:24,  8.67it/s][h264 @ 0x55f3d28f6380] mmco: unref short failure
[h264 @ 0x55f3d28f6380] mmco: unref short failure
  5%|▌         | 12/221 [00:03<02:29,  1.40it/s]  6%|▌         | 13/221 [00:04<01:56,  1.78it/s]  6%|▋         | 14/221 [00:04<01:32,  2.24it/s]  7%|▋         | 15/221 [00:04<01:16,  2.68it/s]  7%|▋         | 16/221 [00:04<01:04,  3.20it/s]  8%|▊         | 17/221 [00:04<01:02,  3.27it/s]  8%|▊         | 18/221 [00:05<01:01,  3.33it/s]  9%|▊         | 19/221 [00:05<00:50,  4.02it/s]  9%|▉         | 20/221 [00:05<00:57,  3.47it/s] 10%|▉         | 21/221 [00:05<00:50,  3.97it/s][h264 @ 0x557ae1c0cb80] mmco: unref short failure
[h264 @ 0x557ae1c0cb80] mmco: unref short failure
 10%|▉         | 22/221 [00:10<05:11,  1.57s/it] 10%|█         | 23/221 [00:10<03:44,  1.13s/it] 11%|█▏        | 25/221 [00:10<02:09,  1.52it/s] 12%|█▏        | 26/221 [00:11<01:45,  1.86it/s] 12%|█▏        | 27/221 [00:11<01:22,  2.34it/s] 13%|█▎        | 28/221 [00:11<01:07,  2.87it/s] 13%|█▎        | 29/221 [00:11<00:54,  3.51it/s] 14%|█▎        | 30/221 [00:11<00:48,  3.94it/s] 14%|█▍        | 31/221 [00:11<00:44,  4.30it/s] 14%|█▍        | 32/221 [00:11<00:38,  4.94it/s] 15%|█▍        | 33/221 [00:12<00:36,  5.17it/s] 15%|█▌        | 34/221 [00:12<00:33,  5.62it/s] 16%|█▌        | 35/221 [00:12<00:28,  6.44it/s] 16%|█▋        | 36/221 [00:12<00:26,  6.88it/s] 17%|█▋        | 37/221 [00:12<00:27,  6.81it/s] 18%|█▊        | 39/221 [00:12<00:24,  7.47it/s] 18%|█▊        | 40/221 [00:12<00:23,  7.82it/s] 19%|█▉        | 42/221 [00:13<00:21,  8.47it/s] 19%|█▉        | 43/221 [00:13<00:22,  8.00it/s] 20%|█▉        | 44/221 [00:13<00:23,  7.50it/s] 20%|██        | 45/221 [00:13<00:39,  4.41it/s] 21%|██        | 46/221 [00:14<00:38,  4.53it/s][h264 @ 0x56176f9e4940] mmco: unref short failure
[h264 @ 0x56176f9e4940] mmco: unref short failure
[h264 @ 0x561766478c80] mmco: unref short failure
[h264 @ 0x561766478c80] mmco: unref short failure
 21%|██▏       | 47/221 [00:18<04:21,  1.50s/it] 22%|██▏       | 48/221 [00:19<03:13,  1.12s/it] 22%|██▏       | 49/221 [00:19<02:27,  1.17it/s] 23%|██▎       | 50/221 [00:19<01:50,  1.55it/s] 24%|██▎       | 52/221 [00:19<01:08,  2.47it/s] 24%|██▍       | 53/221 [00:19<00:57,  2.93it/s] 24%|██▍       | 54/221 [00:23<03:37,  1.30s/it] 25%|██▍       | 55/221 [00:25<03:48,  1.38s/it] 25%|██▌       | 56/221 [00:25<02:52,  1.04s/it] 26%|██▌       | 58/221 [00:25<01:41,  1.60it/s] 27%|██▋       | 60/221 [00:26<01:09,  2.31it/s] 28%|██▊       | 62/221 [00:26<00:51,  3.10it/s] 29%|██▊       | 63/221 [00:26<00:46,  3.37it/s] 29%|██▉       | 64/221 [00:26<00:40,  3.84it/s] 30%|██▉       | 66/221 [00:31<02:58,  1.15s/it] 30%|███       | 67/221 [00:31<02:27,  1.05it/s] 31%|███       | 69/221 [00:32<01:36,  1.57it/s][h264 @ 0x561768c234c0] mmco: unref short failure
[h264 @ 0x561768c234c0] mmco: unref short failure
 32%|███▏      | 71/221 [00:32<01:07,  2.22it/s] 33%|███▎      | 72/221 [00:32<00:57,  2.61it/s] 33%|███▎      | 73/221 [00:32<00:47,  3.10it/s] 33%|███▎      | 74/221 [00:32<00:41,  3.57it/s] 34%|███▍      | 75/221 [00:32<00:35,  4.06it/s] 35%|███▍      | 77/221 [00:33<00:26,  5.50it/s] 35%|███▌      | 78/221 [00:33<00:28,  5.06it/s] 36%|███▌      | 79/221 [00:33<00:32,  4.40it/s] 36%|███▌      | 80/221 [00:33<00:28,  4.96it/s] 37%|███▋      | 81/221 [00:33<00:25,  5.43it/s] 37%|███▋      | 82/221 [00:34<00:27,  5.11it/s] 38%|███▊      | 83/221 [00:34<00:23,  5.92it/s] 38%|███▊      | 84/221 [00:34<00:21,  6.44it/s] 38%|███▊      | 85/221 [00:34<00:20,  6.71it/s] 39%|███▉      | 86/221 [00:34<00:18,  7.41it/s] 40%|███▉      | 88/221 [00:34<00:18,  7.28it/s] 40%|████      | 89/221 [00:35<00:22,  5.89it/s] 41%|████      | 90/221 [00:35<00:20,  6.43it/s] 42%|████▏     | 92/221 [00:35<00:16,  7.80it/s] 42%|████▏     | 93/221 [00:35<00:24,  5.15it/s] 43%|████▎     | 94/221 [00:36<00:22,  5.61it/s] 43%|████▎     | 95/221 [00:36<00:21,  5.93it/s] 43%|████▎     | 96/221 [00:36<00:28,  4.45it/s] 44%|████▍     | 97/221 [00:36<00:27,  4.58it/s] 44%|████▍     | 98/221 [00:37<00:29,  4.16it/s] 45%|████▍     | 99/221 [00:37<00:26,  4.57it/s] 46%|████▌     | 101/221 [00:37<00:19,  6.12it/s] 46%|████▌     | 102/221 [00:37<00:18,  6.53it/s] 47%|████▋     | 104/221 [00:37<00:16,  7.08it/s] 48%|████▊     | 105/221 [00:38<00:18,  6.23it/s] 48%|████▊     | 106/221 [00:38<00:29,  3.96it/s] 48%|████▊     | 107/221 [00:38<00:24,  4.65it/s] 49%|████▉     | 108/221 [00:38<00:21,  5.28it/s] 49%|████▉     | 109/221 [00:38<00:18,  6.02it/s] 50%|████▉     | 110/221 [00:39<00:16,  6.64it/s] 50%|█████     | 111/221 [00:39<00:20,  5.35it/s] 51%|█████     | 112/221 [00:39<00:18,  5.77it/s] 51%|█████     | 113/221 [00:39<00:20,  5.18it/s] 52%|█████▏    | 114/221 [00:39<00:18,  5.69it/s] 52%|█████▏    | 115/221 [00:39<00:16,  6.24it/s] 52%|█████▏    | 116/221 [00:45<02:52,  1.65s/it] 53%|█████▎    | 117/221 [00:45<02:15,  1.30s/it] 53%|█████▎    | 118/221 [00:45<01:38,  1.04it/s] 54%|█████▍    | 120/221 [00:45<00:56,  1.79it/s] 55%|█████▌    | 122/221 [00:46<00:37,  2.64it/s] 56%|█████▌    | 123/221 [00:46<00:31,  3.11it/s] 56%|█████▌    | 124/221 [00:46<00:27,  3.58it/s] 57%|█████▋    | 125/221 [00:46<00:28,  3.37it/s] 57%|█████▋    | 126/221 [00:47<00:32,  2.91it/s] 57%|█████▋    | 127/221 [00:47<00:31,  2.98it/s] 58%|█████▊    | 128/221 [00:47<00:33,  2.80it/s] 58%|█████▊    | 129/221 [00:48<00:26,  3.48it/s] 59%|█████▉    | 130/221 [00:48<00:21,  4.18it/s] 60%|█████▉    | 132/221 [00:48<00:16,  5.38it/s] 60%|██████    | 133/221 [00:48<00:17,  5.11it/s] 61%|██████    | 134/221 [00:48<00:20,  4.19it/s][h264 @ 0x5615d34fc240] mmco: unref short failure
[h264 @ 0x5615d34fc240] mmco: unref short failure
[h264 @ 0x557ae8bc8d00] mmco: unref short failure
[h264 @ 0x557ae8bc8d00] mmco: unref short failure
 61%|██████    | 135/221 [00:50<00:50,  1.71it/s] 62%|██████▏   | 136/221 [00:51<00:47,  1.78it/s][h264 @ 0x557ae45e28c0] mmco: unref short failure
[h264 @ 0x557ae45e28c0] mmco: unref short failure
 62%|██████▏   | 137/221 [00:53<01:32,  1.10s/it] 62%|██████▏   | 138/221 [00:53<01:12,  1.14it/s] 63%|██████▎   | 139/221 [00:54<01:01,  1.34it/s][h264 @ 0x5615d2d97040] mmco: unref short failure
 63%|██████▎   | 140/221 [00:54<00:50,  1.62it/s] 64%|██████▍   | 141/221 [00:54<00:40,  1.99it/s] 64%|██████▍   | 142/221 [00:54<00:33,  2.39it/s] 65%|██████▌   | 144/221 [00:55<00:19,  3.86it/s] 66%|██████▌   | 146/221 [00:55<00:13,  5.45it/s] 67%|██████▋   | 147/221 [00:55<00:12,  5.81it/s] 67%|██████▋   | 148/221 [00:55<00:12,  5.81it/s] 68%|██████▊   | 150/221 [00:55<00:12,  5.68it/s] 68%|██████▊   | 151/221 [00:56<00:14,  4.99it/s] 69%|██████▉   | 153/221 [00:56<00:10,  6.50it/s] 70%|███████   | 155/221 [00:56<00:08,  8.11it/s] 71%|███████   | 156/221 [01:01<01:13,  1.12s/it] 71%|███████   | 157/221 [01:01<00:57,  1.11it/s] 72%|███████▏  | 159/221 [01:01<00:35,  1.76it/s] 73%|███████▎  | 162/221 [01:02<00:20,  2.84it/s] 74%|███████▍  | 164/221 [01:02<00:15,  3.58it/s] 75%|███████▌  | 166/221 [01:07<00:53,  1.03it/s] 76%|███████▌  | 167/221 [01:09<01:06,  1.23s/it] 76%|███████▋  | 169/221 [01:10<00:46,  1.13it/s] 77%|███████▋  | 170/221 [01:10<00:38,  1.34it/s] 78%|███████▊  | 173/221 [01:10<00:20,  2.34it/s] 79%|███████▉  | 175/221 [01:10<00:14,  3.12it/s] 80%|████████  | 177/221 [01:10<00:10,  4.03it/s] 81%|████████  | 179/221 [01:11<00:14,  2.95it/s] 82%|████████▏ | 182/221 [01:12<00:08,  4.36it/s] 83%|████████▎ | 184/221 [01:12<00:06,  5.50it/s] 85%|████████▍ | 187/221 [01:12<00:04,  7.56it/s] 86%|████████▌ | 189/221 [01:12<00:04,  7.75it/s] 87%|████████▋ | 193/221 [01:12<00:02, 11.39it/s] 89%|████████▊ | 196/221 [01:12<00:01, 13.74it/s] 90%|█████████ | 199/221 [01:13<00:01, 14.69it/s] 91%|█████████ | 201/221 [01:13<00:01, 14.55it/s] 92%|█████████▏| 204/221 [01:13<00:01, 16.78it/s] 93%|█████████▎| 206/221 [01:13<00:01, 14.09it/s] 94%|█████████▍| 208/221 [01:13<00:00, 14.89it/s] 95%|█████████▌| 210/221 [01:13<00:00, 15.46it/s] 96%|█████████▌| 212/221 [01:13<00:00, 14.18it/s] 97%|█████████▋| 214/221 [01:14<00:00,  8.73it/s] 98%|█████████▊| 216/221 [01:16<00:01,  2.75it/s] 99%|█████████▊| 218/221 [01:16<00:00,  3.65it/s]100%|█████████▉| 220/221 [01:21<00:00,  1.06it/s]100%|█████████▉| 220/221 [01:21<00:00,  2.70it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<01:04,  3.40it/s]  1%|          | 2/221 [00:00<01:04,  3.40it/s]  1%|▏         | 3/221 [00:00<01:04,  3.40it/s]  2%|▏         | 4/221 [00:01<01:03,  3.40it/s]  2%|▏         | 5/221 [00:01<01:03,  3.40it/s]  3%|▎         | 6/221 [00:01<01:03,  3.41it/s]  3%|▎         | 7/221 [00:02<01:02,  3.41it/s]  4%|▎         | 8/221 [00:02<01:02,  3.41it/s]  4%|▍         | 9/221 [00:02<01:02,  3.41it/s]  5%|▍         | 10/221 [00:02<01:01,  3.41it/s]  5%|▍         | 11/221 [00:03<01:01,  3.41it/s]  5%|▌         | 12/221 [00:03<01:01,  3.41it/s]  6%|▌         | 13/221 [00:03<01:01,  3.41it/s]  6%|▋         | 14/221 [00:04<01:00,  3.41it/s]  7%|▋         | 15/221 [00:04<01:00,  3.41it/s]  7%|▋         | 16/221 [00:04<01:00,  3.41it/s]  8%|▊         | 17/221 [00:04<00:59,  3.41it/s]  8%|▊         | 18/221 [00:05<00:59,  3.41it/s]  9%|▊         | 19/221 [00:05<00:59,  3.41it/s]  9%|▉         | 20/221 [00:05<00:59,  3.41it/s] 10%|▉         | 21/221 [00:06<00:58,  3.41it/s] 10%|▉         | 22/221 [00:06<00:58,  3.41it/s] 10%|█         | 23/221 [00:06<00:58,  3.41it/s] 11%|█         | 24/221 [00:07<00:57,  3.41it/s] 11%|█▏        | 25/221 [00:07<00:57,  3.41it/s] 12%|█▏        | 26/221 [00:07<00:57,  3.41it/s] 12%|█▏        | 27/221 [00:07<00:56,  3.41it/s] 13%|█▎        | 28/221 [00:08<00:56,  3.41it/s] 13%|█▎        | 29/221 [00:08<00:56,  3.41it/s] 14%|█▎        | 30/221 [00:08<00:56,  3.41it/s] 14%|█▍        | 31/221 [00:09<00:55,  3.41it/s] 14%|█▍        | 32/221 [00:09<00:55,  3.41it/s] 15%|█▍        | 33/221 [00:09<00:55,  3.41it/s] 15%|█▌        | 34/221 [00:09<00:54,  3.41it/s] 16%|█▌        | 35/221 [00:10<00:54,  3.41it/s] 16%|█▋        | 36/221 [00:10<00:54,  3.41it/s] 17%|█▋        | 37/221 [00:10<00:54,  3.41it/s] 17%|█▋        | 38/221 [00:11<00:53,  3.41it/s] 18%|█▊        | 39/221 [00:11<00:53,  3.41it/s] 18%|█▊        | 40/221 [00:11<00:53,  3.41it/s] 19%|█▊        | 41/221 [00:12<00:52,  3.41it/s] 19%|█▉        | 42/221 [00:12<00:52,  3.41it/s] 19%|█▉        | 43/221 [00:12<00:52,  3.41it/s] 20%|█▉        | 44/221 [00:12<00:51,  3.41it/s] 20%|██        | 45/221 [00:13<00:51,  3.41it/s] 21%|██        | 46/221 [00:13<00:51,  3.41it/s] 21%|██▏       | 47/221 [00:13<00:51,  3.41it/s] 22%|██▏       | 48/221 [00:14<00:50,  3.41it/s] 22%|██▏       | 49/221 [00:14<00:50,  3.41it/s] 23%|██▎       | 50/221 [00:14<00:50,  3.41it/s] 23%|██▎       | 51/221 [00:14<00:49,  3.41it/s] 24%|██▎       | 52/221 [00:15<00:49,  3.41it/s] 24%|██▍       | 53/221 [00:15<00:49,  3.41it/s] 24%|██▍       | 54/221 [00:15<00:49,  3.41it/s] 25%|██▍       | 55/221 [00:16<00:48,  3.41it/s] 25%|██▌       | 56/221 [00:16<00:48,  3.41it/s] 26%|██▌       | 57/221 [00:16<00:48,  3.41it/s] 26%|██▌       | 58/221 [00:17<00:47,  3.41it/s] 27%|██▋       | 59/221 [00:17<00:47,  3.41it/s] 27%|██▋       | 60/221 [00:17<00:47,  3.41it/s] 28%|██▊       | 61/221 [00:17<00:46,  3.41it/s] 28%|██▊       | 62/221 [00:18<00:46,  3.41it/s] 29%|██▊       | 63/221 [00:18<00:46,  3.41it/s] 29%|██▉       | 64/221 [00:18<00:46,  3.41it/s] 29%|██▉       | 65/221 [00:19<00:45,  3.41it/s] 30%|██▉       | 66/221 [00:19<00:45,  3.41it/s] 30%|███       | 67/221 [00:19<00:45,  3.41it/s] 31%|███       | 68/221 [00:19<00:44,  3.41it/s] 31%|███       | 69/221 [00:20<00:44,  3.41it/s] 32%|███▏      | 70/221 [00:20<00:44,  3.41it/s] 32%|███▏      | 71/221 [00:20<00:44,  3.41it/s] 33%|███▎      | 72/221 [00:21<00:43,  3.41it/s] 33%|███▎      | 73/221 [00:21<00:43,  3.41it/s] 33%|███▎      | 74/221 [00:21<00:43,  3.41it/s] 34%|███▍      | 75/221 [00:22<00:42,  3.41it/s] 34%|███▍      | 76/221 [00:22<00:42,  3.41it/s] 35%|███▍      | 77/221 [00:22<00:42,  3.41it/s] 35%|███▌      | 78/221 [00:22<00:41,  3.41it/s] 36%|███▌      | 79/221 [00:23<00:41,  3.41it/s] 36%|███▌      | 80/221 [00:23<00:41,  3.41it/s] 37%|███▋      | 81/221 [00:23<00:41,  3.41it/s] 37%|███▋      | 82/221 [00:24<00:40,  3.41it/s] 38%|███▊      | 83/221 [00:24<00:40,  3.41it/s] 38%|███▊      | 84/221 [00:24<00:40,  3.41it/s] 38%|███▊      | 85/221 [00:24<00:39,  3.41it/s] 39%|███▉      | 86/221 [00:25<00:39,  3.41it/s] 39%|███▉      | 87/221 [00:25<00:39,  3.41it/s] 40%|███▉      | 88/221 [00:25<00:39,  3.41it/s] 40%|████      | 89/221 [00:26<00:38,  3.41it/s] 41%|████      | 90/221 [00:26<00:38,  3.41it/s] 41%|████      | 91/221 [00:26<00:38,  3.41it/s] 42%|████▏     | 92/221 [00:27<00:37,  3.41it/s] 42%|████▏     | 93/221 [00:27<00:37,  3.41it/s] 43%|████▎     | 94/221 [00:27<00:37,  3.41it/s] 43%|████▎     | 95/221 [00:27<00:36,  3.41it/s] 43%|████▎     | 96/221 [00:28<00:36,  3.41it/s] 44%|████▍     | 97/221 [00:28<00:36,  3.41it/s] 44%|████▍     | 98/221 [00:28<00:36,  3.41it/s] 45%|████▍     | 99/221 [00:29<00:35,  3.41it/s] 45%|████▌     | 100/221 [00:29<00:35,  3.41it/s] 46%|████▌     | 101/221 [00:29<00:35,  3.41it/s] 46%|████▌     | 102/221 [00:29<00:34,  3.41it/s] 47%|████▋     | 103/221 [00:30<00:34,  3.41it/s] 47%|████▋     | 104/221 [00:30<00:34,  3.41it/s] 48%|████▊     | 105/221 [00:30<00:34,  3.41it/s] 48%|████▊     | 106/221 [00:31<00:33,  3.41it/s] 48%|████▊     | 107/221 [00:31<00:33,  3.41it/s] 49%|████▉     | 108/221 [00:31<00:33,  3.41it/s] 49%|████▉     | 109/221 [00:31<00:32,  3.41it/s] 50%|████▉     | 110/221 [00:32<00:32,  3.41it/s] 50%|█████     | 111/221 [00:32<00:32,  3.41it/s] 51%|█████     | 112/221 [00:32<00:31,  3.41it/s] 51%|█████     | 113/221 [00:33<00:31,  3.41it/s] 52%|█████▏    | 114/221 [00:33<00:31,  3.41it/s] 52%|█████▏    | 115/221 [00:33<00:31,  3.41it/s] 52%|█████▏    | 116/221 [00:34<00:30,  3.41it/s] 53%|█████▎    | 117/221 [00:34<00:30,  3.41it/s] 53%|█████▎    | 118/221 [00:34<00:30,  3.41it/s] 54%|█████▍    | 119/221 [00:34<00:29,  3.41it/s] 54%|█████▍    | 120/221 [00:35<00:29,  3.41it/s] 55%|█████▍    | 121/221 [00:35<00:29,  3.41it/s] 55%|█████▌    | 122/221 [00:35<00:29,  3.41it/s] 56%|█████▌    | 123/221 [00:36<00:28,  3.41it/s] 56%|█████▌    | 124/221 [00:36<00:28,  3.41it/s] 57%|█████▋    | 125/221 [00:36<00:28,  3.41it/s] 57%|█████▋    | 126/221 [00:36<00:27,  3.41it/s] 57%|█████▋    | 127/221 [00:37<00:27,  3.41it/s] 58%|█████▊    | 128/221 [00:37<00:27,  3.41it/s] 58%|█████▊    | 129/221 [00:37<00:27,  3.41it/s] 59%|█████▉    | 130/221 [00:38<00:26,  3.41it/s] 59%|█████▉    | 131/221 [00:38<00:26,  3.41it/s] 60%|█████▉    | 132/221 [00:38<00:26,  3.41it/s] 60%|██████    | 133/221 [00:39<00:25,  3.41it/s] 61%|██████    | 134/221 [00:39<00:25,  3.41it/s] 61%|██████    | 135/221 [00:39<00:25,  3.41it/s] 62%|██████▏   | 136/221 [00:39<00:24,  3.41it/s] 62%|██████▏   | 137/221 [00:40<00:24,  3.41it/s] 62%|██████▏   | 138/221 [00:40<00:24,  3.41it/s] 63%|██████▎   | 139/221 [00:40<00:24,  3.41it/s] 63%|██████▎   | 140/221 [00:41<00:23,  3.41it/s] 64%|██████▍   | 141/221 [00:41<00:23,  3.41it/s] 64%|██████▍   | 142/221 [00:41<00:23,  3.41it/s] 65%|██████▍   | 143/221 [00:41<00:22,  3.41it/s] 65%|██████▌   | 144/221 [00:42<00:22,  3.41it/s] 66%|██████▌   | 145/221 [00:42<00:22,  3.41it/s] 66%|██████▌   | 146/221 [00:42<00:22,  3.41it/s] 67%|██████▋   | 147/221 [00:43<00:21,  3.41it/s] 67%|██████▋   | 148/221 [00:43<00:21,  3.41it/s] 67%|██████▋   | 149/221 [00:43<00:21,  3.41it/s] 68%|██████▊   | 150/221 [00:44<00:20,  3.41it/s] 68%|██████▊   | 151/221 [00:44<00:20,  3.41it/s] 69%|██████▉   | 152/221 [00:44<00:20,  3.41it/s] 69%|██████▉   | 153/221 [00:44<00:19,  3.41it/s] 70%|██████▉   | 154/221 [00:45<00:19,  3.41it/s] 70%|███████   | 155/221 [00:45<00:19,  3.41it/s] 71%|███████   | 156/221 [00:45<00:19,  3.41it/s] 71%|███████   | 157/221 [00:46<00:18,  3.41it/s] 71%|███████▏  | 158/221 [00:46<00:18,  3.41it/s] 72%|███████▏  | 159/221 [00:46<00:18,  3.41it/s] 72%|███████▏  | 160/221 [00:46<00:17,  3.41it/s] 73%|███████▎  | 161/221 [00:47<00:17,  3.41it/s] 73%|███████▎  | 162/221 [00:47<00:17,  3.41it/s] 74%|███████▍  | 163/221 [00:47<00:17,  3.41it/s] 74%|███████▍  | 164/221 [00:48<00:16,  3.41it/s] 75%|███████▍  | 165/221 [00:48<00:16,  3.41it/s] 75%|███████▌  | 166/221 [00:48<00:16,  3.41it/s] 76%|███████▌  | 167/221 [00:49<00:15,  3.41it/s] 76%|███████▌  | 168/221 [00:49<00:15,  3.41it/s] 76%|███████▋  | 169/221 [00:49<00:15,  3.41it/s] 77%|███████▋  | 170/221 [00:49<00:14,  3.41it/s] 77%|███████▋  | 171/221 [00:50<00:14,  3.41it/s] 78%|███████▊  | 172/221 [00:50<00:14,  3.41it/s] 78%|███████▊  | 173/221 [00:50<00:14,  3.41it/s] 79%|███████▊  | 174/221 [00:51<00:13,  3.41it/s] 79%|███████▉  | 175/221 [00:51<00:13,  3.41it/s] 80%|███████▉  | 176/221 [00:51<00:13,  3.41it/s] 80%|████████  | 177/221 [00:51<00:12,  3.41it/s] 81%|████████  | 178/221 [00:52<00:12,  3.41it/s] 81%|████████  | 179/221 [00:52<00:12,  3.41it/s] 81%|████████▏ | 180/221 [00:52<00:12,  3.41it/s] 82%|████████▏ | 181/221 [00:53<00:11,  3.41it/s] 82%|████████▏ | 182/221 [00:53<00:11,  3.41it/s] 83%|████████▎ | 183/221 [00:53<00:11,  3.41it/s] 83%|████████▎ | 184/221 [00:54<00:10,  3.41it/s] 84%|████████▎ | 185/221 [00:54<00:10,  3.41it/s] 84%|████████▍ | 186/221 [00:54<00:10,  3.41it/s] 85%|████████▍ | 187/221 [00:54<00:09,  3.41it/s] 85%|████████▌ | 188/221 [00:55<00:09,  3.41it/s] 86%|████████▌ | 189/221 [00:55<00:09,  3.41it/s] 86%|████████▌ | 190/221 [00:55<00:09,  3.41it/s] 86%|████████▋ | 191/221 [00:56<00:08,  3.41it/s] 87%|████████▋ | 192/221 [00:56<00:08,  3.41it/s] 87%|████████▋ | 193/221 [00:56<00:08,  3.41it/s] 88%|████████▊ | 194/221 [00:56<00:07,  3.41it/s] 88%|████████▊ | 195/221 [00:57<00:07,  3.41it/s] 89%|████████▊ | 196/221 [00:57<00:07,  3.41it/s] 89%|████████▉ | 197/221 [00:57<00:07,  3.41it/s] 90%|████████▉ | 198/221 [00:58<00:06,  3.41it/s] 90%|█████████ | 199/221 [00:58<00:06,  3.41it/s] 90%|█████████ | 200/221 [00:58<00:06,  3.41it/s] 91%|█████████ | 201/221 [00:59<00:05,  3.41it/s] 91%|█████████▏| 202/221 [00:59<00:05,  3.41it/s] 92%|█████████▏| 203/221 [00:59<00:05,  3.41it/s] 92%|█████████▏| 204/221 [00:59<00:04,  3.41it/s] 93%|█████████▎| 205/221 [01:00<00:04,  3.41it/s] 93%|█████████▎| 206/221 [01:00<00:04,  3.41it/s] 94%|█████████▎| 207/221 [01:00<00:04,  3.41it/s] 94%|█████████▍| 208/221 [01:01<00:03,  3.41it/s] 95%|█████████▍| 209/221 [01:01<00:03,  3.41it/s] 95%|█████████▌| 210/221 [01:01<00:03,  3.41it/s] 95%|█████████▌| 211/221 [01:01<00:02,  3.41it/s] 96%|█████████▌| 212/221 [01:02<00:02,  3.41it/s] 96%|█████████▋| 213/221 [01:02<00:02,  3.41it/s] 97%|█████████▋| 214/221 [01:02<00:02,  3.41it/s] 97%|█████████▋| 215/221 [01:03<00:01,  3.41it/s] 98%|█████████▊| 216/221 [01:03<00:01,  3.41it/s] 98%|█████████▊| 217/221 [01:03<00:01,  3.41it/s] 99%|█████████▊| 218/221 [01:03<00:00,  3.41it/s] 99%|█████████▉| 219/221 [01:04<00:00,  3.41it/s]100%|█████████▉| 220/221 [01:04<00:00,  3.41it/s]100%|██████████| 221/221 [01:04<00:00,  3.41it/s]100%|██████████| 221/221 [01:04<00:00,  3.41it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<00:42,  5.12it/s]  1%|          | 2/221 [00:00<01:05,  3.32it/s]  1%|▏         | 3/221 [00:01<01:19,  2.75it/s]  2%|▏         | 4/221 [00:01<01:14,  2.89it/s]  2%|▏         | 5/221 [00:01<01:02,  3.43it/s]  3%|▎         | 6/221 [00:01<00:51,  4.15it/s]  3%|▎         | 7/221 [00:01<00:47,  4.49it/s]  4%|▎         | 8/221 [00:02<00:48,  4.39it/s]  4%|▍         | 9/221 [00:02<01:20,  2.63it/s]  5%|▍         | 10/221 [00:03<01:19,  2.67it/s]  5%|▍         | 11/221 [00:03<01:30,  2.31it/s]  5%|▌         | 12/221 [00:04<01:23,  2.50it/s]  6%|▌         | 13/221 [00:04<01:39,  2.09it/s]  6%|▋         | 14/221 [00:04<01:23,  2.48it/s]  7%|▋         | 15/221 [00:05<01:15,  2.71it/s]  7%|▋         | 16/221 [00:05<01:06,  3.09it/s]  8%|▊         | 17/221 [00:05<01:18,  2.59it/s]  8%|▊         | 18/221 [00:06<01:20,  2.52it/s]  9%|▊         | 19/221 [00:06<01:07,  3.00it/s]  9%|▉         | 20/221 [00:06<00:58,  3.41it/s] 10%|▉         | 22/221 [00:07<00:49,  4.06it/s] 10%|█         | 23/221 [00:07<00:45,  4.35it/s] 11%|█         | 24/221 [00:07<00:47,  4.15it/s] 11%|█▏        | 25/221 [00:07<00:46,  4.25it/s] 12%|█▏        | 26/221 [00:08<00:42,  4.58it/s] 12%|█▏        | 27/221 [00:08<00:48,  3.98it/s] 13%|█▎        | 28/221 [00:08<00:52,  3.66it/s] 13%|█▎        | 29/221 [00:09<01:07,  2.83it/s] 14%|█▎        | 30/221 [00:09<00:56,  3.36it/s] 14%|█▍        | 31/221 [00:09<00:50,  3.79it/s] 14%|█▍        | 32/221 [00:09<00:56,  3.37it/s] 15%|█▍        | 33/221 [00:10<00:49,  3.77it/s] 15%|█▌        | 34/221 [00:10<00:50,  3.72it/s] 16%|█▌        | 35/221 [00:10<00:44,  4.15it/s] 16%|█▋        | 36/221 [00:10<00:49,  3.72it/s] 17%|█▋        | 37/221 [00:11<00:50,  3.66it/s] 17%|█▋        | 38/221 [00:11<00:46,  3.94it/s] 18%|█▊        | 39/221 [00:11<00:42,  4.32it/s] 18%|█▊        | 40/221 [00:11<00:38,  4.69it/s] 19%|█▊        | 41/221 [00:11<00:34,  5.23it/s] 19%|█▉        | 42/221 [00:12<00:31,  5.74it/s] 19%|█▉        | 43/221 [00:12<00:40,  4.42it/s] 20%|█▉        | 44/221 [00:12<00:41,  4.23it/s] 20%|██        | 45/221 [00:13<01:01,  2.86it/s] 21%|██        | 46/221 [00:13<01:00,  2.90it/s] 21%|██▏       | 47/221 [00:13<00:49,  3.55it/s] 22%|██▏       | 48/221 [00:13<00:41,  4.18it/s] 23%|██▎       | 50/221 [00:14<00:39,  4.35it/s] 23%|██▎       | 51/221 [00:14<00:37,  4.54it/s] 24%|██▎       | 52/221 [00:14<00:33,  5.09it/s] 24%|██▍       | 53/221 [00:15<00:47,  3.54it/s] 24%|██▍       | 54/221 [00:15<00:45,  3.66it/s] 25%|██▍       | 55/221 [00:15<00:43,  3.83it/s] 25%|██▌       | 56/221 [00:15<00:43,  3.78it/s] 26%|██▌       | 57/221 [00:16<00:45,  3.57it/s] 26%|██▌       | 58/221 [00:16<00:37,  4.34it/s] 27%|██▋       | 59/221 [00:16<00:37,  4.28it/s] 27%|██▋       | 60/221 [00:16<00:35,  4.56it/s] 28%|██▊       | 61/221 [00:17<00:43,  3.72it/s] 28%|██▊       | 62/221 [00:17<00:38,  4.08it/s] 29%|██▊       | 63/221 [00:17<00:38,  4.10it/s] 29%|██▉       | 64/221 [00:17<00:33,  4.68it/s] 29%|██▉       | 65/221 [00:17<00:32,  4.77it/s] 30%|██▉       | 66/221 [00:18<00:37,  4.13it/s] 30%|███       | 67/221 [00:18<00:41,  3.69it/s] 31%|███       | 68/221 [00:18<00:43,  3.52it/s] 31%|███       | 69/221 [00:19<01:07,  2.25it/s] 32%|███▏      | 70/221 [00:19<00:57,  2.63it/s] 32%|███▏      | 71/221 [00:20<01:00,  2.49it/s] 33%|███▎      | 72/221 [00:20<00:54,  2.75it/s] 33%|███▎      | 73/221 [00:20<00:47,  3.12it/s] 33%|███▎      | 74/221 [00:21<00:40,  3.66it/s] 34%|███▍      | 75/221 [00:21<00:42,  3.41it/s] 34%|███▍      | 76/221 [00:21<00:43,  3.36it/s] 35%|███▍      | 77/221 [00:21<00:36,  3.98it/s] 35%|███▌      | 78/221 [00:22<00:40,  3.57it/s] 36%|███▌      | 79/221 [00:22<00:33,  4.20it/s] 36%|███▌      | 80/221 [00:22<00:33,  4.24it/s] 37%|███▋      | 81/221 [00:22<00:31,  4.51it/s] 37%|███▋      | 82/221 [00:23<00:36,  3.83it/s] 38%|███▊      | 83/221 [00:23<00:35,  3.86it/s] 38%|███▊      | 84/221 [00:23<00:40,  3.38it/s] 38%|███▊      | 85/221 [00:24<00:44,  3.07it/s] 39%|███▉      | 86/221 [00:24<00:36,  3.67it/s] 39%|███▉      | 87/221 [00:24<00:41,  3.22it/s] 40%|███▉      | 88/221 [00:24<00:37,  3.57it/s] 40%|████      | 89/221 [00:25<00:46,  2.85it/s] 41%|████      | 90/221 [00:25<00:44,  2.92it/s] 41%|████      | 91/221 [00:25<00:35,  3.66it/s] 42%|████▏     | 92/221 [00:25<00:30,  4.23it/s] 42%|████▏     | 93/221 [00:26<00:38,  3.33it/s] 43%|████▎     | 94/221 [00:26<00:35,  3.62it/s] 43%|████▎     | 95/221 [00:27<00:57,  2.18it/s] 43%|████▎     | 96/221 [00:27<00:50,  2.47it/s] 44%|████▍     | 97/221 [00:28<00:52,  2.35it/s] 44%|████▍     | 98/221 [00:28<00:46,  2.62it/s] 45%|████▍     | 99/221 [00:28<00:42,  2.85it/s] 45%|████▌     | 100/221 [00:29<00:46,  2.60it/s] 46%|████▌     | 101/221 [00:29<00:40,  2.97it/s] 46%|████▌     | 102/221 [00:29<00:38,  3.09it/s] 47%|████▋     | 103/221 [00:30<00:33,  3.53it/s] 48%|████▊     | 105/221 [00:30<00:25,  4.62it/s] 48%|████▊     | 106/221 [00:30<00:29,  3.84it/s] 49%|████▉     | 108/221 [00:30<00:19,  5.70it/s] 49%|████▉     | 109/221 [00:30<00:20,  5.56it/s] 50%|████▉     | 110/221 [00:31<00:22,  4.87it/s] 50%|█████     | 111/221 [00:31<00:24,  4.46it/s] 51%|█████     | 112/221 [00:31<00:30,  3.62it/s] 51%|█████     | 113/221 [00:32<00:30,  3.59it/s] 52%|█████▏    | 114/221 [00:32<00:24,  4.29it/s] 52%|█████▏    | 116/221 [00:32<00:21,  4.83it/s] 53%|█████▎    | 117/221 [00:32<00:22,  4.58it/s] 53%|█████▎    | 118/221 [00:33<00:23,  4.30it/s] 54%|█████▍    | 119/221 [00:33<00:26,  3.85it/s] 54%|█████▍    | 120/221 [00:33<00:30,  3.33it/s] 55%|█████▍    | 121/221 [00:34<00:28,  3.56it/s] 55%|█████▌    | 122/221 [00:34<00:34,  2.84it/s] 56%|█████▌    | 123/221 [00:34<00:29,  3.28it/s] 56%|█████▌    | 124/221 [00:35<00:38,  2.54it/s] 57%|█████▋    | 125/221 [00:36<00:43,  2.21it/s] 57%|█████▋    | 126/221 [00:36<00:37,  2.54it/s] 57%|█████▋    | 127/221 [00:36<00:34,  2.70it/s] 58%|█████▊    | 128/221 [00:36<00:31,  2.98it/s] 58%|█████▊    | 129/221 [00:37<00:24,  3.70it/s] 59%|█████▉    | 130/221 [00:37<00:23,  3.84it/s] 59%|█████▉    | 131/221 [00:37<00:20,  4.44it/s] 60%|█████▉    | 132/221 [00:37<00:26,  3.39it/s] 60%|██████    | 133/221 [00:38<00:32,  2.74it/s] 61%|██████    | 134/221 [00:38<00:29,  2.99it/s] 61%|██████    | 135/221 [00:38<00:25,  3.38it/s] 62%|██████▏   | 136/221 [00:39<00:27,  3.06it/s] 62%|██████▏   | 137/221 [00:39<00:26,  3.18it/s] 62%|██████▏   | 138/221 [00:39<00:24,  3.34it/s] 63%|██████▎   | 139/221 [00:40<00:30,  2.70it/s] 63%|██████▎   | 140/221 [00:40<00:25,  3.21it/s] 64%|██████▍   | 141/221 [00:40<00:25,  3.16it/s] 64%|██████▍   | 142/221 [00:41<00:22,  3.52it/s] 65%|██████▍   | 143/221 [00:41<00:23,  3.34it/s] 65%|██████▌   | 144/221 [00:41<00:22,  3.39it/s] 66%|██████▌   | 145/221 [00:42<00:26,  2.85it/s] 66%|██████▌   | 146/221 [00:42<00:25,  2.90it/s] 67%|██████▋   | 147/221 [00:42<00:20,  3.53it/s] 67%|██████▋   | 148/221 [00:43<00:36,  2.01it/s] 67%|██████▋   | 149/221 [00:43<00:28,  2.50it/s] 68%|██████▊   | 150/221 [00:44<00:23,  3.02it/s] 68%|██████▊   | 151/221 [00:45<00:37,  1.88it/s] 69%|██████▉   | 152/221 [00:45<00:41,  1.68it/s] 69%|██████▉   | 153/221 [00:46<00:35,  1.94it/s] 70%|██████▉   | 154/221 [00:46<00:28,  2.38it/s] 70%|███████   | 155/221 [00:46<00:25,  2.56it/s] 71%|███████   | 156/221 [00:46<00:23,  2.74it/s] 71%|███████   | 157/221 [00:47<00:21,  2.96it/s] 71%|███████▏  | 158/221 [00:47<00:22,  2.86it/s] 72%|███████▏  | 159/221 [00:47<00:19,  3.20it/s] 72%|███████▏  | 160/221 [00:48<00:20,  3.02it/s] 73%|███████▎  | 161/221 [00:48<00:16,  3.70it/s] 74%|███████▍  | 163/221 [00:48<00:12,  4.70it/s] 74%|███████▍  | 164/221 [00:48<00:12,  4.44it/s] 75%|███████▍  | 165/221 [00:49<00:12,  4.40it/s] 75%|███████▌  | 166/221 [00:49<00:16,  3.34it/s] 76%|███████▌  | 167/221 [00:49<00:14,  3.65it/s] 76%|███████▌  | 168/221 [00:50<00:14,  3.61it/s] 77%|███████▋  | 170/221 [00:50<00:14,  3.51it/s] 77%|███████▋  | 171/221 [00:51<00:15,  3.18it/s] 78%|███████▊  | 172/221 [00:51<00:14,  3.49it/s] 78%|███████▊  | 173/221 [00:51<00:13,  3.67it/s] 79%|███████▊  | 174/221 [00:51<00:13,  3.48it/s] 79%|███████▉  | 175/221 [00:52<00:13,  3.37it/s] 80%|███████▉  | 176/221 [00:52<00:11,  4.00it/s] 80%|████████  | 177/221 [00:52<00:09,  4.60it/s] 81%|████████  | 178/221 [00:52<00:13,  3.17it/s] 81%|████████  | 179/221 [00:53<00:13,  3.13it/s] 81%|████████▏ | 180/221 [00:53<00:10,  3.76it/s] 82%|████████▏ | 181/221 [00:53<00:12,  3.32it/s] 82%|████████▏ | 182/221 [00:53<00:09,  4.00it/s] 83%|████████▎ | 183/221 [00:54<00:09,  3.95it/s] 83%|████████▎ | 184/221 [00:54<00:10,  3.48it/s] 84%|████████▎ | 185/221 [00:54<00:10,  3.53it/s] 84%|████████▍ | 186/221 [00:55<00:12,  2.80it/s] 85%|████████▌ | 188/221 [00:55<00:08,  3.74it/s] 86%|████████▌ | 189/221 [00:55<00:07,  4.01it/s] 86%|████████▌ | 190/221 [00:56<00:08,  3.78it/s] 87%|████████▋ | 192/221 [00:56<00:06,  4.44it/s] 87%|████████▋ | 193/221 [00:56<00:07,  3.97it/s] 88%|████████▊ | 194/221 [00:57<00:07,  3.85it/s] 88%|████████▊ | 195/221 [00:57<00:06,  3.97it/s] 89%|████████▊ | 196/221 [00:57<00:08,  3.08it/s] 89%|████████▉ | 197/221 [00:58<00:06,  3.49it/s] 90%|████████▉ | 198/221 [00:58<00:06,  3.62it/s] 90%|█████████ | 199/221 [00:58<00:05,  4.21it/s] 90%|█████████ | 200/221 [00:58<00:04,  4.50it/s] 91%|█████████ | 201/221 [00:58<00:03,  5.33it/s] 91%|█████████▏| 202/221 [00:59<00:03,  5.23it/s] 92%|█████████▏| 203/221 [00:59<00:03,  5.28it/s] 92%|█████████▏| 204/221 [00:59<00:03,  4.90it/s] 93%|█████████▎| 205/221 [00:59<00:03,  4.93it/s] 93%|█████████▎| 206/221 [00:59<00:03,  4.16it/s] 94%|█████████▎| 207/221 [01:00<00:04,  3.17it/s] 94%|█████████▍| 208/221 [01:00<00:03,  3.93it/s] 95%|█████████▍| 209/221 [01:00<00:03,  3.93it/s] 95%|█████████▌| 210/221 [01:01<00:03,  3.41it/s] 95%|█████████▌| 211/221 [01:01<00:02,  3.36it/s] 96%|█████████▌| 212/221 [01:01<00:02,  3.20it/s] 96%|█████████▋| 213/221 [01:02<00:02,  3.28it/s] 97%|█████████▋| 214/221 [01:02<00:02,  2.64it/s] 97%|█████████▋| 215/221 [01:02<00:02,  2.85it/s] 98%|█████████▊| 216/221 [01:03<00:01,  2.74it/s] 98%|█████████▊| 217/221 [01:03<00:01,  2.80it/s] 99%|█████████▊| 218/221 [01:04<00:01,  2.37it/s] 99%|█████████▉| 219/221 [01:04<00:00,  2.45it/s]100%|█████████▉| 220/221 [01:04<00:00,  2.77it/s]100%|██████████| 221/221 [01:05<00:00,  2.73it/s]100%|██████████| 221/221 [01:05<00:00,  3.38it/s]
09/16/2024 16:12:36 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_area_forward========

09/16/2024 16:12:36 - INFO - __main__ -   {'area_r1': 24.1, 'area_recall': '24.1/43.1/50.6', 'area_ravg': 39.3}
09/16/2024 16:12:36 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_area_backard========

09/16/2024 16:12:36 - INFO - __main__ -   {'forward_r1': 33.3, 'forward_recall': '33.3/64.7/75.1', 'forward_ravg': 57.7}
09/16/2024 16:12:36 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video========

09/16/2024 16:12:36 - INFO - __main__ -   {'area_video_r1': 33.3, 'area_video_recall': '33.3/64.4/75.9', 'area_video_ravg': 57.8}
09/16/2024 16:12:36 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_itm_area========

09/16/2024 16:12:36 - INFO - __main__ -   {'area_video_r1': 47.2, 'area_video_recall': '47.2/66.7/72.6', 'area_video_ravg': 62.2, 'area_video_back_r1': 43.9, 'area_video_back_recall': '43.9/68.9/78.2', 'area_video_back_ravg': 63.7}
09/16/2024 16:12:36 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_itc_tva========

09/16/2024 16:12:36 - INFO - __main__ -   {'video_r1': 35.7, 'video_recall': '35.7/63.6/72.5', 'video_ravg': 57.3}
09/16/2024 16:12:36 - INFO - __main__ -   ==== evaluation--ret%tva--msrvtt_ret_ret_itm_tva========

09/16/2024 16:12:36 - INFO - __main__ -   {'video_r1': 49.3, 'video_recall': '49.3/70.8/80.0', 'video_ravg': 66.7}
  0%|          | 0/2755 [00:00<?, ?it/s][h264 @ 0x561765e56a00] mmco: unref short failure
[h264 @ 0x561765e56a00] mmco: unref short failure
09/16/2024 16:12:45 - INFO - __main__ -   current idx OkqwGq_SiKs.41 from finetune_area returns wrong image/video, use 14454 instead.
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/2755 [00:09<7:06:06,  9.28s/it]09/16/2024 16:12:49 - INFO - __main__ -   current idx 1sjlAUHmEHE.6 from finetune_area returns wrong image/video, use 23749 instead.
  0%|          | 2/2755 [00:13<4:48:46,  6.29s/it][h264 @ 0x55f3d135b0c0] mmco: unref short failure
[h264 @ 0x55f3d135b0c0] mmco: unref short failure
[h264 @ 0x5615d4106440] mmco: unref short failure
[h264 @ 0x5615d4106440] mmco: unref short failure
  0%|          | 3/2755 [00:17<3:59:29,  5.22s/it][h264 @ 0x55f3d7f3a180] mmco: unref short failure
  0%|          | 4/2755 [00:21<3:42:59,  4.86s/it][h264 @ 0x557ae8946d00] mmco: unref short failure
  0%|          | 5/2755 [00:26<3:42:30,  4.85s/it][h264 @ 0x557ae8946d00] mmco: unref short failure
[h264 @ 0x557ae8946d00] mmco: unref short failure
  0%|          | 6/2755 [00:31<3:47:00,  4.95s/it]  0%|          | 7/2755 [00:37<4:05:28,  5.36s/it][h264 @ 0x5615dc66da40] mmco: unref short failure
  0%|          | 8/2755 [00:43<4:04:11,  5.33s/it]  0%|          | 9/2755 [00:48<4:03:32,  5.32s/it][h264 @ 0x5615d44e6480] mmco: unref short failure
  0%|          | 10/2755 [00:53<4:01:18,  5.27s/it][h264 @ 0x5615d3c03b00] mmco: unref short failure
[h264 @ 0x5615d3c03b00] mmco: unref short failure
  0%|          | 11/2755 [00:58<3:59:50,  5.24s/it][h264 @ 0x557adca31500] mmco: unref short failure
[h264 @ 0x557adca31500] mmco: unref short failure
  0%|          | 12/2755 [01:03<3:53:58,  5.12s/it][h264 @ 0x55f3d2496a40] mmco: unref short failure
[h264 @ 0x55f3d2496a40] mmco: unref short failure
  0%|          | 13/2755 [01:09<3:57:42,  5.20s/it]  1%|          | 14/2755 [01:14<4:03:01,  5.32s/it][h264 @ 0x557add538b00] mmco: unref short failure
[h264 @ 0x557add538b00] mmco: unref short failure
[h264 @ 0x557add538b00] mmco: unref short failure
[h264 @ 0x557add538b00] mmco: unref short failure
[h264 @ 0x55f3d252c740] mmco: unref short failure
[h264 @ 0x55f3d252c740] mmco: unref short failure
  1%|          | 15/2755 [01:20<4:05:51,  5.38s/it][h264 @ 0x55f3d313e880] mmco: unref short failure
[h264 @ 0x5617693cc340] mmco: unref short failure
[h264 @ 0x5617693cc340] mmco: unref short failure
[h264 @ 0x56176e204c80] mmco: unref short failure
[h264 @ 0x56176e204c80] mmco: unref short failure
[h264 @ 0x561769384380] mmco: unref short failure
[h264 @ 0x55f3de215e40] mmco: unref short failure
[h264 @ 0x5615d82f51c0] mmco: unref short failure
not have audios 8-qwaveiHMM.3
  1%|          | 16/2755 [02:07<13:48:32, 18.15s/it][h264 @ 0x55f3d4568080] mmco: unref short failure
[h264 @ 0x55f3d4568080] mmco: unref short failure
[h264 @ 0x55f3d4568080] mmco: unref short failure
[h264 @ 0x55f3d4568080] mmco: unref short failure
[h264 @ 0x557ae7f7b580] mmco: unref short failure
  1%|          | 17/2755 [02:14<11:02:35, 14.52s/it][h264 @ 0x557adec7cc40] mmco: unref short failure
  1%|          | 18/2755 [02:32<11:54:20, 15.66s/it][h264 @ 0x557ae074fc00] mmco: unref short failure
[h264 @ 0x5615d58d12c0] mmco: unref short failure
[h264 @ 0x5615d58d12c0] mmco: unref short failure
  1%|          | 19/2755 [02:38<9:38:50, 12.69s/it] [h264 @ 0x561767c63300] mmco: unref short failure
[h264 @ 0x561767c63300] mmco: unref short failure
  1%|          | 20/2755 [02:46<8:44:53, 11.52s/it][h264 @ 0x55f3d2a97180] mmco: unref short failure
[h264 @ 0x55f3d2a97180] mmco: unref short failure
[h264 @ 0x5617654dee00] mmco: unref short failure
[h264 @ 0x5617654dee00] mmco: unref short failure
[h264 @ 0x5617654dee00] mmco: unref short failure
[h264 @ 0x56176d71d300] mmco: unref short failure
[h264 @ 0x56176d71d300] mmco: unref short failure
[h264 @ 0x56176d71d300] mmco: unref short failure
[h264 @ 0x56176d71d300] mmco: unref short failure
  1%|          | 21/2755 [02:52<7:17:19,  9.60s/it][h264 @ 0x55f3dea43600] mmco: unref short failure
[h264 @ 0x561773a25cc0] mmco: unref short failure
[h264 @ 0x561773a25cc0] mmco: unref short failure
[h264 @ 0x5615d777aac0] mmco: unref short failure
[h264 @ 0x5615d8dbee80] mmco: unref short failure
[h264 @ 0x5615d8dbee80] mmco: unref short failure
  1%|          | 22/2755 [03:11<9:26:40, 12.44s/it][h264 @ 0x5615d2eeef00] mmco: unref short failure
[h264 @ 0x5615d2eeef00] mmco: unref short failure
  1%|          | 23/2755 [03:17<8:03:12, 10.61s/it][h264 @ 0x55f3d8a22040] mmco: unref short failure
[h264 @ 0x55f3d8a22040] mmco: unref short failure
[h264 @ 0x557ae1aa6600] mmco: unref short failure
[h264 @ 0x5615dd112cc0] mmco: unref short failure
[h264 @ 0x5615dd112cc0] mmco: unref short failure
[h264 @ 0x56176fd06e80] mmco: unref short failure
[h264 @ 0x56176fd06e80] mmco: unref short failure
[h264 @ 0x561766247ac0] mmco: unref short failure
[h264 @ 0x561768bcb4c0] mmco: unref short failure
[h264 @ 0x561768bcb4c0] mmco: unref short failure
[h264 @ 0x55f3e1b0b1c0] mmco: unref short failure
[h264 @ 0x557adca73300] mmco: unref short failure
[h264 @ 0x55f3de824040] mmco: unref short failure
[h264 @ 0x55f3de824040] mmco: unref short failure
[h264 @ 0x557ae6b11a80] mmco: unref short failure
[h264 @ 0x557ae6b11a80] mmco: unref short failure
[h264 @ 0x5615e03e6a80] mmco: unref short failure
[h264 @ 0x5615e03e6a80] mmco: unref short failure
[h264 @ 0x557aedbaba00] mmco: unref short failure
[h264 @ 0x557aedbaba00] mmco: unref short failure
[h264 @ 0x5615d777b3c0] mmco: unref short failure
[h264 @ 0x5615d777b3c0] mmco: unref short failure
[h264 @ 0x55f3d9f7a080] mmco: unref short failure
[h264 @ 0x55f3d9f7a080] mmco: unref short failure
[h264 @ 0x55f3d9f7a080] mmco: unref short failure
[h264 @ 0x56176d859280] mmco: unref short failure
[h264 @ 0x56176d859280] mmco: unref short failure
[h264 @ 0x56176d859280] mmco: unref short failure
[h264 @ 0x56176d859280] mmco: unref short failure
  1%|          | 24/2755 [04:31<22:30:10, 29.66s/it][h264 @ 0x557adfb0d000] mmco: unref short failure
  1%|          | 25/2755 [04:36<16:58:14, 22.38s/it][h264 @ 0x561765cbb780] mmco: unref short failure
[h264 @ 0x5615e0bc86c0] mmco: unref short failure
[h264 @ 0x5615e0bc86c0] mmco: unref short failure
[h264 @ 0x5615d5784480] mmco: unref short failure
[h264 @ 0x5615d5784480] mmco: unref short failure
[h264 @ 0x55f3d5980580] mmco: unref short failure
[h264 @ 0x5615d777b600] mmco: unref short failure
[h264 @ 0x5615de126b00] mmco: unref short failure
  1%|          | 26/2755 [04:57<16:32:11, 21.81s/it]  1%|          | 27/2755 [05:03<12:50:08, 16.94s/it][h264 @ 0x55f3d55caa80] mmco: unref short failure
[h264 @ 0x5615da336180] mmco: unref short failure
[h264 @ 0x5615da336180] mmco: unref short failure
[h264 @ 0x5615e0d305c0] mmco: unref short failure
[h264 @ 0x5615e0d305c0] mmco: unref short failure
[h264 @ 0x5615e5c03240] mmco: unref short failure
[h264 @ 0x5615e5c03240] mmco: unref short failure
[h264 @ 0x557aebcc1880] mmco: unref short failure
[h264 @ 0x557aebcc1880] mmco: unref short failure
[h264 @ 0x5615d58be940] mmco: unref short failure
[h264 @ 0x5615d58be940] mmco: unref short failure
  1%|          | 28/2755 [05:26<14:17:47, 18.87s/it][h264 @ 0x55f3d1ce9c00] mmco: unref short failure
[h264 @ 0x55f3d1ce9c00] mmco: unref short failure
  1%|          | 29/2755 [05:32<11:24:03, 15.06s/it]09/16/2024 16:18:09 - INFO - __main__ -   current idx KPOxRziYDzs.2 from finetune_area returns wrong image/video, use 118323 instead.
09/16/2024 16:18:14 - INFO - __main__ -   current idx 9W2zPcc4Kl8.5 from finetune_area returns wrong image/video, use 52344 instead.
  1%|          | 30/2755 [05:38<9:18:17, 12.29s/it] [h264 @ 0x5615ddd091c0] mmco: unref short failure
[h264 @ 0x5615ddd091c0] mmco: unref short failure
  1%|          | 31/2755 [05:43<7:46:31, 10.28s/it][h264 @ 0x5615dc835540] mmco: unref short failure
[h264 @ 0x56176b3c7100] mmco: unref short failure
[h264 @ 0x55f3d48ddf80] mmco: unref short failure
[h264 @ 0x55f3d48ddf80] mmco: unref short failure
[h264 @ 0x55f3de383c80] mmco: unref short failure
[h264 @ 0x55f3da7584c0] mmco: unref short failure
[h264 @ 0x557aef0cba40] mmco: unref short failure
[h264 @ 0x5615dcab9740] mmco: unref short failure
[h264 @ 0x5615d2c1c300] mmco: unref short failure
[h264 @ 0x55f3e10b6640] mmco: unref short failure
[h264 @ 0x55f3e10b6640] mmco: unref short failure
[h264 @ 0x5615dc6ac940] mmco: unref short failure
[h264 @ 0x557ae5a34800] mmco: unref short failure
[h264 @ 0x5617699d6e40] mmco: unref short failure
[h264 @ 0x5615d72cef40] mmco: unref short failure
  1%|          | 32/2755 [06:59<22:33:08, 29.82s/it]  1%|          | 33/2755 [07:04<16:54:20, 22.36s/it][h264 @ 0x557ae505fd40] mmco: unref short failure
  1%|          | 34/2755 [07:19<15:19:56, 20.29s/it][h264 @ 0x55f3d42ac540] mmco: unref short failure
09/16/2024 16:20:06 - INFO - __main__ -   current idx LWlVnAUqfa4.19 from finetune_area returns wrong image/video, use 57817 instead.
  1%|▏         | 35/2755 [07:31<13:20:51, 17.67s/it][h264 @ 0x56176939ffc0] mmco: unref short failure
[h264 @ 0x5617685a5980] mmco: unref short failure
  1%|▏         | 36/2755 [07:38<10:56:57, 14.50s/it][h264 @ 0x557adb461cc0] mmco: unref short failure
[h264 @ 0x557adb461cc0] mmco: unref short failure
  1%|▏         | 37/2755 [07:43<8:49:30, 11.69s/it] [h264 @ 0x56176b92fe00] mmco: unref short failure
[h264 @ 0x56176b92fe00] mmco: unref short failure
[h264 @ 0x5615e5a466c0] mmco: unref short failure
[h264 @ 0x5615e5a466c0] mmco: unref short failure
[h264 @ 0x55f3d7969a00] mmco: unref short failure
[h264 @ 0x55f3d7969a00] mmco: unref short failure
[h264 @ 0x55f3dab156c0] mmco: unref short failure
[h264 @ 0x5615dac84580] mmco: unref short failure
[h264 @ 0x5615dac84580] mmco: unref short failure
  1%|▏         | 38/2755 [07:56<9:04:22, 12.02s/it][h264 @ 0x5615e88b1b00] mmco: unref short failure
[h264 @ 0x5615e88b1b00] mmco: unref short failure
[h264 @ 0x557ae04788c0] mmco: unref short failure
[h264 @ 0x561767d0b8c0] mmco: unref short failure
[h264 @ 0x561775d8d280] mmco: unref short failure
[h264 @ 0x561775d8d280] mmco: unref short failure
[h264 @ 0x5615dd922500] mmco: unref short failure
  1%|▏         | 39/2755 [08:09<9:19:41, 12.36s/it][h264 @ 0x55f3d68b9600] mmco: unref short failure
[h264 @ 0x55f3d68b9600] mmco: unref short failure
[h264 @ 0x5617759603c0] mmco: unref short failure
[h264 @ 0x5617759603c0] mmco: unref short failure
[h264 @ 0x557ae7581900] mmco: unref short failure
[h264 @ 0x557ae7581900] mmco: unref short failure
[h264 @ 0x56176c2009c0] mmco: unref short failure
[h264 @ 0x56176c2009c0] mmco: unref short failure
[h264 @ 0x55f3da6897c0] mmco: unref short failure
[h264 @ 0x55f3da6897c0] mmco: unref short failure
[h264 @ 0x5615d98ba9c0] mmco: unref short failure
[h264 @ 0x5615d98ba9c0] mmco: unref short failure
[h264 @ 0x5615e15cf2c0] mmco: unref short failure
[h264 @ 0x55f3d1f5ac40] mmco: unref short failure
[h264 @ 0x55f3d1f5ac40] mmco: unref short failure
[h264 @ 0x55f3d1f5ac40] mmco: unref short failure
[h264 @ 0x55f3d1f5ac40] mmco: unref short failure
[h264 @ 0x56177a41a3c0] mmco: unref short failure
[h264 @ 0x56177a41a3c0] mmco: unref short failure
[h264 @ 0x5615d20b3700] mmco: unref short failure
[h264 @ 0x5615dcab3340] mmco: unref short failure
[h264 @ 0x557ae5ff4580] mmco: unref short failure
[h264 @ 0x557aeefc0840] mmco: unref short failure
[h264 @ 0x557aeefc0840] mmco: unref short failure
[h264 @ 0x5615d5d90840] mmco: unref short failure
[h264 @ 0x5615d5d90840] mmco: unref short failure
[h264 @ 0x55f3d4189d40] mmco: unref short failure
[h264 @ 0x55f3d4189d40] mmco: unref short failure
[h264 @ 0x55f3d5056f00] mmco: unref short failure
[h264 @ 0x55f3d76c0940] mmco: unref short failure
[h264 @ 0x55f3d76c0940] mmco: unref short failure
[h264 @ 0x55f3d76c0940] mmco: unref short failure
[h264 @ 0x55f3d76c0940] mmco: unref short failure
  1%|▏         | 40/2755 [09:19<22:23:45, 29.70s/it]09/16/2024 16:21:57 - INFO - __main__ -   current idx TVmM3kKwp3o.36 from finetune_area returns wrong image/video, use 140906 instead.
  1%|▏         | 41/2755 [09:25<16:59:24, 22.54s/it][h264 @ 0x5615d4d803c0] mmco: unref short failure
[h264 @ 0x55f3dc4044c0] mmco: unref short failure
[h264 @ 0x55f3dc4044c0] mmco: unref short failure
[h264 @ 0x55f3d22b83c0] mmco: unref short failure
[h264 @ 0x55f3d22b83c0] mmco: unref short failure
[h264 @ 0x55f3d231be00] mmco: unref short failure
  2%|▏         | 42/2755 [09:54<18:22:24, 24.38s/it][h264 @ 0x55f3e6f98600] mmco: unref short failure
[h264 @ 0x55f3e6f98600] mmco: unref short failure
  2%|▏         | 43/2755 [10:07<15:45:21, 20.91s/it][h264 @ 0x56177659b5c0] mmco: unref short failure
[h264 @ 0x557aefa93480] mmco: unref short failure
[h264 @ 0x557aefa93480] mmco: unref short failure
[h264 @ 0x557aefa93480] mmco: unref short failure
[h264 @ 0x557aefa93480] mmco: unref short failure
[h264 @ 0x557aec93f0c0] mmco: unref short failure
[h264 @ 0x557aec93f0c0] mmco: unref short failure
[h264 @ 0x5615e4aae680] mmco: unref short failure
[h264 @ 0x5615e4aae680] mmco: unref short failure
  2%|▏         | 44/2755 [10:12<12:15:36, 16.28s/it]  2%|▏         | 45/2755 [10:17<9:44:06, 12.93s/it]   2%|▏         | 46/2755 [10:23<8:02:43, 10.69s/it]09/16/2024 16:23:00 - INFO - __main__ -   current idx Lzt-UMekcLY.51 from finetune_area returns wrong image/video, use 131934 instead.
  2%|▏         | 47/2755 [10:29<7:00:34,  9.32s/it][h264 @ 0x557aed962f40] mmco: unref short failure
[h264 @ 0x557aed962f40] mmco: unref short failure
[h264 @ 0x557ae0092c40] mmco: unref short failure
[h264 @ 0x557ae5187540] mmco: unref short failure
[h264 @ 0x56176b3b7c40] mmco: unref short failure
[h264 @ 0x56176b3b7c40] mmco: unref short failure
[h264 @ 0x55f3dfbc7d00] mmco: unref short failure
[h264 @ 0x557ae90697c0] mmco: unref short failure
[h264 @ 0x557adf30a980] mmco: unref short failure
[h264 @ 0x557adf30a980] mmco: unref short failure
09/16/2024 16:23:50 - INFO - __main__ -   current idx fIRuJ6-_rSc.19 from finetune_area returns wrong image/video, use 82333 instead.
[h264 @ 0x55f3dd0cae40] mmco: unref short failure
[h264 @ 0x55f3dd0cae40] mmco: unref short failure
[h264 @ 0x561765fea100] mmco: unref short failure
[h264 @ 0x561765fea100] mmco: unref short failure
09/16/2024 16:24:03 - INFO - __main__ -   current idx ev5QuqL57aY.55 from finetune_area returns wrong image/video, use 54967 instead.
[h264 @ 0x557adc251440] mmco: unref short failure
  2%|▏         | 48/2755 [11:40<20:57:38, 27.88s/it][h264 @ 0x557aed755440] mmco: unref short failure
[h264 @ 0x557aed755440] mmco: unref short failure
[h264 @ 0x5615e19910c0] mmco: unref short failure
[h264 @ 0x557ae1052600] mmco: unref short failure
[h264 @ 0x561765fd5ec0] mmco: unref short failure
[h264 @ 0x557ae57df5c0] mmco: unref short failure
[h264 @ 0x557ae57df5c0] mmco: unref short failure
[h264 @ 0x5615e5849640] mmco: unref short failure
  2%|▏         | 49/2755 [11:57<18:26:57, 24.54s/it]09/16/2024 16:24:33 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
09/16/2024 16:24:33 - INFO - __main__ -   start running ret%tva validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x561766fad180] mmco: unref short failure
[h264 @ 0x561766fad180] mmco: unref short failure
[h264 @ 0x55f3e07b8600] mmco: unref short failure
[h264 @ 0x55f3e07b8600] mmco: unref short failure
[h264 @ 0x55f3e07b8600] mmco: unref short failure
[h264 @ 0x55f3d2a96b00] mmco: unref short failure
[h264 @ 0x55f3d2a96b00] mmco: unref short failure
[h264 @ 0x55f3e0c64c80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/16/2024 16:25:18 - INFO - __main__ -   current idx P9R03t9SYow.10 from finetune_area returns wrong image/video, use 81459 instead.
[h264 @ 0x561774107f40] mmco: unref short failure
[h264 @ 0x55f3da9278c0] mmco: unref short failure
[h264 @ 0x55f3da9278c0] mmco: unref short failure
[h264 @ 0x55f3d60273c0] mmco: unref short failure
[h264 @ 0x55f3d19db8c0] mmco: unref short failure
[h264 @ 0x557ae226d440] mmco: unref short failure
[h264 @ 0x557ae226d440] mmco: unref short failure
[h264 @ 0x5615e45a8680] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:33,  6.63it/s][A
  1%|          | 2/221 [00:00<00:37,  5.81it/s][A
  1%|▏         | 3/221 [00:00<00:46,  4.68it/s][A
  2%|▏         | 4/221 [00:00<00:37,  5.83it/s][A
  2%|▏         | 5/221 [00:00<00:36,  5.99it/s][A
  3%|▎         | 6/221 [00:01<00:35,  5.99it/s][A
  3%|▎         | 7/221 [00:01<00:46,  4.60it/s][A
  4%|▎         | 8/221 [00:01<00:48,  4.40it/s][A
  4%|▍         | 9/221 [00:01<00:47,  4.50it/s][A
  5%|▍         | 10/221 [00:02<00:41,  5.13it/s][A[h264 @ 0x5615dbccf240] mmco: unref short failure
[h264 @ 0x5615dbccf240] mmco: unref short failure

  5%|▌         | 12/221 [00:03<01:32,  2.25it/s][A
  6%|▌         | 13/221 [00:03<01:16,  2.71it/s][A
  7%|▋         | 15/221 [00:03<00:59,  3.45it/s][A
  7%|▋         | 16/221 [00:04<00:50,  4.06it/s][A
  8%|▊         | 17/221 [00:04<00:54,  3.73it/s][A
  8%|▊         | 18/221 [00:04<00:50,  4.01it/s][A
  9%|▊         | 19/221 [00:04<00:44,  4.49it/s][A
  9%|▉         | 20/221 [00:04<00:39,  5.15it/s][A
 10%|▉         | 21/221 [00:05<00:37,  5.30it/s][A09/16/2024 16:27:03 - INFO - __main__ -   current idx OdzQ0Tb5DqY.54 from finetune_area returns wrong image/video, use 130307 instead.

 10%|▉         | 22/221 [00:08<03:17,  1.01it/s][A
 11%|█         | 24/221 [00:08<01:56,  1.68it/s][A
 11%|█▏        | 25/221 [00:08<01:33,  2.11it/s][A
 12%|█▏        | 26/221 [00:08<01:21,  2.39it/s][A
 12%|█▏        | 27/221 [00:08<01:13,  2.64it/s][A
 13%|█▎        | 28/221 [00:09<01:00,  3.17it/s][A
 13%|█▎        | 29/221 [00:09<00:49,  3.86it/s][A
 14%|█▎        | 30/221 [00:09<00:41,  4.61it/s][A
 14%|█▍        | 31/221 [00:09<00:38,  4.89it/s][A
 15%|█▍        | 33/221 [00:09<00:29,  6.47it/s][A
 15%|█▌        | 34/221 [00:09<00:28,  6.66it/s][A
 16%|█▌        | 35/221 [00:09<00:27,  6.78it/s][A
 16%|█▋        | 36/221 [00:10<00:25,  7.32it/s][A
 17%|█▋        | 37/221 [00:10<00:26,  7.01it/s][A
 17%|█▋        | 38/221 [00:10<00:27,  6.65it/s][A[h264 @ 0x5615da6d1840] mmco: unref short failure

 18%|█▊        | 39/221 [00:10<00:24,  7.32it/s][A
 18%|█▊        | 40/221 [00:10<00:24,  7.32it/s][A
 19%|█▊        | 41/221 [00:10<00:28,  6.35it/s][A
 19%|█▉        | 42/221 [00:11<00:34,  5.23it/s][A
 19%|█▉        | 43/221 [00:11<00:36,  4.88it/s][A
 20%|██        | 45/221 [00:11<00:40,  4.34it/s][A
 21%|██        | 46/221 [00:11<00:36,  4.74it/s][A
 21%|██▏       | 47/221 [00:16<04:14,  1.46s/it][A
 22%|██▏       | 49/221 [00:17<02:32,  1.13it/s][A
 23%|██▎       | 50/221 [00:17<02:00,  1.42it/s][A
 23%|██▎       | 51/221 [00:17<01:34,  1.80it/s][A
 24%|██▎       | 52/221 [00:17<01:14,  2.27it/s][A[h264 @ 0x56176680aa80] mmco: unref short failure
[h264 @ 0x56176680aa80] mmco: unref short failure

 24%|██▍       | 54/221 [00:22<03:22,  1.21s/it][A
 25%|██▍       | 55/221 [00:22<02:55,  1.06s/it][A
 25%|██▌       | 56/221 [00:22<02:17,  1.20it/s][A
 26%|██▌       | 57/221 [00:22<01:47,  1.52it/s][A
 26%|██▌       | 58/221 [00:23<01:22,  1.97it/s][A
 27%|██▋       | 59/221 [00:23<01:04,  2.52it/s][A
 27%|██▋       | 60/221 [00:23<00:56,  2.86it/s][A
 28%|██▊       | 61/221 [00:23<00:46,  3.44it/s][A
 28%|██▊       | 62/221 [00:23<00:37,  4.22it/s][A
 29%|██▊       | 63/221 [00:23<00:31,  4.94it/s][A
 29%|██▉       | 64/221 [00:23<00:28,  5.49it/s][A
 29%|██▉       | 65/221 [00:24<00:26,  5.79it/s][A[h264 @ 0x55f3d26b5ec0] mmco: unref short failure
[h264 @ 0x55f3d26b5ec0] mmco: unref short failure

 30%|██▉       | 66/221 [00:28<03:28,  1.34s/it][A
 30%|███       | 67/221 [00:28<02:38,  1.03s/it][A
 31%|███       | 69/221 [00:28<01:36,  1.58it/s][A
 32%|███▏      | 70/221 [00:28<01:21,  1.86it/s][A
 32%|███▏      | 71/221 [00:29<01:03,  2.35it/s][A
 33%|███▎      | 72/221 [00:29<00:51,  2.90it/s][A
 33%|███▎      | 73/221 [00:29<00:49,  3.01it/s][A
 33%|███▎      | 74/221 [00:29<00:39,  3.68it/s][A
 34%|███▍      | 75/221 [00:30<00:43,  3.37it/s][A
 34%|███▍      | 76/221 [00:30<00:36,  3.97it/s][A
 35%|███▍      | 77/221 [00:30<00:31,  4.56it/s][A
 35%|███▌      | 78/221 [00:30<00:26,  5.32it/s][A
 36%|███▌      | 79/221 [00:30<00:31,  4.51it/s][A
 36%|███▌      | 80/221 [00:30<00:26,  5.35it/s][A
 37%|███▋      | 81/221 [00:30<00:23,  5.89it/s][A
 37%|███▋      | 82/221 [00:31<00:25,  5.54it/s][A
 38%|███▊      | 84/221 [00:31<00:21,  6.43it/s][A
 38%|███▊      | 85/221 [00:31<00:22,  6.02it/s][A
 39%|███▉      | 86/221 [00:31<00:26,  5.18it/s][A
 39%|███▉      | 87/221 [00:32<00:24,  5.40it/s][A[h264 @ 0x561774109b40] mmco: unref short failure

 40%|███▉      | 88/221 [00:32<00:30,  4.42it/s][A
 40%|████      | 89/221 [00:32<00:29,  4.51it/s][A
 41%|████      | 90/221 [00:32<00:25,  5.14it/s][A
 41%|████      | 91/221 [00:32<00:22,  5.79it/s][A
 42%|████▏     | 92/221 [00:32<00:20,  6.25it/s][A
 42%|████▏     | 93/221 [00:33<00:31,  4.13it/s][A
 43%|████▎     | 95/221 [00:33<00:25,  4.94it/s][A
 43%|████▎     | 96/221 [00:33<00:26,  4.76it/s][A
 44%|████▍     | 97/221 [00:34<00:22,  5.50it/s][A
 44%|████▍     | 98/221 [00:34<00:20,  5.95it/s][A
 45%|████▍     | 99/221 [00:34<00:19,  6.26it/s][A
 45%|████▌     | 100/221 [00:34<00:18,  6.58it/s][A
 46%|████▌     | 101/221 [00:34<00:16,  7.29it/s][A
 46%|████▌     | 102/221 [00:34<00:16,  7.28it/s][A
 47%|████▋     | 103/221 [00:34<00:14,  7.87it/s][A
 48%|████▊     | 105/221 [00:34<00:13,  8.80it/s][A
 48%|████▊     | 106/221 [00:35<00:25,  4.50it/s][A
 48%|████▊     | 107/221 [00:35<00:24,  4.65it/s][A
 49%|████▉     | 108/221 [00:35<00:23,  4.81it/s][A
 49%|████▉     | 109/221 [00:36<00:22,  5.03it/s][A
 50%|████▉     | 110/221 [00:36<00:20,  5.35it/s][A
 50%|█████     | 111/221 [00:36<00:25,  4.30it/s][A
 51%|█████     | 112/221 [00:36<00:23,  4.63it/s][A
 51%|█████     | 113/221 [00:36<00:19,  5.40it/s][A
 52%|█████▏    | 114/221 [00:36<00:17,  6.23it/s][A
 52%|█████▏    | 115/221 [00:37<00:15,  6.87it/s][A
 52%|█████▏    | 116/221 [00:42<02:52,  1.64s/it][A
 53%|█████▎    | 117/221 [00:42<02:10,  1.26s/it][A
 53%|█████▎    | 118/221 [00:42<01:34,  1.09it/s][A
 54%|█████▍    | 119/221 [00:42<01:10,  1.44it/s][A
 54%|█████▍    | 120/221 [00:43<00:54,  1.84it/s][A
 55%|█████▍    | 121/221 [00:43<00:44,  2.25it/s][A
 55%|█████▌    | 122/221 [00:43<00:34,  2.84it/s][A
 56%|█████▌    | 123/221 [00:43<00:27,  3.50it/s][A
 56%|█████▌    | 124/221 [00:43<00:22,  4.22it/s][A
 57%|█████▋    | 125/221 [00:44<00:27,  3.43it/s][A
 57%|█████▋    | 126/221 [00:44<00:27,  3.50it/s][A
 57%|█████▋    | 127/221 [00:45<00:36,  2.55it/s][A
 58%|█████▊    | 128/221 [00:45<00:49,  1.86it/s][A
 58%|█████▊    | 129/221 [00:46<00:37,  2.43it/s][A
 59%|█████▉    | 131/221 [00:46<00:24,  3.62it/s][A
 60%|█████▉    | 132/221 [00:46<00:25,  3.53it/s][A
 60%|██████    | 133/221 [00:46<00:25,  3.42it/s][A
 61%|██████    | 134/221 [00:47<00:26,  3.31it/s][A[h264 @ 0x561770d4c3c0] mmco: unref short failure
[h264 @ 0x561770d4c3c0] mmco: unref short failure

 61%|██████    | 135/221 [00:47<00:36,  2.38it/s][A
 62%|██████▏   | 136/221 [00:49<00:57,  1.47it/s][A09/16/2024 16:27:51 - INFO - __main__ -   current idx 2sAhL_rJYwk.29 from finetune_area returns wrong image/video, use 137884 instead.

 62%|██████▏   | 137/221 [00:54<02:38,  1.89s/it][A
 62%|██████▏   | 138/221 [00:54<02:05,  1.52s/it][A
 63%|██████▎   | 139/221 [00:55<01:41,  1.24s/it][A
 63%|██████▎   | 140/221 [00:55<01:17,  1.04it/s][A
 64%|██████▍   | 141/221 [00:56<01:05,  1.23it/s][A
 64%|██████▍   | 142/221 [00:56<00:51,  1.52it/s][A
 65%|██████▍   | 143/221 [00:56<00:40,  1.92it/s][A
 65%|██████▌   | 144/221 [00:56<00:31,  2.46it/s][A
 66%|██████▌   | 145/221 [00:56<00:25,  3.00it/s][A
 66%|██████▌   | 146/221 [00:57<00:21,  3.51it/s][A
 67%|██████▋   | 147/221 [00:57<00:18,  4.05it/s][A
 67%|██████▋   | 148/221 [00:57<00:21,  3.47it/s][A
 67%|██████▋   | 149/221 [00:57<00:18,  3.84it/s][A
[h264 @ 0x557ae5bcd880] mmco: unref short failure
 68%|██████▊   | 150/221 [00:57<00:17,  4.04it/s][A
 68%|██████▊   | 151/221 [00:58<00:17,  3.91it/s][A
 69%|██████▉   | 152/221 [00:58<00:20,  3.34it/s][A
 70%|██████▉   | 154/221 [00:58<00:13,  5.02it/s][A
 70%|███████   | 155/221 [00:58<00:11,  5.65it/s][A
 71%|███████   | 156/221 [00:59<00:10,  6.34it/s][A
 71%|███████   | 157/221 [01:04<01:37,  1.52s/it][A
 71%|███████▏  | 158/221 [01:04<01:12,  1.16s/it][A
 72%|███████▏  | 159/221 [01:04<00:53,  1.15it/s][A
 72%|███████▏  | 160/221 [01:04<00:41,  1.47it/s][A
 73%|███████▎  | 162/221 [01:05<00:24,  2.40it/s][A
 74%|███████▍  | 163/221 [01:05<00:21,  2.64it/s][A
 74%|███████▍  | 164/221 [01:05<00:18,  3.07it/s][A[h264 @ 0x55f3ea6af180] mmco: unref short failure
[h264 @ 0x55f3ea6af180] mmco: unref short failure
[h264 @ 0x557adf495b00] mmco: unref short failure
[h264 @ 0x557adf495b00] mmco: unref short failure

 75%|███████▌  | 166/221 [01:10<01:11,  1.29s/it][A
 76%|███████▌  | 167/221 [01:10<00:54,  1.02s/it][A[h264 @ 0x557ae3f4a140] mmco: unref short failure

 76%|███████▌  | 168/221 [01:14<01:35,  1.80s/it][A
 77%|███████▋  | 170/221 [01:15<00:58,  1.14s/it][A
 77%|███████▋  | 171/221 [01:15<00:47,  1.06it/s][A
 78%|███████▊  | 173/221 [01:15<00:29,  1.65it/s][A
 79%|███████▉  | 175/221 [01:15<00:19,  2.35it/s][A
 80%|████████  | 177/221 [01:16<00:13,  3.31it/s][A
 81%|████████  | 178/221 [01:16<00:12,  3.55it/s][A
 81%|████████  | 179/221 [01:20<00:49,  1.19s/it][A
 82%|████████▏ | 181/221 [01:20<00:29,  1.33it/s][A
 83%|████████▎ | 183/221 [01:21<00:20,  1.88it/s][A
 84%|████████▎ | 185/221 [01:21<00:13,  2.68it/s][A
 85%|████████▍ | 187/221 [01:21<00:09,  3.67it/s][A[h264 @ 0x5615d243eac0] mmco: unref short failure

 86%|████████▌ | 189/221 [01:21<00:06,  4.69it/s][A[h264 @ 0x5615d243eac0] mmco: unref short failure

 86%|████████▋ | 191/221 [01:21<00:06,  4.80it/s][A
 87%|████████▋ | 193/221 [01:22<00:04,  5.83it/s][A
 88%|████████▊ | 195/221 [01:22<00:03,  7.08it/s][A
 89%|████████▉ | 197/221 [01:22<00:02,  8.68it/s][A
 90%|█████████ | 199/221 [01:22<00:02,  9.58it/s][A
 91%|█████████ | 201/221 [01:22<00:01, 10.59it/s][A
 92%|█████████▏| 203/221 [01:22<00:01,  9.92it/s][A
 93%|█████████▎| 205/221 [01:23<00:01, 10.73it/s][A
 94%|█████████▎| 207/221 [01:23<00:01,  8.26it/s][A
 95%|█████████▍| 209/221 [01:23<00:01,  9.12it/s][A
 95%|█████████▌| 211/221 [01:23<00:01,  8.08it/s][A
 96%|█████████▌| 212/221 [01:23<00:01,  8.31it/s][A
 97%|█████████▋| 214/221 [01:24<00:01,  6.90it/s][A
 98%|█████████▊| 216/221 [01:24<00:00,  8.75it/s][A09/16/2024 16:28:22 - INFO - __main__ -   current idx TlZDDACt8Nw.3 from finetune_area returns wrong image/video, use 104570 instead.
[h264 @ 0x557af0b09a80] mmco: unref short failure
[h264 @ 0x557af0b09a80] mmco: unref short failure

 99%|█████████▊| 218/221 [01:29<00:02,  1.13it/s][A
 99%|█████████▉| 219/221 [01:29<00:01,  1.35it/s][A
100%|█████████▉| 220/221 [01:34<00:01,  1.64s/it][A100%|██████████| 221/221 [01:34<00:00,  2.33it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:10,  3.11it/s][A[h264 @ 0x55f3d694f380] mmco: unref short failure
[h264 @ 0x55f3d694f380] mmco: unref short failure

  1%|          | 2/221 [00:00<01:07,  3.22it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.28it/s][A
  2%|▏         | 4/221 [00:01<01:05,  3.30it/s][A
  2%|▏         | 5/221 [00:01<01:05,  3.29it/s][A
  3%|▎         | 6/221 [00:01<01:04,  3.31it/s][A
  3%|▎         | 7/221 [00:02<01:04,  3.34it/s][A
  4%|▎         | 8/221 [00:02<01:03,  3.35it/s][A
  4%|▍         | 9/221 [00:02<01:03,  3.36it/s][A
  5%|▍         | 10/221 [00:03<01:02,  3.37it/s][A
  5%|▍         | 11/221 [00:03<01:02,  3.37it/s][A
  5%|▌         | 12/221 [00:03<01:01,  3.38it/s][A
  6%|▌         | 13/221 [00:03<01:01,  3.38it/s][A
  6%|▋         | 14/221 [00:04<01:01,  3.38it/s][A
  7%|▋         | 15/221 [00:04<01:00,  3.39it/s][A
  7%|▋         | 16/221 [00:04<01:00,  3.38it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.26it/s][A
  8%|▊         | 18/221 [00:05<01:01,  3.29it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.28it/s][A
  9%|▉         | 20/221 [00:06<01:00,  3.31it/s][A
 10%|▉         | 21/221 [00:06<01:00,  3.33it/s][A
 10%|▉         | 22/221 [00:06<00:59,  3.34it/s][A
 10%|█         | 23/221 [00:06<00:59,  3.35it/s][A
 11%|█         | 24/221 [00:07<00:58,  3.36it/s][A
 11%|█▏        | 25/221 [00:07<00:58,  3.37it/s][A
 12%|█▏        | 26/221 [00:07<00:57,  3.38it/s][A
 12%|█▏        | 27/221 [00:08<00:57,  3.38it/s][A
 13%|█▎        | 28/221 [00:08<00:57,  3.38it/s][A
 13%|█▎        | 29/221 [00:08<00:56,  3.38it/s][A
 14%|█▎        | 30/221 [00:08<00:56,  3.38it/s][A
 14%|█▍        | 31/221 [00:09<00:56,  3.38it/s][A
 14%|█▍        | 32/221 [00:09<00:55,  3.38it/s][A
 15%|█▍        | 33/221 [00:09<00:55,  3.39it/s][A
 15%|█▌        | 34/221 [00:10<00:55,  3.39it/s][A
 16%|█▌        | 35/221 [00:10<00:54,  3.39it/s][A
 16%|█▋        | 36/221 [00:10<00:54,  3.39it/s][A
 17%|█▋        | 37/221 [00:11<00:54,  3.39it/s][A
 17%|█▋        | 38/221 [00:11<00:53,  3.39it/s][A
 18%|█▊        | 39/221 [00:11<00:53,  3.39it/s][A
 18%|█▊        | 40/221 [00:11<00:53,  3.39it/s][A
 19%|█▊        | 41/221 [00:12<00:53,  3.39it/s][A
 19%|█▉        | 42/221 [00:12<00:52,  3.39it/s][A
 19%|█▉        | 43/221 [00:12<00:52,  3.40it/s][A
 20%|█▉        | 44/221 [00:13<00:52,  3.40it/s][A
 20%|██        | 45/221 [00:13<00:51,  3.40it/s][A
 21%|██        | 46/221 [00:13<00:51,  3.40it/s][A
 21%|██▏       | 47/221 [00:13<00:51,  3.40it/s][A
 22%|██▏       | 48/221 [00:14<00:50,  3.40it/s][A
 22%|██▏       | 49/221 [00:14<00:50,  3.40it/s][A
 23%|██▎       | 50/221 [00:14<00:50,  3.40it/s][A
 23%|██▎       | 51/221 [00:15<00:49,  3.40it/s][A
 24%|██▎       | 52/221 [00:15<00:49,  3.40it/s][A
 24%|██▍       | 53/221 [00:15<00:49,  3.40it/s][A
 24%|██▍       | 54/221 [00:16<00:49,  3.40it/s][A
 25%|██▍       | 55/221 [00:16<00:48,  3.40it/s][A
 25%|██▌       | 56/221 [00:16<00:48,  3.40it/s][A
 26%|██▌       | 57/221 [00:16<00:48,  3.40it/s][A
 26%|██▌       | 58/221 [00:17<00:47,  3.40it/s][A
 27%|██▋       | 59/221 [00:17<00:47,  3.40it/s][A
 27%|██▋       | 60/221 [00:17<00:47,  3.41it/s][A
 28%|██▊       | 61/221 [00:18<00:46,  3.41it/s][A
 28%|██▊       | 62/221 [00:18<00:46,  3.41it/s][A
 29%|██▊       | 63/221 [00:18<00:46,  3.41it/s][A
 29%|██▉       | 64/221 [00:18<00:46,  3.41it/s][A
 29%|██▉       | 65/221 [00:19<00:45,  3.41it/s][A
 30%|██▉       | 66/221 [00:19<00:45,  3.41it/s][A
 30%|███       | 67/221 [00:19<00:45,  3.41it/s][A
 31%|███       | 68/221 [00:20<00:44,  3.41it/s][A
 31%|███       | 69/221 [00:20<00:44,  3.41it/s][A
 32%|███▏      | 70/221 [00:20<00:44,  3.41it/s][A
 32%|███▏      | 71/221 [00:21<00:44,  3.41it/s][A
 33%|███▎      | 72/221 [00:21<00:43,  3.41it/s][A
 33%|███▎      | 73/221 [00:21<00:43,  3.41it/s][A
 33%|███▎      | 74/221 [00:21<00:43,  3.41it/s][A
 34%|███▍      | 75/221 [00:22<00:42,  3.41it/s][A
 34%|███▍      | 76/221 [00:22<00:42,  3.41it/s][A
 35%|███▍      | 77/221 [00:22<00:42,  3.41it/s][A
 35%|███▌      | 78/221 [00:23<00:41,  3.41it/s][A
 36%|███▌      | 79/221 [00:23<00:41,  3.41it/s][A
 36%|███▌      | 80/221 [00:23<00:41,  3.41it/s][A
 37%|███▋      | 81/221 [00:23<00:41,  3.41it/s][A
 37%|███▋      | 82/221 [00:24<00:40,  3.41it/s][A
 38%|███▊      | 83/221 [00:24<00:40,  3.41it/s][A
 38%|███▊      | 84/221 [00:24<00:40,  3.41it/s][A
 38%|███▊      | 85/221 [00:25<00:39,  3.41it/s][A
 39%|███▉      | 86/221 [00:25<00:39,  3.41it/s][A
 39%|███▉      | 87/221 [00:25<00:39,  3.41it/s][A
 40%|███▉      | 88/221 [00:26<00:39,  3.41it/s][A
 40%|████      | 89/221 [00:26<00:38,  3.41it/s][A
 41%|████      | 90/221 [00:26<00:38,  3.41it/s][A
 41%|████      | 91/221 [00:26<00:38,  3.41it/s][A
 42%|████▏     | 92/221 [00:27<00:37,  3.41it/s][A
 42%|████▏     | 93/221 [00:27<00:37,  3.41it/s][A
 43%|████▎     | 94/221 [00:27<00:37,  3.41it/s][A
 43%|████▎     | 95/221 [00:28<00:36,  3.41it/s][A
 43%|████▎     | 96/221 [00:28<00:36,  3.41it/s][A
 44%|████▍     | 97/221 [00:28<00:36,  3.41it/s][A
 44%|████▍     | 98/221 [00:28<00:36,  3.41it/s][A
 45%|████▍     | 99/221 [00:29<00:35,  3.41it/s][A
 45%|████▌     | 100/221 [00:29<00:35,  3.41it/s][A
 46%|████▌     | 101/221 [00:29<00:35,  3.41it/s][A
 46%|████▌     | 102/221 [00:30<00:34,  3.41it/s][A
 47%|████▋     | 103/221 [00:30<00:34,  3.41it/s][A
 47%|████▋     | 104/221 [00:30<00:34,  3.41it/s][A
 48%|████▊     | 105/221 [00:31<00:34,  3.41it/s][A
 48%|████▊     | 106/221 [00:31<00:33,  3.41it/s][A
 48%|████▊     | 107/221 [00:31<00:33,  3.41it/s][A
 49%|████▉     | 108/221 [00:31<00:33,  3.41it/s][A
 49%|████▉     | 109/221 [00:32<00:32,  3.41it/s][A
 50%|████▉     | 110/221 [00:32<00:32,  3.41it/s][A
 50%|█████     | 111/221 [00:32<00:32,  3.41it/s][A
 51%|█████     | 112/221 [00:33<00:31,  3.41it/s][A
 51%|█████     | 113/221 [00:33<00:31,  3.41it/s][A
 52%|█████▏    | 114/221 [00:33<00:31,  3.41it/s][A
 52%|█████▏    | 115/221 [00:33<00:31,  3.41it/s][A
 52%|█████▏    | 116/221 [00:34<00:30,  3.41it/s][A
 53%|█████▎    | 117/221 [00:34<00:30,  3.41it/s][A
 53%|█████▎    | 118/221 [00:34<00:30,  3.41it/s][A
 54%|█████▍    | 119/221 [00:35<00:29,  3.41it/s][A
 54%|█████▍    | 120/221 [00:35<00:29,  3.41it/s][A
 55%|█████▍    | 121/221 [00:35<00:29,  3.41it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.41it/s][A
 56%|█████▌    | 123/221 [00:36<00:28,  3.41it/s][A
 56%|█████▌    | 124/221 [00:36<00:28,  3.41it/s][A
 57%|█████▋    | 125/221 [00:36<00:28,  3.41it/s][A
 57%|█████▋    | 126/221 [00:37<00:27,  3.41it/s][A
 57%|█████▋    | 127/221 [00:37<00:27,  3.41it/s][A
 58%|█████▊    | 128/221 [00:37<00:27,  3.41it/s][A
 58%|█████▊    | 129/221 [00:38<00:27,  3.41it/s][A
 59%|█████▉    | 130/221 [00:38<00:26,  3.41it/s][A
 59%|█████▉    | 131/221 [00:38<00:26,  3.41it/s][A
 60%|█████▉    | 132/221 [00:38<00:26,  3.41it/s][A
 60%|██████    | 133/221 [00:39<00:25,  3.41it/s][A
 61%|██████    | 134/221 [00:39<00:25,  3.41it/s][A
 61%|██████    | 135/221 [00:39<00:25,  3.41it/s][A
 62%|██████▏   | 136/221 [00:40<00:24,  3.41it/s][A
 62%|██████▏   | 137/221 [00:40<00:24,  3.41it/s][A
 62%|██████▏   | 138/221 [00:40<00:24,  3.41it/s][A
 63%|██████▎   | 139/221 [00:40<00:24,  3.41it/s][A
 63%|██████▎   | 140/221 [00:41<00:23,  3.41it/s][A
 64%|██████▍   | 141/221 [00:41<00:23,  3.41it/s][A
 64%|██████▍   | 142/221 [00:41<00:23,  3.41it/s][A
 65%|██████▍   | 143/221 [00:42<00:22,  3.41it/s][A
 65%|██████▌   | 144/221 [00:42<00:22,  3.41it/s][A
 66%|██████▌   | 145/221 [00:42<00:22,  3.41it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.41it/s][A
 67%|██████▋   | 147/221 [00:43<00:21,  3.41it/s][A
 67%|██████▋   | 148/221 [00:43<00:21,  3.41it/s][A
 67%|██████▋   | 149/221 [00:43<00:21,  3.41it/s][A
 68%|██████▊   | 150/221 [00:44<00:20,  3.41it/s][A
 68%|██████▊   | 151/221 [00:44<00:20,  3.41it/s][A
 69%|██████▉   | 152/221 [00:44<00:20,  3.41it/s][A
 69%|██████▉   | 153/221 [00:45<00:19,  3.41it/s][A
 70%|██████▉   | 154/221 [00:45<00:19,  3.41it/s][A
 70%|███████   | 155/221 [00:45<00:19,  3.41it/s][A
 71%|███████   | 156/221 [00:45<00:19,  3.41it/s][A
 71%|███████   | 157/221 [00:46<00:18,  3.41it/s][A
 71%|███████▏  | 158/221 [00:46<00:18,  3.41it/s][A
 72%|███████▏  | 159/221 [00:46<00:18,  3.41it/s][A
 72%|███████▏  | 160/221 [00:47<00:17,  3.40it/s][A
 73%|███████▎  | 161/221 [00:47<00:17,  3.40it/s][A
 73%|███████▎  | 162/221 [00:47<00:17,  3.41it/s][A
 74%|███████▍  | 163/221 [00:48<00:17,  3.41it/s][A
 74%|███████▍  | 164/221 [00:48<00:16,  3.41it/s][A
 75%|███████▍  | 165/221 [00:48<00:16,  3.41it/s][A
 75%|███████▌  | 166/221 [00:48<00:16,  3.41it/s][A
 76%|███████▌  | 167/221 [00:49<00:15,  3.41it/s][A
 76%|███████▌  | 168/221 [00:49<00:15,  3.41it/s][A
 76%|███████▋  | 169/221 [00:49<00:15,  3.41it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.41it/s][A
 77%|███████▋  | 171/221 [00:50<00:14,  3.41it/s][A
 78%|███████▊  | 172/221 [00:50<00:14,  3.41it/s][A
 78%|███████▊  | 173/221 [00:50<00:14,  3.41it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.41it/s][A
 79%|███████▉  | 175/221 [00:51<00:13,  3.41it/s][A
 80%|███████▉  | 176/221 [00:51<00:13,  3.41it/s][A
 80%|████████  | 177/221 [00:52<00:12,  3.41it/s][A
 81%|████████  | 178/221 [00:52<00:12,  3.41it/s][A
 81%|████████  | 179/221 [00:52<00:12,  3.41it/s][A
 81%|████████▏ | 180/221 [00:53<00:12,  3.41it/s][A
 82%|████████▏ | 181/221 [00:53<00:11,  3.41it/s][A
 82%|████████▏ | 182/221 [00:53<00:11,  3.41it/s][A
 83%|████████▎ | 183/221 [00:53<00:11,  3.41it/s][A
 83%|████████▎ | 184/221 [00:54<00:10,  3.41it/s][A
 84%|████████▎ | 185/221 [00:54<00:10,  3.41it/s][A
 84%|████████▍ | 186/221 [00:54<00:10,  3.41it/s][A
 85%|████████▍ | 187/221 [00:55<00:09,  3.41it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.41it/s][A
 86%|████████▌ | 189/221 [00:55<00:09,  3.41it/s][A
 86%|████████▌ | 190/221 [00:55<00:09,  3.41it/s][A
 86%|████████▋ | 191/221 [00:56<00:08,  3.41it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.41it/s][A
 87%|████████▋ | 193/221 [00:56<00:08,  3.41it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.41it/s][A
 88%|████████▊ | 195/221 [00:57<00:07,  3.41it/s][A
 89%|████████▊ | 196/221 [00:57<00:07,  3.41it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.41it/s][A
 90%|████████▉ | 198/221 [00:58<00:06,  3.41it/s][A
 90%|█████████ | 199/221 [00:58<00:06,  3.41it/s][A
 90%|█████████ | 200/221 [00:58<00:06,  3.41it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.41it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.41it/s][A
 92%|█████████▏| 203/221 [00:59<00:05,  3.41it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.41it/s][A
 93%|█████████▎| 205/221 [01:00<00:04,  3.41it/s][A
 93%|█████████▎| 206/221 [01:00<00:04,  3.41it/s][A
 94%|█████████▎| 207/221 [01:00<00:04,  3.41it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▍| 209/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▌| 210/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.41it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.41it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.41it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.41it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.41it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.41it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.41it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.41it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  3.41it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.41it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.41it/s][A100%|██████████| 221/221 [01:05<00:00,  3.40it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:30,  7.14it/s][A
  1%|          | 2/221 [00:00<01:14,  2.93it/s][A
  1%|▏         | 3/221 [00:01<01:29,  2.42it/s][A
  2%|▏         | 4/221 [00:01<01:18,  2.76it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.25it/s][A
  3%|▎         | 6/221 [00:01<00:53,  4.02it/s][A
  3%|▎         | 7/221 [00:01<00:44,  4.76it/s][A
  4%|▎         | 8/221 [00:02<00:46,  4.54it/s][A
  4%|▍         | 9/221 [00:02<01:21,  2.60it/s][A
  5%|▍         | 10/221 [00:03<01:21,  2.58it/s][A
  5%|▍         | 11/221 [00:03<01:23,  2.51it/s][A
  5%|▌         | 12/221 [00:03<01:17,  2.68it/s][A
  6%|▌         | 13/221 [00:04<01:36,  2.16it/s][A
  6%|▋         | 14/221 [00:04<01:22,  2.52it/s][A
  7%|▋         | 15/221 [00:05<01:19,  2.61it/s][A
  7%|▋         | 16/221 [00:05<01:11,  2.88it/s][A
  8%|▊         | 17/221 [00:06<01:22,  2.47it/s][A
  8%|▊         | 18/221 [00:06<01:22,  2.47it/s][A
  9%|▊         | 19/221 [00:06<01:13,  2.74it/s][A
  9%|▉         | 20/221 [00:06<01:03,  3.18it/s][A
 10%|▉         | 22/221 [00:07<00:51,  3.89it/s][A
 10%|█         | 23/221 [00:07<00:48,  4.08it/s][A
 11%|█         | 24/221 [00:07<00:45,  4.36it/s][A
 11%|█▏        | 25/221 [00:07<00:43,  4.52it/s][A
 12%|█▏        | 26/221 [00:08<00:40,  4.80it/s][A
 12%|█▏        | 27/221 [00:08<00:46,  4.15it/s][A
 13%|█▎        | 28/221 [00:08<00:51,  3.71it/s][A
 13%|█▎        | 29/221 [00:09<01:07,  2.86it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.28it/s][A
 14%|█▍        | 31/221 [00:09<00:52,  3.63it/s][A
 14%|█▍        | 32/221 [00:10<00:58,  3.22it/s][A
 15%|█▍        | 33/221 [00:10<00:51,  3.62it/s][A
 15%|█▌        | 34/221 [00:10<00:50,  3.71it/s][A
 16%|█▌        | 35/221 [00:10<00:42,  4.33it/s][A
 16%|█▋        | 36/221 [00:11<00:54,  3.42it/s][A
 17%|█▋        | 37/221 [00:11<00:52,  3.52it/s][A
 17%|█▋        | 38/221 [00:11<00:50,  3.65it/s][A
 18%|█▊        | 39/221 [00:11<00:47,  3.83it/s][A
 18%|█▊        | 40/221 [00:12<00:43,  4.15it/s][A
 19%|█▊        | 41/221 [00:12<00:40,  4.49it/s][A
 19%|█▉        | 42/221 [00:12<00:35,  5.03it/s][A
 19%|█▉        | 43/221 [00:12<00:44,  3.98it/s][A
 20%|█▉        | 44/221 [00:13<00:45,  3.87it/s][A
 20%|██        | 45/221 [00:13<01:04,  2.75it/s][A
 21%|██        | 46/221 [00:14<01:05,  2.67it/s][A
 21%|██▏       | 47/221 [00:14<00:53,  3.28it/s][A
 22%|██▏       | 48/221 [00:14<00:43,  4.01it/s][A
 23%|██▎       | 50/221 [00:14<00:43,  3.94it/s][A
 23%|██▎       | 51/221 [00:15<00:40,  4.18it/s][A
 24%|██▎       | 52/221 [00:15<00:36,  4.67it/s][A
 24%|██▍       | 53/221 [00:15<00:44,  3.77it/s][A
 24%|██▍       | 54/221 [00:15<00:47,  3.53it/s][A
 25%|██▍       | 55/221 [00:16<00:42,  3.95it/s][A
 25%|██▌       | 56/221 [00:16<00:44,  3.72it/s][A
 26%|██▌       | 57/221 [00:16<00:46,  3.51it/s][A
 26%|██▌       | 58/221 [00:16<00:40,  3.98it/s][A
 27%|██▋       | 59/221 [00:17<00:39,  4.08it/s][A
 27%|██▋       | 60/221 [00:17<00:36,  4.43it/s][A
 28%|██▊       | 61/221 [00:17<00:42,  3.73it/s][A
 28%|██▊       | 62/221 [00:17<00:41,  3.87it/s][A
 29%|██▊       | 63/221 [00:18<00:39,  4.00it/s][A
 29%|██▉       | 64/221 [00:18<00:33,  4.64it/s][A
 29%|██▉       | 65/221 [00:18<00:35,  4.35it/s][A
 30%|██▉       | 66/221 [00:18<00:44,  3.50it/s][A
 30%|███       | 67/221 [00:19<00:48,  3.20it/s][A
 31%|███       | 68/221 [00:19<00:51,  2.96it/s][A
 31%|███       | 69/221 [00:20<01:15,  2.02it/s][A
 32%|███▏      | 70/221 [00:20<01:03,  2.37it/s][A
 32%|███▏      | 71/221 [00:21<01:00,  2.49it/s][A
 33%|███▎      | 72/221 [00:21<00:55,  2.71it/s][A
 33%|███▎      | 73/221 [00:21<00:53,  2.77it/s][A
 33%|███▎      | 74/221 [00:21<00:44,  3.31it/s][A
 34%|███▍      | 75/221 [00:22<00:46,  3.16it/s][A
 34%|███▍      | 76/221 [00:22<00:46,  3.14it/s][A
 35%|███▍      | 77/221 [00:22<00:38,  3.76it/s][A
 35%|███▌      | 78/221 [00:23<00:42,  3.35it/s][A
 36%|███▌      | 79/221 [00:23<00:35,  4.01it/s][A
 36%|███▌      | 80/221 [00:23<00:34,  4.10it/s][A
 37%|███▋      | 81/221 [00:23<00:31,  4.48it/s][A
 37%|███▋      | 82/221 [00:24<00:34,  4.03it/s][A
 38%|███▊      | 83/221 [00:24<00:33,  4.14it/s][A
 38%|███▊      | 84/221 [00:24<00:43,  3.15it/s][A
 38%|███▊      | 85/221 [00:25<00:43,  3.13it/s][A
 39%|███▉      | 86/221 [00:25<00:34,  3.89it/s][A
 39%|███▉      | 87/221 [00:25<00:44,  3.00it/s][A
 40%|███▉      | 88/221 [00:25<00:41,  3.24it/s][A
 40%|████      | 89/221 [00:26<00:47,  2.80it/s][A
 41%|████      | 90/221 [00:26<00:48,  2.71it/s][A
 41%|████      | 91/221 [00:26<00:38,  3.35it/s][A
 42%|████▏     | 92/221 [00:27<00:35,  3.60it/s][A
 42%|████▏     | 93/221 [00:27<00:40,  3.17it/s][A
 43%|████▎     | 94/221 [00:27<00:41,  3.06it/s][A
 43%|████▎     | 95/221 [00:28<00:58,  2.16it/s][A
 43%|████▎     | 96/221 [00:29<00:52,  2.38it/s][A
 44%|████▍     | 97/221 [00:29<00:48,  2.55it/s][A
 44%|████▍     | 98/221 [00:29<00:50,  2.44it/s][A
 45%|████▍     | 99/221 [00:30<00:45,  2.68it/s][A
 45%|████▌     | 100/221 [00:30<00:46,  2.62it/s][A
 46%|████▌     | 101/221 [00:30<00:39,  3.00it/s][A
 46%|████▌     | 102/221 [00:31<00:39,  2.99it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.33it/s][A
 48%|████▊     | 105/221 [00:31<00:26,  4.41it/s][A
 48%|████▊     | 106/221 [00:31<00:30,  3.79it/s][A
 49%|████▉     | 108/221 [00:32<00:21,  5.22it/s][A
 49%|████▉     | 109/221 [00:32<00:21,  5.17it/s][A
 50%|████▉     | 110/221 [00:32<00:22,  4.87it/s][A
 50%|█████     | 111/221 [00:32<00:25,  4.26it/s][A
 51%|█████     | 112/221 [00:33<00:30,  3.63it/s][A
 51%|█████     | 113/221 [00:33<00:29,  3.64it/s][A
 52%|█████▏    | 114/221 [00:33<00:24,  4.41it/s][A
 52%|█████▏    | 116/221 [00:33<00:20,  5.11it/s][A
 53%|█████▎    | 117/221 [00:34<00:20,  4.96it/s][A
 53%|█████▎    | 118/221 [00:34<00:23,  4.37it/s][A
 54%|█████▍    | 119/221 [00:34<00:28,  3.58it/s][A
 54%|█████▍    | 120/221 [00:35<00:30,  3.27it/s][A
 55%|█████▍    | 121/221 [00:35<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:35<00:30,  3.27it/s][A
 56%|█████▌    | 123/221 [00:36<00:26,  3.66it/s][A
 56%|█████▌    | 124/221 [00:36<00:34,  2.84it/s][A
 57%|█████▋    | 125/221 [00:37<00:37,  2.57it/s][A
 57%|█████▋    | 126/221 [00:37<00:33,  2.81it/s][A
 57%|█████▋    | 127/221 [00:37<00:33,  2.82it/s][A
 58%|█████▊    | 128/221 [00:37<00:30,  3.08it/s][A
 58%|█████▊    | 129/221 [00:38<00:24,  3.75it/s][A
 59%|█████▉    | 130/221 [00:38<00:22,  4.01it/s][A
 59%|█████▉    | 131/221 [00:38<00:20,  4.32it/s][A
 60%|█████▉    | 132/221 [00:38<00:27,  3.25it/s][A
 60%|██████    | 133/221 [00:39<00:30,  2.85it/s][A
 61%|██████    | 134/221 [00:39<00:31,  2.74it/s][A
 61%|██████    | 135/221 [00:40<00:28,  2.99it/s][A
 62%|██████▏   | 136/221 [00:40<00:28,  2.98it/s][A
 62%|██████▏   | 137/221 [00:40<00:26,  3.15it/s][A
 62%|██████▏   | 138/221 [00:40<00:24,  3.45it/s][A
 63%|██████▎   | 139/221 [00:41<00:29,  2.82it/s][A
 63%|██████▎   | 140/221 [00:41<00:25,  3.18it/s][A
 64%|██████▍   | 141/221 [00:41<00:23,  3.34it/s][A
 64%|██████▍   | 142/221 [00:42<00:20,  3.78it/s][A
 65%|██████▍   | 143/221 [00:42<00:23,  3.28it/s][A
 65%|██████▌   | 144/221 [00:42<00:21,  3.54it/s][A
 66%|██████▌   | 145/221 [00:43<00:27,  2.76it/s][A
 66%|██████▌   | 146/221 [00:43<00:25,  2.94it/s][A
 67%|██████▋   | 147/221 [00:43<00:20,  3.59it/s][A
 67%|██████▋   | 148/221 [00:44<00:29,  2.46it/s][A
 67%|██████▋   | 149/221 [00:44<00:24,  2.93it/s][A
 68%|██████▊   | 150/221 [00:44<00:21,  3.23it/s][A
 68%|██████▊   | 151/221 [00:45<00:34,  2.03it/s][A
 69%|██████▉   | 152/221 [00:46<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [00:46<00:31,  2.16it/s][A
 70%|██████▉   | 154/221 [00:46<00:25,  2.64it/s][A
 70%|███████   | 155/221 [00:47<00:22,  2.91it/s][A
 71%|███████   | 156/221 [00:47<00:20,  3.20it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.36it/s][A
 71%|███████▏  | 158/221 [00:47<00:18,  3.42it/s][A
 72%|███████▏  | 159/221 [00:48<00:16,  3.80it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.23it/s][A
 73%|███████▎  | 162/221 [00:48<00:12,  4.68it/s][A
 74%|███████▍  | 163/221 [00:48<00:12,  4.65it/s][A
 74%|███████▍  | 164/221 [00:49<00:13,  4.34it/s][A
 75%|███████▍  | 165/221 [00:49<00:13,  4.07it/s][A
 75%|███████▌  | 166/221 [00:49<00:16,  3.37it/s][A
 76%|███████▌  | 167/221 [00:50<00:14,  3.70it/s][A
 76%|███████▌  | 168/221 [00:50<00:13,  3.82it/s][A
 77%|███████▋  | 170/221 [00:50<00:13,  3.89it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.20it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.43it/s][A
 78%|███████▊  | 173/221 [00:51<00:14,  3.34it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.26it/s][A
 79%|███████▉  | 175/221 [00:52<00:15,  3.06it/s][A
 80%|███████▉  | 176/221 [00:52<00:12,  3.74it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.16it/s][A
 81%|████████  | 178/221 [00:53<00:15,  2.79it/s][A
 81%|████████  | 179/221 [00:53<00:14,  2.84it/s][A
 81%|████████▏ | 180/221 [00:54<00:11,  3.53it/s][A
 82%|████████▏ | 181/221 [00:54<00:12,  3.21it/s][A
 82%|████████▏ | 182/221 [00:54<00:10,  3.85it/s][A
 83%|████████▎ | 183/221 [00:54<00:10,  3.73it/s][A
 83%|████████▎ | 184/221 [00:55<00:10,  3.42it/s][A
 84%|████████▎ | 185/221 [00:55<00:09,  3.65it/s][A
 84%|████████▍ | 186/221 [00:55<00:12,  2.90it/s][A
 85%|████████▌ | 188/221 [00:56<00:08,  3.87it/s][A
 86%|████████▌ | 189/221 [00:56<00:07,  4.12it/s][A
 86%|████████▌ | 190/221 [00:56<00:08,  3.79it/s][A
 86%|████████▋ | 191/221 [00:56<00:06,  4.47it/s][A
 87%|████████▋ | 192/221 [00:57<00:06,  4.43it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.73it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.48it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.52it/s][A
 89%|████████▊ | 196/221 [00:58<00:08,  2.97it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.29it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  3.05it/s][A
 90%|█████████ | 199/221 [00:59<00:06,  3.49it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.88it/s][A
 91%|█████████ | 201/221 [00:59<00:04,  4.35it/s][A
 91%|█████████▏| 202/221 [00:59<00:04,  4.66it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.49it/s][A
 92%|█████████▏| 204/221 [01:00<00:03,  4.51it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.68it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  3.82it/s][A
 94%|█████████▎| 207/221 [01:01<00:04,  3.12it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.44it/s][A
 95%|█████████▍| 209/221 [01:01<00:03,  3.38it/s][A
 95%|█████████▌| 210/221 [01:02<00:03,  3.44it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.62it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.58it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.53it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  2.65it/s][A
 97%|█████████▋| 215/221 [01:03<00:02,  2.88it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  2.83it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  2.88it/s][A
 99%|█████████▊| 218/221 [01:04<00:01,  2.70it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  2.69it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.11it/s][A
100%|██████████| 221/221 [01:05<00:00,  2.82it/s][A100%|██████████| 221/221 [01:05<00:00,  3.35it/s]
09/16/2024 16:30:46 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 49--===========

09/16/2024 16:30:46 - INFO - __main__ -   {'area_r1': 33.3, 'area_recall': '33.3/54.4/60.5', 'area_ravg': 49.4}
09/16/2024 16:30:46 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 49--===========

09/16/2024 16:30:46 - INFO - __main__ -   {'forward_r1': 36.1, 'forward_recall': '36.1/67.1/77.6', 'forward_ravg': 60.3}
09/16/2024 16:30:46 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video=====step 49--===========

09/16/2024 16:30:46 - INFO - __main__ -   {'area_video_r1': 36.9, 'area_video_recall': '36.9/67.2/77.9', 'area_video_ravg': 60.7}
09/16/2024 16:30:46 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video====history best step: 49=======

09/16/2024 16:30:46 - INFO - __main__ -   {'area_video_r1': 36.9, 'area_video_recall': '36.9/67.2/77.9', 'area_video_ravg': 60.7}
09/16/2024 16:30:46 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_area=====step 49--===========

09/16/2024 16:30:46 - INFO - __main__ -   {'area_video_r1': 49.3, 'area_video_recall': '49.3/69.6/76.6', 'area_video_ravg': 65.2, 'area_video_back_r1': 45.4, 'area_video_back_recall': '45.4/70.5/80.0', 'area_video_back_ravg': 65.3}
09/16/2024 16:30:46 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_area====history best step: 49=======

09/16/2024 16:30:46 - INFO - __main__ -   {'area_video_r1': 49.3, 'area_video_recall': '49.3/69.6/76.6', 'area_video_ravg': 65.2, 'area_video_back_r1': 45.4, 'area_video_back_recall': '45.4/70.5/80.0', 'area_video_back_ravg': 65.3}
09/16/2024 16:30:46 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 49--===========

09/16/2024 16:30:46 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/65.4/73.8', 'video_ravg': 58.7}
09/16/2024 16:30:46 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 49=======

09/16/2024 16:30:46 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/65.4/73.8', 'video_ravg': 58.7}
09/16/2024 16:30:46 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 49--===========

09/16/2024 16:30:46 - INFO - __main__ -   {'video_r1': 50.8, 'video_recall': '50.8/71.6/79.4', 'video_ravg': 67.3}
09/16/2024 16:30:46 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 49=======

09/16/2024 16:30:46 - INFO - __main__ -   {'video_r1': 50.8, 'video_recall': '50.8/71.6/79.4', 'video_ravg': 67.3}
09/16/2024 16:31:25 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.00734258908778429, 'loss_ret%tv%ta--finetune_area/loss_area': 4.413893699645996, 'loss_ret%tv%ta--finetune_area/total_loss': 4.421236515045166}
  2%|▏         | 50/2755 [18:50<106:11:04, 141.32s/it][h264 @ 0x557add9ef740] mmco: unref short failure
  2%|▏         | 51/2755 [18:54<75:07:13, 100.01s/it]   2%|▏         | 52/2755 [18:58<53:23:56, 71.12s/it] [h264 @ 0x557ae7683100] mmco: unref short failure
  2%|▏         | 53/2755 [19:02<38:18:53, 51.05s/it][h264 @ 0x5617678903c0] mmco: unref short failure
[h264 @ 0x5617678903c0] mmco: unref short failure
  2%|▏         | 54/2755 [19:06<27:48:01, 37.05s/it]  2%|▏         | 55/2755 [19:11<20:28:21, 27.30s/it]  2%|▏         | 56/2755 [19:16<15:27:02, 20.61s/it][h264 @ 0x56176ba50800] mmco: unref short failure
  2%|▏         | 57/2755 [19:22<12:09:55, 16.23s/it]  2%|▏         | 58/2755 [19:28<9:51:47, 13.17s/it] [h264 @ 0x5615e3191880] mmco: unref short failure
[h264 @ 0x5615e3191880] mmco: unref short failure
  2%|▏         | 59/2755 [19:34<8:14:34, 11.01s/it]09/16/2024 16:32:13 - INFO - __main__ -   current idx eBDMGQmep9I.8 from finetune_area returns wrong image/video, use 115224 instead.
[h264 @ 0x557ae05ad5c0] mmco: unref short failure
[h264 @ 0x557ae05ad5c0] mmco: unref short failure
  2%|▏         | 60/2755 [19:40<7:04:17,  9.45s/it][h264 @ 0x5615e2b68140] mmco: unref short failure
  2%|▏         | 61/2755 [19:45<6:06:27,  8.16s/it]  2%|▏         | 62/2755 [19:51<5:35:17,  7.47s/it]  2%|▏         | 63/2755 [19:56<5:00:58,  6.71s/it][h264 @ 0x56177ec9f940] mmco: unref short failure
[h264 @ 0x56177ec9f940] mmco: unref short failure
  2%|▏         | 64/2755 [20:01<4:46:05,  6.38s/it][h264 @ 0x55f3da68a100] mmco: unref short failure
  2%|▏         | 65/2755 [20:07<4:34:20,  6.12s/it][h264 @ 0x55f3eb8dc640] mmco: unref short failure
[h264 @ 0x55f3eb8dc640] mmco: unref short failure
[h264 @ 0x55f3eb8dc640] mmco: unref short failure
[h264 @ 0x55f3eb8dc640] mmco: unref short failure
[h264 @ 0x56176e317440] mmco: unref short failure
[h264 @ 0x56176e317440] mmco: unref short failure
[h264 @ 0x557af1e80040] mmco: unref short failure
[h264 @ 0x5615e2cc6cc0] mmco: unref short failure
[h264 @ 0x557adf70b340] mmco: unref short failure
[h264 @ 0x557adf70b340] mmco: unref short failure
[h264 @ 0x56177dacfdc0] mmco: unref short failure
[h264 @ 0x56177dacfdc0] mmco: unref short failure
[h264 @ 0x56177dacfdc0] mmco: unref short failure
[h264 @ 0x56177dacfdc0] mmco: unref short failure
[h264 @ 0x56177dacfdc0] mmco: unref short failure
[h264 @ 0x56177dacfdc0] mmco: unref short failure
[h264 @ 0x5615e7d73cc0] mmco: unref short failure
[h264 @ 0x5615e7d73cc0] mmco: unref short failure
[h264 @ 0x5615e7d73cc0] mmco: unref short failure
[h264 @ 0x5615e7d73cc0] mmco: unref short failure
[h264 @ 0x56176788ff40] mmco: unref short failure
[h264 @ 0x56176788ff40] mmco: unref short failure
[h264 @ 0x55f3d70e1400] mmco: unref short failure
[h264 @ 0x55f3d70e1400] mmco: unref short failure
[h264 @ 0x56176c2a0400] mmco: unref short failure
[h264 @ 0x56176c2a0400] mmco: unref short failure
[h264 @ 0x55f3dea55000] mmco: unref short failure
[h264 @ 0x55f3e5cccac0] mmco: unref short failure
[h264 @ 0x55f3e5cccac0] mmco: unref short failure
  2%|▏         | 66/2755 [20:51<13:00:45, 17.42s/it][h264 @ 0x557af337b400] mmco: unref short failure
[h264 @ 0x557af337b400] mmco: unref short failure
[h264 @ 0x557adb5753c0] mmco: unref short failure
[h264 @ 0x557adb5753c0] mmco: unref short failure
[h264 @ 0x561766168ec0] mmco: unref short failure
[h264 @ 0x561766168ec0] mmco: unref short failure
[h264 @ 0x5615d2e8da40] mmco: unref short failure
[h264 @ 0x5615d2e8da40] mmco: unref short failure
  2%|▏         | 67/2755 [21:08<12:58:05, 17.37s/it][h264 @ 0x56177238ea40] mmco: unref short failure
[h264 @ 0x56177238ea40] mmco: unref short failure
  2%|▏         | 68/2755 [21:21<12:01:51, 16.12s/it][h264 @ 0x5615d8a1c0c0] mmco: unref short failure
[h264 @ 0x5615d8a1c0c0] mmco: unref short failure
[h264 @ 0x557ae5e02c80] mmco: unref short failure
[h264 @ 0x557ae5e02c80] mmco: unref short failure
[h264 @ 0x561775d10680] mmco: unref short failure
[h264 @ 0x561775d10680] mmco: unref short failure
  3%|▎         | 69/2755 [21:28<9:57:44, 13.35s/it] [h264 @ 0x561775d10680] mmco: unref short failure
[h264 @ 0x561775d10680] mmco: unref short failure
[h264 @ 0x55f3e343c940] mmco: unref short failure
[h264 @ 0x55f3e343c940] mmco: unref short failure
09/16/2024 16:34:13 - INFO - __main__ -   current idx bLVzqC3fftg.6 from finetune_area returns wrong image/video, use 78415 instead.
  3%|▎         | 70/2755 [21:38<9:12:17, 12.34s/it]  3%|▎         | 71/2755 [21:48<8:38:36, 11.59s/it][h264 @ 0x557ae34edf80] mmco: unref short failure
[h264 @ 0x557ae34edf80] mmco: unref short failure
  3%|▎         | 72/2755 [21:53<7:19:34,  9.83s/it][h264 @ 0x56177d2e5640] mmco: unref short failure
  3%|▎         | 73/2755 [21:59<6:23:31,  8.58s/it][h264 @ 0x5615d20b6dc0] mmco: unref short failure
09/16/2024 16:34:44 - INFO - __main__ -   current idx _9yTXjXc3Ko.34 from finetune_area returns wrong image/video, use 45215 instead.
[h264 @ 0x56176ebef900] mmco: unref short failure
[h264 @ 0x56176ebef900] mmco: unref short failure
[h264 @ 0x5615dc9d4940] mmco: unref short failure
[h264 @ 0x557af6c39640] mmco: unref short failure
[h264 @ 0x557aeb86bb00] mmco: unref short failure
[h264 @ 0x557aeb86bb00] mmco: unref short failure
[h264 @ 0x5615dcabb340] mmco: unref short failure
[h264 @ 0x5615dcabb340] mmco: unref short failure
[h264 @ 0x56177871b5c0] mmco: unref short failure
[h264 @ 0x56177db8d5c0] mmco: unref short failure
[h264 @ 0x55f3d5855800] mmco: unref short failure
09/16/2024 16:35:29 - INFO - __main__ -   current idx fbV8HQ0iF2U.27 from finetune_area returns wrong image/video, use 60650 instead.
[h264 @ 0x55f3db89b480] mmco: unref short failure
[h264 @ 0x56177dacf480] mmco: unref short failure
[h264 @ 0x56177dacf480] mmco: unref short failure
  3%|▎         | 74/2755 [23:11<20:30:52, 27.55s/it][h264 @ 0x557ae87ddc80] mmco: unref short failure
[h264 @ 0x557ae87ddc80] mmco: unref short failure
  3%|▎         | 75/2755 [23:25<17:29:32, 23.50s/it][h264 @ 0x56177d211b40] mmco: unref short failure
  3%|▎         | 76/2755 [23:45<16:47:08, 22.56s/it]  3%|▎         | 77/2755 [23:51<12:59:01, 17.45s/it]09/16/2024 16:36:28 - INFO - __main__ -   current idx KKgWOFJTjqo.8 from finetune_area returns wrong image/video, use 46959 instead.
[h264 @ 0x5615e49cee40] mmco: unref short failure
[h264 @ 0x5615dc5e5440] mmco: unref short failure
[h264 @ 0x5615dc5e5440] mmco: unref short failure
  3%|▎         | 78/2755 [24:04<11:59:09, 16.12s/it]09/16/2024 16:36:42 - INFO - __main__ -   current idx fYJGKzrM0os.20 from finetune_area returns wrong image/video, use 119137 instead.
  3%|▎         | 79/2755 [24:09<9:32:46, 12.84s/it]   3%|▎         | 80/2755 [24:15<7:58:44, 10.74s/it]  3%|▎         | 81/2755 [24:20<6:41:49,  9.02s/it][h264 @ 0x55f3eed7c480] mmco: unref short failure
[h264 @ 0x55f3ed08f7c0] mmco: unref short failure
[h264 @ 0x557ae48ede40] mmco: unref short failure
[h264 @ 0x557ae48ede40] mmco: unref short failure
[h264 @ 0x557aea375540] mmco: unref short failure
[h264 @ 0x5617660a18c0] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x557ae0087e00] mmco: unref short failure
[h264 @ 0x55f3d90b3b80] mmco: unref short failure
[h264 @ 0x55f3d90b3b80] mmco: unref short failure
[h264 @ 0x557adf309440] mmco: unref short failure
[h264 @ 0x557aee4d9700] mmco: unref short failure
[h264 @ 0x557aee4d9700] mmco: unref short failure
09/16/2024 16:37:44 - INFO - __main__ -   current idx EavSblb031o.10 from finetune_area returns wrong image/video, use 42861 instead.
[h264 @ 0x557aee4d9700] mmco: unref short failure
[h264 @ 0x557aee4d9700] mmco: unref short failure
[h264 @ 0x557aeba8e500] mmco: unref short failure
[h264 @ 0x557aeba8e500] mmco: unref short failure
[h264 @ 0x5615e7b2ee00] mmco: unref short failure
[h264 @ 0x557af0c64f40] mmco: unref short failure
[h264 @ 0x5615e314df40] mmco: unref short failure
[h264 @ 0x5615f13a32c0] mmco: unref short failure
[h264 @ 0x5615f13a32c0] mmco: unref short failure
  3%|▎         | 82/2755 [25:34<21:12:43, 28.57s/it][h264 @ 0x55f3d2d4ac00] mmco: unref short failure
[h264 @ 0x5615d201b480] mmco: unref short failure
[h264 @ 0x5615d2e8d5c0] mmco: unref short failure
[h264 @ 0x557adb510cc0] mmco: unref short failure
[h264 @ 0x557adb510cc0] mmco: unref short failure
[h264 @ 0x5615d60bfec0] mmco: unref short failure
[h264 @ 0x5615d60bfec0] mmco: unref short failure
[h264 @ 0x557ae438fd40] mmco: unref short failure
[h264 @ 0x561781532fc0] mmco: unref short failure
[h264 @ 0x561781532fc0] mmco: unref short failure
[h264 @ 0x557af75c8fc0] mmco: unref short failure
not have audios GAwav3sZcGw.4
  3%|▎         | 83/2755 [25:55<19:29:23, 26.26s/it][h264 @ 0x5615df102140] mmco: unref short failure
[h264 @ 0x56177fade840] mmco: unref short failure
[h264 @ 0x56177fade840] mmco: unref short failure
  3%|▎         | 84/2755 [26:03<15:22:25, 20.72s/it]  3%|▎         | 85/2755 [26:10<12:26:56, 16.79s/it][h264 @ 0x55f3d4dbde80] mmco: unref short failure
[h264 @ 0x55f3d4dbde80] mmco: unref short failure
[h264 @ 0x561779266f40] mmco: unref short failure
[h264 @ 0x561779266f40] mmco: unref short failure
[h264 @ 0x5615d60ed680] mmco: unref short failure
[h264 @ 0x55f3e9593940] mmco: unref short failure
  3%|▎         | 86/2755 [26:30<12:58:32, 17.50s/it][h264 @ 0x557af6f6f880] mmco: unref short failure
  3%|▎         | 87/2755 [26:35<10:23:55, 14.03s/it][h264 @ 0x557af0422840] mmco: unref short failure
[h264 @ 0x557af0422840] mmco: unref short failure
09/16/2024 16:39:18 - INFO - __main__ -   current idx H3sDc6_8nAc.67 from finetune_area returns wrong image/video, use 68934 instead.
  3%|▎         | 88/2755 [26:42<8:40:25, 11.71s/it] 09/16/2024 16:39:21 - INFO - __main__ -   current idx 1ryAZ-G9Vu8.4 from finetune_area returns wrong image/video, use 122841 instead.
  3%|▎         | 89/2755 [26:47<7:19:56,  9.90s/it][h264 @ 0x561769521940] mmco: unref short failure
[h264 @ 0x5615e1766900] mmco: unref short failure
[h264 @ 0x5615e1766900] mmco: unref short failure
[h264 @ 0x5617668d78c0] mmco: unref short failure
[h264 @ 0x5617668d78c0] mmco: unref short failure
[h264 @ 0x557af24629c0] mmco: unref short failure
[h264 @ 0x557af24629c0] mmco: unref short failure
[h264 @ 0x55f3edb963c0] mmco: unref short failure
[h264 @ 0x55f3eadc3740] mmco: unref short failure
[h264 @ 0x56177d1cbf40] mmco: unref short failure
[h264 @ 0x56177d1cbf40] mmco: unref short failure
[h264 @ 0x56177904a080] mmco: unref short failure
[h264 @ 0x557aeab79cc0] mmco: unref short failure
[h264 @ 0x557aeab79cc0] mmco: unref short failure
[h264 @ 0x5615ddf26b40] mmco: unref short failure
[h264 @ 0x5615da64ae00] mmco: unref short failure
  3%|▎         | 90/2755 [27:56<20:18:35, 27.44s/it]09/16/2024 16:40:33 - INFO - __main__ -   current idx 9Dbcu8BMaBw.7 from finetune_area returns wrong image/video, use 78237 instead.
[h264 @ 0x557ae1913540] mmco: unref short failure
[h264 @ 0x557ae1913540] mmco: unref short failure
[h264 @ 0x55f3d2810b80] mmco: unref short failure
[h264 @ 0x55f3d2810b80] mmco: unref short failure
[h264 @ 0x55f3df362e00] mmco: unref short failure
  3%|▎         | 91/2755 [28:14<18:19:26, 24.76s/it][h264 @ 0x5615de60ae80] mmco: unref short failure
[h264 @ 0x557ae52730c0] mmco: unref short failure
  3%|▎         | 92/2755 [28:23<14:37:58, 19.78s/it][h264 @ 0x5617824d9580] mmco: unref short failure
[h264 @ 0x5617824d9580] mmco: unref short failure
[h264 @ 0x5617824d9580] mmco: unref short failure
[h264 @ 0x5617824d9580] mmco: unref short failure
[h264 @ 0x5615d3f965c0] mmco: unref short failure
  3%|▎         | 93/2755 [28:35<12:57:53, 17.53s/it][h264 @ 0x55f3e1baa640] mmco: unref short failure
09/16/2024 16:41:16 - INFO - __main__ -   current idx WdVJ8VSAKso.55 from finetune_area returns wrong image/video, use 58713 instead.
[h264 @ 0x561770af5400] mmco: unref short failure
[h264 @ 0x55f3d23e0680] mmco: unref short failure
[h264 @ 0x55f3d23e0680] mmco: unref short failure
[h264 @ 0x56177d6d8a80] mmco: unref short failure
  3%|▎         | 94/2755 [28:52<12:54:32, 17.46s/it]  3%|▎         | 95/2755 [28:57<10:10:52, 13.78s/it][h264 @ 0x55f3d4da2bc0] mmco: unref short failure
[h264 @ 0x55f3d4da2bc0] mmco: unref short failure
  3%|▎         | 96/2755 [29:04<8:31:34, 11.54s/it] [h264 @ 0x55f3ee086f80] mmco: unref short failure
[h264 @ 0x5615eb758080] mmco: unref short failure
[h264 @ 0x5615eb758080] mmco: unref short failure
[h264 @ 0x55f3ee086f80] mmco: unref short failure
  4%|▎         | 97/2755 [29:09<7:09:44,  9.70s/it][h264 @ 0x55f3d74a11c0] mmco: unref short failure
[h264 @ 0x55f3d74a11c0] mmco: unref short failure
09/16/2024 16:42:03 - INFO - __main__ -   current idx ANmim71qNrM.86 from finetune_area returns wrong image/video, use 128394 instead.
[h264 @ 0x5615e8e0a340] mmco: unref short failure
[h264 @ 0x5615d8aedbc0] mmco: unref short failure
[h264 @ 0x5615d8aedbc0] mmco: unref short failure
[h264 @ 0x5615ecdd0ec0] mmco: unref short failure
[h264 @ 0x5615ecdd0ec0] mmco: unref short failure
[h264 @ 0x557ae2419080] mmco: unref short failure
[h264 @ 0x557ae2419080] mmco: unref short failure
[h264 @ 0x557ae2419080] mmco: unref short failure
[h264 @ 0x557ae2419080] mmco: unref short failure
09/16/2024 16:42:26 - INFO - __main__ -   current idx 2N3kUMwK_eA.16 from finetune_area returns wrong image/video, use 54473 instead.
[h264 @ 0x557aee8e0300] mmco: unref short failure
[h264 @ 0x5615e2f9f140] mmco: unref short failure
[h264 @ 0x5615d148df00] mmco: unref short failure
[h264 @ 0x5615d148df00] mmco: unref short failure
[h264 @ 0x5615d148df00] mmco: unref short failure
[h264 @ 0x5615d148df00] mmco: unref short failure
[h264 @ 0x5615db695580] mmco: unref short failure
[h264 @ 0x5615db695580] mmco: unref short failure
09/16/2024 16:42:49 - INFO - __main__ -   current idx HHAHhge9hOk.66 from finetune_area returns wrong image/video, use 109821 instead.
  4%|▎         | 98/2755 [30:18<20:18:28, 27.52s/it][h264 @ 0x55f3d9c74540] mmco: unref short failure
[h264 @ 0x55f3d9c74540] mmco: unref short failure
[h264 @ 0x557aedbc2580] mmco: unref short failure
[h264 @ 0x557aedbc2580] mmco: unref short failure
[h264 @ 0x55f3f0ec6bc0] mmco: unref short failure
[h264 @ 0x5615ecdd0c80] mmco: unref short failure
[h264 @ 0x5615ecdd0c80] mmco: unref short failure
[h264 @ 0x5615ecdd0c80] mmco: unref short failure
[h264 @ 0x5615ecdd0c80] mmco: unref short failure
09/16/2024 16:43:07 - INFO - __main__ -   current idx hDAA1fIvxDY.2 from finetune_area returns wrong image/video, use 54335 instead.
[h264 @ 0x55f3ee125040] mmco: unref short failure
[h264 @ 0x55f3d8afb3c0] mmco: unref short failure
[h264 @ 0x55f3d8afb3c0] mmco: unref short failure
[h264 @ 0x55f3d8afb3c0] mmco: unref short failure
[h264 @ 0x55f3d8afb3c0] mmco: unref short failure
[h264 @ 0x5615efb67840] mmco: unref short failure
  4%|▎         | 99/2755 [30:38<18:37:00, 25.23s/it]09/16/2024 16:43:15 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
09/16/2024 16:43:15 - INFO - __main__ -   start running ret%tva validation...
[h264 @ 0x5615d7d70e40] mmco: unref short failure
[h264 @ 0x5615d7d70e40] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55f3e76a59c0] mmco: unref short failure
[h264 @ 0x55f3e76a59c0] mmco: unref short failure
[h264 @ 0x55f3e9958940] mmco: unref short failure
[h264 @ 0x55f3e9958940] mmco: unref short failure
[h264 @ 0x5615f0ada800] mmco: unref short failure
[h264 @ 0x5615f0ada800] mmco: unref short failure
[h264 @ 0x561774214e80] mmco: unref short failure
[h264 @ 0x561774214e80] mmco: unref short failure
[h264 @ 0x55f3e6a7de40] mmco: unref short failure
[h264 @ 0x55f3e6a7de40] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55f3f0dfc180] mmco: unref short failure
[h264 @ 0x55f3f0dfc180] mmco: unref short failure
[h264 @ 0x557ae83b8a80] mmco: unref short failure
[h264 @ 0x557ae83b8a80] mmco: unref short failure
[h264 @ 0x5615da6dcd00] mmco: unref short failure
[h264 @ 0x5615da6dcd00] mmco: unref short failure
[h264 @ 0x5615f0aa9940] mmco: unref short failure
[h264 @ 0x5615f0aa9940] mmco: unref short failure
[h264 @ 0x5615f0aa9940] mmco: unref short failure
[h264 @ 0x5615f0aa9940] mmco: unref short failure
[h264 @ 0x5617750a6f00] mmco: unref short failure
[h264 @ 0x5617750a6f00] mmco: unref short failure
[h264 @ 0x56177fa024c0] mmco: unref short failure
[h264 @ 0x56177fa024c0] mmco: unref short failure
[h264 @ 0x561765df7980] mmco: unref short failure
[h264 @ 0x561765df7980] mmco: unref short failure
[h264 @ 0x5615d3815840] mmco: unref short failure
[h264 @ 0x5615d3815840] mmco: unref short failure
[h264 @ 0x5617812c4700] mmco: unref short failure
[h264 @ 0x5617812c4700] mmco: unref short failure
[h264 @ 0x56176b6cf180] mmco: unref short failure
[h264 @ 0x557afa13a6c0] mmco: unref short failure
[h264 @ 0x557afa13a6c0] mmco: unref short failure
[h264 @ 0x55f3e0bbcac0] mmco: unref short failure
[h264 @ 0x55f3e0bbcac0] mmco: unref short failure
[h264 @ 0x55f3e0bbcac0] mmco: unref short failure
[h264 @ 0x55f3e0bbcac0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:32,  2.38it/s][A
  1%|          | 2/221 [00:00<01:31,  2.38it/s][A
  1%|▏         | 3/221 [00:01<01:30,  2.42it/s][A
  2%|▏         | 4/221 [00:01<01:08,  3.16it/s][A
  2%|▏         | 5/221 [00:01<01:03,  3.43it/s][A
  3%|▎         | 7/221 [00:02<00:49,  4.34it/s][A
  4%|▎         | 8/221 [00:02<00:52,  4.10it/s][A
  4%|▍         | 9/221 [00:02<00:49,  4.27it/s][A
  5%|▍         | 10/221 [00:02<00:53,  3.95it/s][A
  5%|▌         | 12/221 [00:03<00:46,  4.51it/s][A
  6%|▌         | 13/221 [00:03<00:44,  4.70it/s][A
  6%|▋         | 14/221 [00:03<00:44,  4.69it/s][A
  7%|▋         | 15/221 [00:03<00:43,  4.73it/s][A
  7%|▋         | 16/221 [00:04<00:56,  3.63it/s][A
  8%|▊         | 17/221 [00:04<01:14,  2.73it/s][A
  8%|▊         | 18/221 [00:05<01:10,  2.90it/s][A
  9%|▊         | 19/221 [00:05<00:59,  3.42it/s][A
  9%|▉         | 20/221 [00:05<00:51,  3.92it/s][A
 10%|▉         | 21/221 [00:05<00:50,  3.96it/s][A
 10%|▉         | 22/221 [00:06<01:06,  3.00it/s][A[h264 @ 0x557af2e7fec0] mmco: unref short failure
[h264 @ 0x557af2e7fec0] mmco: unref short failure

 11%|█         | 24/221 [00:06<00:47,  4.13it/s][A[h264 @ 0x557af2752a40] mmco: unref short failure

 11%|█▏        | 25/221 [00:06<00:48,  4.06it/s][A
 12%|█▏        | 26/221 [00:07<01:00,  3.24it/s][A
 12%|█▏        | 27/221 [00:07<00:52,  3.70it/s][A
 13%|█▎        | 28/221 [00:07<01:09,  2.79it/s][A
 13%|█▎        | 29/221 [00:08<01:03,  3.02it/s][A
 14%|█▎        | 30/221 [00:08<01:12,  2.62it/s][A
 14%|█▍        | 31/221 [00:09<01:09,  2.74it/s][A
 14%|█▍        | 32/221 [00:09<00:58,  3.24it/s][A09/16/2024 16:45:37 - INFO - __main__ -   current idx HnCSd-QjKvs.136 from finetune_area returns wrong image/video, use 28556 instead.

 15%|█▍        | 33/221 [00:09<01:01,  3.07it/s][A
 15%|█▌        | 34/221 [00:09<00:57,  3.26it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.27it/s][A
 16%|█▋        | 36/221 [00:10<01:00,  3.06it/s][A
 17%|█▋        | 37/221 [00:10<01:02,  2.93it/s][A
 17%|█▋        | 38/221 [00:11<01:05,  2.78it/s][A
 18%|█▊        | 39/221 [00:11<00:52,  3.45it/s][A
 18%|█▊        | 40/221 [00:11<00:51,  3.51it/s][A
 19%|█▊        | 41/221 [00:11<00:47,  3.80it/s][A[h264 @ 0x5615e6e79fc0] mmco: unref short failure
[h264 @ 0x5615e6e79fc0] mmco: unref short failure

 19%|█▉        | 42/221 [00:12<01:11,  2.50it/s][A
 19%|█▉        | 43/221 [00:12<00:57,  3.09it/s][A
 20%|█▉        | 44/221 [00:13<00:54,  3.24it/s][A
 20%|██        | 45/221 [00:13<01:08,  2.58it/s][A
 21%|██        | 46/221 [00:14<01:10,  2.47it/s][A
 21%|██▏       | 47/221 [00:16<02:42,  1.07it/s][A
 22%|██▏       | 48/221 [00:16<02:00,  1.44it/s][A
 22%|██▏       | 49/221 [00:16<01:37,  1.77it/s][A
 23%|██▎       | 50/221 [00:16<01:24,  2.02it/s][A
 23%|██▎       | 51/221 [00:17<01:08,  2.48it/s][A
 24%|██▎       | 52/221 [00:17<01:07,  2.49it/s][A
 24%|██▍       | 53/221 [00:17<00:59,  2.83it/s][A
 24%|██▍       | 54/221 [00:18<01:33,  1.79it/s][A
 25%|██▍       | 55/221 [00:19<01:29,  1.85it/s][A
 25%|██▌       | 56/221 [00:19<01:08,  2.42it/s][A
 26%|██▌       | 57/221 [00:19<01:01,  2.65it/s][A
 26%|██▌       | 58/221 [00:19<00:54,  3.01it/s][A
 27%|██▋       | 59/221 [00:20<00:47,  3.39it/s][A
 27%|██▋       | 60/221 [00:20<01:06,  2.42it/s][A
 28%|██▊       | 61/221 [00:21<00:56,  2.85it/s][A
 28%|██▊       | 62/221 [00:21<00:57,  2.76it/s][A
 29%|██▊       | 63/221 [00:21<00:49,  3.16it/s][A
 29%|██▉       | 64/221 [00:22<01:02,  2.52it/s][A
 29%|██▉       | 65/221 [00:22<00:54,  2.84it/s][A
 30%|██▉       | 66/221 [00:23<01:05,  2.36it/s][A
 30%|███       | 67/221 [00:23<00:59,  2.60it/s][A
 31%|███       | 68/221 [00:23<00:51,  2.97it/s][A
 31%|███       | 69/221 [00:24<01:13,  2.06it/s][A
 32%|███▏      | 70/221 [00:24<01:02,  2.42it/s][A
 32%|███▏      | 71/221 [00:24<00:51,  2.92it/s][A[h264 @ 0x55f3e98d9800] mmco: unref short failure
[h264 @ 0x55f3e98d9800] mmco: unref short failure

 33%|███▎      | 72/221 [00:25<00:57,  2.60it/s][A
 33%|███▎      | 73/221 [00:25<01:03,  2.34it/s][A
 33%|███▎      | 74/221 [00:26<00:51,  2.87it/s][A
 34%|███▍      | 75/221 [00:26<00:55,  2.63it/s][A
 34%|███▍      | 76/221 [00:26<00:49,  2.94it/s][A
 35%|███▍      | 77/221 [00:27<00:50,  2.87it/s][A
 35%|███▌      | 78/221 [00:27<00:41,  3.48it/s][A
 36%|███▌      | 79/221 [00:27<00:46,  3.04it/s][A
 36%|███▌      | 80/221 [00:28<00:45,  3.11it/s][A[h264 @ 0x5615e9aef100] mmco: unref short failure

 37%|███▋      | 81/221 [00:28<00:47,  2.93it/s][A
 37%|███▋      | 82/221 [00:28<00:51,  2.67it/s][A
 38%|███▊      | 83/221 [00:29<00:55,  2.47it/s][A
 38%|███▊      | 84/221 [00:29<00:48,  2.84it/s][A
 39%|███▉      | 86/221 [00:29<00:36,  3.74it/s][A[h264 @ 0x5615d99f08c0] mmco: unref short failure
[h264 @ 0x5615d99f08c0] mmco: unref short failure
[h264 @ 0x5615d99f08c0] mmco: unref short failure
[h264 @ 0x5615d99f08c0] mmco: unref short failure

 39%|███▉      | 87/221 [00:30<01:01,  2.20it/s][A
 40%|███▉      | 88/221 [00:31<01:05,  2.02it/s][A
 40%|████      | 89/221 [00:31<00:57,  2.30it/s][A
 41%|████      | 90/221 [00:32<00:50,  2.61it/s][A
 41%|████      | 91/221 [00:32<00:44,  2.93it/s][A
 42%|████▏     | 92/221 [00:32<00:47,  2.72it/s][A
 42%|████▏     | 93/221 [00:33<00:49,  2.57it/s][A
 43%|████▎     | 94/221 [00:33<00:49,  2.57it/s][A
 43%|████▎     | 95/221 [00:33<00:45,  2.75it/s][A[h264 @ 0x55f3d8e87a40] mmco: unref short failure
[h264 @ 0x55f3d8e87a40] mmco: unref short failure

 43%|████▎     | 96/221 [00:34<01:01,  2.05it/s][A
 44%|████▍     | 97/221 [00:34<00:54,  2.29it/s][A
 44%|████▍     | 98/221 [00:35<01:08,  1.79it/s][A[h264 @ 0x5615e04f44c0] mmco: unref short failure

 45%|████▍     | 99/221 [00:36<00:59,  2.05it/s][A
 45%|████▌     | 100/221 [00:36<00:53,  2.27it/s][A
 46%|████▌     | 101/221 [00:36<00:48,  2.49it/s][A
 46%|████▌     | 102/221 [00:37<00:48,  2.45it/s][A
 47%|████▋     | 103/221 [00:37<00:37,  3.12it/s][A
 47%|████▋     | 104/221 [00:37<00:31,  3.73it/s][A
 48%|████▊     | 105/221 [00:37<00:34,  3.33it/s][A
 48%|████▊     | 106/221 [00:38<00:55,  2.05it/s][A
 48%|████▊     | 107/221 [00:38<00:46,  2.43it/s][A
 49%|████▉     | 108/221 [00:39<00:40,  2.80it/s][A
 49%|████▉     | 109/221 [00:39<00:40,  2.80it/s][A
 50%|████▉     | 110/221 [00:39<00:31,  3.50it/s][A[h264 @ 0x56176f705a80] mmco: unref short failure
[h264 @ 0x56176f705a80] mmco: unref short failure
[h264 @ 0x56176f705a80] mmco: unref short failure
[h264 @ 0x56176f705a80] mmco: unref short failure

 50%|█████     | 111/221 [00:40<00:33,  3.25it/s][A[h264 @ 0x56176f705a80] mmco: unref short failure

 51%|█████     | 112/221 [00:40<00:35,  3.06it/s][A
 51%|█████     | 113/221 [00:40<00:36,  2.99it/s][A
 52%|█████▏    | 114/221 [00:40<00:30,  3.56it/s][A
 52%|█████▏    | 115/221 [00:41<00:28,  3.68it/s][A[h264 @ 0x55f3d2358700] mmco: unref short failure

 52%|█████▏    | 116/221 [00:42<00:47,  2.23it/s][A
 53%|█████▎    | 117/221 [00:42<00:44,  2.34it/s][A
 53%|█████▎    | 118/221 [00:42<00:36,  2.81it/s][A
 54%|█████▍    | 119/221 [00:42<00:36,  2.79it/s][A
 54%|█████▍    | 120/221 [00:43<00:32,  3.15it/s][A
 55%|█████▍    | 121/221 [00:43<00:28,  3.53it/s][A
 55%|█████▌    | 122/221 [00:43<00:30,  3.25it/s][A
 56%|█████▌    | 123/221 [00:43<00:28,  3.44it/s][A
 56%|█████▌    | 124/221 [00:44<00:31,  3.03it/s][A
 57%|█████▋    | 125/221 [00:44<00:37,  2.56it/s][A
 57%|█████▋    | 126/221 [00:45<00:36,  2.57it/s][A
 57%|█████▋    | 127/221 [00:46<00:59,  1.57it/s][A[h264 @ 0x557af6c03800] mmco: unref short failure

 58%|█████▊    | 128/221 [00:46<00:53,  1.75it/s][A
 58%|█████▊    | 129/221 [00:47<00:40,  2.27it/s][A[h264 @ 0x55f3e614b840] mmco: unref short failure

 59%|█████▉    | 130/221 [00:47<00:32,  2.78it/s][A[h264 @ 0x55f3e614b840] mmco: unref short failure

 59%|█████▉    | 131/221 [00:47<00:27,  3.24it/s][A
 60%|█████▉    | 132/221 [00:48<00:48,  1.83it/s][A
 60%|██████    | 133/221 [00:48<00:44,  1.99it/s][A
 61%|██████    | 134/221 [00:49<00:52,  1.65it/s][A
 61%|██████    | 135/221 [00:50<00:53,  1.60it/s][A
 62%|██████▏   | 136/221 [00:50<00:48,  1.76it/s][A
 62%|██████▏   | 137/221 [00:51<00:45,  1.84it/s][A[h264 @ 0x557af883b940] mmco: unref short failure
[h264 @ 0x557af883b940] mmco: unref short failure

 62%|██████▏   | 138/221 [00:51<00:43,  1.92it/s][A
 63%|██████▎   | 139/221 [00:52<00:53,  1.53it/s][A
 63%|██████▎   | 140/221 [00:53<00:48,  1.66it/s][A
 64%|██████▍   | 141/221 [00:54<00:50,  1.59it/s][A
 64%|██████▍   | 142/221 [00:54<00:47,  1.67it/s][A
 65%|██████▍   | 143/221 [00:54<00:43,  1.81it/s][A
 65%|██████▌   | 144/221 [00:55<00:36,  2.13it/s][A
 66%|██████▌   | 145/221 [00:55<00:28,  2.69it/s][A[h264 @ 0x55f3e034dac0] mmco: unref short failure

 66%|██████▌   | 146/221 [00:55<00:22,  3.34it/s][A
 67%|██████▋   | 147/221 [00:55<00:22,  3.25it/s][A
 67%|██████▋   | 148/221 [00:56<00:30,  2.40it/s][A
 67%|██████▋   | 149/221 [00:57<00:32,  2.19it/s][A
 68%|██████▊   | 150/221 [00:57<00:28,  2.47it/s][A
 68%|██████▊   | 151/221 [00:57<00:27,  2.56it/s][A
 69%|██████▉   | 152/221 [00:58<00:33,  2.07it/s][A
 69%|██████▉   | 153/221 [00:58<00:25,  2.64it/s][A[h264 @ 0x5615ee24ca40] mmco: unref short failure

 70%|██████▉   | 154/221 [00:58<00:22,  2.98it/s][A
 70%|███████   | 155/221 [00:59<00:20,  3.26it/s][A
 71%|███████   | 156/221 [00:59<00:21,  2.96it/s][A
 71%|███████   | 157/221 [01:01<00:45,  1.41it/s][A
 71%|███████▏  | 158/221 [01:01<00:36,  1.72it/s][A
 72%|███████▏  | 159/221 [01:01<00:28,  2.19it/s][A[h264 @ 0x56176d0ce440] mmco: unref short failure

 72%|███████▏  | 160/221 [01:01<00:23,  2.59it/s][A
 73%|███████▎  | 161/221 [01:01<00:18,  3.23it/s][A
 73%|███████▎  | 162/221 [01:01<00:15,  3.76it/s][A
 74%|███████▍  | 163/221 [01:02<00:16,  3.47it/s][A
 74%|███████▍  | 164/221 [01:02<00:15,  3.69it/s][A
 75%|███████▍  | 165/221 [01:03<00:18,  3.01it/s][A
 75%|███████▌  | 166/221 [01:04<00:35,  1.56it/s][A
 76%|███████▌  | 167/221 [01:04<00:27,  2.00it/s][A
 76%|███████▌  | 168/221 [01:06<00:42,  1.25it/s][A
 76%|███████▋  | 169/221 [01:06<00:30,  1.69it/s][A
 77%|███████▋  | 170/221 [01:06<00:25,  1.98it/s][A[h264 @ 0x557ae4c437c0] mmco: unref short failure

 77%|███████▋  | 171/221 [01:07<00:26,  1.90it/s][A
 78%|███████▊  | 172/221 [01:07<00:21,  2.27it/s][A
 78%|███████▊  | 173/221 [01:07<00:21,  2.23it/s][A
 79%|███████▊  | 174/221 [01:07<00:17,  2.75it/s][A
 79%|███████▉  | 175/221 [01:08<00:15,  2.89it/s][A
 80%|███████▉  | 176/221 [01:08<00:13,  3.45it/s][A
 80%|████████  | 177/221 [01:08<00:11,  3.75it/s][A
 81%|████████  | 178/221 [01:09<00:21,  2.01it/s][A
 81%|████████  | 179/221 [01:10<00:21,  1.92it/s][A
 81%|████████▏ | 180/221 [01:10<00:16,  2.46it/s][A
 82%|████████▏ | 181/221 [01:10<00:12,  3.16it/s][A
 82%|████████▏ | 182/221 [01:10<00:11,  3.32it/s][A
 83%|████████▎ | 183/221 [01:11<00:12,  3.13it/s][A
 83%|████████▎ | 184/221 [01:11<00:12,  2.86it/s][A
 84%|████████▍ | 186/221 [01:12<00:12,  2.85it/s][A
 85%|████████▍ | 187/221 [01:12<00:11,  2.94it/s][A
 85%|████████▌ | 188/221 [01:12<00:10,  3.22it/s][A
 86%|████████▌ | 189/221 [01:13<00:09,  3.21it/s][A
 86%|████████▌ | 190/221 [01:13<00:10,  3.08it/s][A
 86%|████████▋ | 191/221 [01:13<00:07,  3.83it/s][A
 87%|████████▋ | 192/221 [01:13<00:07,  4.11it/s][A
 87%|████████▋ | 193/221 [01:13<00:05,  4.92it/s][A[h264 @ 0x557ae4c43300] mmco: unref short failure
[h264 @ 0x557ae4c43300] mmco: unref short failure

 88%|████████▊ | 194/221 [01:15<00:16,  1.59it/s][A
 88%|████████▊ | 195/221 [01:15<00:13,  1.98it/s][A[h264 @ 0x55f3e84e1600] mmco: unref short failure
[h264 @ 0x55f3e84e1600] mmco: unref short failure

 89%|████████▊ | 196/221 [01:15<00:09,  2.51it/s][A
 89%|████████▉ | 197/221 [01:16<00:08,  2.70it/s][A
 90%|████████▉ | 198/221 [01:16<00:07,  3.20it/s][A
 90%|█████████ | 199/221 [01:16<00:07,  3.05it/s][A
 90%|█████████ | 200/221 [01:16<00:05,  3.67it/s][A
 91%|█████████ | 201/221 [01:17<00:05,  3.92it/s][A
 91%|█████████▏| 202/221 [01:17<00:04,  3.81it/s][A
 92%|█████████▏| 203/221 [01:17<00:04,  3.82it/s][A
 92%|█████████▏| 204/221 [01:17<00:04,  4.07it/s][A
 93%|█████████▎| 205/221 [01:18<00:03,  4.02it/s][A
 93%|█████████▎| 206/221 [01:18<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:18<00:03,  3.54it/s][A
 95%|█████████▍| 209/221 [01:19<00:03,  3.60it/s][A
 95%|█████████▌| 210/221 [01:19<00:02,  4.24it/s][A
 95%|█████████▌| 211/221 [01:19<00:02,  3.46it/s][A
 96%|█████████▌| 212/221 [01:20<00:02,  3.39it/s][A
 96%|█████████▋| 213/221 [01:20<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:20<00:02,  2.89it/s][A
 97%|█████████▋| 215/221 [01:20<00:01,  3.51it/s][A
 98%|█████████▊| 216/221 [01:21<00:01,  3.49it/s][A
 98%|█████████▊| 217/221 [01:21<00:01,  2.69it/s][A
 99%|█████████▊| 218/221 [01:22<00:00,  3.01it/s][A
 99%|█████████▉| 219/221 [01:22<00:00,  2.98it/s][A
100%|█████████▉| 220/221 [01:23<00:00,  1.80it/s][A100%|██████████| 221/221 [01:23<00:00,  2.64it/s]
[h264 @ 0x55f3e6595740] mmco: unref short failure
[h264 @ 0x55f3d14202c0] mmco: unref short failure
[h264 @ 0x55f3e63def00] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.31it/s][A
  1%|          | 2/221 [00:00<01:06,  3.27it/s][A
  1%|▏         | 3/221 [00:00<01:05,  3.32it/s][A
  2%|▏         | 4/221 [00:01<01:04,  3.35it/s][A
  2%|▏         | 5/221 [00:01<01:04,  3.36it/s][A
  3%|▎         | 6/221 [00:01<01:03,  3.37it/s][A[h264 @ 0x557ae21e1340] mmco: unref short failure

  3%|▎         | 7/221 [00:02<01:03,  3.38it/s][A
  4%|▎         | 8/221 [00:02<01:03,  3.38it/s][A
  4%|▍         | 9/221 [00:02<01:03,  3.34it/s][A
  5%|▍         | 10/221 [00:02<01:03,  3.31it/s][A
  5%|▍         | 11/221 [00:03<01:04,  3.27it/s][A
  5%|▌         | 12/221 [00:03<01:03,  3.30it/s][A
  6%|▌         | 13/221 [00:03<01:03,  3.28it/s][A
  6%|▋         | 14/221 [00:04<01:03,  3.26it/s][A
  7%|▋         | 15/221 [00:04<01:02,  3.30it/s][A
  7%|▋         | 16/221 [00:04<01:01,  3.31it/s][A
  8%|▊         | 17/221 [00:05<01:01,  3.31it/s][A
  8%|▊         | 18/221 [00:05<01:01,  3.33it/s][A
  9%|▊         | 19/221 [00:05<01:00,  3.34it/s][A
  9%|▉         | 20/221 [00:06<01:00,  3.35it/s][A
 10%|▉         | 21/221 [00:06<00:59,  3.35it/s][A
 10%|▉         | 22/221 [00:06<00:59,  3.35it/s][A
 10%|█         | 23/221 [00:06<00:59,  3.35it/s][A
 11%|█         | 24/221 [00:07<00:59,  3.32it/s][A
 11%|█▏        | 25/221 [00:07<00:59,  3.29it/s][A
 12%|█▏        | 26/221 [00:07<00:59,  3.28it/s][A
 12%|█▏        | 27/221 [00:08<00:58,  3.31it/s][A
 13%|█▎        | 28/221 [00:08<00:58,  3.28it/s][A[h264 @ 0x557af8716280] mmco: unref short failure
[h264 @ 0x557af8716280] mmco: unref short failure
[h264 @ 0x557af8716280] mmco: unref short failure
[h264 @ 0x557af8716280] mmco: unref short failure

 13%|█▎        | 29/221 [00:08<00:58,  3.28it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.29it/s][A
 14%|█▍        | 31/221 [00:09<00:57,  3.30it/s][A
 14%|█▍        | 32/221 [00:09<00:56,  3.33it/s][A
 15%|█▍        | 33/221 [00:09<00:56,  3.34it/s][A
 15%|█▌        | 34/221 [00:10<00:55,  3.35it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.32it/s][A
 16%|█▋        | 36/221 [00:10<00:56,  3.27it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.28it/s][A[h264 @ 0x55f3ec52be00] mmco: unref short failure
[h264 @ 0x55f3ec52be00] mmco: unref short failure

 17%|█▋        | 38/221 [00:11<00:55,  3.30it/s][A
 18%|█▊        | 39/221 [00:11<00:54,  3.33it/s][A
 18%|█▊        | 40/221 [00:12<00:54,  3.34it/s][A
 19%|█▊        | 41/221 [00:12<00:53,  3.36it/s][A
 19%|█▉        | 42/221 [00:12<00:53,  3.34it/s][A
 19%|█▉        | 43/221 [00:12<00:53,  3.36it/s][A
 20%|█▉        | 44/221 [00:13<00:52,  3.37it/s][A
 20%|██        | 45/221 [00:13<00:52,  3.36it/s][A
 21%|██        | 46/221 [00:13<00:52,  3.32it/s][A[h264 @ 0x557af266c300] mmco: unref short failure
[h264 @ 0x557af266c300] mmco: unref short failure

 21%|██▏       | 47/221 [00:14<00:52,  3.34it/s][A
 22%|██▏       | 48/221 [00:14<00:51,  3.34it/s][A
 22%|██▏       | 49/221 [00:14<00:52,  3.26it/s][A
 23%|██▎       | 50/221 [00:15<00:52,  3.27it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.30it/s][A
 24%|██▎       | 52/221 [00:15<00:51,  3.28it/s][A
 24%|██▍       | 53/221 [00:15<00:50,  3.31it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.31it/s][A
 25%|██▍       | 55/221 [00:16<00:49,  3.33it/s][A
 25%|██▌       | 56/221 [00:16<00:50,  3.27it/s][A
 26%|██▌       | 57/221 [00:17<00:51,  3.17it/s][A
 26%|██▌       | 58/221 [00:17<00:50,  3.20it/s][A
 27%|██▋       | 59/221 [00:17<00:50,  3.21it/s][A
 27%|██▋       | 60/221 [00:18<00:49,  3.25it/s][A
 28%|██▊       | 61/221 [00:18<00:49,  3.26it/s][A
 28%|██▊       | 62/221 [00:18<00:49,  3.19it/s][A
 29%|██▊       | 63/221 [00:19<00:48,  3.25it/s][A
 29%|██▉       | 64/221 [00:19<00:48,  3.25it/s][A
 29%|██▉       | 65/221 [00:19<00:48,  3.23it/s][A
 30%|██▉       | 66/221 [00:19<00:47,  3.24it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.28it/s][A
 31%|███       | 68/221 [00:20<00:46,  3.28it/s][A
 31%|███       | 69/221 [00:20<00:46,  3.30it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.30it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.33it/s][A
 33%|███▎      | 72/221 [00:21<00:45,  3.27it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.30it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.33it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.25it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.24it/s][A
 35%|███▍      | 77/221 [00:23<00:44,  3.22it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.27it/s][A
 36%|███▌      | 79/221 [00:23<00:42,  3.30it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.33it/s][A
 37%|███▋      | 81/221 [00:24<00:41,  3.34it/s][A
 37%|███▋      | 82/221 [00:24<00:41,  3.33it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.30it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.32it/s][A
 38%|███▊      | 85/221 [00:25<00:40,  3.34it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.35it/s][A
 39%|███▉      | 87/221 [00:26<00:39,  3.36it/s][A
 40%|███▉      | 88/221 [00:26<00:39,  3.37it/s][A
 40%|████      | 89/221 [00:26<00:39,  3.38it/s][A
 41%|████      | 90/221 [00:27<00:38,  3.38it/s][A[h264 @ 0x5615eea3cd80] mmco: unref short failure

 41%|████      | 91/221 [00:27<00:38,  3.38it/s][A
 42%|████▏     | 92/221 [00:27<00:38,  3.39it/s][A
 42%|████▏     | 93/221 [00:28<00:37,  3.39it/s][A
 43%|████▎     | 94/221 [00:28<00:37,  3.39it/s][A
 43%|████▎     | 95/221 [00:28<00:37,  3.39it/s][A
 43%|████▎     | 96/221 [00:28<00:36,  3.39it/s][A
 44%|████▍     | 97/221 [00:29<00:36,  3.39it/s][A
 44%|████▍     | 98/221 [00:29<00:36,  3.39it/s][A
 45%|████▍     | 99/221 [00:29<00:35,  3.39it/s][A
 45%|████▌     | 100/221 [00:30<00:35,  3.39it/s][A
 46%|████▌     | 101/221 [00:30<00:35,  3.39it/s][A
 46%|████▌     | 102/221 [00:30<00:35,  3.39it/s][A
 47%|████▋     | 103/221 [00:31<00:34,  3.40it/s][A
 47%|████▋     | 104/221 [00:31<00:34,  3.40it/s][A
 48%|████▊     | 105/221 [00:31<00:34,  3.40it/s][A
 48%|████▊     | 106/221 [00:31<00:33,  3.40it/s][A
 48%|████▊     | 107/221 [00:32<00:33,  3.40it/s][A
 49%|████▉     | 108/221 [00:32<00:33,  3.40it/s][A
 49%|████▉     | 109/221 [00:32<00:32,  3.40it/s][A
 50%|████▉     | 110/221 [00:33<00:32,  3.40it/s][A
 50%|█████     | 111/221 [00:33<00:32,  3.40it/s][A
 51%|█████     | 112/221 [00:33<00:32,  3.41it/s][A
 51%|█████     | 113/221 [00:33<00:31,  3.40it/s][A
 52%|█████▏    | 114/221 [00:34<00:31,  3.41it/s][A
 52%|█████▏    | 115/221 [00:34<00:31,  3.40it/s][A
 52%|█████▏    | 116/221 [00:34<00:30,  3.41it/s][A
 53%|█████▎    | 117/221 [00:35<00:30,  3.41it/s][A
 53%|█████▎    | 118/221 [00:35<00:30,  3.41it/s][A
 54%|█████▍    | 119/221 [00:35<00:29,  3.41it/s][A
 54%|█████▍    | 120/221 [00:36<00:29,  3.41it/s][A
 55%|█████▍    | 121/221 [00:36<00:29,  3.41it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.41it/s][A
 56%|█████▌    | 123/221 [00:36<00:28,  3.41it/s][A
 56%|█████▌    | 124/221 [00:37<00:28,  3.41it/s][A
 57%|█████▋    | 125/221 [00:37<00:28,  3.41it/s][A
 57%|█████▋    | 126/221 [00:37<00:27,  3.41it/s][A
 57%|█████▋    | 127/221 [00:38<00:27,  3.41it/s][A
 58%|█████▊    | 128/221 [00:38<00:27,  3.41it/s][A
 58%|█████▊    | 129/221 [00:38<00:27,  3.41it/s][A
 59%|█████▉    | 130/221 [00:38<00:26,  3.41it/s][A
 59%|█████▉    | 131/221 [00:39<00:26,  3.41it/s][A
 60%|█████▉    | 132/221 [00:39<00:26,  3.41it/s][A
 60%|██████    | 133/221 [00:39<00:25,  3.41it/s][A
 61%|██████    | 134/221 [00:40<00:25,  3.41it/s][A
 61%|██████    | 135/221 [00:40<00:25,  3.41it/s][A
 62%|██████▏   | 136/221 [00:40<00:24,  3.41it/s][A
 62%|██████▏   | 137/221 [00:41<00:24,  3.41it/s][A
 62%|██████▏   | 138/221 [00:41<00:24,  3.41it/s][A
 63%|██████▎   | 139/221 [00:41<00:24,  3.41it/s][A
 63%|██████▎   | 140/221 [00:41<00:23,  3.41it/s][A
 64%|██████▍   | 141/221 [00:42<00:23,  3.40it/s][A
 64%|██████▍   | 142/221 [00:42<00:23,  3.40it/s][A
 65%|██████▍   | 143/221 [00:42<00:22,  3.41it/s][A
 65%|██████▌   | 144/221 [00:43<00:22,  3.41it/s][A
 66%|██████▌   | 145/221 [00:43<00:22,  3.41it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.41it/s][A
 67%|██████▋   | 147/221 [00:43<00:21,  3.41it/s][A
 67%|██████▋   | 148/221 [00:44<00:21,  3.41it/s][A
 67%|██████▋   | 149/221 [00:44<00:21,  3.41it/s][A
 68%|██████▊   | 150/221 [00:44<00:20,  3.41it/s][A
 68%|██████▊   | 151/221 [00:45<00:20,  3.41it/s][A
 69%|██████▉   | 152/221 [00:45<00:20,  3.41it/s][A
 69%|██████▉   | 153/221 [00:45<00:19,  3.41it/s][A
 70%|██████▉   | 154/221 [00:46<00:19,  3.41it/s][A
 70%|███████   | 155/221 [00:46<00:19,  3.41it/s][A
 71%|███████   | 156/221 [00:46<00:19,  3.41it/s][A
 71%|███████   | 157/221 [00:46<00:18,  3.41it/s][A
 71%|███████▏  | 158/221 [00:47<00:18,  3.41it/s][A
 72%|███████▏  | 159/221 [00:47<00:18,  3.41it/s][A
 72%|███████▏  | 160/221 [00:47<00:17,  3.41it/s][A
 73%|███████▎  | 161/221 [00:48<00:17,  3.41it/s][A
 73%|███████▎  | 162/221 [00:48<00:17,  3.41it/s][A
 74%|███████▍  | 163/221 [00:48<00:17,  3.41it/s][A
 74%|███████▍  | 164/221 [00:48<00:16,  3.41it/s][A
 75%|███████▍  | 165/221 [00:49<00:16,  3.41it/s][A
 75%|███████▌  | 166/221 [00:49<00:16,  3.41it/s][A
 76%|███████▌  | 167/221 [00:49<00:15,  3.41it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.41it/s][A
 76%|███████▋  | 169/221 [00:50<00:15,  3.41it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.41it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.41it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.41it/s][A
 78%|███████▊  | 173/221 [00:51<00:14,  3.41it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.41it/s][A
 79%|███████▉  | 175/221 [00:52<00:13,  3.41it/s][A
 80%|███████▉  | 176/221 [00:52<00:13,  3.41it/s][A
 80%|████████  | 177/221 [00:52<00:12,  3.41it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.41it/s][A
 81%|████████  | 179/221 [00:53<00:12,  3.41it/s][A
 81%|████████▏ | 180/221 [00:53<00:12,  3.41it/s][A
 82%|████████▏ | 181/221 [00:53<00:11,  3.41it/s][A
 82%|████████▏ | 182/221 [00:54<00:11,  3.41it/s][A
 83%|████████▎ | 183/221 [00:54<00:11,  3.41it/s][A
 83%|████████▎ | 184/221 [00:54<00:10,  3.41it/s][A
 84%|████████▎ | 185/221 [00:55<00:10,  3.41it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.41it/s][A
 85%|████████▍ | 187/221 [00:55<00:09,  3.41it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.41it/s][A
 86%|████████▌ | 189/221 [00:56<00:09,  3.41it/s][A
 86%|████████▌ | 190/221 [00:56<00:09,  3.41it/s][A
 86%|████████▋ | 191/221 [00:56<00:08,  3.41it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.41it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.41it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.41it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.41it/s][A
 89%|████████▊ | 196/221 [00:58<00:07,  3.41it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.41it/s][A
 90%|████████▉ | 198/221 [00:58<00:06,  3.41it/s][A
 90%|█████████ | 199/221 [00:59<00:06,  3.41it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.41it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.41it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.41it/s][A
 92%|█████████▏| 203/221 [01:00<00:05,  3.41it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.41it/s][A
 93%|█████████▎| 205/221 [01:00<00:04,  3.41it/s][A
 93%|█████████▎| 206/221 [01:01<00:04,  3.41it/s][A
 94%|█████████▎| 207/221 [01:01<00:04,  3.41it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▍| 209/221 [01:02<00:03,  3.41it/s][A
 95%|█████████▌| 210/221 [01:02<00:03,  3.41it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.41it/s][A
 96%|█████████▌| 212/221 [01:03<00:02,  3.41it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  3.41it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.41it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.41it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  3.41it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.41it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.41it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  3.41it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.41it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.41it/s][A100%|██████████| 221/221 [01:05<00:00,  3.36it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:24,  8.98it/s][A
  1%|          | 2/221 [00:00<01:01,  3.57it/s][A
  1%|▏         | 3/221 [00:01<01:28,  2.48it/s][A
  2%|▏         | 4/221 [00:01<01:02,  3.49it/s][A
  2%|▏         | 5/221 [00:01<00:58,  3.71it/s][A
  3%|▎         | 6/221 [00:01<00:52,  4.07it/s][A
  3%|▎         | 7/221 [00:01<00:51,  4.18it/s][A
  4%|▎         | 8/221 [00:02<01:00,  3.51it/s][A
  4%|▍         | 9/221 [00:02<01:03,  3.33it/s][A
  5%|▍         | 10/221 [00:03<01:19,  2.65it/s][A
  5%|▍         | 11/221 [00:03<01:17,  2.70it/s][A
  5%|▌         | 12/221 [00:03<01:09,  3.00it/s][A
  6%|▌         | 13/221 [00:04<01:15,  2.74it/s][A
  6%|▋         | 14/221 [00:04<01:14,  2.77it/s][A
  7%|▋         | 15/221 [00:04<01:21,  2.51it/s][A
  7%|▋         | 16/221 [00:05<01:16,  2.68it/s][A
  8%|▊         | 17/221 [00:05<01:20,  2.55it/s][A
  8%|▊         | 18/221 [00:06<01:14,  2.73it/s][A
  9%|▊         | 19/221 [00:06<01:11,  2.84it/s][A
  9%|▉         | 20/221 [00:06<00:56,  3.54it/s][A
 10%|▉         | 21/221 [00:06<00:50,  3.98it/s][A
 10%|▉         | 22/221 [00:06<00:46,  4.32it/s][A
 10%|█         | 23/221 [00:06<00:38,  5.12it/s][A
 11%|█         | 24/221 [00:07<00:35,  5.60it/s][A
 11%|█▏        | 25/221 [00:07<00:37,  5.22it/s][A
 12%|█▏        | 26/221 [00:07<00:41,  4.75it/s][A
 12%|█▏        | 27/221 [00:07<00:40,  4.82it/s][A
 13%|█▎        | 28/221 [00:08<00:49,  3.87it/s][A
 13%|█▎        | 29/221 [00:08<00:59,  3.24it/s][A
 14%|█▎        | 30/221 [00:08<01:04,  2.95it/s][A
 14%|█▍        | 31/221 [00:09<00:58,  3.24it/s][A
 14%|█▍        | 32/221 [00:09<01:00,  3.11it/s][A
 15%|█▍        | 33/221 [00:09<00:54,  3.43it/s][A
 15%|█▌        | 34/221 [00:09<00:44,  4.19it/s][A
 16%|█▋        | 36/221 [00:10<00:46,  4.01it/s][A
 17%|█▋        | 37/221 [00:10<00:47,  3.90it/s][A
 17%|█▋        | 38/221 [00:11<00:50,  3.61it/s][A
 18%|█▊        | 39/221 [00:11<00:46,  3.93it/s][A
 18%|█▊        | 40/221 [00:11<00:50,  3.56it/s][A
 19%|█▊        | 41/221 [00:11<00:51,  3.47it/s][A
 19%|█▉        | 42/221 [00:12<00:45,  3.93it/s][A
 19%|█▉        | 43/221 [00:12<00:48,  3.66it/s][A
 20%|█▉        | 44/221 [00:12<00:41,  4.26it/s][A
 20%|██        | 45/221 [00:12<00:52,  3.34it/s][A
 21%|██        | 46/221 [00:13<00:55,  3.17it/s][A
 21%|██▏       | 47/221 [00:13<00:46,  3.76it/s][A
 22%|██▏       | 49/221 [00:13<00:30,  5.60it/s][A
 23%|██▎       | 50/221 [00:14<00:48,  3.52it/s][A
 23%|██▎       | 51/221 [00:14<00:47,  3.62it/s][A
 24%|██▎       | 52/221 [00:14<00:40,  4.16it/s][A
 24%|██▍       | 53/221 [00:14<00:42,  3.92it/s][A
 24%|██▍       | 54/221 [00:15<00:39,  4.20it/s][A
 25%|██▍       | 55/221 [00:15<00:34,  4.80it/s][A
 25%|██▌       | 56/221 [00:15<00:35,  4.66it/s][A
 26%|██▌       | 57/221 [00:15<00:40,  4.08it/s][A
 27%|██▋       | 59/221 [00:16<00:33,  4.87it/s][A
 27%|██▋       | 60/221 [00:16<00:30,  5.32it/s][A
 28%|██▊       | 61/221 [00:16<00:30,  5.27it/s][A
 28%|██▊       | 62/221 [00:16<00:38,  4.17it/s][A
 29%|██▉       | 64/221 [00:17<00:37,  4.19it/s][A
 29%|██▉       | 65/221 [00:17<00:33,  4.63it/s][A
 30%|██▉       | 66/221 [00:17<00:39,  3.89it/s][A
 30%|███       | 67/221 [00:18<00:40,  3.78it/s][A
 31%|███       | 68/221 [00:18<00:45,  3.39it/s][A
 31%|███       | 69/221 [00:19<00:59,  2.57it/s][A
 32%|███▏      | 70/221 [00:19<00:51,  2.96it/s][A
 32%|███▏      | 71/221 [00:19<00:49,  3.04it/s][A
 33%|███▎      | 72/221 [00:20<00:53,  2.79it/s][A
 33%|███▎      | 73/221 [00:20<00:53,  2.78it/s][A
 33%|███▎      | 74/221 [00:20<00:43,  3.41it/s][A
 34%|███▍      | 75/221 [00:20<00:46,  3.14it/s][A
 34%|███▍      | 76/221 [00:21<00:47,  3.04it/s][A
 35%|███▍      | 77/221 [00:21<00:43,  3.30it/s][A
 35%|███▌      | 78/221 [00:21<00:40,  3.51it/s][A
 36%|███▌      | 79/221 [00:21<00:36,  3.88it/s][A
 36%|███▌      | 80/221 [00:22<00:39,  3.55it/s][A
 37%|███▋      | 81/221 [00:22<00:38,  3.65it/s][A
 37%|███▋      | 82/221 [00:22<00:34,  4.00it/s][A
 38%|███▊      | 84/221 [00:23<00:38,  3.52it/s][A
 38%|███▊      | 85/221 [00:23<00:32,  4.17it/s][A
 39%|███▉      | 86/221 [00:23<00:32,  4.21it/s][A
 39%|███▉      | 87/221 [00:24<00:40,  3.34it/s][A
 40%|███▉      | 88/221 [00:24<00:44,  2.98it/s][A
 40%|████      | 89/221 [00:25<00:48,  2.70it/s][A
 41%|████      | 90/221 [00:25<00:50,  2.59it/s][A
 41%|████      | 91/221 [00:25<00:42,  3.08it/s][A
 42%|████▏     | 92/221 [00:26<00:41,  3.09it/s][A
 42%|████▏     | 93/221 [00:26<00:37,  3.37it/s][A
 43%|████▎     | 94/221 [00:26<00:37,  3.40it/s][A
 43%|████▎     | 95/221 [00:27<00:53,  2.37it/s][A
 43%|████▎     | 96/221 [00:27<00:52,  2.38it/s][A
 44%|████▍     | 97/221 [00:28<00:50,  2.46it/s][A
 44%|████▍     | 98/221 [00:28<01:01,  2.01it/s][A
 45%|████▍     | 99/221 [00:29<00:54,  2.25it/s][A
 45%|████▌     | 100/221 [00:29<00:54,  2.24it/s][A
 46%|████▌     | 101/221 [00:29<00:52,  2.30it/s][A
 46%|████▌     | 102/221 [00:30<00:46,  2.58it/s][A
 47%|████▋     | 103/221 [00:30<00:36,  3.20it/s][A
 48%|████▊     | 105/221 [00:30<00:31,  3.65it/s][A
 48%|████▊     | 106/221 [00:31<00:35,  3.28it/s][A
 48%|████▊     | 107/221 [00:31<00:30,  3.69it/s][A
 49%|████▉     | 108/221 [00:31<00:29,  3.89it/s][A
 49%|████▉     | 109/221 [00:31<00:30,  3.63it/s][A
 50%|████▉     | 110/221 [00:32<00:27,  4.01it/s][A
 50%|█████     | 111/221 [00:32<00:31,  3.52it/s][A
 51%|█████     | 112/221 [00:32<00:30,  3.60it/s][A
 51%|█████     | 113/221 [00:32<00:27,  3.90it/s][A
 52%|█████▏    | 115/221 [00:33<00:21,  4.92it/s][A
 52%|█████▏    | 116/221 [00:33<00:20,  5.03it/s][A
 53%|█████▎    | 117/221 [00:33<00:22,  4.72it/s][A
 53%|█████▎    | 118/221 [00:33<00:25,  4.08it/s][A
 54%|█████▍    | 119/221 [00:34<00:29,  3.41it/s][A
 54%|█████▍    | 120/221 [00:34<00:29,  3.46it/s][A
 55%|█████▍    | 121/221 [00:35<00:32,  3.07it/s][A
 55%|█████▌    | 122/221 [00:35<00:33,  2.93it/s][A
 56%|█████▌    | 123/221 [00:35<00:27,  3.51it/s][A
 56%|█████▌    | 124/221 [00:36<00:31,  3.09it/s][A
 57%|█████▋    | 125/221 [00:36<00:33,  2.84it/s][A
 57%|█████▋    | 126/221 [00:36<00:28,  3.28it/s][A
 57%|█████▋    | 127/221 [00:37<00:30,  3.06it/s][A
 58%|█████▊    | 128/221 [00:37<00:28,  3.32it/s][A
 58%|█████▊    | 129/221 [00:37<00:22,  4.11it/s][A
 59%|█████▉    | 130/221 [00:37<00:26,  3.47it/s][A
 59%|█████▉    | 131/221 [00:37<00:23,  3.80it/s][A
 60%|█████▉    | 132/221 [00:38<00:31,  2.84it/s][A
 60%|██████    | 133/221 [00:38<00:27,  3.20it/s][A
 61%|██████    | 134/221 [00:39<00:31,  2.79it/s][A
 61%|██████    | 135/221 [00:39<00:25,  3.41it/s][A
 62%|██████▏   | 136/221 [00:39<00:25,  3.34it/s][A
 62%|██████▏   | 137/221 [00:39<00:25,  3.36it/s][A
 62%|██████▏   | 138/221 [00:40<00:23,  3.60it/s][A
 63%|██████▎   | 139/221 [00:40<00:24,  3.30it/s][A
 63%|██████▎   | 140/221 [00:40<00:23,  3.52it/s][A
 64%|██████▍   | 141/221 [00:41<00:26,  3.05it/s][A
 64%|██████▍   | 142/221 [00:41<00:23,  3.32it/s][A
 65%|██████▍   | 143/221 [00:41<00:22,  3.46it/s][A
 65%|██████▌   | 144/221 [00:41<00:20,  3.84it/s][A
 66%|██████▌   | 145/221 [00:42<00:22,  3.33it/s][A
 66%|██████▌   | 146/221 [00:42<00:20,  3.62it/s][A
 67%|██████▋   | 148/221 [00:43<00:27,  2.64it/s][A
 67%|██████▋   | 149/221 [00:43<00:27,  2.60it/s][A
 68%|██████▊   | 150/221 [00:44<00:27,  2.56it/s][A
 68%|██████▊   | 151/221 [00:45<00:37,  1.85it/s][A
 69%|██████▉   | 152/221 [00:45<00:34,  1.99it/s][A
 69%|██████▉   | 153/221 [00:45<00:28,  2.37it/s][A
 70%|██████▉   | 154/221 [00:46<00:26,  2.56it/s][A
 70%|███████   | 155/221 [00:46<00:21,  3.06it/s][A
 71%|███████   | 156/221 [00:46<00:20,  3.18it/s][A
 71%|███████   | 157/221 [00:46<00:19,  3.30it/s][A
 71%|███████▏  | 158/221 [00:47<00:19,  3.17it/s][A
 72%|███████▏  | 159/221 [00:47<00:17,  3.62it/s][A
 72%|███████▏  | 160/221 [00:47<00:16,  3.79it/s][A
 74%|███████▍  | 163/221 [00:48<00:10,  5.51it/s][A
 74%|███████▍  | 164/221 [00:48<00:10,  5.46it/s][A
 75%|███████▍  | 165/221 [00:48<00:12,  4.40it/s][A
 75%|███████▌  | 166/221 [00:48<00:13,  3.96it/s][A
 76%|███████▌  | 167/221 [00:49<00:13,  4.14it/s][A
 76%|███████▌  | 168/221 [00:49<00:13,  4.04it/s][A
 77%|███████▋  | 170/221 [00:49<00:10,  4.64it/s][A
 77%|███████▋  | 171/221 [00:50<00:16,  3.04it/s][A
 78%|███████▊  | 172/221 [00:50<00:16,  2.94it/s][A
 78%|███████▊  | 173/221 [00:51<00:17,  2.82it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.50it/s][A
 79%|███████▉  | 175/221 [00:51<00:12,  3.74it/s][A
 80%|███████▉  | 176/221 [00:51<00:10,  4.36it/s][A
 80%|████████  | 177/221 [00:52<00:11,  3.93it/s][A
 81%|████████  | 178/221 [00:52<00:17,  2.40it/s][A
 81%|████████  | 179/221 [00:53<00:16,  2.56it/s][A
 82%|████████▏ | 181/221 [00:53<00:10,  3.76it/s][A
 82%|████████▏ | 182/221 [00:53<00:10,  3.78it/s][A
 83%|████████▎ | 183/221 [00:53<00:10,  3.60it/s][A
 83%|████████▎ | 184/221 [00:54<00:11,  3.13it/s][A
 84%|████████▍ | 186/221 [00:54<00:09,  3.58it/s][A
 85%|████████▍ | 187/221 [00:55<00:08,  4.03it/s][A
 85%|████████▌ | 188/221 [00:55<00:07,  4.53it/s][A
 86%|████████▌ | 189/221 [00:55<00:06,  4.65it/s][A
 86%|████████▌ | 190/221 [00:55<00:08,  3.69it/s][A
 86%|████████▋ | 191/221 [00:55<00:06,  4.43it/s][A
 87%|████████▋ | 192/221 [00:56<00:06,  4.23it/s][A
 87%|████████▋ | 193/221 [00:56<00:08,  3.41it/s][A
 88%|████████▊ | 194/221 [00:57<00:09,  2.80it/s][A
 88%|████████▊ | 195/221 [00:57<00:09,  2.63it/s][A
 89%|████████▊ | 196/221 [00:57<00:08,  2.82it/s][A
 89%|████████▉ | 197/221 [00:58<00:09,  2.48it/s][A
 90%|████████▉ | 198/221 [00:58<00:08,  2.70it/s][A
 90%|█████████ | 199/221 [00:58<00:07,  2.83it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.41it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.63it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.56it/s][A
 92%|█████████▏| 203/221 [00:59<00:05,  3.46it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.25it/s][A
 93%|█████████▎| 205/221 [01:00<00:04,  3.82it/s][A
 93%|█████████▎| 206/221 [01:00<00:04,  3.37it/s][A
 94%|█████████▎| 207/221 [01:01<00:04,  3.43it/s][A
 94%|█████████▍| 208/221 [01:01<00:04,  3.18it/s][A
 95%|█████████▍| 209/221 [01:01<00:03,  3.53it/s][A
 95%|█████████▌| 210/221 [01:01<00:02,  3.87it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.66it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.33it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.70it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.42it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.44it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.54it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.20it/s][A
 99%|█████████▊| 218/221 [01:04<00:01,  2.75it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  2.69it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.29it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.43it/s][A100%|██████████| 221/221 [01:05<00:00,  3.39it/s]
09/16/2024 16:49:09 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 99--===========

09/16/2024 16:49:09 - INFO - __main__ -   {'area_r1': 41.5, 'area_recall': '41.5/69.0/79.4', 'area_ravg': 63.3}
09/16/2024 16:49:09 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 99--===========

09/16/2024 16:49:09 - INFO - __main__ -   {'forward_r1': 36.2, 'forward_recall': '36.2/64.8/74.8', 'forward_ravg': 58.6}
09/16/2024 16:49:09 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video=====step 99--===========

09/16/2024 16:49:09 - INFO - __main__ -   {'area_video_r1': 35.9, 'area_video_recall': '35.9/64.8/75.6', 'area_video_ravg': 58.7}
09/16/2024 16:49:09 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video====history best step: 49=======

09/16/2024 16:49:09 - INFO - __main__ -   {'area_video_r1': 36.9, 'area_video_recall': '36.9/67.2/77.9', 'area_video_ravg': 60.7}
09/16/2024 16:49:09 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_area=====step 99--===========

09/16/2024 16:49:09 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/75.2/83.3', 'area_video_ravg': 70.2, 'area_video_back_r1': 49.8, 'area_video_back_recall': '49.8/73.5/80.5', 'area_video_back_ravg': 67.9}
09/16/2024 16:49:09 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_area====history best step: 99=======

09/16/2024 16:49:09 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/75.2/83.3', 'area_video_ravg': 70.2, 'area_video_back_r1': 49.8, 'area_video_back_recall': '49.8/73.5/80.5', 'area_video_back_ravg': 67.9}
09/16/2024 16:49:09 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 99--===========

09/16/2024 16:49:09 - INFO - __main__ -   {'video_r1': 35.2, 'video_recall': '35.2/65.0/74.9', 'video_ravg': 58.4}
09/16/2024 16:49:09 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 49=======

09/16/2024 16:49:09 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/65.4/73.8', 'video_ravg': 58.7}
09/16/2024 16:49:09 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 99--===========

09/16/2024 16:49:09 - INFO - __main__ -   {'video_r1': 51.7, 'video_recall': '51.7/73.9/81.4', 'video_ravg': 69.0}
09/16/2024 16:49:09 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 99=======

09/16/2024 16:49:09 - INFO - __main__ -   {'video_r1': 51.7, 'video_recall': '51.7/73.9/81.4', 'video_ravg': 69.0}
09/16/2024 16:49:37 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.010558332316577435, 'loss_ret%tv%ta--finetune_area/loss_area': 2.651104211807251, 'loss_ret%tv%ta--finetune_area/total_loss': 2.6616625785827637}
  4%|▎         | 100/2755 [37:03<98:11:59, 133.15s/it]  4%|▎         | 101/2755 [37:07<69:30:27, 94.28s/it]   4%|▎         | 102/2755 [37:10<49:29:58, 67.17s/it]  4%|▎         | 103/2755 [37:15<35:32:04, 48.24s/it]  4%|▍         | 104/2755 [37:19<25:47:26, 35.02s/it]  4%|▍         | 105/2755 [37:24<19:10:34, 26.05s/it][h264 @ 0x55f3e55dd6c0] mmco: unref short failure
[h264 @ 0x557af4f392c0] mmco: unref short failure
[h264 @ 0x557af4f392c0] mmco: unref short failure
[h264 @ 0x557af4f392c0] mmco: unref short failure
[h264 @ 0x557af4f392c0] mmco: unref short failure
  4%|▍         | 106/2755 [37:29<14:37:36, 19.88s/it]  4%|▍         | 107/2755 [37:35<11:24:20, 15.51s/it][h264 @ 0x557af4b5d780] mmco: unref short failure
  4%|▍         | 108/2755 [37:40<9:14:59, 12.58s/it] 09/16/2024 16:50:20 - INFO - __main__ -   current idx 7vvB1s2Ia00.73 from finetune_area returns wrong image/video, use 134800 instead.
[h264 @ 0x56178114c500] mmco: unref short failure
[h264 @ 0x56178114c500] mmco: unref short failure
  4%|▍         | 109/2755 [37:46<7:39:10, 10.41s/it][h264 @ 0x5615ed841d40] mmco: unref short failure
[h264 @ 0x5615ed841d40] mmco: unref short failure
  4%|▍         | 110/2755 [37:51<6:35:28,  8.97s/it]  4%|▍         | 111/2755 [37:56<5:44:40,  7.82s/it]  4%|▍         | 112/2755 [38:02<5:08:44,  7.01s/it][h264 @ 0x55f3db711d00] mmco: unref short failure
[h264 @ 0x55f3db711d00] mmco: unref short failure
  4%|▍         | 113/2755 [38:07<4:43:07,  6.43s/it][h264 @ 0x5615df299540] mmco: unref short failure
  4%|▍         | 114/2755 [38:12<4:33:57,  6.22s/it][h264 @ 0x557ae58cb140] mmco: unref short failure
[h264 @ 0x5615d94991c0] mmco: unref short failure
  4%|▍         | 115/2755 [38:17<4:15:51,  5.82s/it][h264 @ 0x557ae956b740] mmco: unref short failure
[h264 @ 0x55f3db648840] mmco: unref short failure
[h264 @ 0x55f3db648840] mmco: unref short failure
[h264 @ 0x55f3eb3ae840] mmco: unref short failure
[h264 @ 0x55f3d2426b40] mmco: unref short failure
[h264 @ 0x55f3d2426b40] mmco: unref short failure
[h264 @ 0x55f3eab0df00] mmco: unref short failure
[h264 @ 0x55f3eab0df00] mmco: unref short failure
[h264 @ 0x55f3eab0df00] mmco: unref short failure
[h264 @ 0x55f3eab0df00] mmco: unref short failure
09/16/2024 16:51:06 - INFO - __main__ -   current idx _QteUi76_ZA.36 from finetune_area returns wrong image/video, use 47029 instead.
[h264 @ 0x55f3e98d9600] mmco: unref short failure
[h264 @ 0x55f3e98d9600] mmco: unref short failure
[h264 @ 0x5615ed06eac0] mmco: unref short failure
[h264 @ 0x55f3e2050b00] mmco: unref short failure
[h264 @ 0x55f3e2050b00] mmco: unref short failure
[h264 @ 0x561779910480] mmco: unref short failure
[h264 @ 0x5617718c5600] mmco: unref short failure
[h264 @ 0x5617718c5600] mmco: unref short failure
  4%|▍         | 116/2755 [39:01<12:42:04, 17.33s/it][h264 @ 0x5615e3447e40] mmco: unref short failure
[h264 @ 0x5615e3447e40] mmco: unref short failure
[h264 @ 0x56177502bd40] mmco: unref short failure
[h264 @ 0x56177502bd40] mmco: unref short failure
[h264 @ 0x557af2b776c0] mmco: unref short failure
[h264 @ 0x557af2b776c0] mmco: unref short failure
[h264 @ 0x557af2b776c0] mmco: unref short failure
[h264 @ 0x557af2b776c0] mmco: unref short failure
[h264 @ 0x557ade76b540] mmco: unref short failure
[h264 @ 0x5615e4a82b40] mmco: unref short failure
[h264 @ 0x5615e4a82b40] mmco: unref short failure
[h264 @ 0x55f3eece0100] mmco: unref short failure
  4%|▍         | 117/2755 [39:18<12:37:27, 17.23s/it][h264 @ 0x557afabd1c40] mmco: unref short failure
09/16/2024 16:52:10 - INFO - __main__ -   current idx O7bldPqp3YQ.24 from finetune_area returns wrong image/video, use 49134 instead.
  4%|▍         | 118/2755 [39:37<12:51:33, 17.56s/it][h264 @ 0x55f3db6fe580] mmco: unref short failure
  4%|▍         | 119/2755 [39:43<10:17:16, 14.05s/it]  4%|▍         | 120/2755 [39:48<8:22:42, 11.45s/it] [h264 @ 0x5617746e0a40] mmco: unref short failure
  4%|▍         | 121/2755 [39:55<7:24:52, 10.13s/it]  4%|▍         | 122/2755 [40:01<6:25:41,  8.79s/it][h264 @ 0x5617746e0100] mmco: unref short failure
[h264 @ 0x55f3edbe8e40] mmco: unref short failure
[h264 @ 0x55f3edbe8e40] mmco: unref short failure
[h264 @ 0x55f3ec392100] mmco: unref short failure
[h264 @ 0x55f3ec392100] mmco: unref short failure
  4%|▍         | 123/2755 [40:07<5:56:52,  8.14s/it][h264 @ 0x55f3db7259c0] mmco: unref short failure
[h264 @ 0x5615ec0daf40] mmco: unref short failure
[h264 @ 0x5615ec0daf40] mmco: unref short failure
[h264 @ 0x5615ea92f280] mmco: unref short failure
[h264 @ 0x5615ea92f280] mmco: unref short failure
[h264 @ 0x561766546600] mmco: unref short failure
[h264 @ 0x56178037cec0] mmco: unref short failure
[h264 @ 0x56178037cec0] mmco: unref short failure
[h264 @ 0x557af2df7dc0] mmco: unref short failure
[h264 @ 0x557af2df7dc0] mmco: unref short failure
[h264 @ 0x557afa4f66c0] mmco: unref short failure
[h264 @ 0x557afa4f66c0] mmco: unref short failure
[h264 @ 0x561782594a40] mmco: unref short failure
[h264 @ 0x557af236d100] mmco: unref short failure
[h264 @ 0x5615d818e300] mmco: unref short failure
[h264 @ 0x561783fe75c0] mmco: unref short failure
[h264 @ 0x56177ff1d8c0] mmco: unref short failure
[h264 @ 0x557adedb2780] mmco: unref short failure
[h264 @ 0x557adedb2780] mmco: unref short failure
[h264 @ 0x5615d4a26140] mmco: unref short failure
[h264 @ 0x5615d4a26140] mmco: unref short failure
[h264 @ 0x55f3ee7f6840] mmco: unref short failure
[h264 @ 0x55f3ee7f6840] mmco: unref short failure
[h264 @ 0x55f3ee7f6840] mmco: unref short failure
[h264 @ 0x55f3ee7f6840] mmco: unref short failure
[h264 @ 0x55f3e65395c0] mmco: unref short failure
[h264 @ 0x557ae43d1780] mmco: unref short failure
[h264 @ 0x557ae43d1780] mmco: unref short failure
[h264 @ 0x557ae43d1780] mmco: unref short failure
[h264 @ 0x557ae43d1780] mmco: unref short failure
  5%|▍         | 124/2755 [41:24<20:59:31, 28.72s/it][h264 @ 0x557adbdae940] mmco: unref short failure
  5%|▍         | 125/2755 [41:36<17:13:24, 23.58s/it][h264 @ 0x561767ae7400] mmco: unref short failure
[h264 @ 0x561767ae7400] mmco: unref short failure
  5%|▍         | 126/2755 [42:00<17:19:29, 23.72s/it][h264 @ 0x5615ed65c2c0] mmco: unref short failure
[h264 @ 0x5615ed65c2c0] mmco: unref short failure
[h264 @ 0x55f3ee086f80] mmco: unref short failure
[h264 @ 0x55f3ee086f80] mmco: unref short failure
[h264 @ 0x55f3ee086f80] mmco: unref short failure
[h264 @ 0x55f3ee086f80] mmco: unref short failure
  5%|▍         | 127/2755 [42:05<13:16:07, 18.18s/it]  5%|▍         | 128/2755 [42:11<10:30:42, 14.41s/it]  5%|▍         | 129/2755 [42:16<8:35:17, 11.77s/it]   5%|▍         | 130/2755 [42:22<7:12:33,  9.89s/it]  5%|▍         | 131/2755 [42:30<6:56:08,  9.52s/it][h264 @ 0x55f3e5e51880] mmco: unref short failure
[h264 @ 0x55f3e5e51880] mmco: unref short failure
[h264 @ 0x5615ee196e00] mmco: unref short failure
[h264 @ 0x5615ee196e00] mmco: unref short failure
[h264 @ 0x56177d033680] mmco: unref short failure
[h264 @ 0x5615ed3ab040] mmco: unref short failure
[h264 @ 0x5615ed3ab040] mmco: unref short failure
09/16/2024 16:55:50 - INFO - __main__ -   current idx RqGCXAHa3g8.17 from finetune_area returns wrong image/video, use 20291 instead.
[h264 @ 0x56176a2df540] mmco: unref short failure
[h264 @ 0x56176a2df540] mmco: unref short failure
[h264 @ 0x55f3daac2cc0] mmco: unref short failure
[h264 @ 0x55f3daac2cc0] mmco: unref short failure
[h264 @ 0x55f3e1246500] mmco: unref short failure
[h264 @ 0x56177c866040] mmco: unref short failure
[h264 @ 0x557afa849640] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x56177b60e100] mmco: unref short failure
[h264 @ 0x557aea484cc0] mmco: unref short failure
[h264 @ 0x557aea484cc0] mmco: unref short failure
[h264 @ 0x55f3eea05940] mmco: unref short failure
[h264 @ 0x5615f3887400] mmco: unref short failure
[h264 @ 0x561769b02f80] mmco: unref short failure
[h264 @ 0x561769b02f80] mmco: unref short failure
[h264 @ 0x561773197700] mmco: unref short failure
[h264 @ 0x561773197700] mmco: unref short failure
[h264 @ 0x557aef152800] mmco: unref short failure
[h264 @ 0x55f3e3a3d980] mmco: unref short failure
[h264 @ 0x55f3dea55000] mmco: unref short failure
[h264 @ 0x55f3dea55000] mmco: unref short failure
[h264 @ 0x55f3dea55000] mmco: unref short failure
[h264 @ 0x55f3dea55000] mmco: unref short failure
  5%|▍         | 132/2755 [43:46<21:17:40, 29.23s/it][h264 @ 0x55f3dbf5f380] mmco: unref short failure
  5%|▍         | 133/2755 [43:59<17:53:22, 24.56s/it][h264 @ 0x561768151c80] mmco: unref short failure
  5%|▍         | 134/2755 [44:19<16:52:11, 23.17s/it]09/16/2024 16:56:57 - INFO - __main__ -   current idx E-kDX2OZOQY.26 from finetune_area returns wrong image/video, use 109609 instead.
  5%|▍         | 135/2755 [44:25<13:07:10, 18.03s/it][h264 @ 0x5615dfb5e500] mmco: unref short failure
[h264 @ 0x5615dfb5e500] mmco: unref short failure
[h264 @ 0x5615e68538c0] mmco: unref short failure
[h264 @ 0x5615e68538c0] mmco: unref short failure
[h264 @ 0x5615ece1a000] mmco: unref short failure
[h264 @ 0x5615ece1a000] mmco: unref short failure
[h264 @ 0x557af6dd0000] mmco: unref short failure
[h264 @ 0x557af6dd0000] mmco: unref short failure
  5%|▍         | 136/2755 [44:30<10:19:22, 14.19s/it][h264 @ 0x5615db37e440] mmco: unref short failure
  5%|▍         | 137/2755 [44:44<10:05:58, 13.89s/it][h264 @ 0x5615d78b00c0] mmco: unref short failure
  5%|▌         | 138/2755 [44:50<8:22:00, 11.51s/it] [h264 @ 0x5615ef457080] mmco: unref short failure
[h264 @ 0x5615ef457080] mmco: unref short failure
[h264 @ 0x557ae55bcbc0] mmco: unref short failure
[h264 @ 0x557ae55bcbc0] mmco: unref short failure
  5%|▌         | 139/2755 [44:55<7:05:50,  9.77s/it][h264 @ 0x561786157e80] mmco: unref short failure
[h264 @ 0x5615d6b98980] mmco: unref short failure
[h264 @ 0x5615d6b98980] mmco: unref short failure
[h264 @ 0x5615f1969c00] mmco: unref short failure
[h264 @ 0x5615f1969c00] mmco: unref short failure
[h264 @ 0x55f3d2426680] mmco: unref short failure
[h264 @ 0x56177208dec0] mmco: unref short failure
[h264 @ 0x56177208dec0] mmco: unref short failure
[h264 @ 0x5615d9112080] mmco: unref short failure
[h264 @ 0x5615d9112080] mmco: unref short failure
[h264 @ 0x5615d9112080] mmco: unref short failure
[h264 @ 0x5615d9112080] mmco: unref short failure
[h264 @ 0x55f3d76c0bc0] mmco: unref short failure
[h264 @ 0x55f3d76c0bc0] mmco: unref short failure
[h264 @ 0x55f3d76c0bc0] mmco: unref short failure
[h264 @ 0x55f3d76c0bc0] mmco: unref short failure
[h264 @ 0x5615f25d9400] mmco: unref short failure
[h264 @ 0x5615f25d9400] mmco: unref short failure
[h264 @ 0x55f3ec12a140] mmco: unref short failure
[h264 @ 0x55f3ec12a140] mmco: unref short failure
[h264 @ 0x5615d73c0680] mmco: unref short failure
[h264 @ 0x5615d73c0680] mmco: unref short failure
[h264 @ 0x55f3e1ab9a80] mmco: unref short failure
[h264 @ 0x55f3e1ab9a80] mmco: unref short failure
[h264 @ 0x5615f6b7e480] mmco: unref short failure
[h264 @ 0x5615f6b7e480] mmco: unref short failure
  5%|▌         | 140/2755 [46:19<23:07:10, 31.83s/it][h264 @ 0x5617794e5d80] mmco: unref short failure
[h264 @ 0x5617794e5d80] mmco: unref short failure
[h264 @ 0x5617794e5d80] mmco: unref short failure
  5%|▌         | 141/2755 [46:24<17:23:59, 23.96s/it][h264 @ 0x5617849a54c0] mmco: unref short failure
[h264 @ 0x5615d46c7d00] mmco: unref short failure
  5%|▌         | 142/2755 [46:41<15:51:32, 21.85s/it][h264 @ 0x561766c7b2c0] mmco: unref short failure
[h264 @ 0x561766c7b2c0] mmco: unref short failure
[h264 @ 0x561776825900] mmco: unref short failure
[h264 @ 0x561776825900] mmco: unref short failure
  5%|▌         | 143/2755 [46:48<12:33:44, 17.31s/it]  5%|▌         | 144/2755 [46:53<9:54:53, 13.67s/it] [h264 @ 0x5617812cf540] mmco: unref short failure
[h264 @ 0x5615f46494c0] mmco: unref short failure
[h264 @ 0x5615d80f9c80] mmco: unref short failure
[h264 @ 0x5615d80f9c80] mmco: unref short failure
  5%|▌         | 145/2755 [47:05<9:34:31, 13.21s/it]  5%|▌         | 146/2755 [47:10<7:49:56, 10.81s/it][h264 @ 0x5617861579c0] mmco: unref short failure
[h264 @ 0x557af0f7b480] mmco: unref short failure
  5%|▌         | 147/2755 [47:16<6:49:18,  9.42s/it][h264 @ 0x56177b52fc80] mmco: unref short failure
[h264 @ 0x56177b52fc80] mmco: unref short failure
[h264 @ 0x56177b52fc80] mmco: unref short failure
[h264 @ 0x56177b52fc80] mmco: unref short failure
09/16/2024 16:59:56 - INFO - __main__ -   current idx Shp-TGprhM8.19 from finetune_area returns wrong image/video, use 71867 instead.
[h264 @ 0x55f3eae346c0] mmco: unref short failure
[h264 @ 0x55f3eae346c0] mmco: unref short failure
[h264 @ 0x5615d2d3aa80] mmco: unref short failure
[h264 @ 0x55f3df2a9100] mmco: unref short failure
[h264 @ 0x55f3df2a9100] mmco: unref short failure
[h264 @ 0x55f3df2a9100] mmco: unref short failure
[h264 @ 0x55f3df2a9100] mmco: unref short failure
[h264 @ 0x557af4a7de80] mmco: unref short failure
[h264 @ 0x56178462a780] mmco: unref short failure
[h264 @ 0x56178462a780] mmco: unref short failure
[h264 @ 0x557af69c3140] mmco: unref short failure
[h264 @ 0x557af69c3140] mmco: unref short failure
[h264 @ 0x5615eec6d4c0] mmco: unref short failure
[h264 @ 0x5615eec6d4c0] mmco: unref short failure
[h264 @ 0x55f3d9d8d6c0] mmco: unref short failure
[h264 @ 0x55f3d9d8d6c0] mmco: unref short failure
[h264 @ 0x557ae7b87840] mmco: unref short failure
[h264 @ 0x557ae7b87840] mmco: unref short failure
[h264 @ 0x55f3d18e3940] mmco: unref short failure
[h264 @ 0x55f3d18e3940] mmco: unref short failure
[h264 @ 0x55f3e745d880] mmco: unref short failure
[h264 @ 0x557ae3a4dd80] mmco: unref short failure
[h264 @ 0x557ade5fab80] mmco: unref short failure
[h264 @ 0x557ade5fab80] mmco: unref short failure
[h264 @ 0x557adb5f4940] mmco: unref short failure
[h264 @ 0x557adb5f4940] mmco: unref short failure
[h264 @ 0x56177a360080] mmco: unref short failure
[h264 @ 0x56177a360080] mmco: unref short failure
[h264 @ 0x56177a360080] mmco: unref short failure
[h264 @ 0x56177a360080] mmco: unref short failure
[h264 @ 0x55f3f0893b40] mmco: unref short failure
[h264 @ 0x55f3f0893b40] mmco: unref short failure
[h264 @ 0x55f3f0893b40] mmco: unref short failure
[h264 @ 0x5617806e3600] mmco: unref short failure
[h264 @ 0x56177b654e80] mmco: unref short failure
[h264 @ 0x56177b654e80] mmco: unref short failure
09/16/2024 17:01:07 - INFO - __main__ -   current idx E852WPZZnME.32 from finetune_area returns wrong image/video, use 115042 instead.
[h264 @ 0x557af87bae80] mmco: unref short failure
[h264 @ 0x5615dfb5e700] mmco: unref short failure
[h264 @ 0x5615dfb5e700] mmco: unref short failure
09/16/2024 17:01:21 - INFO - __main__ -   current idx 6wN4IYAiKIg.50 from finetune_area returns wrong image/video, use 137925 instead.
[h264 @ 0x5615d547fe40] mmco: unref short failure
[h264 @ 0x5615d547fe40] mmco: unref short failure
[h264 @ 0x5615d8f08480] mmco: unref short failure
[h264 @ 0x5615d8f08480] mmco: unref short failure
  5%|▌         | 148/2755 [48:50<25:09:36, 34.74s/it]  5%|▌         | 149/2755 [48:56<18:53:01, 26.09s/it]09/16/2024 17:01:33 - INFO - __main__ -   evaluate on ret%tva--msrvtt_ret task
09/16/2024 17:01:33 - INFO - __main__ -   start running ret%tva validation...
[h264 @ 0x55f3e07c0540] mmco: unref short failure
[h264 @ 0x557af15b4240] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x557af15b4240] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5615d91501c0] mmco: unref short failure
[h264 @ 0x5615d574c2c0] mmco: unref short failure
[h264 @ 0x5615d574c2c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x56177b654e80] mmco: unref short failure
[h264 @ 0x56177b654e80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x557ae2895f80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5615d8f5d480] mmco: unref short failure
[h264 @ 0x56176bcf60c0] mmco: unref short failure
[h264 @ 0x56176bcf60c0] mmco: unref short failure
[h264 @ 0x557afa4f66c0] mmco: unref short failure
[h264 @ 0x557afa4f66c0] mmco: unref short failure
[h264 @ 0x56177e064cc0] mmco: unref short failure
[h264 @ 0x56177e064cc0] mmco: unref short failure
[h264 @ 0x557af7e10740] mmco: unref short failure
[h264 @ 0x557af16afb00] mmco: unref short failure
[h264 @ 0x557af16afb00] mmco: unref short failure
[h264 @ 0x557af16afb00] mmco: unref short failure
[h264 @ 0x557af16afb00] mmco: unref short failure
[h264 @ 0x5617665ee3c0] mmco: unref short failure
[h264 @ 0x5617665ee3c0] mmco: unref short failure
[h264 @ 0x5617665ee3c0] mmco: unref short failure
[h264 @ 0x5617665ee3c0] mmco: unref short failure
[h264 @ 0x5617665ee3c0] mmco: unref short failure
[h264 @ 0x5617665ee3c0] mmco: unref short failure
[h264 @ 0x56178388fac0] mmco: unref short failure
[h264 @ 0x56177b52de40] mmco: unref short failure
[h264 @ 0x56177b52de40] mmco: unref short failure
[h264 @ 0x56176f9e1dc0] mmco: unref short failure
[h264 @ 0x55f3f0a95940] mmco: unref short failure
[h264 @ 0x55f3f0a95940] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:43,  2.12it/s][A
  1%|          | 2/221 [00:00<01:48,  2.01it/s][A
  1%|▏         | 3/221 [00:01<01:51,  1.96it/s][A
  2%|▏         | 4/221 [00:01<01:25,  2.54it/s][A
  2%|▏         | 5/221 [00:01<01:09,  3.10it/s][A
  3%|▎         | 6/221 [00:02<00:57,  3.76it/s][A
  3%|▎         | 7/221 [00:02<00:59,  3.63it/s][A
  4%|▎         | 8/221 [00:02<00:51,  4.12it/s][A
  4%|▍         | 9/221 [00:02<00:55,  3.84it/s][A
  5%|▍         | 10/221 [00:03<01:11,  2.96it/s][A
  5%|▍         | 11/221 [00:03<00:58,  3.59it/s][A
  5%|▌         | 12/221 [00:03<00:58,  3.59it/s][A
  6%|▌         | 13/221 [00:03<00:53,  3.91it/s][A
  6%|▋         | 14/221 [00:04<00:50,  4.12it/s][A
  7%|▋         | 15/221 [00:04<00:51,  3.97it/s][A
  7%|▋         | 16/221 [00:04<01:04,  3.19it/s][A
  8%|▊         | 17/221 [00:05<01:23,  2.45it/s][A
  8%|▊         | 18/221 [00:05<01:16,  2.66it/s][A
  9%|▊         | 19/221 [00:06<01:05,  3.10it/s][A
  9%|▉         | 20/221 [00:06<00:55,  3.60it/s][A
 10%|▉         | 21/221 [00:06<01:02,  3.21it/s][A
 10%|▉         | 22/221 [00:07<01:06,  3.00it/s][A
 10%|█         | 23/221 [00:07<00:53,  3.71it/s][A
 11%|█         | 24/221 [00:07<00:48,  4.07it/s][A
 11%|█▏        | 25/221 [00:07<00:45,  4.34it/s][A
 12%|█▏        | 26/221 [00:07<00:55,  3.50it/s][A
 12%|█▏        | 27/221 [00:08<00:45,  4.30it/s][A
 13%|█▎        | 28/221 [00:08<01:11,  2.68it/s][A
 13%|█▎        | 29/221 [00:09<01:05,  2.92it/s][A
 14%|█▎        | 30/221 [00:09<01:09,  2.74it/s][A
 14%|█▍        | 31/221 [00:09<01:11,  2.67it/s][A
 14%|█▍        | 32/221 [00:09<00:55,  3.39it/s][A
 15%|█▍        | 33/221 [00:10<00:48,  3.85it/s][A
 16%|█▌        | 35/221 [00:10<00:42,  4.39it/s][A
 16%|█▋        | 36/221 [00:10<00:50,  3.68it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.25it/s][A
 17%|█▋        | 38/221 [00:11<01:03,  2.90it/s][A
 18%|█▊        | 39/221 [00:11<00:50,  3.60it/s][A
 18%|█▊        | 40/221 [00:12<00:48,  3.70it/s][A
 19%|█▊        | 41/221 [00:12<00:43,  4.15it/s][A
 19%|█▉        | 42/221 [00:12<00:56,  3.16it/s][A
 19%|█▉        | 43/221 [00:12<00:48,  3.71it/s][A
 20%|█▉        | 44/221 [00:13<00:42,  4.19it/s][A
 20%|██        | 45/221 [00:13<00:56,  3.10it/s][A
 21%|██        | 46/221 [00:13<00:51,  3.38it/s][A
 21%|██▏       | 47/221 [00:14<01:22,  2.11it/s][A
 22%|██▏       | 48/221 [00:14<01:07,  2.57it/s][A
 22%|██▏       | 49/221 [00:15<01:09,  2.49it/s][A[h264 @ 0x557adbb55a80] mmco: unref short failure

 23%|██▎       | 50/221 [00:15<01:13,  2.32it/s][A
 23%|██▎       | 51/221 [00:16<01:13,  2.31it/s][A
 24%|██▎       | 52/221 [00:16<01:05,  2.59it/s][A
 24%|██▍       | 53/221 [00:16<00:54,  3.07it/s][A
 24%|██▍       | 54/221 [00:18<01:55,  1.45it/s][A
 25%|██▍       | 55/221 [00:18<01:39,  1.68it/s][A
 25%|██▌       | 56/221 [00:18<01:16,  2.17it/s][A
 26%|██▌       | 57/221 [00:19<01:03,  2.58it/s][A
 26%|██▌       | 58/221 [00:19<00:52,  3.12it/s][A
 27%|██▋       | 59/221 [00:19<00:43,  3.70it/s][A
 27%|██▋       | 60/221 [00:20<01:13,  2.19it/s][A
 28%|██▊       | 61/221 [00:20<01:01,  2.61it/s][A
 28%|██▊       | 62/221 [00:20<00:58,  2.71it/s][A
 29%|██▊       | 63/221 [00:20<00:50,  3.13it/s][A
 29%|██▉       | 64/221 [00:21<00:59,  2.66it/s][A
 29%|██▉       | 65/221 [00:21<00:55,  2.81it/s][A[h264 @ 0x5615dd160c00] mmco: unref short failure
[h264 @ 0x5615dd160c00] mmco: unref short failure
[h264 @ 0x5615dd160c00] mmco: unref short failure

 30%|██▉       | 66/221 [00:22<01:12,  2.15it/s][A
 30%|███       | 67/221 [00:22<01:06,  2.32it/s][A
 31%|███       | 68/221 [00:23<01:01,  2.48it/s][A
 31%|███       | 69/221 [00:24<01:26,  1.75it/s][A
 32%|███▏      | 70/221 [00:24<01:12,  2.07it/s][A
 32%|███▏      | 71/221 [00:24<01:00,  2.47it/s][A
 33%|███▎      | 72/221 [00:25<01:01,  2.42it/s][A
 33%|███▎      | 73/221 [00:25<01:02,  2.35it/s][A
 33%|███▎      | 74/221 [00:25<00:50,  2.92it/s][A
 34%|███▍      | 75/221 [00:26<00:53,  2.75it/s][A
 34%|███▍      | 76/221 [00:26<00:53,  2.72it/s][A
 35%|███▍      | 77/221 [00:26<00:56,  2.54it/s][A
 35%|███▌      | 78/221 [00:27<00:44,  3.20it/s][A
 36%|███▌      | 79/221 [00:27<00:47,  2.96it/s][A
 36%|███▌      | 80/221 [00:27<00:44,  3.19it/s][A[h264 @ 0x55f3d26b9e40] mmco: unref short failure

 37%|███▋      | 81/221 [00:28<00:46,  2.98it/s][A
 37%|███▋      | 82/221 [00:28<00:52,  2.65it/s][A
 38%|███▊      | 83/221 [00:29<01:09,  2.00it/s][A
 38%|███▊      | 84/221 [00:29<01:04,  2.14it/s][A
 38%|███▊      | 85/221 [00:29<00:50,  2.69it/s][A
 39%|███▉      | 86/221 [00:30<00:42,  3.18it/s][A
 39%|███▉      | 87/221 [00:31<01:17,  1.73it/s][A
 40%|███▉      | 88/221 [00:31<01:20,  1.64it/s][A
 40%|████      | 89/221 [00:32<01:06,  1.98it/s][A
 41%|████      | 90/221 [00:32<00:56,  2.32it/s][A
 41%|████      | 91/221 [00:32<00:45,  2.85it/s][A
 42%|████▏     | 92/221 [00:32<00:41,  3.09it/s][A
 42%|████▏     | 93/221 [00:33<00:45,  2.80it/s][A
 43%|████▎     | 94/221 [00:33<00:42,  3.01it/s][A
 43%|████▎     | 95/221 [00:34<00:43,  2.91it/s][A[h264 @ 0x557af3100c40] mmco: unref short failure
[h264 @ 0x557af3100c40] mmco: unref short failure

 43%|████▎     | 96/221 [00:34<00:53,  2.34it/s][A[h264 @ 0x557af3100c40] mmco: unref short failure
[h264 @ 0x557af3100c40] mmco: unref short failure

 44%|████▍     | 97/221 [00:35<00:55,  2.23it/s][A
 44%|████▍     | 98/221 [00:36<01:13,  1.68it/s][A
 45%|████▍     | 99/221 [00:36<01:03,  1.93it/s][A
 45%|████▌     | 100/221 [00:36<00:54,  2.24it/s][A
 46%|████▌     | 101/221 [00:36<00:46,  2.57it/s][A
 46%|████▌     | 102/221 [00:37<00:51,  2.30it/s][A
 47%|████▋     | 103/221 [00:37<00:41,  2.87it/s][A
 47%|████▋     | 104/221 [00:37<00:37,  3.11it/s][A
 48%|████▊     | 105/221 [00:38<00:42,  2.73it/s][A[h264 @ 0x557ae28e61c0] mmco: unref short failure
[h264 @ 0x557ae28e61c0] mmco: unref short failure

 48%|████▊     | 106/221 [00:39<00:59,  1.95it/s][A
 48%|████▊     | 107/221 [00:39<00:51,  2.19it/s][A
 49%|████▉     | 108/221 [00:39<00:45,  2.50it/s][A
 49%|████▉     | 109/221 [00:40<00:39,  2.82it/s][A
 50%|████▉     | 110/221 [00:40<00:36,  3.04it/s][A
 50%|█████     | 111/221 [00:40<00:38,  2.88it/s][A
 51%|█████     | 112/221 [00:41<00:36,  3.01it/s][A
 51%|█████     | 113/221 [00:41<00:34,  3.13it/s][A
 52%|█████▏    | 114/221 [00:41<00:28,  3.79it/s][A
 52%|█████▏    | 115/221 [00:41<00:24,  4.37it/s][A
 52%|█████▏    | 116/221 [00:42<00:38,  2.71it/s][A
 53%|█████▎    | 117/221 [00:42<00:39,  2.62it/s][A
 53%|█████▎    | 118/221 [00:42<00:34,  2.97it/s][A
 54%|█████▍    | 119/221 [00:43<00:34,  2.98it/s][A
 54%|█████▍    | 120/221 [00:43<00:30,  3.27it/s][A
 55%|█████▍    | 121/221 [00:43<00:29,  3.38it/s][A
 55%|█████▌    | 122/221 [00:44<00:36,  2.72it/s][A
 56%|█████▌    | 123/221 [00:44<00:35,  2.76it/s][A
 56%|█████▌    | 124/221 [00:45<00:40,  2.40it/s][A
 57%|█████▋    | 125/221 [00:45<00:44,  2.16it/s][A
 57%|█████▋    | 126/221 [00:46<00:40,  2.37it/s][A
 57%|█████▋    | 127/221 [00:47<01:06,  1.41it/s][A
 58%|█████▊    | 128/221 [00:47<00:56,  1.65it/s][A
 58%|█████▊    | 129/221 [00:48<00:43,  2.10it/s][A
 59%|█████▉    | 130/221 [00:48<00:34,  2.62it/s][A
 59%|█████▉    | 131/221 [00:48<00:30,  2.92it/s][A
 60%|█████▉    | 132/221 [00:49<00:56,  1.58it/s][A
 60%|██████    | 133/221 [00:50<00:49,  1.76it/s][A
 61%|██████    | 134/221 [00:51<00:58,  1.49it/s][A
 61%|██████    | 135/221 [00:51<00:57,  1.50it/s][A
 62%|██████▏   | 136/221 [00:52<00:49,  1.72it/s][A
 62%|██████▏   | 137/221 [00:52<00:41,  2.00it/s][A
 62%|██████▏   | 138/221 [00:52<00:40,  2.06it/s][A
 63%|██████▎   | 139/221 [00:53<00:47,  1.72it/s][A
 63%|██████▎   | 140/221 [00:54<00:43,  1.88it/s][A
 64%|██████▍   | 141/221 [00:54<00:38,  2.10it/s][A[h264 @ 0x56177d125740] mmco: unref short failure

 64%|██████▍   | 142/221 [00:54<00:34,  2.27it/s][A
 65%|██████▍   | 143/221 [00:55<00:36,  2.12it/s][A
 65%|██████▌   | 144/221 [00:55<00:33,  2.31it/s][A
 66%|██████▌   | 145/221 [00:55<00:26,  2.91it/s][A
 66%|██████▌   | 146/221 [00:55<00:20,  3.65it/s][A
 67%|██████▋   | 147/221 [00:56<00:21,  3.47it/s][A
 67%|██████▋   | 148/221 [00:56<00:24,  3.02it/s][A
 67%|██████▋   | 149/221 [00:57<00:29,  2.46it/s][A
 68%|██████▊   | 150/221 [00:57<00:24,  2.86it/s][A
 68%|██████▊   | 151/221 [00:58<00:29,  2.39it/s][A
 69%|██████▉   | 152/221 [00:59<00:41,  1.67it/s][A
 69%|██████▉   | 153/221 [00:59<00:31,  2.16it/s][A
 70%|██████▉   | 154/221 [00:59<00:26,  2.51it/s][A
 70%|███████   | 155/221 [00:59<00:24,  2.70it/s][A
 71%|███████   | 156/221 [01:00<00:26,  2.45it/s][A
 71%|███████   | 157/221 [01:00<00:28,  2.28it/s][A
 71%|███████▏  | 158/221 [01:01<00:24,  2.58it/s][A
 72%|███████▏  | 159/221 [01:01<00:19,  3.11it/s][A
 72%|███████▏  | 160/221 [01:01<00:17,  3.56it/s][A
 73%|███████▎  | 161/221 [01:01<00:15,  4.00it/s][A
 73%|███████▎  | 162/221 [01:01<00:12,  4.63it/s][A
 74%|███████▍  | 163/221 [01:02<00:13,  4.22it/s][A
 74%|███████▍  | 164/221 [01:02<00:14,  3.93it/s][A
 75%|███████▍  | 165/221 [01:02<00:14,  3.99it/s][A
 75%|███████▌  | 166/221 [01:03<00:18,  3.00it/s][A
 76%|███████▌  | 167/221 [01:03<00:14,  3.68it/s][A
 76%|███████▌  | 168/221 [01:03<00:20,  2.57it/s][A
 76%|███████▋  | 169/221 [01:03<00:16,  3.24it/s][A[h264 @ 0x56177aac1140] mmco: unref short failure

 77%|███████▋  | 170/221 [01:04<00:14,  3.45it/s][A
 77%|███████▋  | 171/221 [01:04<00:17,  2.94it/s][A
 78%|███████▊  | 172/221 [01:04<00:15,  3.24it/s][A
 78%|███████▊  | 173/221 [01:05<00:17,  2.70it/s][A
 79%|███████▊  | 174/221 [01:05<00:14,  3.20it/s][A
 79%|███████▉  | 175/221 [01:05<00:14,  3.28it/s][A
 80%|███████▉  | 176/221 [01:06<00:11,  3.76it/s][A
 80%|████████  | 177/221 [01:06<00:10,  4.14it/s][A
 81%|████████  | 178/221 [01:07<00:19,  2.18it/s][A
 81%|████████  | 179/221 [01:07<00:18,  2.21it/s][A
 81%|████████▏ | 180/221 [01:07<00:14,  2.82it/s][A
 82%|████████▏ | 182/221 [01:08<00:10,  3.81it/s][A
[h264 @ 0x56176599e240] mmco: unref short failure
 83%|████████▎ | 183/221 [01:08<00:10,  3.60it/s][A[h264 @ 0x56176599e240] mmco: unref short failure
[h264 @ 0x56176599e240] mmco: unref short failure
[h264 @ 0x56176599e240] mmco: unref short failure
[h264 @ 0x56176599e240] mmco: unref short failure

 83%|████████▎ | 184/221 [01:08<00:11,  3.27it/s][A
 84%|████████▍ | 186/221 [01:09<00:11,  2.96it/s][A
 85%|████████▍ | 187/221 [01:09<00:10,  3.10it/s][A
 85%|████████▌ | 188/221 [01:10<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [01:10<00:10,  3.18it/s][A
 86%|████████▌ | 190/221 [01:10<00:11,  2.78it/s][A
 86%|████████▋ | 191/221 [01:11<00:08,  3.41it/s][A
 87%|████████▋ | 192/221 [01:11<00:08,  3.36it/s][A
 87%|████████▋ | 193/221 [01:11<00:07,  3.66it/s][A
 88%|████████▊ | 194/221 [01:13<00:19,  1.37it/s][A
 88%|████████▊ | 195/221 [01:13<00:14,  1.76it/s][A
 89%|████████▊ | 196/221 [01:13<00:11,  2.13it/s][A
 89%|████████▉ | 197/221 [01:14<00:09,  2.42it/s][A
 90%|████████▉ | 198/221 [01:14<00:08,  2.86it/s][A
 90%|█████████ | 199/221 [01:14<00:07,  2.98it/s][A
 90%|█████████ | 200/221 [01:14<00:06,  3.48it/s][A
 91%|█████████ | 201/221 [01:14<00:05,  3.86it/s][A
 91%|█████████▏| 202/221 [01:15<00:05,  3.56it/s][A
 92%|█████████▏| 203/221 [01:15<00:04,  3.69it/s][A
 92%|█████████▏| 204/221 [01:15<00:04,  3.96it/s][A
 93%|█████████▎| 205/221 [01:15<00:03,  4.47it/s][A
 93%|█████████▎| 206/221 [01:16<00:04,  3.25it/s][A
 94%|█████████▍| 208/221 [01:16<00:03,  3.69it/s][A
 95%|█████████▍| 209/221 [01:17<00:03,  3.88it/s][A
 95%|█████████▌| 211/221 [01:17<00:02,  4.16it/s][A
 96%|█████████▌| 212/221 [01:17<00:02,  3.86it/s][A
 96%|█████████▋| 213/221 [01:18<00:01,  4.14it/s][A
 97%|█████████▋| 214/221 [01:18<00:02,  3.02it/s][A
 97%|█████████▋| 215/221 [01:18<00:01,  3.46it/s][A
 98%|█████████▊| 216/221 [01:19<00:01,  3.49it/s][A
 98%|█████████▊| 217/221 [01:19<00:01,  2.97it/s][A
 99%|█████████▊| 218/221 [01:19<00:00,  3.09it/s][A
 99%|█████████▉| 219/221 [01:20<00:00,  3.17it/s][A
100%|█████████▉| 220/221 [01:20<00:00,  2.13it/s][A
100%|██████████| 221/221 [01:21<00:00,  2.74it/s][A100%|██████████| 221/221 [01:21<00:00,  2.73it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:04,  3.40it/s][A
  1%|          | 2/221 [00:00<01:04,  3.39it/s][A
  1%|▏         | 3/221 [00:00<01:04,  3.39it/s][A
  2%|▏         | 4/221 [00:01<01:04,  3.36it/s][A[h264 @ 0x56176a7a1580] mmco: unref short failure

  2%|▏         | 5/221 [00:01<01:04,  3.37it/s][A
  3%|▎         | 6/221 [00:01<01:04,  3.33it/s][A
  3%|▎         | 7/221 [00:02<01:04,  3.29it/s][A
  4%|▎         | 8/221 [00:02<01:06,  3.20it/s][A
  4%|▍         | 9/221 [00:02<01:05,  3.26it/s][A
  5%|▍         | 10/221 [00:03<01:04,  3.25it/s][A
  5%|▍         | 11/221 [00:03<01:03,  3.29it/s][A
  5%|▌         | 12/221 [00:03<01:03,  3.29it/s][A
  6%|▌         | 13/221 [00:03<01:02,  3.32it/s][A
  6%|▋         | 14/221 [00:04<01:02,  3.33it/s][A
  7%|▋         | 15/221 [00:04<01:02,  3.30it/s][A
  7%|▋         | 16/221 [00:04<01:02,  3.26it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.27it/s][A
  8%|▊         | 18/221 [00:05<01:01,  3.28it/s][A
  9%|▊         | 19/221 [00:05<01:02,  3.25it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.28it/s][A
 10%|▉         | 21/221 [00:06<01:00,  3.29it/s][A
 10%|▉         | 22/221 [00:06<01:00,  3.28it/s][A
 10%|█         | 23/221 [00:06<01:00,  3.27it/s][A
 11%|█         | 24/221 [00:07<01:00,  3.26it/s][A
 11%|█▏        | 25/221 [00:07<00:59,  3.29it/s][A
 12%|█▏        | 26/221 [00:07<00:58,  3.32it/s][A
 12%|█▏        | 27/221 [00:08<00:58,  3.34it/s][A
 13%|█▎        | 28/221 [00:08<00:57,  3.35it/s][A
 13%|█▎        | 29/221 [00:08<00:57,  3.36it/s][A
 14%|█▎        | 30/221 [00:09<00:56,  3.37it/s][A
 14%|█▍        | 31/221 [00:09<00:56,  3.33it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.30it/s][A
 15%|█▍        | 33/221 [00:09<00:56,  3.32it/s][A
 15%|█▌        | 34/221 [00:10<00:56,  3.31it/s][A
 16%|█▌        | 35/221 [00:10<00:55,  3.33it/s][A
 16%|█▋        | 36/221 [00:10<00:58,  3.16it/s][A
 17%|█▋        | 37/221 [00:11<00:57,  3.21it/s][A
 17%|█▋        | 38/221 [00:11<00:56,  3.26it/s][A[h264 @ 0x55f3d1c3aa40] mmco: unref short failure
[h264 @ 0x55f3d1c3aa40] mmco: unref short failure

 18%|█▊        | 39/221 [00:11<00:55,  3.28it/s][A
 18%|█▊        | 40/221 [00:12<00:55,  3.28it/s][A
 19%|█▊        | 41/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 42/221 [00:12<00:53,  3.33it/s][A
 19%|█▉        | 43/221 [00:13<00:53,  3.35it/s][A
 20%|█▉        | 44/221 [00:13<00:52,  3.36it/s][A
 20%|██        | 45/221 [00:13<00:52,  3.37it/s][A
 21%|██        | 46/221 [00:13<00:52,  3.35it/s][A[h264 @ 0x55f3d168ae40] mmco: unref short failure
[h264 @ 0x55f3d168ae40] mmco: unref short failure

 21%|██▏       | 47/221 [00:14<00:51,  3.35it/s][A[h264 @ 0x557ae0877080] mmco: unref short failure

 22%|██▏       | 48/221 [00:14<00:51,  3.33it/s][A
 22%|██▏       | 49/221 [00:14<00:52,  3.31it/s][A[h264 @ 0x557adb3b2700] mmco: unref short failure

 23%|██▎       | 50/221 [00:15<00:51,  3.33it/s][A
 23%|██▎       | 51/221 [00:15<00:50,  3.35it/s][A
 24%|██▎       | 52/221 [00:15<00:50,  3.36it/s][A
 24%|██▍       | 53/221 [00:16<00:49,  3.36it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.33it/s][A
 25%|██▍       | 55/221 [00:16<00:49,  3.34it/s][A09/16/2024 17:05:33 - INFO - __main__ -   current idx aJ7i4YUrYo8.49 from finetune_area returns wrong image/video, use 101648 instead.

 25%|██▌       | 56/221 [00:16<00:49,  3.36it/s][A
 26%|██▌       | 57/221 [00:17<00:48,  3.37it/s][A
 26%|██▌       | 58/221 [00:17<00:48,  3.37it/s][A
 27%|██▋       | 59/221 [00:17<00:48,  3.33it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.33it/s][A
 28%|██▊       | 62/221 [00:18<00:47,  3.35it/s][A
 29%|██▊       | 63/221 [00:19<00:47,  3.35it/s][A
 29%|██▉       | 64/221 [00:19<00:46,  3.36it/s][A
 29%|██▉       | 65/221 [00:19<00:46,  3.37it/s][A
 30%|██▉       | 66/221 [00:19<00:45,  3.38it/s][A
 30%|███       | 67/221 [00:20<00:45,  3.38it/s][A
 31%|███       | 68/221 [00:20<00:45,  3.38it/s][A
 31%|███       | 69/221 [00:20<00:44,  3.39it/s][A
 32%|███▏      | 70/221 [00:21<00:44,  3.39it/s][A[h264 @ 0x5615e3579440] mmco: unref short failure

 32%|███▏      | 71/221 [00:21<00:44,  3.39it/s][A
 33%|███▎      | 72/221 [00:21<00:43,  3.39it/s][A
 33%|███▎      | 73/221 [00:21<00:43,  3.39it/s][A
 33%|███▎      | 74/221 [00:22<00:43,  3.39it/s][A
 34%|███▍      | 75/221 [00:22<00:43,  3.39it/s][A
 34%|███▍      | 76/221 [00:22<00:42,  3.39it/s][A
 35%|███▍      | 77/221 [00:23<00:42,  3.39it/s][A
 35%|███▌      | 78/221 [00:23<00:42,  3.39it/s][A
 36%|███▌      | 79/221 [00:23<00:41,  3.39it/s][A
 36%|███▌      | 80/221 [00:24<00:41,  3.40it/s][A
 37%|███▋      | 81/221 [00:24<00:41,  3.40it/s][A
 37%|███▋      | 82/221 [00:24<00:40,  3.40it/s][A
 38%|███▊      | 83/221 [00:24<00:40,  3.40it/s][A
 38%|███▊      | 84/221 [00:25<00:40,  3.40it/s][A
 38%|███▊      | 85/221 [00:25<00:40,  3.40it/s][A
 39%|███▉      | 86/221 [00:25<00:39,  3.40it/s][A
 39%|███▉      | 87/221 [00:26<00:39,  3.40it/s][A
 40%|███▉      | 88/221 [00:26<00:39,  3.40it/s][A
 40%|████      | 89/221 [00:26<00:38,  3.40it/s][A
 41%|████      | 90/221 [00:26<00:38,  3.40it/s][A
 41%|████      | 91/221 [00:27<00:38,  3.40it/s][A
 42%|████▏     | 92/221 [00:27<00:37,  3.40it/s][A
 42%|████▏     | 93/221 [00:27<00:37,  3.40it/s][A
 43%|████▎     | 94/221 [00:28<00:37,  3.41it/s][A
 43%|████▎     | 95/221 [00:28<00:37,  3.41it/s][A
 43%|████▎     | 96/221 [00:28<00:36,  3.40it/s][A
 44%|████▍     | 97/221 [00:29<00:36,  3.40it/s][A
 44%|████▍     | 98/221 [00:29<00:36,  3.41it/s][A
 45%|████▍     | 99/221 [00:29<00:35,  3.40it/s][A
 45%|████▌     | 100/221 [00:29<00:35,  3.41it/s][A
 46%|████▌     | 101/221 [00:30<00:35,  3.41it/s][A
 46%|████▌     | 102/221 [00:30<00:34,  3.41it/s][A
 47%|████▋     | 103/221 [00:30<00:34,  3.41it/s][A
 47%|████▋     | 104/221 [00:31<00:34,  3.41it/s][A
 48%|████▊     | 105/221 [00:31<00:34,  3.41it/s][A
 48%|████▊     | 106/221 [00:31<00:33,  3.41it/s][A
 48%|████▊     | 107/221 [00:31<00:33,  3.41it/s][A
 49%|████▉     | 108/221 [00:32<00:33,  3.41it/s][A
 49%|████▉     | 109/221 [00:32<00:32,  3.41it/s][A
 50%|████▉     | 110/221 [00:32<00:32,  3.41it/s][A
 50%|█████     | 111/221 [00:33<00:32,  3.41it/s][A
 51%|█████     | 112/221 [00:33<00:32,  3.41it/s][A
 51%|█████     | 113/221 [00:33<00:31,  3.41it/s][A
 52%|█████▏    | 114/221 [00:34<00:31,  3.41it/s][A
 52%|█████▏    | 115/221 [00:34<00:31,  3.41it/s][A
 52%|█████▏    | 116/221 [00:34<00:30,  3.41it/s][A
 53%|█████▎    | 117/221 [00:34<00:30,  3.41it/s][A
 53%|█████▎    | 118/221 [00:35<00:30,  3.40it/s][A
 54%|█████▍    | 119/221 [00:35<00:29,  3.41it/s][A
 54%|█████▍    | 120/221 [00:35<00:29,  3.41it/s][A
 55%|█████▍    | 121/221 [00:36<00:29,  3.41it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.41it/s][A
 56%|█████▌    | 123/221 [00:36<00:28,  3.41it/s][A
 56%|█████▌    | 124/221 [00:36<00:28,  3.41it/s][A
 57%|█████▋    | 125/221 [00:37<00:28,  3.40it/s][A
 57%|█████▋    | 126/221 [00:37<00:27,  3.40it/s][A
 57%|█████▋    | 127/221 [00:37<00:27,  3.40it/s][A
 58%|█████▊    | 128/221 [00:38<00:27,  3.41it/s][A
 58%|█████▊    | 129/221 [00:38<00:27,  3.41it/s][A
 59%|█████▉    | 130/221 [00:38<00:26,  3.41it/s][A
 59%|█████▉    | 131/221 [00:38<00:26,  3.41it/s][A
 60%|█████▉    | 132/221 [00:39<00:26,  3.41it/s][A
 60%|██████    | 133/221 [00:39<00:25,  3.41it/s][A
 61%|██████    | 134/221 [00:39<00:25,  3.41it/s][A
 61%|██████    | 135/221 [00:40<00:25,  3.41it/s][A
 62%|██████▏   | 136/221 [00:40<00:24,  3.41it/s][A
 62%|██████▏   | 137/221 [00:40<00:24,  3.41it/s][A
 62%|██████▏   | 138/221 [00:41<00:24,  3.41it/s][A
 63%|██████▎   | 139/221 [00:41<00:24,  3.41it/s][A
 63%|██████▎   | 140/221 [00:41<00:23,  3.41it/s][A
 64%|██████▍   | 141/221 [00:41<00:23,  3.41it/s][A
 64%|██████▍   | 142/221 [00:42<00:23,  3.41it/s][A
 65%|██████▍   | 143/221 [00:42<00:22,  3.41it/s][A
 65%|██████▌   | 144/221 [00:42<00:22,  3.41it/s][A
 66%|██████▌   | 145/221 [00:43<00:22,  3.41it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.41it/s][A
 67%|██████▋   | 147/221 [00:43<00:21,  3.41it/s][A
 67%|██████▋   | 148/221 [00:43<00:21,  3.41it/s][A
 67%|██████▋   | 149/221 [00:44<00:21,  3.41it/s][A
 68%|██████▊   | 150/221 [00:44<00:20,  3.41it/s][A
 68%|██████▊   | 151/221 [00:44<00:20,  3.41it/s][A
 69%|██████▉   | 152/221 [00:45<00:20,  3.41it/s][A
 69%|██████▉   | 153/221 [00:45<00:19,  3.41it/s][A
 70%|██████▉   | 154/221 [00:45<00:19,  3.41it/s][A
 70%|███████   | 155/221 [00:46<00:19,  3.41it/s][A
 71%|███████   | 156/221 [00:46<00:19,  3.41it/s][A
 71%|███████   | 157/221 [00:46<00:18,  3.41it/s][A
 71%|███████▏  | 158/221 [00:46<00:18,  3.41it/s][A
 72%|███████▏  | 159/221 [00:47<00:18,  3.41it/s][A
 72%|███████▏  | 160/221 [00:47<00:17,  3.41it/s][A
 73%|███████▎  | 161/221 [00:47<00:17,  3.41it/s][A
 73%|███████▎  | 162/221 [00:48<00:17,  3.41it/s][A
 74%|███████▍  | 163/221 [00:48<00:17,  3.41it/s][A
 74%|███████▍  | 164/221 [00:48<00:16,  3.41it/s][A
 75%|███████▍  | 165/221 [00:48<00:16,  3.41it/s][A
 75%|███████▌  | 166/221 [00:49<00:16,  3.41it/s][A
 76%|███████▌  | 167/221 [00:49<00:15,  3.41it/s][A
 76%|███████▌  | 168/221 [00:49<00:15,  3.41it/s][A
 76%|███████▋  | 169/221 [00:50<00:15,  3.41it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.41it/s][A
 77%|███████▋  | 171/221 [00:50<00:14,  3.41it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.41it/s][A
 78%|███████▊  | 173/221 [00:51<00:14,  3.41it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.41it/s][A
 79%|███████▉  | 175/221 [00:51<00:13,  3.41it/s][A
 80%|███████▉  | 176/221 [00:52<00:13,  3.41it/s][A
 80%|████████  | 177/221 [00:52<00:12,  3.41it/s][A
 81%|████████  | 178/221 [00:52<00:12,  3.41it/s][A
 81%|████████  | 179/221 [00:53<00:12,  3.41it/s][A
 81%|████████▏ | 180/221 [00:53<00:12,  3.41it/s][A
 82%|████████▏ | 181/221 [00:53<00:11,  3.41it/s][A
 82%|████████▏ | 182/221 [00:53<00:11,  3.41it/s][A
 83%|████████▎ | 183/221 [00:54<00:11,  3.41it/s][A
 83%|████████▎ | 184/221 [00:54<00:10,  3.41it/s][A
 84%|████████▎ | 185/221 [00:54<00:10,  3.41it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.41it/s][A
 85%|████████▍ | 187/221 [00:55<00:09,  3.41it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.41it/s][A
 86%|████████▌ | 189/221 [00:56<00:09,  3.41it/s][A
 86%|████████▌ | 190/221 [00:56<00:09,  3.41it/s][A
 86%|████████▋ | 191/221 [00:56<00:08,  3.41it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.41it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.41it/s][A
 88%|████████▊ | 194/221 [00:57<00:07,  3.41it/s][A
 88%|████████▊ | 195/221 [00:57<00:07,  3.41it/s][A
 89%|████████▊ | 196/221 [00:58<00:07,  3.41it/s][A
 89%|████████▉ | 197/221 [00:58<00:07,  3.41it/s][A
 90%|████████▉ | 198/221 [00:58<00:06,  3.41it/s][A
 90%|█████████ | 199/221 [00:58<00:06,  3.41it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.41it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.41it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.41it/s][A
 92%|█████████▏| 203/221 [01:00<00:05,  3.41it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.41it/s][A
 93%|█████████▎| 205/221 [01:00<00:04,  3.41it/s][A
 93%|█████████▎| 206/221 [01:01<00:04,  3.41it/s][A
 94%|█████████▎| 207/221 [01:01<00:04,  3.41it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▍| 209/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▌| 210/221 [01:02<00:03,  3.41it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.41it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.41it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  3.41it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.41it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.41it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.41it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.41it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.41it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  3.41it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.41it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.41it/s][A100%|██████████| 221/221 [01:05<00:00,  3.38it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:25,  8.62it/s][A
  1%|          | 2/221 [00:00<00:53,  4.10it/s][A
  1%|▏         | 3/221 [00:01<01:35,  2.29it/s][A
  2%|▏         | 4/221 [00:01<01:07,  3.23it/s][A
  2%|▏         | 5/221 [00:01<00:58,  3.68it/s][A
  3%|▎         | 6/221 [00:01<00:53,  4.04it/s][A
  3%|▎         | 7/221 [00:01<00:48,  4.40it/s][A
  4%|▎         | 8/221 [00:02<00:50,  4.19it/s][A
  4%|▍         | 9/221 [00:02<00:58,  3.64it/s][A
  5%|▍         | 10/221 [00:03<01:25,  2.46it/s][A
  5%|▍         | 11/221 [00:03<01:21,  2.59it/s][A
  5%|▌         | 12/221 [00:03<01:10,  2.96it/s][A
  6%|▌         | 13/221 [00:04<01:22,  2.53it/s][A
  6%|▋         | 14/221 [00:04<01:13,  2.82it/s][A
  7%|▋         | 15/221 [00:05<01:25,  2.42it/s][A
  7%|▋         | 16/221 [00:05<01:17,  2.66it/s][A
  8%|▊         | 17/221 [00:05<01:22,  2.47it/s][A
  8%|▊         | 18/221 [00:06<01:15,  2.69it/s][A
  9%|▊         | 19/221 [00:06<01:06,  3.02it/s][A
  9%|▉         | 20/221 [00:06<00:54,  3.69it/s][A
 10%|▉         | 21/221 [00:06<00:47,  4.18it/s][A
 10%|▉         | 22/221 [00:06<00:42,  4.71it/s][A
 10%|█         | 23/221 [00:06<00:35,  5.54it/s][A
 11%|█         | 24/221 [00:07<00:33,  5.91it/s][A
 11%|█▏        | 25/221 [00:07<00:37,  5.30it/s][A
 12%|█▏        | 26/221 [00:07<00:45,  4.28it/s][A
 12%|█▏        | 27/221 [00:07<00:45,  4.23it/s][A
 13%|█▎        | 28/221 [00:08<00:56,  3.40it/s][A
 13%|█▎        | 29/221 [00:08<00:56,  3.42it/s][A
 14%|█▎        | 30/221 [00:08<00:54,  3.49it/s][A
 14%|█▍        | 31/221 [00:09<00:54,  3.52it/s][A
 14%|█▍        | 32/221 [00:09<00:51,  3.65it/s][A
 15%|█▍        | 33/221 [00:09<00:49,  3.83it/s][A
 15%|█▌        | 34/221 [00:09<00:42,  4.39it/s][A
 16%|█▌        | 35/221 [00:09<00:36,  5.15it/s][A
 16%|█▋        | 36/221 [00:10<00:55,  3.32it/s][A
 17%|█▋        | 37/221 [00:10<00:54,  3.39it/s][A
 17%|█▋        | 38/221 [00:11<00:55,  3.33it/s][A
 18%|█▊        | 39/221 [00:11<00:44,  4.10it/s][A
 18%|█▊        | 40/221 [00:11<00:46,  3.89it/s][A
 19%|█▊        | 41/221 [00:11<00:45,  3.96it/s][A
 19%|█▉        | 42/221 [00:11<00:40,  4.43it/s][A
 19%|█▉        | 43/221 [00:12<00:53,  3.32it/s][A
 20%|█▉        | 44/221 [00:12<00:47,  3.73it/s][A
 20%|██        | 45/221 [00:12<00:53,  3.31it/s][A
 21%|██        | 46/221 [00:13<00:54,  3.21it/s][A
 21%|██▏       | 47/221 [00:13<00:44,  3.93it/s][A
 22%|██▏       | 49/221 [00:13<00:29,  5.77it/s][A
 23%|██▎       | 50/221 [00:14<00:46,  3.70it/s][A
 23%|██▎       | 51/221 [00:14<00:45,  3.75it/s][A
 24%|██▎       | 52/221 [00:14<00:39,  4.29it/s][A
 24%|██▍       | 53/221 [00:14<00:45,  3.67it/s][A
 24%|██▍       | 54/221 [00:15<00:43,  3.82it/s][A
 25%|██▍       | 55/221 [00:15<00:37,  4.42it/s][A
 25%|██▌       | 56/221 [00:15<00:38,  4.26it/s][A
 26%|██▌       | 57/221 [00:15<00:39,  4.11it/s][A
 27%|██▋       | 59/221 [00:16<00:34,  4.67it/s][A
 27%|██▋       | 60/221 [00:16<00:31,  5.14it/s][A
 28%|██▊       | 61/221 [00:16<00:31,  5.07it/s][A
 28%|██▊       | 62/221 [00:16<00:37,  4.22it/s][A
 29%|██▉       | 64/221 [00:17<00:31,  4.91it/s][A
 29%|██▉       | 65/221 [00:17<00:29,  5.31it/s][A
 30%|██▉       | 66/221 [00:17<00:34,  4.52it/s][A
 30%|███       | 67/221 [00:17<00:39,  3.92it/s][A
 31%|███       | 68/221 [00:18<00:36,  4.22it/s][A
 31%|███       | 69/221 [00:18<00:50,  3.02it/s][A
 32%|███▏      | 70/221 [00:18<00:44,  3.42it/s][A
 32%|███▏      | 71/221 [00:19<00:44,  3.34it/s][A
 33%|███▎      | 72/221 [00:19<00:51,  2.87it/s][A
 33%|███▎      | 73/221 [00:19<00:48,  3.04it/s][A
 33%|███▎      | 74/221 [00:20<00:39,  3.70it/s][A
 34%|███▍      | 75/221 [00:20<00:46,  3.15it/s][A
 34%|███▍      | 76/221 [00:20<00:44,  3.27it/s][A
 35%|███▍      | 77/221 [00:21<00:43,  3.31it/s][A
 35%|███▌      | 78/221 [00:21<00:40,  3.49it/s][A
 36%|███▌      | 79/221 [00:21<00:38,  3.73it/s][A
 36%|███▌      | 80/221 [00:21<00:36,  3.88it/s][A
 37%|███▋      | 81/221 [00:22<00:36,  3.79it/s][A
 37%|███▋      | 82/221 [00:22<00:34,  4.08it/s][A
 38%|███▊      | 83/221 [00:22<00:30,  4.49it/s][A
 38%|███▊      | 84/221 [00:22<00:35,  3.84it/s][A
 38%|███▊      | 85/221 [00:22<00:29,  4.64it/s][A
 39%|███▉      | 86/221 [00:22<00:26,  5.16it/s][A
 39%|███▉      | 87/221 [00:23<00:34,  3.83it/s][A
 40%|███▉      | 88/221 [00:23<00:41,  3.22it/s][A
 40%|████      | 89/221 [00:24<00:48,  2.74it/s][A
 41%|████      | 90/221 [00:24<00:48,  2.70it/s][A
 41%|████      | 91/221 [00:24<00:41,  3.12it/s][A
 42%|████▏     | 92/221 [00:25<00:39,  3.29it/s][A
 42%|████▏     | 93/221 [00:25<00:40,  3.20it/s][A
 43%|████▎     | 94/221 [00:25<00:34,  3.67it/s][A
 43%|████▎     | 95/221 [00:26<00:55,  2.27it/s][A
 43%|████▎     | 96/221 [00:26<00:48,  2.60it/s][A
 44%|████▍     | 97/221 [00:27<00:46,  2.68it/s][A
 44%|████▍     | 98/221 [00:27<00:49,  2.47it/s][A
 45%|████▍     | 99/221 [00:27<00:44,  2.73it/s][A
 45%|████▌     | 100/221 [00:28<00:45,  2.66it/s][A
 46%|████▌     | 101/221 [00:28<00:48,  2.47it/s][A
 46%|████▌     | 102/221 [00:29<00:42,  2.77it/s][A
 47%|████▋     | 103/221 [00:29<00:34,  3.41it/s][A
 48%|████▊     | 105/221 [00:29<00:26,  4.33it/s][A
 48%|████▊     | 106/221 [00:29<00:30,  3.77it/s][A
 48%|████▊     | 107/221 [00:30<00:27,  4.07it/s][A
 49%|████▉     | 108/221 [00:30<00:32,  3.46it/s][A
 49%|████▉     | 109/221 [00:30<00:31,  3.55it/s][A
 50%|████▉     | 110/221 [00:30<00:28,  3.89it/s][A
 50%|█████     | 111/221 [00:31<00:32,  3.40it/s][A
 51%|█████     | 112/221 [00:31<00:33,  3.29it/s][A
 51%|█████     | 113/221 [00:31<00:29,  3.62it/s][A
 52%|█████▏    | 115/221 [00:32<00:21,  4.85it/s][A
 52%|█████▏    | 116/221 [00:32<00:21,  4.94it/s][A
 53%|█████▎    | 117/221 [00:32<00:22,  4.72it/s][A
 53%|█████▎    | 118/221 [00:32<00:27,  3.81it/s][A
 54%|█████▍    | 119/221 [00:33<00:31,  3.26it/s][A
 54%|█████▍    | 120/221 [00:33<00:30,  3.30it/s][A
 55%|█████▍    | 121/221 [00:33<00:32,  3.06it/s][A
 55%|█████▌    | 122/221 [00:34<00:33,  2.97it/s][A
 56%|█████▌    | 123/221 [00:34<00:28,  3.47it/s][A
 56%|█████▌    | 124/221 [00:34<00:33,  2.90it/s][A
 57%|█████▋    | 125/221 [00:35<00:37,  2.59it/s][A
 57%|█████▋    | 126/221 [00:35<00:30,  3.09it/s][A
 57%|█████▋    | 127/221 [00:36<00:36,  2.60it/s][A
 58%|█████▊    | 128/221 [00:36<00:31,  2.97it/s][A
 59%|█████▉    | 130/221 [00:36<00:23,  3.82it/s][A
 59%|█████▉    | 131/221 [00:36<00:21,  4.10it/s][A
 60%|█████▉    | 132/221 [00:37<00:28,  3.11it/s][A
 60%|██████    | 133/221 [00:37<00:28,  3.12it/s][A
 61%|██████    | 134/221 [00:38<00:33,  2.56it/s][A
 61%|██████    | 135/221 [00:38<00:28,  2.97it/s][A
 62%|██████▏   | 136/221 [00:38<00:28,  3.00it/s][A
 62%|██████▏   | 137/221 [00:39<00:26,  3.20it/s][A
 62%|██████▏   | 138/221 [00:39<00:24,  3.40it/s][A
 63%|██████▎   | 139/221 [00:39<00:25,  3.16it/s][A
 63%|██████▎   | 140/221 [00:40<00:24,  3.29it/s][A
 64%|██████▍   | 141/221 [00:40<00:26,  3.06it/s][A
 64%|██████▍   | 142/221 [00:40<00:24,  3.29it/s][A
 65%|██████▍   | 143/221 [00:40<00:23,  3.39it/s][A
 65%|██████▌   | 144/221 [00:41<00:20,  3.73it/s][A
 66%|██████▌   | 145/221 [00:41<00:22,  3.35it/s][A
 66%|██████▌   | 146/221 [00:41<00:20,  3.64it/s][A
 67%|██████▋   | 147/221 [00:41<00:16,  4.46it/s][A
 67%|██████▋   | 148/221 [00:42<00:33,  2.19it/s][A
 67%|██████▋   | 149/221 [00:43<00:27,  2.63it/s][A
 68%|██████▊   | 150/221 [00:43<00:24,  2.86it/s][A
 68%|██████▊   | 151/221 [00:44<00:43,  1.62it/s][A
 69%|██████▉   | 152/221 [00:45<00:40,  1.70it/s][A
 69%|██████▉   | 153/221 [00:45<00:33,  2.05it/s][A
 70%|██████▉   | 154/221 [00:45<00:29,  2.28it/s][A
 70%|███████   | 155/221 [00:45<00:24,  2.70it/s][A
 71%|███████   | 156/221 [00:46<00:22,  2.89it/s][A
 71%|███████   | 157/221 [00:46<00:20,  3.16it/s][A
 71%|███████▏  | 158/221 [00:46<00:21,  2.93it/s][A
 72%|███████▏  | 159/221 [00:47<00:18,  3.35it/s][A
 72%|███████▏  | 160/221 [00:47<00:19,  3.17it/s][A
 73%|███████▎  | 162/221 [00:47<00:11,  4.97it/s][A
 74%|███████▍  | 163/221 [00:47<00:12,  4.77it/s][A
 74%|███████▍  | 164/221 [00:47<00:12,  4.65it/s][A
 75%|███████▍  | 165/221 [00:48<00:12,  4.45it/s][A
 75%|███████▌  | 166/221 [00:48<00:13,  3.97it/s][A
 76%|███████▌  | 168/221 [00:48<00:11,  4.57it/s][A
 77%|███████▋  | 170/221 [00:49<00:10,  4.86it/s][A
 77%|███████▋  | 171/221 [00:49<00:14,  3.37it/s][A
 78%|███████▊  | 172/221 [00:50<00:13,  3.53it/s][A
 78%|███████▊  | 173/221 [00:50<00:15,  3.19it/s][A
 79%|███████▉  | 175/221 [00:50<00:11,  4.00it/s][A
 80%|███████▉  | 176/221 [00:51<00:10,  4.21it/s][A
 80%|████████  | 177/221 [00:51<00:11,  3.85it/s][A
 81%|████████  | 178/221 [00:52<00:17,  2.49it/s][A
 81%|████████  | 179/221 [00:52<00:16,  2.59it/s][A
 82%|████████▏ | 181/221 [00:52<00:10,  3.67it/s][A
 82%|████████▏ | 182/221 [00:53<00:10,  3.71it/s][A
 83%|████████▎ | 183/221 [00:53<00:10,  3.65it/s][A
 83%|████████▎ | 184/221 [00:53<00:11,  3.30it/s][A
 84%|████████▍ | 186/221 [00:54<00:09,  3.67it/s][A
 85%|████████▍ | 187/221 [00:54<00:08,  4.19it/s][A
 85%|████████▌ | 188/221 [00:54<00:07,  4.37it/s][A
 86%|████████▌ | 189/221 [00:54<00:07,  4.41it/s][A
 86%|████████▌ | 190/221 [00:55<00:09,  3.42it/s][A
 86%|████████▋ | 191/221 [00:55<00:07,  4.05it/s][A
 87%|████████▋ | 192/221 [00:55<00:08,  3.45it/s][A
 87%|████████▋ | 193/221 [00:56<00:08,  3.39it/s][A
 88%|████████▊ | 194/221 [00:56<00:09,  2.77it/s][A
 88%|████████▊ | 195/221 [00:56<00:09,  2.63it/s][A
 89%|████████▊ | 196/221 [00:57<00:09,  2.59it/s][A
 89%|████████▉ | 197/221 [00:58<00:11,  2.09it/s][A
 90%|████████▉ | 198/221 [00:58<00:09,  2.33it/s][A
 90%|█████████ | 199/221 [00:58<00:07,  2.82it/s][A
 90%|█████████ | 200/221 [00:58<00:06,  3.32it/s][A
 91%|█████████ | 201/221 [00:58<00:05,  3.76it/s][A
 91%|█████████▏| 202/221 [00:59<00:05,  3.37it/s][A
 92%|█████████▏| 203/221 [00:59<00:05,  3.57it/s][A
 92%|█████████▏| 204/221 [00:59<00:04,  4.07it/s][A
 93%|█████████▎| 205/221 [00:59<00:04,  3.94it/s][A
 93%|█████████▎| 206/221 [01:00<00:04,  3.26it/s][A
 94%|█████████▎| 207/221 [01:00<00:04,  3.49it/s][A
 94%|█████████▍| 208/221 [01:00<00:03,  3.52it/s][A
 95%|█████████▍| 209/221 [01:01<00:03,  3.33it/s][A
 95%|█████████▌| 210/221 [01:01<00:02,  3.95it/s][A
 95%|█████████▌| 211/221 [01:01<00:02,  3.81it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.37it/s][A
 96%|█████████▋| 213/221 [01:02<00:02,  3.56it/s][A
 97%|█████████▋| 214/221 [01:02<00:02,  3.08it/s][A
 97%|█████████▋| 215/221 [01:03<00:01,  3.04it/s][A
 98%|█████████▊| 216/221 [01:03<00:01,  3.30it/s][A
 98%|█████████▊| 217/221 [01:03<00:01,  3.08it/s][A
 99%|█████████▊| 218/221 [01:04<00:01,  2.45it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  2.63it/s][A
100%|█████████▉| 220/221 [01:04<00:00,  3.21it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.44it/s][A100%|██████████| 221/221 [01:05<00:00,  3.40it/s]
09/16/2024 17:07:29 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_forward=====step 149--===========

09/16/2024 17:07:29 - INFO - __main__ -   {'area_r1': 42.3, 'area_recall': '42.3/70.4/80.1', 'area_ravg': 64.3}
09/16/2024 17:07:29 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_backard=====step 149--===========

09/16/2024 17:07:29 - INFO - __main__ -   {'forward_r1': 37.9, 'forward_recall': '37.9/65.5/76.4', 'forward_ravg': 59.9}
09/16/2024 17:07:29 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video=====step 149--===========

09/16/2024 17:07:29 - INFO - __main__ -   {'area_video_r1': 38.2, 'area_video_recall': '38.2/66.0/76.5', 'area_video_ravg': 60.2}
09/16/2024 17:07:29 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_area_back_with_video====history best step: 149=======

09/16/2024 17:07:29 - INFO - __main__ -   {'area_video_r1': 38.2, 'area_video_recall': '38.2/66.0/76.5', 'area_video_ravg': 60.2}
09/16/2024 17:07:29 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_area=====step 149--===========

09/16/2024 17:07:29 - INFO - __main__ -   {'area_video_r1': 51.5, 'area_video_recall': '51.5/75.0/83.7', 'area_video_ravg': 70.1, 'area_video_back_r1': 50.1, 'area_video_back_recall': '50.1/73.0/80.7', 'area_video_back_ravg': 67.9}
09/16/2024 17:07:29 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_area====history best step: 99=======

09/16/2024 17:07:29 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/75.2/83.3', 'area_video_ravg': 70.2, 'area_video_back_r1': 49.8, 'area_video_back_recall': '49.8/73.5/80.5', 'area_video_back_ravg': 67.9}
09/16/2024 17:07:29 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itc_tva=====step 149--===========

09/16/2024 17:07:29 - INFO - __main__ -   {'video_r1': 36.0, 'video_recall': '36.0/65.2/75.9', 'video_ravg': 59.0}
09/16/2024 17:07:29 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itc_tva====history best step: 49=======

09/16/2024 17:07:29 - INFO - __main__ -   {'video_r1': 37.0, 'video_recall': '37.0/65.4/73.8', 'video_ravg': 58.7}
09/16/2024 17:07:29 - INFO - __main__ -   ====-evaluation--ret%tva--msrvtt_ret_ret_itm_tva=====step 149--===========

09/16/2024 17:07:29 - INFO - __main__ -   {'video_r1': 50.8, 'video_recall': '50.8/74.2/82.0', 'video_ravg': 69.0}
09/16/2024 17:07:29 - INFO - __main__ -   ======evaluation--ret%tva--msrvtt_ret_ret_itm_tva====history best step: 99=======

09/16/2024 17:07:29 - INFO - __main__ -   {'video_r1': 51.7, 'video_recall': '51.7/73.9/81.4', 'video_ravg': 69.0}
09/16/2024 17:07:55 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.012930295430123806, 'loss_ret%tv%ta--finetune_area/loss_area': 2.5129246711730957, 'loss_ret%tv%ta--finetune_area/total_loss': 2.52585506439209}
  5%|▌         | 150/2755 [55:20<96:36:41, 133.51s/it][h264 @ 0x557ae0c37b40] mmco: unref short failure
  5%|▌         | 151/2755 [55:24<68:20:54, 94.49s/it] [h264 @ 0x561785e04a40] mmco: unref short failure
srun: Job step aborted: Waiting up to 92 seconds for job step to finish.
slurmstepd: error: *** JOB 7728185 ON lrdn2614 CANCELLED AT 2024-09-16T17:08:07 ***
