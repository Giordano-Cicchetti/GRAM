NODELIST=lrdn3068
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
3
10
2

DEVICE SET
DEVICE SET
DEVICE SET
DEVICE SET
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/18/2024 14:55:05 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/18/2024 14:55:05 - INFO - __main__ -   ==================model_configs==================

09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_model_type : vast
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_frozen_vision : False
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_frozen_audio : False
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_checkpointing : True
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_max_caption_len : 40
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_contra_dim : 512
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_vision_resolution : 224
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_audio_melbins : 64
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_audio_target_length : 1024
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_beam_size : 3
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_captioner_mode : False
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_generate_nums : 1
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_max_vision_sample_num : 2
09/18/2024 14:55:05 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
09/18/2024 14:55:05 - INFO - __main__ -   ==================run_configs==================

09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_checkpoint : 
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod150kConSubtitles
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_optim : adamw
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_weight_decay : 0.01
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_grad_norm : 2.0
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_resume : False
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_seed : 50
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_fp16 : True
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_bf16 : False
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_zero_shot : False
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_new_lr : 0
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_new_params_name : []
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_valid_freq : 10
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_dataset_mix_type : random
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_first_eval : True
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_num_train_steps : 0
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_save_best : True
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_pin_mem : True
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_vision_resolution : 224
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_use_ddp : False
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_mode : training
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_log_steps : 100
09/18/2024 14:55:05 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
09/18/2024 14:55:05 - INFO - __main__ -   ==================data_configs==================

09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_type : annoindexed
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_training : True
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_name : finetune_area
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_txt : ../vast27m/annotations150k.json
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_vision : ../vast27m/videos/
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_audio : ../vast27m/audios
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_vision_transforms : crop_flip
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_vision_format : video_rawvideo
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_vision_sample_num : 2
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_audio_sample_num : 1
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_task : ret%tv%ta
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_epoch : 5
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_n_workers : 8
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_finetune_area_train_batch_size : 256
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 8
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tvas
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
09/18/2024 14:55:05 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
09/18/2024 14:55:09 - INFO - __main__ -   finetune_area Using clip mean and std.
09/18/2024 14:55:09 - INFO - __main__ -   finetune_area transforms crop_flip
ci sono 149153 labelsci sono 149153 labelsci sono 149153 labels

ci sono 149153 labels

09/18/2024 14:56:23 - INFO - __main__ -   Create Dataset finetune_area Success
09/18/2024 14:56:23 - INFO - __main__ -    loader ret%tv%ta--finetune_area , ratio 2910 , bs_pergpu 64, n_workers 8
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 14:56:25 - INFO - __main__ -   current idx Y6ZqQLFmpGU.39 from finetune_area returns wrong image/video, use 76870 instead.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 14:56:26 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/18/2024 14:56:26 - INFO - __main__ -   msrvtt_ret transforms crop_flip
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
ci sono 884 labels
ci sono 884 labelsci sono 884 labels

ci sono 884 labels
09/18/2024 14:56:27 - INFO - __main__ -   Create Dataset msrvtt_ret Success
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'Please 'pip install xformers'
Please 'pip install xformers'

Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
09/18/2024 14:56:29 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/18/2024 14:56:29 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/18/2024 14:56:29 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/18/2024 14:56:29 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565e5817d00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598dc812440] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb5ea8a8c0] mmco: unref short failure
[h264 @ 0x55cb5ea8a8c0] mmco: unref short failure
[h264 @ 0x55cb5eab1080] mmco: unref short failure
[h264 @ 0x5598dbde4c00] mmco: unref short failure
[h264 @ 0x5598dbde4c00] mmco: unref short failure
[h264 @ 0x5598dbde4c00] mmco: unref short failure
[h264 @ 0x5598dbde4c00] mmco: unref short failure
[h264 @ 0x55d8fea41ec0] mmco: unref short failure
[h264 @ 0x55cb5ec33300] mmco: unref short failure
[h264 @ 0x55cb5ff59940] mmco: unref short failure
09/18/2024 14:57:18 - INFO - __main__ -   current idx d018IFLZh_8.4 from finetune_area returns wrong image/video, use 17473 instead.
[h264 @ 0x5565e581d640] mmco: unref short failure
[h264 @ 0x5565e581d640] mmco: unref short failure
[h264 @ 0x55d8feabdbc0] mmco: unref short failure
09/18/2024 14:57:32 - INFO - __main__ -   current idx GH7kTACMcMI.49 from finetune_area returns wrong image/video, use 13682 instead.
[h264 @ 0x55cb5f9e1cc0] mmco: unref short failure
[h264 @ 0x55cb5f9e1cc0] mmco: unref short failure
09/18/2024 14:57:33 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/18/2024 14:57:34 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
[h264 @ 0x5565e53c40c0] mmco: unref short failure
[h264 @ 0x5565e53c40c0] mmco: unref short failure
09/18/2024 14:57:36 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/18/2024 14:57:36 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/18/2024 14:57:46 - INFO - root -   incompatible_keys.missing_keys: []
09/18/2024 14:57:46 - INFO - root -   incompatible_keys.missing_keys: []
09/18/2024 14:57:46 - INFO - root -   incompatible_keys.missing_keys: []
09/18/2024 14:57:46 - INFO - root -   incompatible_keys.missing_keys: []
09/18/2024 14:57:48 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/18/2024 14:57:48 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/18/2024 14:57:48 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/18/2024 14:57:48 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
[h264 @ 0x55d8ffdcab80] mmco: unref short failure
[h264 @ 0x55d8ffdcab80] mmco: unref short failure
[h264 @ 0x55d8ffdcab80] mmco: unref short failure
[h264 @ 0x55d8ffdcab80] mmco: unref short failure
[h264 @ 0x55cb5efda900] mmco: unref short failure
[h264 @ 0x55d8ff966dc0] mmco: unref short failure
[h264 @ 0x5598dc370040] mmco: unref short failure
[h264 @ 0x5598dc370040] mmco: unref short failure
[h264 @ 0x5598dc370040] mmco: unref short failure
09/18/2024 14:57:59 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/18/2024 14:57:59 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/18/2024 14:57:59 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/18/2024 14:57:59 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
[h264 @ 0x55d8ff7d3dc0] mmco: unref short failure
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.10.crossattention.self.value.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'cls.predictions.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'cls.predictions.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.query.bias', 'cls.predictions.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.query.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'cls.predictions.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[h264 @ 0x55cb5e70f4c0] mmco: unref short failure
[h264 @ 0x55cb5ebffe00] mmco: unref short failure
[h264 @ 0x55cb5ebffe00] mmco: unref short failure
09/18/2024 14:58:27 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/ckpt/model_step_204994.pt
09/18/2024 14:58:27 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast
[h264 @ 0x55cb5f130540] mmco: unref short failure
09/18/2024 14:58:32 - INFO - __main__ -   Unexpected keys ['vision_encoder.text.logit_scale']
09/18/2024 14:58:32 - INFO - __main__ -   missing_keys  ['vision_encoder.logit_scale']
[h264 @ 0x55d8feecbd80] mmco: unref short failure
[h264 @ 0x55d8feecbd80] mmco: unref short failure
[h264 @ 0x55cb62d7abc0] mmco: unref short failure
09/18/2024 14:58:38 - INFO - __main__ -   ==================learning_rate_settings==================

09/18/2024 14:58:38 - INFO - __main__ -     basic_lr : 2e-05
09/18/2024 14:58:38 - INFO - __main__ -     clip_lr_visual : 5e-07
09/18/2024 14:58:38 - INFO - __main__ -     clip_lr_visual_len : 245
09/18/2024 14:58:38 - INFO - __main__ -     new_lr : 0
09/18/2024 14:58:38 - INFO - __main__ -     new_params_name: []
09/18/2024 14:58:38 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 14:58:38 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x5598dcbe8a80] mmco: unref short failure
[h264 @ 0x5598dcbe8a80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb64965540] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598dc3fe000] mmco: unref short failure
[h264 @ 0x5598dc3fe000] mmco: unref short failure
[h264 @ 0x5598dd95ee40] mmco: unref short failure
[h264 @ 0x55cb5f827f40] mmco: unref short failure
[h264 @ 0x55cb5f827f40] mmco: unref short failure
[h264 @ 0x55d8fed41740] mmco: unref short failure
[h264 @ 0x55cb64b0a1c0] mmco: unref short failure
[h264 @ 0x55cb64b0a1c0] mmco: unref short failure
[h264 @ 0x5565e6016a80] mmco: unref short failure
[h264 @ 0x5565e6d64f80] mmco: unref short failure
[h264 @ 0x5565e6d64f80] mmco: unref short failure
[h264 @ 0x55cb5cd19d40] mmco: unref short failure
09/18/2024 14:59:03 - INFO - __main__ -   current idx b1Hz-jQ4IM0.8 from finetune_area returns wrong image/video, use 35266 instead.
[h264 @ 0x55d8ff1f6440] mmco: unref short failure
[h264 @ 0x55d8ff1f6440] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 14:59:17 - INFO - __main__ -   current idx BYYoxWKlR3E.11 from finetune_area returns wrong image/video, use 54238 instead.
[h264 @ 0x55d90355fc80] mmco: unref short failure
[h264 @ 0x5598df364340] mmco: unref short failure
[h264 @ 0x5598df364340] mmco: unref short failure
[h264 @ 0x55d8fe5f6000] mmco: unref short failure
[h264 @ 0x55d8fe5f6000] mmco: unref short failure
[h264 @ 0x5565e8e51a80] mmco: unref short failure
[h264 @ 0x55cb618ec180] mmco: unref short failure
[h264 @ 0x5565e58c7ac0] mmco: unref short failure
[h264 @ 0x5565e58c7ac0] mmco: unref short failure
[h264 @ 0x5598e31cd500] mmco: unref short failure
[h264 @ 0x5598e31cd500] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
[h264 @ 0x5598e5480280] mmco: unref short failure
[h264 @ 0x55d90348a3c0] mmco: unref short failure
[h264 @ 0x5598e2332680] mmco: unref short failure
[h264 @ 0x5598e2332680] mmco: unref short failure
[h264 @ 0x5598e2332680] mmco: unref short failure
[h264 @ 0x5598e07fc300] mmco: unref short failure
[h264 @ 0x5598de0c4780] mmco: unref short failure
09/18/2024 15:00:51 - INFO - __main__ -   current idx bVHcVa51-ds.15 from finetune_area returns wrong image/video, use 87000 instead.
[h264 @ 0x5565eb8d1640] mmco: unref short failure
[h264 @ 0x5565e6e71f40] mmco: unref short failure
[h264 @ 0x5598de0c4300] mmco: unref short failure
[h264 @ 0x5598de0c4300] mmco: unref short failure
[h264 @ 0x55cb5ecef9c0] mmco: unref short failure
[h264 @ 0x55cb5ecef9c0] mmco: unref short failure
[h264 @ 0x55cb5ecef9c0] mmco: unref short failure
[h264 @ 0x55cb5ecef9c0] mmco: unref short failure
[h264 @ 0x55cb5ecef9c0] mmco: unref short failure
[h264 @ 0x55cb5ecef9c0] mmco: unref short failure
[h264 @ 0x55d900396680] mmco: unref short failure
[h264 @ 0x55d900396680] mmco: unref short failure
09/18/2024 15:01:06 - INFO - __main__ -   current idx 1-hr71oylIM.38 from finetune_area returns wrong image/video, use 139748 instead.
[h264 @ 0x55cb5ecefc40] mmco: unref short failure
[h264 @ 0x55cb5ecefc40] mmco: unref short failure
[h264 @ 0x55cb5ecefc40] mmco: unref short failure
[h264 @ 0x55cb5ecefc40] mmco: unref short failure
[h264 @ 0x55cb5ecefc40] mmco: unref short failure
[h264 @ 0x55cb5ecefc40] mmco: unref short failure
[h264 @ 0x5565e6f30fc0] mmco: unref short failure
[h264 @ 0x5565e9b67900] mmco: unref short failure
[h264 @ 0x5565e9b67900] mmco: unref short failure
[h264 @ 0x5565e9b67900] mmco: unref short failure
[h264 @ 0x5565e9b67900] mmco: unref short failure
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<00:23,  9.55it/s]  1%|          | 2/221 [00:00<00:33,  6.61it/s]  1%|▏         | 3/221 [00:00<00:40,  5.42it/s]  2%|▏         | 5/221 [00:00<00:28,  7.49it/s]  3%|▎         | 7/221 [00:00<00:28,  7.46it/s]  4%|▎         | 8/221 [00:01<00:32,  6.65it/s]  5%|▍         | 10/221 [00:01<00:27,  7.72it/s]  5%|▌         | 12/221 [00:04<01:58,  1.76it/s]  6%|▋         | 14/221 [00:04<01:23,  2.48it/s]  7%|▋         | 15/221 [00:04<01:11,  2.89it/s]  8%|▊         | 17/221 [00:04<00:58,  3.52it/s]  8%|▊         | 18/221 [00:04<00:52,  3.86it/s]  9%|▊         | 19/221 [00:04<00:47,  4.24it/s]  9%|▉         | 20/221 [00:05<00:44,  4.51it/s] 10%|▉         | 21/221 [00:05<00:39,  5.07it/s][h264 @ 0x55cb60e76f40] mmco: unref short failure
[h264 @ 0x55cb60e76f40] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
[h264 @ 0x5598de448580] mmco: unref short failure
 10%|▉         | 22/221 [00:10<04:46,  1.44s/it] 11%|█         | 24/221 [00:10<02:50,  1.16it/s] 12%|█▏        | 26/221 [00:10<01:53,  1.71it/s] 13%|█▎        | 28/221 [00:10<01:19,  2.42it/s] 14%|█▎        | 30/221 [00:10<00:58,  3.27it/s] 14%|█▍        | 31/221 [00:11<00:52,  3.62it/s] 14%|█▍        | 32/221 [00:11<00:45,  4.19it/s] 15%|█▍        | 33/221 [00:11<00:39,  4.73it/s] 16%|█▌        | 35/221 [00:11<00:30,  6.19it/s] 16%|█▋        | 36/221 [00:11<00:27,  6.62it/s] 17%|█▋        | 37/221 [00:11<00:27,  6.70it/s] 18%|█▊        | 39/221 [00:11<00:22,  8.24it/s] 18%|█▊        | 40/221 [00:12<00:21,  8.37it/s] 19%|█▊        | 41/221 [00:12<00:21,  8.50it/s] 19%|█▉        | 43/221 [00:12<00:18,  9.87it/s] 20%|██        | 45/221 [00:12<00:29,  5.96it/s] 21%|██        | 46/221 [00:13<00:29,  5.93it/s] 21%|██▏       | 47/221 [00:17<03:38,  1.26s/it] 22%|██▏       | 48/221 [00:18<02:51,  1.01it/s] 22%|██▏       | 49/221 [00:18<02:10,  1.32it/s] 23%|██▎       | 50/221 [00:18<01:39,  1.72it/s] 23%|██▎       | 51/221 [00:18<01:17,  2.18it/s] 24%|██▎       | 52/221 [00:18<01:01,  2.76it/s] 24%|██▍       | 54/221 [00:22<03:12,  1.15s/it] 25%|██▍       | 55/221 [00:24<03:27,  1.25s/it] 26%|██▌       | 57/221 [00:24<02:07,  1.29it/s] 27%|██▋       | 59/221 [00:24<01:27,  1.86it/s] 27%|██▋       | 60/221 [00:25<01:14,  2.15it/s] 28%|██▊       | 61/221 [00:25<01:02,  2.56it/s] 28%|██▊       | 62/221 [00:25<00:51,  3.10it/s] 29%|██▊       | 63/221 [00:25<00:41,  3.77it/s] 29%|██▉       | 64/221 [00:25<00:35,  4.48it/s] 30%|██▉       | 66/221 [00:30<03:15,  1.26s/it] 30%|███       | 67/221 [00:30<02:37,  1.03s/it] 31%|███       | 69/221 [00:31<01:41,  1.49it/s] 32%|███▏      | 70/221 [00:31<01:21,  1.84it/s] 32%|███▏      | 71/221 [00:31<01:06,  2.25it/s][h264 @ 0x55cb5f715740] mmco: unref short failure
[h264 @ 0x55cb5f715740] mmco: unref short failure
 33%|███▎      | 72/221 [00:31<00:55,  2.68it/s] 33%|███▎      | 73/221 [00:31<00:45,  3.25it/s]09/18/2024 15:01:56 - INFO - __main__ -   current idx -c6ksbh044A.74 from finetune_area returns wrong image/video, use 106497 instead.
 33%|███▎      | 74/221 [00:31<00:37,  3.95it/s] 34%|███▍      | 75/221 [00:32<00:31,  4.60it/s] 35%|███▍      | 77/221 [00:32<00:23,  6.06it/s] 36%|███▌      | 79/221 [00:32<00:25,  5.49it/s] 37%|███▋      | 81/221 [00:32<00:20,  6.67it/s] 37%|███▋      | 82/221 [00:33<00:23,  5.86it/s] 38%|███▊      | 83/221 [00:33<00:21,  6.46it/s] 38%|███▊      | 85/221 [00:33<00:17,  7.89it/s] 39%|███▉      | 87/221 [00:33<00:17,  7.82it/s] 40%|███▉      | 88/221 [00:33<00:19,  6.94it/s] 40%|████      | 89/221 [00:34<00:23,  5.66it/s] 41%|████      | 90/221 [00:34<00:23,  5.58it/s] 41%|████      | 91/221 [00:34<00:22,  5.70it/s] 42%|████▏     | 92/221 [00:34<00:21,  6.07it/s] 42%|████▏     | 93/221 [00:35<00:29,  4.35it/s] 43%|████▎     | 95/221 [00:35<00:22,  5.70it/s] 43%|████▎     | 96/221 [00:35<00:24,  5.18it/s] 44%|████▍     | 98/221 [00:35<00:19,  6.39it/s] 45%|████▌     | 100/221 [00:35<00:16,  7.43it/s] 46%|████▌     | 101/221 [00:36<00:16,  7.29it/s] 46%|████▌     | 102/221 [00:36<00:17,  6.75it/s] 47%|████▋     | 103/221 [00:36<00:19,  5.92it/s] 47%|████▋     | 104/221 [00:36<00:18,  6.45it/s] 48%|████▊     | 105/221 [00:36<00:16,  6.98it/s] 48%|████▊     | 106/221 [00:37<00:27,  4.13it/s] 49%|████▉     | 108/221 [00:37<00:20,  5.40it/s] 49%|████▉     | 109/221 [00:37<00:18,  6.00it/s] 50%|█████     | 111/221 [00:37<00:18,  5.82it/s] 51%|█████     | 112/221 [00:38<00:18,  5.81it/s] 52%|█████▏    | 114/221 [00:38<00:15,  7.07it/s] 52%|█████▏    | 115/221 [00:38<00:17,  6.22it/s][h264 @ 0x5598dc3d1980] mmco: unref short failure
[h264 @ 0x5598dc3d1980] mmco: unref short failure
 52%|█████▏    | 116/221 [00:43<02:25,  1.39s/it] 53%|█████▎    | 117/221 [00:44<01:55,  1.11s/it] 53%|█████▎    | 118/221 [00:44<01:28,  1.16it/s] 54%|█████▍    | 119/221 [00:44<01:08,  1.49it/s] 54%|█████▍    | 120/221 [00:44<00:52,  1.94it/s] 55%|█████▍    | 121/221 [00:44<00:40,  2.50it/s] 55%|█████▌    | 122/221 [00:44<00:32,  3.09it/s] 56%|█████▌    | 123/221 [00:44<00:25,  3.83it/s] 56%|█████▌    | 124/221 [00:45<00:21,  4.56it/s] 57%|█████▋    | 125/221 [00:45<00:25,  3.77it/s] 57%|█████▋    | 126/221 [00:45<00:31,  3.06it/s] 57%|█████▋    | 127/221 [00:46<00:30,  3.06it/s] 58%|█████▊    | 128/221 [00:46<00:32,  2.84it/s][h264 @ 0x55d9022bbc00] mmco: unref short failure
 58%|█████▊    | 129/221 [00:46<00:28,  3.24it/s] 59%|█████▉    | 131/221 [00:46<00:18,  4.79it/s] 60%|█████▉    | 132/221 [00:47<00:17,  5.12it/s] 60%|██████    | 133/221 [00:47<00:18,  4.77it/s] 61%|██████    | 134/221 [00:47<00:18,  4.66it/s] 61%|██████    | 135/221 [00:49<00:49,  1.72it/s][h264 @ 0x5598e4c06e00] mmco: unref short failure
 62%|██████▏   | 136/221 [00:49<00:48,  1.76it/s] 62%|██████▏   | 137/221 [00:52<01:34,  1.12s/it] 62%|██████▏   | 138/221 [00:52<01:13,  1.13it/s] 63%|██████▎   | 139/221 [00:52<01:02,  1.32it/s] 63%|██████▎   | 140/221 [00:53<00:50,  1.61it/s] 64%|██████▍   | 141/221 [00:53<00:41,  1.93it/s] 64%|██████▍   | 142/221 [00:53<00:32,  2.42it/s] 65%|██████▍   | 143/221 [00:53<00:26,  2.95it/s] 66%|██████▌   | 145/221 [00:53<00:16,  4.59it/s] 67%|██████▋   | 147/221 [00:54<00:12,  6.08it/s] 67%|██████▋   | 148/221 [00:54<00:12,  5.88it/s] 67%|██████▋   | 149/221 [00:54<00:11,  6.21it/s] 68%|██████▊   | 150/221 [00:54<00:16,  4.25it/s] 68%|██████▊   | 151/221 [00:55<00:17,  3.91it/s] 69%|██████▉   | 153/221 [00:55<00:12,  5.36it/s][h264 @ 0x5565eba9d5c0] mmco: unref short failure
 70%|██████▉   | 154/221 [00:55<00:11,  5.93it/s] 70%|███████   | 155/221 [00:55<00:10,  6.58it/s][h264 @ 0x5598dfd27c40] mmco: unref short failure
[h264 @ 0x5598dfd27c40] mmco: unref short failure
 71%|███████   | 156/221 [01:00<01:37,  1.50s/it] 71%|███████   | 157/221 [01:01<01:13,  1.15s/it] 71%|███████▏  | 158/221 [01:01<00:53,  1.17it/s] 72%|███████▏  | 159/221 [01:01<00:39,  1.56it/s] 73%|███████▎  | 161/221 [01:01<00:23,  2.53it/s][h264 @ 0x55d8ff4d1540] mmco: unref short failure
 73%|███████▎  | 162/221 [01:01<00:20,  2.81it/s] 74%|███████▍  | 163/221 [01:01<00:19,  3.02it/s][h264 @ 0x55d8fed8a580] mmco: unref short failure
 74%|███████▍  | 164/221 [01:02<00:15,  3.60it/s][h264 @ 0x5598e00455c0] mmco: unref short failure
[h264 @ 0x5598e00455c0] mmco: unref short failure
 75%|███████▍  | 165/221 [01:07<01:33,  1.66s/it] 76%|███████▌  | 167/221 [01:09<01:20,  1.49s/it] 76%|███████▋  | 169/221 [01:10<00:51,  1.01it/s] 77%|███████▋  | 170/221 [01:10<00:41,  1.22it/s] 78%|███████▊  | 173/221 [01:10<00:21,  2.25it/s] 79%|███████▉  | 175/221 [01:10<00:15,  3.05it/s] 80%|████████  | 177/221 [01:10<00:11,  3.97it/s] 81%|████████  | 179/221 [01:12<00:14,  2.88it/s] 82%|████████▏ | 182/221 [01:12<00:09,  4.27it/s] 83%|████████▎ | 184/221 [01:12<00:06,  5.42it/s] 85%|████████▍ | 187/221 [01:12<00:04,  7.45it/s] 86%|████████▌ | 189/221 [01:12<00:04,  7.61it/s] 87%|████████▋ | 193/221 [01:12<00:02, 11.19it/s] 89%|████████▊ | 196/221 [01:13<00:01, 13.48it/s] 90%|█████████ | 199/221 [01:13<00:01, 14.35it/s] 91%|█████████ | 201/221 [01:13<00:01, 14.23it/s] 92%|█████████▏| 204/221 [01:13<00:01, 16.44it/s] 93%|█████████▎| 206/221 [01:13<00:01, 13.77it/s] 94%|█████████▍| 208/221 [01:13<00:00, 14.55it/s] 95%|█████████▌| 210/221 [01:13<00:00, 15.13it/s] 96%|█████████▌| 212/221 [01:14<00:00, 13.93it/s] 97%|█████████▋| 214/221 [01:14<00:00,  8.53it/s] 98%|█████████▊| 216/221 [01:16<00:01,  2.68it/s] 99%|█████████▊| 218/221 [01:16<00:00,  3.56it/s]100%|█████████▉| 220/221 [01:21<00:00,  1.03it/s]100%|█████████▉| 220/221 [01:21<00:00,  2.68it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<01:06,  3.30it/s]  1%|          | 2/221 [00:00<01:06,  3.31it/s]  1%|▏         | 3/221 [00:00<01:05,  3.31it/s]  2%|▏         | 4/221 [00:01<01:05,  3.31it/s]  2%|▏         | 5/221 [00:01<01:05,  3.31it/s]  3%|▎         | 6/221 [00:01<01:04,  3.31it/s]  3%|▎         | 7/221 [00:02<01:04,  3.31it/s]  4%|▎         | 8/221 [00:02<01:04,  3.31it/s]  4%|▍         | 9/221 [00:02<01:04,  3.31it/s]  5%|▍         | 10/221 [00:03<01:03,  3.31it/s]  5%|▍         | 11/221 [00:03<01:03,  3.31it/s]  5%|▌         | 12/221 [00:03<01:03,  3.31it/s]  6%|▌         | 13/221 [00:03<01:02,  3.31it/s]  6%|▋         | 14/221 [00:04<01:02,  3.31it/s]  7%|▋         | 15/221 [00:04<01:02,  3.31it/s]  7%|▋         | 16/221 [00:04<01:01,  3.31it/s]  8%|▊         | 17/221 [00:05<01:01,  3.31it/s]  8%|▊         | 18/221 [00:05<01:01,  3.31it/s]  9%|▊         | 19/221 [00:05<01:00,  3.31it/s]  9%|▉         | 20/221 [00:06<01:00,  3.31it/s] 10%|▉         | 21/221 [00:06<01:00,  3.31it/s] 10%|▉         | 22/221 [00:06<01:00,  3.31it/s] 10%|█         | 23/221 [00:06<00:59,  3.31it/s] 11%|█         | 24/221 [00:07<00:59,  3.31it/s] 11%|█▏        | 25/221 [00:07<00:59,  3.31it/s] 12%|█▏        | 26/221 [00:07<00:58,  3.31it/s] 12%|█▏        | 27/221 [00:08<00:58,  3.31it/s] 13%|█▎        | 28/221 [00:08<00:58,  3.31it/s] 13%|█▎        | 29/221 [00:08<00:57,  3.31it/s] 14%|█▎        | 30/221 [00:09<00:57,  3.31it/s] 14%|█▍        | 31/221 [00:09<00:57,  3.31it/s] 14%|█▍        | 32/221 [00:09<00:57,  3.31it/s] 15%|█▍        | 33/221 [00:09<00:56,  3.31it/s] 15%|█▌        | 34/221 [00:10<00:56,  3.31it/s] 16%|█▌        | 35/221 [00:10<00:56,  3.31it/s] 16%|█▋        | 36/221 [00:10<00:55,  3.31it/s] 17%|█▋        | 37/221 [00:11<00:55,  3.31it/s] 17%|█▋        | 38/221 [00:11<00:55,  3.31it/s] 18%|█▊        | 39/221 [00:11<00:54,  3.31it/s] 18%|█▊        | 40/221 [00:12<00:54,  3.31it/s] 19%|█▊        | 41/221 [00:12<00:54,  3.31it/s] 19%|█▉        | 42/221 [00:12<00:54,  3.31it/s] 19%|█▉        | 43/221 [00:12<00:53,  3.31it/s] 20%|█▉        | 44/221 [00:13<00:53,  3.31it/s] 20%|██        | 45/221 [00:13<00:53,  3.31it/s] 21%|██        | 46/221 [00:13<00:52,  3.31it/s] 21%|██▏       | 47/221 [00:14<00:52,  3.31it/s] 22%|██▏       | 48/221 [00:14<00:52,  3.31it/s] 22%|██▏       | 49/221 [00:14<00:51,  3.31it/s] 23%|██▎       | 50/221 [00:15<00:51,  3.31it/s] 23%|██▎       | 51/221 [00:15<00:51,  3.31it/s] 24%|██▎       | 52/221 [00:15<00:51,  3.31it/s] 24%|██▍       | 53/221 [00:16<00:50,  3.31it/s] 24%|██▍       | 54/221 [00:16<00:50,  3.31it/s] 25%|██▍       | 55/221 [00:16<00:50,  3.31it/s] 25%|██▌       | 56/221 [00:16<00:49,  3.31it/s] 26%|██▌       | 57/221 [00:17<00:49,  3.31it/s] 26%|██▌       | 58/221 [00:17<00:49,  3.31it/s] 27%|██▋       | 59/221 [00:17<00:48,  3.31it/s] 27%|██▋       | 60/221 [00:18<00:48,  3.31it/s] 28%|██▊       | 61/221 [00:18<00:48,  3.31it/s] 28%|██▊       | 62/221 [00:18<00:48,  3.31it/s] 29%|██▊       | 63/221 [00:19<00:47,  3.31it/s] 29%|██▉       | 64/221 [00:19<00:47,  3.31it/s] 29%|██▉       | 65/221 [00:19<00:47,  3.31it/s] 30%|██▉       | 66/221 [00:19<00:46,  3.31it/s] 30%|███       | 67/221 [00:20<00:46,  3.31it/s] 31%|███       | 68/221 [00:20<00:46,  3.31it/s] 31%|███       | 69/221 [00:20<00:45,  3.31it/s] 32%|███▏      | 70/221 [00:21<00:45,  3.31it/s] 32%|███▏      | 71/221 [00:21<00:45,  3.31it/s] 33%|███▎      | 72/221 [00:21<00:44,  3.31it/s] 33%|███▎      | 73/221 [00:22<00:44,  3.31it/s] 33%|███▎      | 74/221 [00:22<00:44,  3.31it/s] 34%|███▍      | 75/221 [00:22<00:44,  3.31it/s] 34%|███▍      | 76/221 [00:22<00:43,  3.31it/s] 35%|███▍      | 77/221 [00:23<00:43,  3.31it/s] 35%|███▌      | 78/221 [00:23<00:43,  3.31it/s] 36%|███▌      | 79/221 [00:23<00:42,  3.31it/s] 36%|███▌      | 80/221 [00:24<00:42,  3.31it/s] 37%|███▋      | 81/221 [00:24<00:42,  3.31it/s] 37%|███▋      | 82/221 [00:24<00:41,  3.31it/s] 38%|███▊      | 83/221 [00:25<00:41,  3.31it/s] 38%|███▊      | 84/221 [00:25<00:41,  3.31it/s] 38%|███▊      | 85/221 [00:25<00:41,  3.31it/s] 39%|███▉      | 86/221 [00:25<00:40,  3.31it/s] 39%|███▉      | 87/221 [00:26<00:40,  3.31it/s] 40%|███▉      | 88/221 [00:26<00:40,  3.31it/s] 40%|████      | 89/221 [00:26<00:39,  3.31it/s] 41%|████      | 90/221 [00:27<00:39,  3.31it/s] 41%|████      | 91/221 [00:27<00:39,  3.31it/s] 42%|████▏     | 92/221 [00:27<00:38,  3.31it/s] 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s] 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s] 43%|████▎     | 95/221 [00:28<00:38,  3.31it/s] 43%|████▎     | 96/221 [00:28<00:37,  3.31it/s] 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s] 44%|████▍     | 98/221 [00:29<00:37,  3.31it/s] 45%|████▍     | 99/221 [00:29<00:36,  3.31it/s] 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s] 46%|████▌     | 101/221 [00:30<00:36,  3.31it/s] 46%|████▌     | 102/221 [00:30<00:35,  3.31it/s] 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s] 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s] 48%|████▊     | 105/221 [00:31<00:35,  3.31it/s] 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s] 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s] 49%|████▉     | 108/221 [00:32<00:34,  3.31it/s] 49%|████▉     | 109/221 [00:32<00:33,  3.31it/s] 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s] 50%|█████     | 111/221 [00:33<00:33,  3.31it/s] 51%|█████     | 112/221 [00:33<00:32,  3.31it/s] 51%|█████     | 113/221 [00:34<00:32,  3.31it/s] 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s] 52%|█████▏    | 115/221 [00:34<00:32,  3.31it/s] 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s] 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s] 53%|█████▎    | 118/221 [00:35<00:31,  3.31it/s] 54%|█████▍    | 119/221 [00:35<00:30,  3.31it/s] 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s] 55%|█████▍    | 121/221 [00:36<00:30,  3.31it/s] 55%|█████▌    | 122/221 [00:36<00:29,  3.31it/s] 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s] 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s] 57%|█████▋    | 125/221 [00:37<00:28,  3.31it/s] 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s] 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s] 58%|█████▊    | 128/221 [00:38<00:28,  3.31it/s] 58%|█████▊    | 129/221 [00:38<00:27,  3.31it/s] 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s] 59%|█████▉    | 131/221 [00:39<00:27,  3.31it/s] 60%|█████▉    | 132/221 [00:39<00:26,  3.31it/s] 60%|██████    | 133/221 [00:40<00:26,  3.31it/s] 61%|██████    | 134/221 [00:40<00:26,  3.31it/s] 61%|██████    | 135/221 [00:40<00:25,  3.31it/s] 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s] 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s] 62%|██████▏   | 138/221 [00:41<00:25,  3.31it/s] 63%|██████▎   | 139/221 [00:41<00:24,  3.31it/s] 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s] 64%|██████▍   | 141/221 [00:42<00:24,  3.31it/s] 64%|██████▍   | 142/221 [00:42<00:23,  3.31it/s] 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s] 65%|██████▌   | 144/221 [00:43<00:23,  3.31it/s] 66%|██████▌   | 145/221 [00:43<00:22,  3.31it/s] 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s] 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s] 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s] 67%|██████▋   | 149/221 [00:44<00:21,  3.31it/s] 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s] 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s] 69%|██████▉   | 152/221 [00:45<00:20,  3.31it/s] 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s] 70%|██████▉   | 154/221 [00:46<00:20,  3.31it/s] 70%|███████   | 155/221 [00:46<00:19,  3.31it/s] 71%|███████   | 156/221 [00:47<00:19,  3.31it/s] 71%|███████   | 157/221 [00:47<00:19,  3.31it/s] 71%|███████▏  | 158/221 [00:47<00:19,  3.31it/s] 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s] 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s] 73%|███████▎  | 161/221 [00:48<00:18,  3.31it/s] 73%|███████▎  | 162/221 [00:48<00:17,  3.31it/s] 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s] 74%|███████▍  | 164/221 [00:49<00:17,  3.31it/s] 75%|███████▍  | 165/221 [00:49<00:16,  3.31it/s] 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s] 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s] 76%|███████▌  | 168/221 [00:50<00:16,  3.31it/s] 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s] 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s] 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s] 78%|███████▊  | 172/221 [00:51<00:14,  3.31it/s] 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s] 79%|███████▊  | 174/221 [00:52<00:14,  3.31it/s] 79%|███████▉  | 175/221 [00:52<00:13,  3.31it/s] 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s] 80%|████████  | 177/221 [00:53<00:13,  3.31it/s] 81%|████████  | 178/221 [00:53<00:12,  3.31it/s] 81%|████████  | 179/221 [00:54<00:12,  3.31it/s] 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s] 82%|████████▏ | 181/221 [00:54<00:12,  3.31it/s] 82%|████████▏ | 182/221 [00:54<00:11,  3.31it/s] 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s] 83%|████████▎ | 184/221 [00:55<00:11,  3.31it/s] 84%|████████▎ | 185/221 [00:55<00:10,  3.31it/s] 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s] 85%|████████▍ | 187/221 [00:56<00:10,  3.31it/s] 85%|████████▌ | 188/221 [00:56<00:09,  3.31it/s] 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s] 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s] 86%|████████▋ | 191/221 [00:57<00:09,  3.31it/s] 87%|████████▋ | 192/221 [00:57<00:08,  3.31it/s] 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s] 88%|████████▊ | 194/221 [00:58<00:08,  3.31it/s] 88%|████████▊ | 195/221 [00:58<00:07,  3.31it/s] 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s] 89%|████████▉ | 197/221 [00:59<00:07,  3.31it/s] 90%|████████▉ | 198/221 [00:59<00:06,  3.31it/s] 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s] 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s] 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s] 91%|█████████▏| 202/221 [01:00<00:05,  3.31it/s] 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s] 92%|█████████▏| 204/221 [01:01<00:05,  3.31it/s] 93%|█████████▎| 205/221 [01:01<00:04,  3.31it/s] 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s] 94%|█████████▎| 207/221 [01:02<00:04,  3.31it/s] 94%|█████████▍| 208/221 [01:02<00:03,  3.31it/s] 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s] 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s] 95%|█████████▌| 211/221 [01:03<00:03,  3.31it/s] 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s] 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s] 97%|█████████▋| 214/221 [01:04<00:02,  3.31it/s] 97%|█████████▋| 215/221 [01:04<00:01,  3.31it/s] 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s] 98%|█████████▊| 217/221 [01:05<00:01,  3.31it/s] 99%|█████████▊| 218/221 [01:05<00:00,  3.31it/s] 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s]100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s]100%|██████████| 221/221 [01:06<00:00,  3.31it/s]100%|██████████| 221/221 [01:06<00:00,  3.31it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<00:23,  9.19it/s]  1%|          | 2/221 [00:00<00:50,  4.33it/s]  1%|▏         | 3/221 [00:00<00:39,  5.49it/s]  2%|▏         | 4/221 [00:01<01:17,  2.79it/s]  2%|▏         | 5/221 [00:01<01:26,  2.49it/s]  3%|▎         | 7/221 [00:02<01:04,  3.34it/s]  4%|▎         | 8/221 [00:02<01:23,  2.55it/s]  4%|▍         | 9/221 [00:03<01:27,  2.42it/s]  5%|▍         | 10/221 [00:03<01:22,  2.55it/s]  5%|▍         | 11/221 [00:03<01:27,  2.41it/s]  5%|▌         | 12/221 [00:04<01:17,  2.71it/s]  6%|▌         | 13/221 [00:04<01:39,  2.10it/s]  6%|▋         | 14/221 [00:05<01:41,  2.03it/s]  7%|▋         | 15/221 [00:05<01:20,  2.57it/s]  7%|▋         | 16/221 [00:05<01:04,  3.18it/s]  8%|▊         | 17/221 [00:06<01:42,  1.98it/s]  8%|▊         | 18/221 [00:07<01:30,  2.25it/s]  9%|▊         | 19/221 [00:07<01:16,  2.63it/s] 10%|▉         | 21/221 [00:07<00:49,  4.08it/s] 10%|▉         | 22/221 [00:07<00:42,  4.70it/s] 10%|█         | 23/221 [00:07<00:43,  4.51it/s] 11%|█         | 24/221 [00:08<00:44,  4.43it/s] 11%|█▏        | 25/221 [00:08<00:51,  3.80it/s] 12%|█▏        | 26/221 [00:08<00:59,  3.27it/s] 12%|█▏        | 27/221 [00:09<01:01,  3.14it/s] 13%|█▎        | 28/221 [00:09<01:11,  2.71it/s] 13%|█▎        | 29/221 [00:10<01:30,  2.13it/s] 14%|█▎        | 30/221 [00:10<01:11,  2.66it/s] 14%|█▍        | 31/221 [00:10<00:58,  3.25it/s] 15%|█▍        | 33/221 [00:11<00:52,  3.58it/s] 15%|█▌        | 34/221 [00:11<01:03,  2.94it/s] 16%|█▌        | 35/221 [00:12<01:03,  2.94it/s] 16%|█▋        | 36/221 [00:12<01:08,  2.72it/s] 17%|█▋        | 37/221 [00:12<01:09,  2.66it/s] 17%|█▋        | 38/221 [00:13<00:59,  3.10it/s] 18%|█▊        | 39/221 [00:13<01:12,  2.51it/s] 18%|█▊        | 40/221 [00:13<01:08,  2.66it/s] 19%|█▉        | 42/221 [00:14<00:50,  3.55it/s] 19%|█▉        | 43/221 [00:14<00:45,  3.91it/s] 20%|█▉        | 44/221 [00:14<00:41,  4.29it/s] 20%|██        | 45/221 [00:15<00:51,  3.43it/s] 21%|██        | 46/221 [00:15<00:43,  4.02it/s] 21%|██▏       | 47/221 [00:15<00:41,  4.23it/s] 22%|██▏       | 48/221 [00:15<00:38,  4.46it/s] 22%|██▏       | 49/221 [00:15<00:43,  3.95it/s] 23%|██▎       | 50/221 [00:17<01:25,  2.00it/s] 23%|██▎       | 51/221 [00:17<01:09,  2.43it/s] 24%|██▎       | 52/221 [00:17<00:58,  2.89it/s] 24%|██▍       | 53/221 [00:17<00:46,  3.64it/s] 24%|██▍       | 54/221 [00:18<01:00,  2.76it/s] 25%|██▌       | 56/221 [00:18<00:38,  4.30it/s] 26%|██▌       | 57/221 [00:18<00:35,  4.56it/s] 26%|██▌       | 58/221 [00:18<00:46,  3.50it/s] 27%|██▋       | 59/221 [00:19<00:52,  3.07it/s] 27%|██▋       | 60/221 [00:19<00:44,  3.66it/s] 28%|██▊       | 61/221 [00:19<00:36,  4.34it/s] 28%|██▊       | 62/221 [00:19<00:31,  5.05it/s] 29%|██▊       | 63/221 [00:19<00:33,  4.78it/s] 29%|██▉       | 64/221 [00:20<00:31,  4.91it/s] 29%|██▉       | 65/221 [00:20<00:37,  4.16it/s] 30%|██▉       | 66/221 [00:20<00:31,  4.94it/s] 30%|███       | 67/221 [00:21<00:43,  3.56it/s] 31%|███       | 68/221 [00:21<00:34,  4.41it/s] 31%|███       | 69/221 [00:21<00:41,  3.70it/s] 32%|███▏      | 70/221 [00:21<00:42,  3.60it/s] 32%|███▏      | 71/221 [00:22<00:44,  3.39it/s] 33%|███▎      | 72/221 [00:22<00:49,  3.01it/s] 33%|███▎      | 73/221 [00:22<00:44,  3.32it/s] 33%|███▎      | 74/221 [00:23<00:41,  3.57it/s] 34%|███▍      | 75/221 [00:23<00:43,  3.35it/s] 34%|███▍      | 76/221 [00:23<00:35,  4.09it/s] 35%|███▍      | 77/221 [00:24<00:46,  3.09it/s] 35%|███▌      | 78/221 [00:24<00:52,  2.73it/s] 36%|███▌      | 79/221 [00:24<00:50,  2.83it/s] 36%|███▌      | 80/221 [00:25<00:43,  3.22it/s] 37%|███▋      | 81/221 [00:25<00:39,  3.51it/s] 37%|███▋      | 82/221 [00:26<01:04,  2.16it/s] 38%|███▊      | 83/221 [00:26<01:08,  2.00it/s] 38%|███▊      | 84/221 [00:27<01:01,  2.22it/s] 38%|███▊      | 85/221 [00:27<00:56,  2.42it/s] 39%|███▉      | 86/221 [00:27<00:48,  2.77it/s] 39%|███▉      | 87/221 [00:28<00:49,  2.69it/s] 40%|███▉      | 88/221 [00:28<00:39,  3.40it/s] 40%|████      | 89/221 [00:28<00:45,  2.87it/s] 41%|████      | 90/221 [00:28<00:37,  3.50it/s] 42%|████▏     | 92/221 [00:29<00:28,  4.50it/s] 42%|████▏     | 93/221 [00:29<00:36,  3.51it/s] 43%|████▎     | 94/221 [00:29<00:31,  4.05it/s] 43%|████▎     | 95/221 [00:30<00:41,  3.02it/s] 43%|████▎     | 96/221 [00:30<00:37,  3.38it/s] 44%|████▍     | 97/221 [00:30<00:32,  3.81it/s] 44%|████▍     | 98/221 [00:30<00:30,  4.09it/s] 45%|████▍     | 99/221 [00:31<00:30,  4.00it/s] 45%|████▌     | 100/221 [00:31<00:28,  4.25it/s] 46%|████▌     | 101/221 [00:31<00:28,  4.25it/s] 46%|████▌     | 102/221 [00:32<00:39,  3.02it/s] 47%|████▋     | 103/221 [00:32<00:31,  3.71it/s] 47%|████▋     | 104/221 [00:32<00:30,  3.81it/s] 48%|████▊     | 105/221 [00:32<00:26,  4.35it/s] 48%|████▊     | 106/221 [00:32<00:25,  4.56it/s] 49%|████▉     | 108/221 [00:32<00:19,  5.92it/s] 49%|████▉     | 109/221 [00:33<00:22,  4.99it/s] 50%|████▉     | 110/221 [00:33<00:33,  3.30it/s] 50%|█████     | 111/221 [00:34<00:39,  2.75it/s] 51%|█████     | 112/221 [00:34<00:42,  2.55it/s] 51%|█████     | 113/221 [00:35<00:34,  3.09it/s] 52%|█████▏    | 114/221 [00:35<00:31,  3.41it/s] 52%|█████▏    | 116/221 [00:35<00:26,  3.95it/s] 53%|█████▎    | 117/221 [00:35<00:24,  4.16it/s] 53%|█████▎    | 118/221 [00:36<00:35,  2.93it/s] 54%|█████▍    | 119/221 [00:36<00:30,  3.35it/s] 54%|█████▍    | 120/221 [00:36<00:28,  3.54it/s] 55%|█████▍    | 121/221 [00:37<00:25,  3.88it/s] 55%|█████▌    | 122/221 [00:37<00:22,  4.43it/s] 56%|█████▌    | 124/221 [00:37<00:21,  4.55it/s] 57%|█████▋    | 125/221 [00:37<00:21,  4.53it/s] 57%|█████▋    | 126/221 [00:38<00:20,  4.67it/s] 57%|█████▋    | 127/221 [00:38<00:21,  4.47it/s] 58%|█████▊    | 128/221 [00:38<00:17,  5.18it/s] 58%|█████▊    | 129/221 [00:38<00:16,  5.60it/s] 59%|█████▉    | 130/221 [00:38<00:16,  5.49it/s] 60%|█████▉    | 132/221 [00:38<00:11,  7.44it/s] 60%|██████    | 133/221 [00:39<00:15,  5.53it/s] 61%|██████    | 134/221 [00:39<00:13,  6.25it/s] 61%|██████    | 135/221 [00:39<00:16,  5.16it/s] 62%|██████▏   | 136/221 [00:40<00:21,  4.01it/s] 62%|██████▏   | 137/221 [00:40<00:20,  4.19it/s] 62%|██████▏   | 138/221 [00:40<00:17,  4.80it/s] 63%|██████▎   | 139/221 [00:40<00:24,  3.34it/s] 63%|██████▎   | 140/221 [00:41<00:25,  3.15it/s] 64%|██████▍   | 141/221 [00:41<00:22,  3.57it/s] 64%|██████▍   | 142/221 [00:41<00:22,  3.44it/s] 65%|██████▍   | 143/221 [00:41<00:19,  4.00it/s] 66%|██████▌   | 145/221 [00:42<00:20,  3.65it/s] 66%|██████▌   | 146/221 [00:42<00:22,  3.34it/s] 67%|██████▋   | 147/221 [00:43<00:18,  4.01it/s] 67%|██████▋   | 148/221 [00:43<00:17,  4.23it/s] 67%|██████▋   | 149/221 [00:43<00:14,  4.92it/s] 68%|██████▊   | 150/221 [00:43<00:13,  5.41it/s] 68%|██████▊   | 151/221 [00:43<00:19,  3.63it/s] 69%|██████▉   | 152/221 [00:45<00:36,  1.90it/s] 69%|██████▉   | 153/221 [00:45<00:31,  2.19it/s] 70%|██████▉   | 154/221 [00:45<00:25,  2.60it/s] 70%|███████   | 155/221 [00:45<00:22,  2.96it/s] 71%|███████   | 156/221 [00:46<00:21,  2.99it/s] 71%|███████   | 157/221 [00:46<00:18,  3.49it/s] 72%|███████▏  | 159/221 [00:46<00:13,  4.68it/s] 72%|███████▏  | 160/221 [00:46<00:12,  4.81it/s] 73%|███████▎  | 162/221 [00:47<00:12,  4.56it/s] 74%|███████▍  | 163/221 [00:47<00:12,  4.57it/s] 74%|███████▍  | 164/221 [00:48<00:18,  3.03it/s] 75%|███████▌  | 166/221 [00:48<00:18,  2.96it/s] 76%|███████▌  | 168/221 [00:49<00:13,  3.97it/s] 76%|███████▋  | 169/221 [00:49<00:13,  3.92it/s] 77%|███████▋  | 170/221 [00:50<00:19,  2.64it/s] 77%|███████▋  | 171/221 [00:50<00:15,  3.19it/s] 78%|███████▊  | 172/221 [00:50<00:14,  3.40it/s] 79%|███████▊  | 174/221 [00:51<00:19,  2.43it/s] 79%|███████▉  | 175/221 [00:52<00:19,  2.32it/s] 80%|███████▉  | 176/221 [00:52<00:16,  2.68it/s] 80%|████████  | 177/221 [00:52<00:14,  2.98it/s] 81%|████████  | 178/221 [00:52<00:12,  3.42it/s] 81%|████████  | 179/221 [00:52<00:11,  3.79it/s] 81%|████████▏ | 180/221 [00:53<00:11,  3.72it/s] 82%|████████▏ | 181/221 [00:53<00:11,  3.59it/s] 82%|████████▏ | 182/221 [00:53<00:09,  4.32it/s] 83%|████████▎ | 183/221 [00:53<00:07,  4.79it/s] 83%|████████▎ | 184/221 [00:53<00:06,  5.29it/s] 84%|████████▎ | 185/221 [00:54<00:08,  4.23it/s] 84%|████████▍ | 186/221 [00:54<00:08,  4.16it/s] 85%|████████▍ | 187/221 [00:54<00:07,  4.57it/s] 85%|████████▌ | 188/221 [00:55<00:10,  3.17it/s] 86%|████████▌ | 189/221 [00:55<00:14,  2.20it/s] 86%|████████▌ | 190/221 [00:56<00:14,  2.07it/s] 87%|████████▋ | 192/221 [00:56<00:08,  3.28it/s] 87%|████████▋ | 193/221 [00:56<00:07,  3.57it/s] 88%|████████▊ | 194/221 [00:57<00:08,  3.18it/s] 88%|████████▊ | 195/221 [00:57<00:07,  3.48it/s] 89%|████████▊ | 196/221 [00:58<00:10,  2.35it/s] 89%|████████▉ | 197/221 [00:58<00:09,  2.53it/s] 90%|████████▉ | 198/221 [00:58<00:07,  2.98it/s] 90%|█████████ | 199/221 [00:59<00:06,  3.49it/s] 90%|█████████ | 200/221 [00:59<00:05,  3.54it/s] 91%|█████████ | 201/221 [00:59<00:05,  3.49it/s] 92%|█████████▏| 203/221 [00:59<00:03,  4.99it/s] 92%|█████████▏| 204/221 [01:00<00:04,  3.77it/s] 93%|█████████▎| 205/221 [01:00<00:03,  4.40it/s] 93%|█████████▎| 206/221 [01:00<00:03,  4.31it/s] 94%|█████████▎| 207/221 [01:01<00:04,  3.22it/s] 94%|█████████▍| 208/221 [01:01<00:03,  3.31it/s] 95%|█████████▍| 209/221 [01:01<00:03,  3.01it/s] 95%|█████████▌| 210/221 [01:01<00:03,  3.62it/s] 95%|█████████▌| 211/221 [01:02<00:02,  4.16it/s] 96%|█████████▌| 212/221 [01:02<00:02,  4.43it/s] 96%|█████████▋| 213/221 [01:03<00:03,  2.28it/s] 97%|█████████▋| 214/221 [01:03<00:03,  2.12it/s] 97%|█████████▋| 215/221 [01:04<00:02,  2.24it/s] 98%|█████████▊| 216/221 [01:04<00:01,  2.80it/s] 98%|█████████▊| 217/221 [01:04<00:01,  3.53it/s] 99%|█████████▊| 218/221 [01:04<00:00,  4.18it/s] 99%|█████████▉| 219/221 [01:05<00:00,  2.99it/s]100%|█████████▉| 220/221 [01:05<00:00,  2.52it/s]100%|██████████| 221/221 [01:06<00:00,  1.96it/s]100%|██████████| 221/221 [01:06<00:00,  3.33it/s]
09/18/2024 15:05:09 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_forward========

09/18/2024 15:05:09 - INFO - __main__ -   {'area_r1': 24.1, 'area_recall': '24.1/43.1/50.6', 'area_ravg': 39.3}
09/18/2024 15:05:09 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_backard========

09/18/2024 15:05:09 - INFO - __main__ -   {'forward_r1': 33.3, 'forward_recall': '33.3/64.7/75.1', 'forward_ravg': 57.7}
09/18/2024 15:05:09 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video========

09/18/2024 15:05:09 - INFO - __main__ -   {'area_video_r1': 33.3, 'area_video_recall': '33.3/64.4/75.9', 'area_video_ravg': 57.8}
09/18/2024 15:05:09 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itm_area========

09/18/2024 15:05:09 - INFO - __main__ -   {'area_video_r1': 48.9, 'area_video_recall': '48.9/68.0/73.2', 'area_video_ravg': 63.3, 'area_video_back_r1': 48.3, 'area_video_back_recall': '48.3/71.2/77.4', 'area_video_back_ravg': 65.6}
09/18/2024 15:05:09 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas========

09/18/2024 15:05:09 - INFO - __main__ -   {'video_r1': 31.0, 'video_recall': '31.0/53.2/63.1', 'video_ravg': 49.1}
09/18/2024 15:05:09 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas========

09/18/2024 15:05:09 - INFO - __main__ -   {'video_r1': 50.7, 'video_recall': '50.7/68.7/74.4', 'video_ravg': 64.6}
  0%|          | 0/2910 [00:00<?, ?it/s][h264 @ 0x55d8ff29ddc0] mmco: unref short failure
[h264 @ 0x55d8ff29ddc0] mmco: unref short failure
[h264 @ 0x55d8ff29ddc0] mmco: unref short failure
[h264 @ 0x55d8ff29ddc0] mmco: unref short failure
[h264 @ 0x55cb627f5e40] mmco: unref short failure
[h264 @ 0x55cb627f5e40] mmco: unref short failure
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/2910 [00:09<7:46:19,  9.62s/it]  0%|          | 2/2910 [00:13<5:04:45,  6.29s/it]  0%|          | 3/2910 [00:17<4:17:35,  5.32s/it]  0%|          | 4/2910 [00:22<4:05:05,  5.06s/it][h264 @ 0x55d8ff0d4480] mmco: unref short failure
[h264 @ 0x55d8ff0d4480] mmco: unref short failure
[h264 @ 0x5565ecea9cc0] mmco: unref short failure
[h264 @ 0x5565ecea9cc0] mmco: unref short failure
[h264 @ 0x5565eca02e00] mmco: unref short failure
  0%|          | 5/2910 [00:26<3:52:23,  4.80s/it][h264 @ 0x5598e153f880] mmco: unref short failure
[h264 @ 0x5598e153f880] mmco: unref short failure
[h264 @ 0x55d907a7d300] mmco: unref short failure
  0%|          | 6/2910 [00:31<3:52:00,  4.79s/it]  0%|          | 7/2910 [00:36<4:00:15,  4.97s/it]09/18/2024 15:05:47 - INFO - __main__ -   current idx KX0TpgJdepY.11 from finetune_area returns wrong image/video, use 56889 instead.
[h264 @ 0x55d8fedd05c0] mmco: unref short failure
[h264 @ 0x55cb6ac82f00] mmco: unref short failure
  0%|          | 8/2910 [00:41<4:01:57,  5.00s/it][h264 @ 0x55cb63fa8340] mmco: unref short failure
[h264 @ 0x55cb63fa8340] mmco: unref short failure
[h264 @ 0x55cb63458b00] mmco: unref short failure
  0%|          | 9/2910 [00:47<4:11:15,  5.20s/it]  0%|          | 10/2910 [00:53<4:26:55,  5.52s/it]  0%|          | 11/2910 [00:59<4:23:05,  5.45s/it]  0%|          | 12/2910 [01:04<4:21:36,  5.42s/it]  0%|          | 13/2910 [01:09<4:19:57,  5.38s/it]09/18/2024 15:06:20 - INFO - __main__ -   current idx f2vz8YAFQms.52 from finetune_area returns wrong image/video, use 47000 instead.
  0%|          | 14/2910 [01:15<4:18:31,  5.36s/it]  1%|          | 15/2910 [01:21<4:29:12,  5.58s/it][h264 @ 0x55d902e1af80] mmco: unref short failure
[h264 @ 0x55d902e1af80] mmco: unref short failure
[h264 @ 0x55d902e1af80] mmco: unref short failure
[h264 @ 0x55d902e1af80] mmco: unref short failure
09/18/2024 15:06:35 - INFO - __main__ -   current idx QzpF1yDPHf0.29 from finetune_area returns wrong image/video, use 139919 instead.
[h264 @ 0x5598e81e8680] mmco: unref short failure
[h264 @ 0x5598e9abc880] mmco: unref short failure
[h264 @ 0x5598e9abc880] mmco: unref short failure
[h264 @ 0x5565eae0e340] mmco: unref short failure
[h264 @ 0x5565eae0e340] mmco: unref short failure
[h264 @ 0x55d8ffb33180] mmco: unref short failure
[h264 @ 0x5565ec66f880] mmco: unref short failure
[h264 @ 0x5565ec66f880] mmco: unref short failure
[h264 @ 0x55d9022b6fc0] mmco: unref short failure
[h264 @ 0x55cb6d619840] mmco: unref short failure
[h264 @ 0x55cb6d619840] mmco: unref short failure
[h264 @ 0x5598e59acf40] mmco: unref short failure
[h264 @ 0x5598e59acf40] mmco: unref short failure
[h264 @ 0x55cb63a3ecc0] mmco: unref short failure
[h264 @ 0x55cb5f7f08c0] mmco: unref short failure
[h264 @ 0x55cb5f7f08c0] mmco: unref short failure
  1%|          | 16/2910 [02:10<15:03:21, 18.73s/it][h264 @ 0x5565f5b1e100] mmco: unref short failure
  1%|          | 17/2910 [02:15<11:51:08, 14.75s/it][h264 @ 0x55d8ffefe8c0] mmco: unref short failure
[h264 @ 0x55d8ffefe8c0] mmco: unref short failure
[h264 @ 0x5598e8ecf740] mmco: unref short failure
[h264 @ 0x5598e8ecf740] mmco: unref short failure
[h264 @ 0x5598e8ecf740] mmco: unref short failure
[h264 @ 0x5598e8ecf740] mmco: unref short failure
[h264 @ 0x5598dd07b8c0] mmco: unref short failure
[h264 @ 0x5565ede53cc0] mmco: unref short failure
  1%|          | 18/2910 [02:34<12:50:02, 15.98s/it][h264 @ 0x5598de283180] mmco: unref short failure
[h264 @ 0x55d9080e47c0] mmco: unref short failure
[h264 @ 0x55d9080e47c0] mmco: unref short failure
  1%|          | 19/2910 [02:44<11:25:42, 14.23s/it][h264 @ 0x55cb66cca280] mmco: unref short failure
[h264 @ 0x55cb66cca280] mmco: unref short failure
  1%|          | 20/2910 [02:57<10:59:28, 13.69s/it][h264 @ 0x55d901bee600] mmco: unref short failure
[h264 @ 0x55d901bee600] mmco: unref short failure
[h264 @ 0x55d901bee600] mmco: unref short failure
[h264 @ 0x55d901bee600] mmco: unref short failure
[h264 @ 0x5565ea1d5fc0] mmco: unref short failure
[h264 @ 0x5565ea1d5fc0] mmco: unref short failure
  1%|          | 21/2910 [03:02<8:53:40, 11.08s/it] [h264 @ 0x55cb64a97480] mmco: unref short failure
[h264 @ 0x55cb64a97480] mmco: unref short failure
[h264 @ 0x55cb6b49b4c0] mmco: unref short failure
[h264 @ 0x55cb6b49b4c0] mmco: unref short failure
  1%|          | 22/2910 [03:09<7:57:08,  9.91s/it][h264 @ 0x55d906b54740] mmco: unref short failure
  1%|          | 23/2910 [03:15<7:04:26,  8.82s/it][h264 @ 0x55d90c38d980] mmco: unref short failure
[h264 @ 0x55d90c38d980] mmco: unref short failure
[h264 @ 0x5598e2b43640] mmco: unref short failure
[h264 @ 0x5598e08e33c0] mmco: unref short failure
[h264 @ 0x5598e08e33c0] mmco: unref short failure
[h264 @ 0x55cb6672b780] mmco: unref short failure
[h264 @ 0x55cb6672b780] mmco: unref short failure
[h264 @ 0x55d905a8eec0] mmco: unref short failure
[h264 @ 0x5565e5827c80] mmco: unref short failure
[h264 @ 0x55d90f52f340] mmco: unref short failure
[h264 @ 0x5598e4ad6600] mmco: unref short failure
[h264 @ 0x55cb6c34fb40] mmco: unref short failure
[h264 @ 0x55cb6c34fb40] mmco: unref short failure
[h264 @ 0x55d8fee81180] mmco: unref short failure
[h264 @ 0x55d8fee81180] mmco: unref short failure
09/18/2024 15:09:43 - INFO - __main__ -   current idx IcQG8e_UOeA.24 from finetune_area returns wrong image/video, use 119424 instead.
[h264 @ 0x5598e9266200] mmco: unref short failure
  1%|          | 24/2910 [04:35<24:00:45, 29.95s/it]  1%|          | 25/2910 [04:39<17:57:44, 22.41s/it][h264 @ 0x55d9087bb740] mmco: unref short failure
[h264 @ 0x5565e5c5b100] mmco: unref short failure
09/18/2024 15:09:58 - INFO - __main__ -   current idx XgAp7Dp862o.1 from finetune_area returns wrong image/video, use 80046 instead.
[h264 @ 0x55d90b81ea40] mmco: unref short failure
[h264 @ 0x55d90b81ea40] mmco: unref short failure
[h264 @ 0x55d8ff480f80] mmco: unref short failure
[h264 @ 0x55d8ff480f80] mmco: unref short failure
[h264 @ 0x55d8ff480f80] mmco: unref short failure
[h264 @ 0x55d8ff480f80] mmco: unref short failure
[h264 @ 0x55d9078c4380] mmco: unref short failure
[h264 @ 0x55cb5efe5140] mmco: unref short failure
[h264 @ 0x55cb5efe5140] mmco: unref short failure
  1%|          | 26/2910 [05:10<20:00:06, 24.97s/it][h264 @ 0x5598e1d48a40] mmco: unref short failure
[h264 @ 0x5598e1d48a40] mmco: unref short failure
[h264 @ 0x5598e1d48a40] mmco: unref short failure
[h264 @ 0x55cb64a1dd80] mmco: unref short failure
[h264 @ 0x55cb64a1dd80] mmco: unref short failure
[h264 @ 0x5565ebf912c0] mmco: unref short failure
[h264 @ 0x55d905628680] mmco: unref short failure
[h264 @ 0x55d905628680] mmco: unref short failure
[h264 @ 0x55d905628680] mmco: unref short failure
[h264 @ 0x55d905628680] mmco: unref short failure
[h264 @ 0x55d90e738480] mmco: unref short failure
  1%|          | 27/2910 [05:25<17:34:43, 21.95s/it][h264 @ 0x5598e67df800] mmco: unref short failure
[h264 @ 0x5598e67df800] mmco: unref short failure
09/18/2024 15:10:40 - INFO - __main__ -   current idx zpVEmyBr_Hg.14 from finetune_area returns wrong image/video, use 62748 instead.
  1%|          | 28/2910 [05:32<13:52:20, 17.33s/it][h264 @ 0x55d90099cb40] mmco: unref short failure
[h264 @ 0x55d90099cb40] mmco: unref short failure
[h264 @ 0x55d90099cb40] mmco: unref short failure
[h264 @ 0x55d90099cb40] mmco: unref short failure
  1%|          | 29/2910 [05:37<10:59:25, 13.73s/it]  1%|          | 30/2910 [05:42<8:57:18, 11.19s/it] [h264 @ 0x5565ef44b1c0] mmco: unref short failure
[h264 @ 0x5565ef44b1c0] mmco: unref short failure
  1%|          | 31/2910 [05:49<7:46:43,  9.73s/it][h264 @ 0x55cb5efe5140] mmco: unref short failure
[h264 @ 0x5598dfce8780] mmco: unref short failure
[h264 @ 0x5598dfce8780] mmco: unref short failure
[h264 @ 0x5565e82842c0] mmco: unref short failure
[h264 @ 0x55cb5e6bdd40] mmco: unref short failure
[h264 @ 0x55cb5e6bdd40] mmco: unref short failure
[h264 @ 0x5598dcb94980] mmco: unref short failure
[h264 @ 0x5598dcb94980] mmco: unref short failure
[h264 @ 0x5598de2b2840] mmco: unref short failure
[h264 @ 0x5598de2b2840] mmco: unref short failure
[h264 @ 0x55d9129c7180] mmco: unref short failure
[h264 @ 0x55d9129c7180] mmco: unref short failure
[h264 @ 0x55cb5e4f32c0] mmco: unref short failure
[h264 @ 0x55cb5e4f32c0] mmco: unref short failure
[h264 @ 0x55d903ba1d40] mmco: unref short failure
[h264 @ 0x55d903ba1d40] mmco: unref short failure
[h264 @ 0x55d903ba1d40] mmco: unref short failure
[h264 @ 0x55d903ba1d40] mmco: unref short failure
[h264 @ 0x5565ee3f6900] mmco: unref short failure
[h264 @ 0x55cb6bc42ec0] mmco: unref short failure
[h264 @ 0x55cb6bc42ec0] mmco: unref short failure
09/18/2024 15:11:36 - INFO - __main__ -   current idx fQszS3V8sxI.1 from finetune_area returns wrong image/video, use 34076 instead.
[h264 @ 0x5598ed51dd80] mmco: unref short failure
[h264 @ 0x5598ed51dd80] mmco: unref short failure
[h264 @ 0x55cb6d81d780] mmco: unref short failure
[h264 @ 0x55cb6d81d780] mmco: unref short failure
[h264 @ 0x55cb6e7f2640] mmco: unref short failure
[h264 @ 0x55cb6e7f2640] mmco: unref short failure
[h264 @ 0x5565f445ae40] mmco: unref short failure
09/18/2024 15:11:55 - INFO - __main__ -   current idx gy0BKAObQyw.14 from finetune_area returns wrong image/video, use 146105 instead.
[h264 @ 0x55d905c47f40] mmco: unref short failure
[h264 @ 0x55d905c47f40] mmco: unref short failure
[h264 @ 0x55cb690e62c0] mmco: unref short failure
[h264 @ 0x55cb690e62c0] mmco: unref short failure
  1%|          | 32/2910 [07:03<23:13:21, 29.05s/it]  1%|          | 33/2910 [07:09<17:41:48, 22.14s/it][h264 @ 0x5598e38d0380] mmco: unref short failure
[h264 @ 0x5598e38d0380] mmco: unref short failure
[h264 @ 0x5598e38d0380] mmco: unref short failure
[h264 @ 0x5598e38d0380] mmco: unref short failure
[h264 @ 0x5565e627fc40] mmco: unref short failure
[h264 @ 0x5565e627fc40] mmco: unref short failure
[h264 @ 0x55cb5f53fa40] mmco: unref short failure
[h264 @ 0x5598f1d13ac0] mmco: unref short failure
[h264 @ 0x5598ece38540] mmco: unref short failure
[h264 @ 0x5598ece38540] mmco: unref short failure
[h264 @ 0x5598df8b4ec0] mmco: unref short failure
[h264 @ 0x5598df8b4ec0] mmco: unref short failure
  1%|          | 34/2910 [07:32<18:02:45, 22.59s/it][h264 @ 0x5565f041a040] mmco: unref short failure
[h264 @ 0x5565f041a040] mmco: unref short failure
[h264 @ 0x5565ef7462c0] mmco: unref short failure
  1%|          | 35/2910 [07:52<17:12:54, 21.56s/it][h264 @ 0x5598e8171cc0] mmco: unref short failure
[h264 @ 0x5598e8171cc0] mmco: unref short failure
  1%|          | 36/2910 [07:57<13:27:10, 16.85s/it]09/18/2024 15:13:12 - INFO - __main__ -   current idx HnCSd-QjKvs.136 from finetune_area returns wrong image/video, use 135996 instead.
  1%|▏         | 37/2910 [08:03<10:46:12, 13.50s/it][h264 @ 0x55cb5ec320c0] mmco: unref short failure
[h264 @ 0x55cb5ec320c0] mmco: unref short failure
  1%|▏         | 38/2910 [08:08<8:46:09, 10.99s/it] [h264 @ 0x5565e7ac9200] mmco: unref short failure
[h264 @ 0x5565e7ac9200] mmco: unref short failure
  1%|▏         | 39/2910 [08:13<7:22:19,  9.24s/it][h264 @ 0x55d905c54b40] mmco: unref short failure
[h264 @ 0x55d905c54b40] mmco: unref short failure
[h264 @ 0x55cb5e4d5180] mmco: unref short failure
[h264 @ 0x55d913a39c80] mmco: unref short failure
[h264 @ 0x5565f2964900] mmco: unref short failure
[h264 @ 0x5565f2964900] mmco: unref short failure
[h264 @ 0x5565e7656880] mmco: unref short failure
[h264 @ 0x55cb5f98e4c0] mmco: unref short failure
[h264 @ 0x55cb5f98e4c0] mmco: unref short failure
[h264 @ 0x5598eb083140] mmco: unref short failure
[h264 @ 0x5598eb083140] mmco: unref short failure
[h264 @ 0x5598eb083140] mmco: unref short failure
[h264 @ 0x5598e4ab27c0] mmco: unref short failure
[h264 @ 0x55d910251900] mmco: unref short failure
[h264 @ 0x55d910251900] mmco: unref short failure
[h264 @ 0x5565ef55bf00] mmco: unref short failure
[h264 @ 0x5565ef55bf00] mmco: unref short failure
[h264 @ 0x55d9103f2b00] mmco: unref short failure
[h264 @ 0x55d9013d52c0] mmco: unref short failure
[h264 @ 0x55d9013d52c0] mmco: unref short failure
[h264 @ 0x5565ed91be40] mmco: unref short failure
[h264 @ 0x5565ed91be40] mmco: unref short failure
[h264 @ 0x55cb6d7c26c0] mmco: unref short failure
[h264 @ 0x55cb6d7c26c0] mmco: unref short failure
[h264 @ 0x55cb6a0bfc80] mmco: unref short failure
[h264 @ 0x55cb6d5b7100] mmco: unref short failure
[h264 @ 0x55cb6d5b7100] mmco: unref short failure
[h264 @ 0x55cb6d5b7100] mmco: unref short failure
  1%|▏         | 40/2910 [09:23<21:45:06, 27.28s/it][h264 @ 0x55cb6bf6ff40] mmco: unref short failure
[h264 @ 0x5565f596fa00] mmco: unref short failure
[h264 @ 0x5565f596fa00] mmco: unref short failure
[h264 @ 0x55d90c40b300] mmco: unref short failure
[h264 @ 0x55d90c40b300] mmco: unref short failure
[h264 @ 0x5565ec0ca840] mmco: unref short failure
[h264 @ 0x5565ec0ca840] mmco: unref short failure
[h264 @ 0x55cb6922fe00] mmco: unref short failure
[h264 @ 0x55cb6922fe00] mmco: unref short failure
  1%|▏         | 41/2910 [09:45<20:29:53, 25.72s/it][h264 @ 0x55cb6db7c6c0] mmco: unref short failure
  1%|▏         | 42/2910 [10:04<18:53:29, 23.71s/it]  1%|▏         | 43/2910 [10:16<16:00:19, 20.10s/it]09/18/2024 15:15:26 - INFO - __main__ -   current idx bcmbKN1aIOU.40 from finetune_area returns wrong image/video, use 123173 instead.
[h264 @ 0x5565f326b280] mmco: unref short failure
  2%|▏         | 44/2910 [10:21<12:30:30, 15.71s/it][h264 @ 0x55d903edfdc0] mmco: unref short failure
[h264 @ 0x55d8ff76cac0] mmco: unref short failure
  2%|▏         | 45/2910 [10:26<10:01:37, 12.60s/it]  2%|▏         | 46/2910 [10:32<8:27:02, 10.62s/it]   2%|▏         | 47/2910 [10:38<7:12:02,  9.05s/it][h264 @ 0x55d9103f2d00] mmco: unref short failure
[h264 @ 0x55cb64a8a900] mmco: unref short failure
[h264 @ 0x5565f74ac780] mmco: unref short failure
[h264 @ 0x55d9100526c0] mmco: unref short failure
[h264 @ 0x55d9100526c0] mmco: unref short failure
[h264 @ 0x5565e84923c0] mmco: unref short failure
[h264 @ 0x55cb6e841c40] mmco: unref short failure
[h264 @ 0x55cb6e841c40] mmco: unref short failure
[h264 @ 0x55d90d4e7400] mmco: unref short failure
[h264 @ 0x55d90d4e7400] mmco: unref short failure
[h264 @ 0x55d9063bb040] mmco: unref short failure
[h264 @ 0x5565f6d18f40] mmco: unref short failure
[h264 @ 0x55cb5ec43740] mmco: unref short failure
[h264 @ 0x5598ee360e00] mmco: unref short failure
[h264 @ 0x55d906bda480] mmco: unref short failure
[h264 @ 0x55d90deaa640] mmco: unref short failure
[h264 @ 0x55d90deaa640] mmco: unref short failure
  2%|▏         | 48/2910 [11:52<22:45:46, 28.63s/it][h264 @ 0x5598e8166a80] mmco: unref short failure
[h264 @ 0x5598e8166a80] mmco: unref short failure
[h264 @ 0x5598e8166a80] mmco: unref short failure
[h264 @ 0x5598e8166a80] mmco: unref short failure
[h264 @ 0x5565f8206f80] mmco: unref short failure
[h264 @ 0x5565f8206f80] mmco: unref short failure
[h264 @ 0x5565f8206f80] mmco: unref short failure
[h264 @ 0x5565f8206f80] mmco: unref short failure
  2%|▏         | 49/2910 [12:04<18:47:51, 23.65s/it]09/18/2024 15:17:14 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 15:17:14 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598e38c3940] mmco: unref short failure
[h264 @ 0x5598e38c3940] mmco: unref short failure
[h264 @ 0x5565f1a48d40] mmco: unref short failure
[h264 @ 0x55cb719b8240] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565e9099dc0] mmco: unref short failure
[h264 @ 0x5565e9099dc0] mmco: unref short failure
[h264 @ 0x5565e9099dc0] mmco: unref short failure
[h264 @ 0x5565e9099dc0] mmco: unref short failure
[h264 @ 0x55cb626f2a80] mmco: unref short failure
[h264 @ 0x55cb626f2a80] mmco: unref short failure
[h264 @ 0x55cb626f2a80] mmco: unref short failure
[h264 @ 0x55cb626f2a80] mmco: unref short failure
[h264 @ 0x55cb626f2a80] mmco: unref short failure
[h264 @ 0x55cb626f2a80] mmco: unref short failure
[h264 @ 0x5565ed032700] mmco: unref short failure
[h264 @ 0x5565ed032700] mmco: unref short failure
[h264 @ 0x55d9006eb200] mmco: unref short failure
[h264 @ 0x5565f5c3d980] mmco: unref short failure
[h264 @ 0x5565f5c3d980] mmco: unref short failure
[h264 @ 0x5565f5c3d980] mmco: unref short failure
[h264 @ 0x5565f5c3d980] mmco: unref short failure
[h264 @ 0x55d90c3459c0] mmco: unref short failure
[h264 @ 0x5565e50cc4c0] mmco: unref short failure
[h264 @ 0x5598efe98f40] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:32,  6.80it/s][A
  1%|          | 2/221 [00:00<00:39,  5.54it/s][A
  1%|▏         | 3/221 [00:00<00:47,  4.63it/s][A
  2%|▏         | 4/221 [00:00<00:38,  5.65it/s][A
  3%|▎         | 6/221 [00:00<00:26,  7.99it/s][A
  3%|▎         | 7/221 [00:01<00:31,  6.88it/s][A
  4%|▎         | 8/221 [00:01<00:29,  7.18it/s][A
  5%|▍         | 10/221 [00:01<00:24,  8.69it/s][A
  5%|▌         | 12/221 [00:02<01:12,  2.87it/s][A
  6%|▌         | 13/221 [00:02<01:02,  3.33it/s][A
  6%|▋         | 14/221 [00:03<00:53,  3.88it/s][A
  7%|▋         | 15/221 [00:03<00:47,  4.32it/s][A
  7%|▋         | 16/221 [00:03<00:41,  4.89it/s][A
  8%|▊         | 17/221 [00:03<00:49,  4.10it/s][A
  8%|▊         | 18/221 [00:03<00:49,  4.11it/s][A
  9%|▊         | 19/221 [00:04<00:47,  4.24it/s][A
  9%|▉         | 20/221 [00:04<00:40,  4.95it/s][A
 10%|▉         | 21/221 [00:04<00:39,  5.09it/s][A
 10%|▉         | 22/221 [00:07<03:25,  1.03s/it][A
 11%|█         | 24/221 [00:07<01:58,  1.66it/s][A
 11%|█▏        | 25/221 [00:07<01:34,  2.08it/s][A
 12%|█▏        | 26/221 [00:07<01:19,  2.45it/s][A
 12%|█▏        | 27/221 [00:08<01:07,  2.89it/s][A
 13%|█▎        | 28/221 [00:08<01:06,  2.92it/s][A
 13%|█▎        | 29/221 [00:08<00:57,  3.33it/s][A
 14%|█▎        | 30/221 [00:08<00:50,  3.75it/s][A
 14%|█▍        | 31/221 [00:09<00:52,  3.64it/s][A
 14%|█▍        | 32/221 [00:09<00:44,  4.25it/s][A
 15%|█▍        | 33/221 [00:09<00:39,  4.79it/s][A
 15%|█▌        | 34/221 [00:09<00:34,  5.49it/s][A
 16%|█▌        | 35/221 [00:09<00:30,  6.02it/s][A
 16%|█▋        | 36/221 [00:09<00:30,  6.10it/s][A
 17%|█▋        | 37/221 [00:10<00:31,  5.82it/s][A
 17%|█▋        | 38/221 [00:10<00:31,  5.89it/s][A
 18%|█▊        | 39/221 [00:10<00:27,  6.72it/s][A
 18%|█▊        | 40/221 [00:10<00:25,  7.12it/s][A
 19%|█▉        | 42/221 [00:10<00:23,  7.70it/s][A
 19%|█▉        | 43/221 [00:10<00:24,  7.23it/s][A
 20%|█▉        | 44/221 [00:10<00:24,  7.37it/s][A
 20%|██        | 45/221 [00:11<00:36,  4.89it/s][A
 21%|██        | 46/221 [00:11<00:37,  4.66it/s][A
 21%|██▏       | 47/221 [00:16<04:42,  1.63s/it][A
 22%|██▏       | 48/221 [00:16<03:29,  1.21s/it][A
 22%|██▏       | 49/221 [00:17<02:33,  1.12it/s][A
 23%|██▎       | 50/221 [00:17<01:52,  1.51it/s][A
 24%|██▎       | 52/221 [00:17<01:07,  2.49it/s][A
 24%|██▍       | 53/221 [00:17<00:55,  3.00it/s][A[h264 @ 0x5565f9a83140] mmco: unref short failure

 24%|██▍       | 54/221 [00:21<03:57,  1.42s/it][A
 25%|██▍       | 55/221 [00:22<03:20,  1.21s/it][A
 25%|██▌       | 56/221 [00:22<02:28,  1.11it/s][A
 26%|██▌       | 58/221 [00:22<01:28,  1.84it/s][A
 27%|██▋       | 60/221 [00:23<01:03,  2.54it/s][A
 28%|██▊       | 61/221 [00:23<00:54,  2.94it/s][A
 29%|██▊       | 63/221 [00:23<00:42,  3.70it/s][A
 29%|██▉       | 64/221 [00:23<00:40,  3.86it/s][A
 29%|██▉       | 65/221 [00:24<00:35,  4.42it/s][A
 30%|██▉       | 66/221 [00:28<03:14,  1.26s/it][A
 30%|███       | 67/221 [00:28<02:35,  1.01s/it][A
 31%|███       | 68/221 [00:28<01:59,  1.28it/s][A
 31%|███       | 69/221 [00:29<01:43,  1.47it/s][A
 32%|███▏      | 70/221 [00:29<01:21,  1.86it/s][A
 32%|███▏      | 71/221 [00:29<01:05,  2.31it/s][A
 33%|███▎      | 72/221 [00:29<00:55,  2.68it/s][A
 33%|███▎      | 73/221 [00:30<00:48,  3.06it/s][A
 34%|███▍      | 75/221 [00:30<00:34,  4.19it/s][A
 34%|███▍      | 76/221 [00:30<00:31,  4.67it/s][A
 35%|███▍      | 77/221 [00:30<00:26,  5.39it/s][A
 36%|███▌      | 79/221 [00:31<00:26,  5.27it/s][A
 36%|███▌      | 80/221 [00:31<00:27,  5.08it/s][A
 37%|███▋      | 81/221 [00:31<00:26,  5.33it/s][A
 37%|███▋      | 82/221 [00:31<00:27,  5.04it/s][A[h264 @ 0x5565fe231e80] mmco: unref short failure

 38%|███▊      | 83/221 [00:31<00:32,  4.30it/s][A
 38%|███▊      | 84/221 [00:32<00:27,  4.96it/s][A
 39%|███▉      | 86/221 [00:32<00:20,  6.52it/s][A
 39%|███▉      | 87/221 [00:32<00:19,  6.91it/s][A
 40%|███▉      | 88/221 [00:32<00:24,  5.47it/s][A
 40%|████      | 89/221 [00:32<00:26,  4.96it/s][A
 41%|████      | 91/221 [00:33<00:20,  6.43it/s][A
 42%|████▏     | 92/221 [00:33<00:19,  6.63it/s][A
 42%|████▏     | 93/221 [00:33<00:27,  4.71it/s][A
 43%|████▎     | 94/221 [00:33<00:25,  5.06it/s][A
 43%|████▎     | 95/221 [00:33<00:24,  5.13it/s][A
 43%|████▎     | 96/221 [00:34<00:29,  4.23it/s][A
 44%|████▍     | 97/221 [00:34<00:25,  4.87it/s][A
 44%|████▍     | 98/221 [00:34<00:22,  5.58it/s][A
 45%|████▍     | 99/221 [00:34<00:20,  6.03it/s][A
 45%|████▌     | 100/221 [00:34<00:17,  6.77it/s][A
 46%|████▌     | 101/221 [00:34<00:16,  7.25it/s][A
 46%|████▌     | 102/221 [00:35<00:15,  7.47it/s][A
 47%|████▋     | 103/221 [00:35<00:15,  7.75it/s][A
 48%|████▊     | 105/221 [00:35<00:13,  8.40it/s][A
 48%|████▊     | 106/221 [00:35<00:24,  4.72it/s][A
 48%|████▊     | 107/221 [00:35<00:21,  5.37it/s][A
 49%|████▉     | 109/221 [00:36<00:16,  6.77it/s][A
 50%|█████     | 111/221 [00:36<00:17,  6.17it/s][A
 51%|█████     | 112/221 [00:36<00:17,  6.35it/s][A
 51%|█████     | 113/221 [00:36<00:15,  6.94it/s][A
 52%|█████▏    | 115/221 [00:36<00:12,  8.39it/s][A
 52%|█████▏    | 116/221 [00:42<02:12,  1.26s/it][A
 53%|█████▎    | 117/221 [00:42<01:47,  1.03s/it][A
 53%|█████▎    | 118/221 [00:42<01:23,  1.23it/s][A
 54%|█████▍    | 119/221 [00:42<01:03,  1.61it/s][A
 54%|█████▍    | 120/221 [00:42<00:48,  2.09it/s][A
 55%|█████▍    | 121/221 [00:43<00:38,  2.60it/s][A
 55%|█████▌    | 122/221 [00:43<00:35,  2.79it/s][A
 56%|█████▌    | 123/221 [00:43<00:28,  3.42it/s][A
 56%|█████▌    | 124/221 [00:43<00:22,  4.22it/s][A
 57%|█████▋    | 125/221 [00:44<00:28,  3.41it/s][A
 57%|█████▋    | 126/221 [00:44<00:28,  3.29it/s][A[h264 @ 0x55cb6aca58c0] mmco: unref short failure
[h264 @ 0x55cb6aca58c0] mmco: unref short failure

 57%|█████▋    | 127/221 [00:44<00:36,  2.57it/s][A
 58%|█████▊    | 128/221 [00:45<00:54,  1.71it/s][A
 58%|█████▊    | 129/221 [00:46<00:40,  2.25it/s][A
 59%|█████▉    | 131/221 [00:46<00:24,  3.66it/s][A
 60%|█████▉    | 132/221 [00:46<00:24,  3.58it/s][A
 60%|██████    | 133/221 [00:46<00:23,  3.71it/s][A
 61%|██████    | 134/221 [00:47<00:24,  3.57it/s][A
 61%|██████    | 135/221 [00:47<00:39,  2.16it/s][A
 62%|██████▏   | 136/221 [00:49<00:58,  1.45it/s][A[h264 @ 0x5598eca5ca40] mmco: unref short failure
[h264 @ 0x5598eca5ca40] mmco: unref short failure

 62%|██████▏   | 137/221 [00:54<02:40,  1.91s/it][A[h264 @ 0x55cb719b86c0] mmco: unref short failure
[h264 @ 0x55cb719b86c0] mmco: unref short failure

 62%|██████▏   | 138/221 [00:54<02:06,  1.53s/it][A
 63%|██████▎   | 139/221 [00:55<01:43,  1.26s/it][A
 63%|██████▎   | 140/221 [00:55<01:19,  1.02it/s][A
 64%|██████▍   | 141/221 [00:56<01:08,  1.17it/s][A
 64%|██████▍   | 142/221 [00:56<00:53,  1.47it/s][A
 65%|██████▍   | 143/221 [00:56<00:41,  1.87it/s][A
 65%|██████▌   | 144/221 [00:56<00:31,  2.41it/s][A
 66%|██████▌   | 145/221 [00:56<00:24,  3.08it/s][A
 66%|██████▌   | 146/221 [00:57<00:19,  3.82it/s][A
 67%|██████▋   | 147/221 [00:57<00:16,  4.42it/s][A
 67%|██████▋   | 148/221 [00:57<00:17,  4.11it/s][A
 67%|██████▋   | 149/221 [00:57<00:15,  4.74it/s][A
 68%|██████▊   | 150/221 [00:57<00:15,  4.52it/s][A
 68%|██████▊   | 151/221 [00:58<00:19,  3.51it/s][A
 69%|██████▉   | 152/221 [00:58<00:16,  4.07it/s][A
 69%|██████▉   | 153/221 [00:58<00:16,  4.22it/s][A
 70%|██████▉   | 154/221 [00:58<00:14,  4.69it/s][A[h264 @ 0x55d9072d5080] mmco: unref short failure
[h264 @ 0x55d9072d5080] mmco: unref short failure
[h264 @ 0x55d9072d5080] mmco: unref short failure
[h264 @ 0x55d9072d5080] mmco: unref short failure

 71%|███████   | 156/221 [01:04<01:27,  1.35s/it][A
 71%|███████   | 157/221 [01:04<01:09,  1.09s/it][A
 71%|███████▏  | 158/221 [01:04<00:52,  1.19it/s][A
 72%|███████▏  | 160/221 [01:04<00:32,  1.87it/s][A
 73%|███████▎  | 161/221 [01:04<00:26,  2.30it/s][A
 73%|███████▎  | 162/221 [01:05<00:23,  2.52it/s][A
 74%|███████▍  | 163/221 [01:05<00:19,  3.00it/s][A[h264 @ 0x5598e6aed640] mmco: unref short failure

 74%|███████▍  | 164/221 [01:05<00:16,  3.36it/s][A[h264 @ 0x5565f2e56900] mmco: unref short failure
[h264 @ 0x5565f2e56900] mmco: unref short failure

 75%|███████▍  | 165/221 [01:10<01:35,  1.71s/it][A
 76%|███████▌  | 167/221 [01:15<01:40,  1.86s/it][A
 76%|███████▌  | 168/221 [01:15<01:16,  1.44s/it][A
 76%|███████▋  | 169/221 [01:15<01:01,  1.18s/it][A
 77%|███████▋  | 170/221 [01:15<00:48,  1.04it/s][A
 77%|███████▋  | 171/221 [01:16<00:36,  1.39it/s][A
 78%|███████▊  | 172/221 [01:16<00:26,  1.83it/s][A
 79%|███████▊  | 174/221 [01:16<00:16,  2.84it/s][A
 79%|███████▉  | 175/221 [01:16<00:14,  3.25it/s][A
 80%|███████▉  | 176/221 [01:16<00:14,  3.21it/s][A
 80%|████████  | 177/221 [01:17<00:13,  3.36it/s][A[h264 @ 0x5565e99c6880] mmco: unref short failure

 81%|████████  | 178/221 [01:21<01:04,  1.50s/it][A
 81%|████████  | 179/221 [01:21<00:46,  1.10s/it][A
 81%|████████▏ | 180/221 [01:21<00:33,  1.23it/s][A
 82%|████████▏ | 181/221 [01:22<00:24,  1.65it/s][A
 82%|████████▏ | 182/221 [01:22<00:18,  2.11it/s][A
 83%|████████▎ | 183/221 [01:22<00:14,  2.65it/s][A
 84%|████████▎ | 185/221 [01:22<00:08,  4.04it/s][A
 85%|████████▍ | 187/221 [01:22<00:06,  5.29it/s][A
 85%|████████▌ | 188/221 [01:22<00:05,  5.56it/s][A
 86%|████████▌ | 189/221 [01:23<00:05,  5.56it/s][A
 86%|████████▋ | 191/221 [01:23<00:04,  7.33it/s][A
 87%|████████▋ | 192/221 [01:23<00:03,  7.76it/s][A
 88%|████████▊ | 194/221 [01:23<00:03,  8.60it/s][A
 88%|████████▊ | 195/221 [01:23<00:03,  7.80it/s][A
 89%|████████▉ | 197/221 [01:23<00:02,  9.18it/s][A
 90%|████████▉ | 198/221 [01:23<00:02,  9.11it/s][A
 90%|█████████ | 199/221 [01:24<00:02,  8.71it/s][A
 91%|█████████ | 201/221 [01:24<00:02,  8.65it/s][A
 91%|█████████▏| 202/221 [01:24<00:02,  7.65it/s][A
 92%|█████████▏| 204/221 [01:24<00:02,  8.27it/s][A
 93%|█████████▎| 205/221 [01:24<00:02,  6.56it/s][A
 94%|█████████▎| 207/221 [01:25<00:01,  7.92it/s][A
 95%|█████████▍| 209/221 [01:25<00:01,  9.27it/s][A
 95%|█████████▌| 210/221 [01:25<00:01,  6.89it/s][A
 95%|█████████▌| 211/221 [01:25<00:01,  7.08it/s][A
 96%|█████████▌| 212/221 [01:25<00:01,  6.16it/s][A09/18/2024 15:20:59 - INFO - __main__ -   current idx 3_mqkjCXK8E.83 from finetune_area returns wrong image/video, use 111731 instead.

 96%|█████████▋| 213/221 [01:26<00:01,  4.84it/s][A
 97%|█████████▋| 215/221 [01:26<00:00,  6.44it/s][A
 98%|█████████▊| 216/221 [01:31<00:06,  1.32s/it][A
 99%|█████████▊| 218/221 [01:31<00:02,  1.19it/s][A09/18/2024 15:21:07 - INFO - __main__ -   current idx 6mjP2GVtPHA.82 from finetune_area returns wrong image/video, use 90531 instead.

 99%|█████████▉| 219/221 [01:37<00:03,  1.81s/it][A100%|█████████▉| 220/221 [01:37<00:00,  2.26it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:10,  3.12it/s][A
  1%|          | 2/221 [00:00<01:08,  3.21it/s][A
  1%|▏         | 3/221 [00:00<01:10,  3.08it/s][A
  2%|▏         | 4/221 [00:01<01:10,  3.09it/s][A
  2%|▏         | 5/221 [00:01<01:08,  3.15it/s][A
  3%|▎         | 6/221 [00:01<01:08,  3.15it/s][A
  3%|▎         | 7/221 [00:02<01:06,  3.20it/s][A
  4%|▎         | 8/221 [00:02<01:07,  3.17it/s][A
  4%|▍         | 9/221 [00:02<01:06,  3.21it/s][A
  5%|▍         | 10/221 [00:03<01:05,  3.23it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.19it/s][A
  5%|▌         | 12/221 [00:03<01:05,  3.21it/s][A
  6%|▌         | 13/221 [00:04<01:05,  3.19it/s][A
  6%|▋         | 14/221 [00:04<01:05,  3.15it/s][A
  7%|▋         | 15/221 [00:04<01:04,  3.18it/s][A
  7%|▋         | 16/221 [00:05<01:03,  3.21it/s][A
  8%|▊         | 17/221 [00:05<01:03,  3.23it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.23it/s][A
  9%|▊         | 19/221 [00:05<01:02,  3.24it/s][A09/18/2024 15:21:19 - INFO - __main__ -   current idx DnpQpBpq4-8.30 from finetune_area returns wrong image/video, use 60115 instead.

  9%|▉         | 20/221 [00:06<01:03,  3.18it/s][A
 10%|▉         | 21/221 [00:06<01:02,  3.21it/s][A
 10%|▉         | 22/221 [00:06<01:03,  3.15it/s][A
 10%|█         | 23/221 [00:07<01:02,  3.17it/s][A
 11%|█         | 24/221 [00:07<01:01,  3.18it/s][A
 11%|█▏        | 25/221 [00:07<01:00,  3.21it/s][A
 12%|█▏        | 26/221 [00:08<01:00,  3.22it/s][A
 12%|█▏        | 27/221 [00:08<00:59,  3.24it/s][A
 13%|█▎        | 28/221 [00:08<01:00,  3.21it/s][A
 13%|█▎        | 29/221 [00:09<00:59,  3.23it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.25it/s][A
 14%|█▍        | 31/221 [00:09<00:58,  3.26it/s][A
 14%|█▍        | 32/221 [00:09<00:58,  3.25it/s][A
 15%|█▍        | 33/221 [00:10<00:58,  3.24it/s][A
 15%|█▌        | 34/221 [00:10<00:57,  3.25it/s][A
 16%|█▌        | 35/221 [00:10<00:57,  3.25it/s][A
 16%|█▋        | 36/221 [00:11<00:56,  3.26it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.27it/s][A
 17%|█▋        | 38/221 [00:11<00:56,  3.25it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.21it/s][A[h264 @ 0x55cb74364480] mmco: unref short failure
[h264 @ 0x55cb74364480] mmco: unref short failure

 18%|█▊        | 40/221 [00:12<00:56,  3.19it/s][A
 19%|█▊        | 41/221 [00:12<00:55,  3.21it/s][A
 19%|█▉        | 42/221 [00:13<00:55,  3.21it/s][A
 19%|█▉        | 43/221 [00:13<00:55,  3.24it/s][A
 20%|█▉        | 44/221 [00:13<00:55,  3.21it/s][A
 20%|██        | 45/221 [00:14<00:54,  3.24it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.22it/s][A
 21%|██▏       | 47/221 [00:14<00:56,  3.10it/s][A
 22%|██▏       | 48/221 [00:15<00:55,  3.12it/s][A
 22%|██▏       | 49/221 [00:15<00:55,  3.10it/s][A
 23%|██▎       | 50/221 [00:15<00:54,  3.16it/s][A
 23%|██▎       | 51/221 [00:15<00:53,  3.17it/s][A
 24%|██▎       | 52/221 [00:16<00:53,  3.18it/s][A
 24%|██▍       | 53/221 [00:16<00:52,  3.21it/s][A
 24%|██▍       | 54/221 [00:16<00:52,  3.19it/s][A
 25%|██▍       | 55/221 [00:17<00:52,  3.18it/s][A
 25%|██▌       | 56/221 [00:17<00:51,  3.20it/s][A[h264 @ 0x55cb63462d00] mmco: unref short failure
[h264 @ 0x55cb63462d00] mmco: unref short failure

 26%|██▌       | 57/221 [00:17<00:50,  3.23it/s][A09/18/2024 15:21:31 - INFO - __main__ -   current idx Lzt-UMekcLY.51 from finetune_area returns wrong image/video, use 144801 instead.

 26%|██▌       | 58/221 [00:18<00:50,  3.25it/s][A
 27%|██▋       | 59/221 [00:18<00:50,  3.24it/s][A
 27%|██▋       | 60/221 [00:18<00:49,  3.25it/s][A
 28%|██▊       | 61/221 [00:19<00:48,  3.27it/s][A
 28%|██▊       | 62/221 [00:19<00:49,  3.23it/s][A
 29%|██▊       | 63/221 [00:19<00:49,  3.22it/s][A
 29%|██▉       | 64/221 [00:19<00:48,  3.23it/s][A
 29%|██▉       | 65/221 [00:20<00:48,  3.25it/s][A
 30%|██▉       | 66/221 [00:20<00:47,  3.26it/s][A
 30%|███       | 67/221 [00:20<00:47,  3.27it/s][A
 31%|███       | 68/221 [00:21<00:46,  3.27it/s][A
 31%|███       | 69/221 [00:21<00:47,  3.17it/s][A
 32%|███▏      | 70/221 [00:21<00:47,  3.18it/s][A
 32%|███▏      | 71/221 [00:22<00:46,  3.21it/s][A
 33%|███▎      | 72/221 [00:22<00:47,  3.17it/s][A
 33%|███▎      | 73/221 [00:22<00:46,  3.18it/s][A
 33%|███▎      | 74/221 [00:23<00:46,  3.17it/s][A
 34%|███▍      | 75/221 [00:23<00:45,  3.21it/s][A
 34%|███▍      | 76/221 [00:23<00:45,  3.21it/s][A
 35%|███▍      | 77/221 [00:24<00:44,  3.21it/s][A
 35%|███▌      | 78/221 [00:24<00:44,  3.23it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.25it/s][A
 36%|███▌      | 80/221 [00:24<00:43,  3.26it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.26it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.27it/s][A
 38%|███▊      | 83/221 [00:25<00:42,  3.28it/s][A
 38%|███▊      | 84/221 [00:26<00:43,  3.17it/s][A
 38%|███▊      | 85/221 [00:26<00:42,  3.21it/s][A
 39%|███▉      | 86/221 [00:26<00:41,  3.24it/s][A
 39%|███▉      | 87/221 [00:27<00:41,  3.25it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.27it/s][A
 40%|████      | 89/221 [00:27<00:40,  3.28it/s][A
 41%|████      | 90/221 [00:28<00:39,  3.28it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.29it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 94/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:30<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:34<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:36<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:37<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:40<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:42<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:44<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:46<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:47<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:48<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:50<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:51<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:52<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:53<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:56<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:57<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:58<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [01:00<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:01<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:02<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:06<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.27it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  1%|          | 2/221 [00:00<00:42,  5.14it/s][A
  2%|▏         | 4/221 [00:01<01:01,  3.53it/s][A
  2%|▏         | 5/221 [00:01<01:14,  2.91it/s][A
  3%|▎         | 7/221 [00:01<01:00,  3.57it/s][A
  4%|▎         | 8/221 [00:02<01:13,  2.90it/s][A
  4%|▍         | 9/221 [00:03<01:23,  2.54it/s][A
  5%|▍         | 10/221 [00:03<01:25,  2.46it/s][A
  5%|▍         | 11/221 [00:04<01:33,  2.25it/s][A
  5%|▌         | 12/221 [00:04<01:19,  2.64it/s][A
  6%|▌         | 13/221 [00:05<01:42,  2.03it/s][A
  6%|▋         | 14/221 [00:05<01:42,  2.03it/s][A
  7%|▋         | 15/221 [00:05<01:23,  2.46it/s][A
  7%|▋         | 16/221 [00:05<01:07,  3.03it/s][A
  8%|▊         | 17/221 [00:06<01:53,  1.80it/s][A
  8%|▊         | 18/221 [00:07<01:35,  2.14it/s][A
  9%|▊         | 19/221 [00:07<01:20,  2.50it/s][A
 10%|▉         | 21/221 [00:07<00:52,  3.81it/s][A
 10%|█         | 23/221 [00:07<00:43,  4.52it/s][A
 11%|█         | 24/221 [00:08<00:45,  4.36it/s][A
 11%|█▏        | 25/221 [00:08<00:45,  4.33it/s][A
 12%|█▏        | 26/221 [00:08<00:52,  3.69it/s][A
 12%|█▏        | 27/221 [00:09<00:59,  3.24it/s][A
 13%|█▎        | 28/221 [00:09<01:15,  2.55it/s][A
 13%|█▎        | 29/221 [00:10<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:10<01:19,  2.39it/s][A
 14%|█▍        | 31/221 [00:11<01:04,  2.96it/s][A
 14%|█▍        | 32/221 [00:11<00:51,  3.70it/s][A
 15%|█▍        | 33/221 [00:11<00:57,  3.29it/s][A
 15%|█▌        | 34/221 [00:12<01:09,  2.69it/s][A
 16%|█▌        | 35/221 [00:12<01:06,  2.81it/s][A
 16%|█▋        | 36/221 [00:12<01:14,  2.49it/s][A
 17%|█▋        | 37/221 [00:13<01:14,  2.48it/s][A
 17%|█▋        | 38/221 [00:13<01:09,  2.63it/s][A
 18%|█▊        | 39/221 [00:14<01:25,  2.14it/s][A
 18%|█▊        | 40/221 [00:14<01:13,  2.45it/s][A
 19%|█▉        | 42/221 [00:14<00:53,  3.38it/s][A
 19%|█▉        | 43/221 [00:15<00:51,  3.48it/s][A
 20%|█▉        | 44/221 [00:15<00:44,  3.98it/s][A
 20%|██        | 45/221 [00:15<00:55,  3.15it/s][A
 21%|██        | 46/221 [00:15<00:46,  3.76it/s][A
 21%|██▏       | 47/221 [00:16<00:46,  3.78it/s][A
 22%|██▏       | 48/221 [00:16<00:42,  4.04it/s][A
 22%|██▏       | 49/221 [00:16<00:46,  3.69it/s][A
 23%|██▎       | 50/221 [00:18<01:37,  1.76it/s][A
 23%|██▎       | 51/221 [00:18<01:17,  2.20it/s][A
 24%|██▎       | 52/221 [00:18<00:59,  2.84it/s][A
 24%|██▍       | 54/221 [00:18<00:55,  3.00it/s][A
 25%|██▌       | 56/221 [00:19<00:38,  4.29it/s][A
 26%|██▌       | 57/221 [00:19<00:36,  4.49it/s][A
 26%|██▌       | 58/221 [00:19<00:51,  3.14it/s][A
 27%|██▋       | 59/221 [00:20<00:53,  3.01it/s][A
 27%|██▋       | 60/221 [00:20<00:44,  3.63it/s][A
 28%|██▊       | 61/221 [00:20<00:39,  4.06it/s][A
 28%|██▊       | 62/221 [00:20<00:34,  4.67it/s][A
 29%|██▊       | 63/221 [00:20<00:37,  4.23it/s][A
 29%|██▉       | 64/221 [00:21<00:32,  4.78it/s][A
 29%|██▉       | 65/221 [00:21<00:39,  3.98it/s][A
 30%|██▉       | 66/221 [00:21<00:33,  4.68it/s][A
 30%|███       | 67/221 [00:22<00:44,  3.43it/s][A
 31%|███       | 68/221 [00:22<00:35,  4.26it/s][A
 31%|███       | 69/221 [00:22<00:35,  4.23it/s][A
 32%|███▏      | 70/221 [00:22<00:36,  4.18it/s][A
 32%|███▏      | 71/221 [00:23<00:42,  3.49it/s][A
 33%|███▎      | 72/221 [00:23<00:47,  3.13it/s][A
 33%|███▎      | 73/221 [00:23<00:43,  3.44it/s][A
 33%|███▎      | 74/221 [00:23<00:40,  3.62it/s][A
 34%|███▍      | 75/221 [00:24<00:44,  3.28it/s][A
 34%|███▍      | 76/221 [00:24<00:35,  4.06it/s][A
 35%|███▍      | 77/221 [00:24<00:37,  3.87it/s][A
 35%|███▌      | 78/221 [00:25<00:39,  3.58it/s][A
 36%|███▌      | 79/221 [00:25<00:42,  3.32it/s][A
 36%|███▌      | 80/221 [00:25<00:37,  3.78it/s][A
 37%|███▋      | 81/221 [00:25<00:34,  4.05it/s][A
 37%|███▋      | 82/221 [00:26<00:59,  2.34it/s][A
 38%|███▊      | 83/221 [00:27<01:05,  2.11it/s][A
 38%|███▊      | 84/221 [00:27<01:00,  2.27it/s][A
 38%|███▊      | 85/221 [00:27<00:53,  2.53it/s][A
 39%|███▉      | 86/221 [00:27<00:42,  3.19it/s][A
 39%|███▉      | 87/221 [00:28<00:45,  2.96it/s][A
 40%|███▉      | 88/221 [00:28<00:36,  3.60it/s][A
 40%|████      | 89/221 [00:28<00:44,  2.97it/s][A
 41%|████      | 90/221 [00:29<00:35,  3.67it/s][A
 41%|████      | 91/221 [00:29<00:29,  4.47it/s][A
 42%|████▏     | 92/221 [00:29<00:30,  4.20it/s][A
 42%|████▏     | 93/221 [00:29<00:35,  3.62it/s][A
 43%|████▎     | 94/221 [00:29<00:28,  4.48it/s][A
 43%|████▎     | 95/221 [00:30<00:41,  3.02it/s][A
 43%|████▎     | 96/221 [00:30<00:35,  3.51it/s][A
 44%|████▍     | 97/221 [00:30<00:29,  4.23it/s][A
 44%|████▍     | 98/221 [00:31<00:30,  4.05it/s][A
 45%|████▍     | 99/221 [00:31<00:28,  4.29it/s][A
 45%|████▌     | 100/221 [00:31<00:28,  4.25it/s][A
 46%|████▌     | 101/221 [00:31<00:27,  4.44it/s][A
 46%|████▌     | 102/221 [00:32<00:34,  3.46it/s][A
 47%|████▋     | 103/221 [00:32<00:28,  4.10it/s][A
 47%|████▋     | 104/221 [00:32<00:27,  4.20it/s][A
 48%|████▊     | 105/221 [00:32<00:24,  4.74it/s][A
 48%|████▊     | 106/221 [00:32<00:24,  4.64it/s][A
 49%|████▉     | 108/221 [00:33<00:18,  6.20it/s][A
 49%|████▉     | 109/221 [00:33<00:22,  4.96it/s][A
 50%|████▉     | 110/221 [00:34<00:33,  3.27it/s][A
 50%|█████     | 111/221 [00:34<00:43,  2.51it/s][A
 51%|█████     | 112/221 [00:35<00:41,  2.65it/s][A
 51%|█████     | 113/221 [00:35<00:34,  3.14it/s][A
 52%|█████▏    | 114/221 [00:35<00:30,  3.56it/s][A
 52%|█████▏    | 116/221 [00:35<00:24,  4.32it/s][A
 53%|█████▎    | 117/221 [00:36<00:25,  4.04it/s][A
 53%|█████▎    | 118/221 [00:36<00:33,  3.09it/s][A
 54%|█████▍    | 119/221 [00:36<00:31,  3.24it/s][A
 54%|█████▍    | 120/221 [00:37<00:28,  3.54it/s][A
 55%|█████▍    | 121/221 [00:37<00:24,  4.12it/s][A
 55%|█████▌    | 122/221 [00:37<00:24,  4.09it/s][A
 56%|█████▌    | 124/221 [00:37<00:23,  4.18it/s][A
 57%|█████▋    | 125/221 [00:38<00:22,  4.25it/s][A
 57%|█████▋    | 126/221 [00:38<00:21,  4.45it/s][A
 57%|█████▋    | 127/221 [00:38<00:21,  4.37it/s][A
 58%|█████▊    | 128/221 [00:38<00:19,  4.71it/s][A
 58%|█████▊    | 129/221 [00:38<00:17,  5.35it/s][A
 59%|█████▉    | 130/221 [00:39<00:18,  4.83it/s][A
 60%|█████▉    | 132/221 [00:39<00:13,  6.80it/s][A
 60%|██████    | 133/221 [00:39<00:16,  5.45it/s][A
 61%|██████    | 134/221 [00:39<00:14,  5.95it/s][A
 61%|██████    | 135/221 [00:39<00:16,  5.14it/s][A
 62%|██████▏   | 136/221 [00:40<00:21,  3.93it/s][A
 62%|██████▏   | 137/221 [00:40<00:19,  4.30it/s][A
 62%|██████▏   | 138/221 [00:40<00:16,  4.98it/s][A
 63%|██████▎   | 139/221 [00:41<00:23,  3.46it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.68it/s][A
 64%|██████▍   | 141/221 [00:41<00:20,  3.99it/s][A
 64%|██████▍   | 142/221 [00:41<00:21,  3.66it/s][A
 65%|██████▍   | 143/221 [00:42<00:19,  4.09it/s][A
 66%|██████▌   | 145/221 [00:42<00:20,  3.64it/s][A
 66%|██████▌   | 146/221 [00:43<00:22,  3.37it/s][A
 67%|██████▋   | 147/221 [00:43<00:18,  3.94it/s][A
 67%|██████▋   | 148/221 [00:43<00:16,  4.47it/s][A
 68%|██████▊   | 150/221 [00:43<00:12,  5.73it/s][A
 68%|██████▊   | 151/221 [00:43<00:15,  4.55it/s][A
 69%|██████▉   | 152/221 [00:45<00:31,  2.19it/s][A
 69%|██████▉   | 153/221 [00:45<00:29,  2.33it/s][A
 70%|██████▉   | 154/221 [00:45<00:24,  2.78it/s][A
 70%|███████   | 155/221 [00:45<00:22,  2.99it/s][A
 71%|███████   | 156/221 [00:46<00:22,  2.87it/s][A
 71%|███████   | 157/221 [00:46<00:18,  3.42it/s][A
 72%|███████▏  | 159/221 [00:46<00:13,  4.59it/s][A
 72%|███████▏  | 160/221 [00:46<00:12,  4.73it/s][A
 73%|███████▎  | 161/221 [00:46<00:10,  5.47it/s][A
 73%|███████▎  | 162/221 [00:47<00:13,  4.37it/s][A
 74%|███████▍  | 163/221 [00:47<00:13,  4.36it/s][A
 74%|███████▍  | 164/221 [00:48<00:21,  2.67it/s][A
 75%|███████▍  | 165/221 [00:48<00:16,  3.36it/s][A
 75%|███████▌  | 166/221 [00:49<00:21,  2.55it/s][A
 76%|███████▌  | 168/221 [00:49<00:13,  3.79it/s][A
 76%|███████▋  | 169/221 [00:49<00:14,  3.56it/s][A
 77%|███████▋  | 170/221 [00:50<00:20,  2.51it/s][A
 77%|███████▋  | 171/221 [00:50<00:16,  3.10it/s][A
 78%|███████▊  | 172/221 [00:50<00:13,  3.63it/s][A
 78%|███████▊  | 173/221 [00:50<00:11,  4.15it/s][A
 79%|███████▊  | 174/221 [00:51<00:21,  2.22it/s][A
 79%|███████▉  | 175/221 [00:52<00:21,  2.11it/s][A
 80%|███████▉  | 176/221 [00:52<00:17,  2.64it/s][A
 80%|████████  | 177/221 [00:52<00:14,  2.95it/s][A
 81%|████████  | 178/221 [00:52<00:13,  3.26it/s][A
 81%|████████  | 179/221 [00:53<00:11,  3.70it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.79it/s][A
 82%|████████▏ | 181/221 [00:53<00:11,  3.42it/s][A
 83%|████████▎ | 183/221 [00:53<00:07,  4.91it/s][A
 83%|████████▎ | 184/221 [00:53<00:06,  5.31it/s][A
 84%|████████▎ | 185/221 [00:54<00:08,  4.04it/s][A
 84%|████████▍ | 186/221 [00:54<00:08,  3.95it/s][A
 85%|████████▍ | 187/221 [00:54<00:07,  4.64it/s][A
 85%|████████▌ | 188/221 [00:55<00:10,  3.19it/s][A
 86%|████████▌ | 189/221 [00:56<00:14,  2.23it/s][A
 86%|████████▌ | 190/221 [00:56<00:14,  2.11it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.28it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.68it/s][A
 88%|████████▊ | 194/221 [00:57<00:08,  3.24it/s][A
 88%|████████▊ | 195/221 [00:57<00:08,  3.22it/s][A
 89%|████████▊ | 196/221 [00:58<00:11,  2.23it/s][A
 89%|████████▉ | 197/221 [00:58<00:09,  2.49it/s][A
 90%|████████▉ | 198/221 [00:59<00:08,  2.78it/s][A
 90%|█████████ | 199/221 [00:59<00:06,  3.44it/s][A
 90%|█████████ | 200/221 [00:59<00:05,  3.62it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.57it/s][A
 92%|█████████▏| 203/221 [00:59<00:03,  5.14it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.96it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.65it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.67it/s][A
 94%|█████████▎| 207/221 [01:01<00:04,  3.46it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.45it/s][A
 95%|█████████▍| 209/221 [01:01<00:04,  2.94it/s][A
 95%|█████████▌| 210/221 [01:02<00:03,  3.56it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  4.21it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  4.45it/s][A
 96%|█████████▋| 213/221 [01:03<00:03,  2.15it/s][A
 97%|█████████▋| 214/221 [01:03<00:03,  2.11it/s][A
 97%|█████████▋| 215/221 [01:04<00:02,  2.24it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  2.69it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.33it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  4.10it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  2.89it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  2.55it/s][A
100%|██████████| 221/221 [01:06<00:00,  1.82it/s][A100%|██████████| 221/221 [01:06<00:00,  3.31it/s]
09/18/2024 15:23:33 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 49--===========

09/18/2024 15:23:33 - INFO - __main__ -   {'area_r1': 33.6, 'area_recall': '33.6/54.0/60.3', 'area_ravg': 49.3}
09/18/2024 15:23:33 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 49--===========

09/18/2024 15:23:33 - INFO - __main__ -   {'forward_r1': 36.1, 'forward_recall': '36.1/66.9/77.4', 'forward_ravg': 60.1}
09/18/2024 15:23:33 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 49--===========

09/18/2024 15:23:33 - INFO - __main__ -   {'area_video_r1': 36.5, 'area_video_recall': '36.5/67.1/77.8', 'area_video_ravg': 60.5}
09/18/2024 15:23:33 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 49=======

09/18/2024 15:23:33 - INFO - __main__ -   {'area_video_r1': 36.5, 'area_video_recall': '36.5/67.1/77.8', 'area_video_ravg': 60.5}
09/18/2024 15:23:33 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 49--===========

09/18/2024 15:23:33 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 15:23:33 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 49=======

09/18/2024 15:23:33 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 15:23:33 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 49--===========

09/18/2024 15:23:33 - INFO - __main__ -   {'video_r1': 31.1, 'video_recall': '31.1/55.1/65.8', 'video_ravg': 50.7}
09/18/2024 15:23:33 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 49=======

09/18/2024 15:23:33 - INFO - __main__ -   {'video_r1': 31.1, 'video_recall': '31.1/55.1/65.8', 'video_ravg': 50.7}
09/18/2024 15:23:33 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 49--===========

09/18/2024 15:23:33 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 15:23:33 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 15:23:33 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 15:24:09 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0006672353483736515, 'loss_ret%tv%ta--finetune_area/loss_area': 4.35585880279541, 'loss_ret%tv%ta--finetune_area/total_loss': 4.356525897979736}
[h264 @ 0x55d903835d40] mmco: unref short failure
[h264 @ 0x55d903835d40] mmco: unref short failure
[h264 @ 0x55d903835d40] mmco: unref short failure
[h264 @ 0x55d903835d40] mmco: unref short failure
[h264 @ 0x55cb67b13e80] mmco: unref short failure
[h264 @ 0x55cb67b13e80] mmco: unref short failure
  2%|▏         | 50/2910 [19:02<112:46:34, 141.96s/it]  2%|▏         | 51/2910 [19:06<79:46:29, 100.45s/it]   2%|▏         | 52/2910 [19:10<56:45:48, 71.50s/it] [h264 @ 0x55cb6711f540] mmco: unref short failure
[h264 @ 0x55d90c9dc340] mmco: unref short failure
[h264 @ 0x55d90c9dc340] mmco: unref short failure
  2%|▏         | 53/2910 [19:13<40:37:03, 51.18s/it]  2%|▏         | 54/2910 [19:19<29:39:46, 37.39s/it]  2%|▏         | 55/2910 [19:23<21:53:47, 27.61s/it][h264 @ 0x55cb6e9fccc0] mmco: unref short failure
  2%|▏         | 56/2910 [19:29<16:38:30, 20.99s/it][h264 @ 0x5598f0f45300] mmco: unref short failure
  2%|▏         | 57/2910 [19:35<13:00:12, 16.41s/it][h264 @ 0x55d907680300] mmco: unref short failure
[h264 @ 0x55cb70c7e040] mmco: unref short failure
  2%|▏         | 58/2910 [19:40<10:15:08, 12.94s/it][h264 @ 0x55cb68e2f480] mmco: unref short failure
[h264 @ 0x55cb71587480] mmco: unref short failure
[h264 @ 0x55cb71587480] mmco: unref short failure
[h264 @ 0x5598f5d0ff00] mmco: unref short failure
[h264 @ 0x5598f5d0ff00] mmco: unref short failure
[h264 @ 0x5598de2b1f00] mmco: unref short failure
[h264 @ 0x5598de2b1f00] mmco: unref short failure
  2%|▏         | 59/2910 [19:46<8:36:08, 10.86s/it] [h264 @ 0x5565eadf1e00] mmco: unref short failure
[h264 @ 0x5565eadf1e00] mmco: unref short failure
  2%|▏         | 60/2910 [19:50<7:07:37,  9.00s/it][h264 @ 0x55cb5fa51740] mmco: unref short failure
[h264 @ 0x55cb5fa51740] mmco: unref short failure
  2%|▏         | 61/2910 [19:56<6:21:12,  8.03s/it][h264 @ 0x5598dd506ac0] mmco: unref short failure
[h264 @ 0x5598dd506ac0] mmco: unref short failure
  2%|▏         | 62/2910 [20:01<5:36:05,  7.08s/it]  2%|▏         | 63/2910 [20:06<5:11:20,  6.56s/it][h264 @ 0x5598ecfcff00] mmco: unref short failure
  2%|▏         | 64/2910 [20:11<4:47:38,  6.06s/it][h264 @ 0x55cb6bff3540] mmco: unref short failure
[h264 @ 0x5598efb23700] mmco: unref short failure
[h264 @ 0x5598efb23700] mmco: unref short failure
  2%|▏         | 65/2910 [20:16<4:32:45,  5.75s/it][h264 @ 0x55cb73d52380] mmco: unref short failure
[h264 @ 0x55cb73d52380] mmco: unref short failure
[h264 @ 0x55cb63462a80] mmco: unref short failure
[h264 @ 0x55cb63462a80] mmco: unref short failure
[h264 @ 0x5598f334f800] mmco: unref short failure
[h264 @ 0x55d90215bd00] mmco: unref short failure
[h264 @ 0x55d909300740] mmco: unref short failure
[h264 @ 0x5598dd336800] mmco: unref short failure
[h264 @ 0x5598dd336800] mmco: unref short failure
[h264 @ 0x5598f030cd00] mmco: unref short failure
[h264 @ 0x5565f40d7300] mmco: unref short failure
[h264 @ 0x5565f40d7300] mmco: unref short failure
[h264 @ 0x5565e65f0140] mmco: unref short failure
[h264 @ 0x5565e65f0140] mmco: unref short failure
09/18/2024 15:25:59 - INFO - __main__ -   current idx LGB6ZEjGm7Q.29 from finetune_area returns wrong image/video, use 134176 instead.
[h264 @ 0x55cb63f56dc0] mmco: unref short failure
[h264 @ 0x55cb63f56dc0] mmco: unref short failure
09/18/2024 15:26:08 - INFO - __main__ -   current idx AqyH6qo2WI0.9 from finetune_area returns wrong image/video, use 117540 instead.
[h264 @ 0x55cb701f1340] mmco: unref short failure
[h264 @ 0x55cb6f8587c0] mmco: unref short failure
[h264 @ 0x55d90ce1b000] mmco: unref short failure
[h264 @ 0x55d90ce1b000] mmco: unref short failure
[h264 @ 0x5565f273f540] mmco: unref short failure
[h264 @ 0x5565f273f540] mmco: unref short failure
[h264 @ 0x55cb6286b780] mmco: unref short failure
[h264 @ 0x55cb6286b780] mmco: unref short failure
[h264 @ 0x55cb6286b780] mmco: unref short failure
[h264 @ 0x5565fce39400] mmco: unref short failure
[h264 @ 0x5598ece8fc40] mmco: unref short failure
  2%|▏         | 66/2910 [21:25<19:26:01, 24.60s/it]  2%|▏         | 67/2910 [21:30<14:44:00, 18.66s/it][h264 @ 0x5565ee32d380] mmco: unref short failure
[h264 @ 0x5565ee32d380] mmco: unref short failure
[h264 @ 0x55d90ce2cd80] mmco: unref short failure
[h264 @ 0x55d90ce2cd80] mmco: unref short failure
  2%|▏         | 68/2910 [21:36<11:49:21, 14.98s/it]  2%|▏         | 69/2910 [21:46<10:35:51, 13.43s/it][h264 @ 0x55cb6ea08980] mmco: unref short failure
[h264 @ 0x55cb6ea08980] mmco: unref short failure
  2%|▏         | 70/2910 [21:53<9:03:18, 11.48s/it] [h264 @ 0x5565fb2b1840] mmco: unref short failure
[h264 @ 0x5565fb2b1840] mmco: unref short failure
[h264 @ 0x5598ef5c6040] mmco: unref short failure
[h264 @ 0x5598ef5c6040] mmco: unref short failure
[h264 @ 0x5565fd458c00] mmco: unref short failure
[h264 @ 0x5598e7642940] mmco: unref short failure
[h264 @ 0x5598e7642940] mmco: unref short failure
  2%|▏         | 71/2910 [22:12<10:52:40, 13.79s/it][h264 @ 0x5565e6ea9080] mmco: unref short failure
[h264 @ 0x5565e6ea9080] mmco: unref short failure
[h264 @ 0x5565f60eb680] mmco: unref short failure
[h264 @ 0x5565f60eb680] mmco: unref short failure
  2%|▏         | 72/2910 [22:17<8:51:17, 11.23s/it]   3%|▎         | 73/2910 [22:22<7:25:56,  9.43s/it][h264 @ 0x55d914e4a1c0] mmco: unref short failure
09/18/2024 15:27:45 - INFO - __main__ -   current idx UGjfq2kyBqs.34 from finetune_area returns wrong image/video, use 93098 instead.
[h264 @ 0x5565fd458c00] mmco: unref short failure
[h264 @ 0x55d908b76d80] mmco: unref short failure
[h264 @ 0x55d908b76d80] mmco: unref short failure
[h264 @ 0x55d90ea61580] mmco: unref short failure
09/18/2024 15:28:14 - INFO - __main__ -   current idx 1cJZfRW1WbY.33 from finetune_area returns wrong image/video, use 109243 instead.
[h264 @ 0x5598efd33440] mmco: unref short failure
[h264 @ 0x5598efd33440] mmco: unref short failure
09/18/2024 15:28:27 - INFO - __main__ -   current idx Yn3caOZk6dk.38 from finetune_area returns wrong image/video, use 36582 instead.
[h264 @ 0x5565fa7bce00] mmco: unref short failure
[h264 @ 0x5565fb6c9100] mmco: unref short failure
[h264 @ 0x5565fb6c9100] mmco: unref short failure
[h264 @ 0x5565fb6c9100] mmco: unref short failure
[h264 @ 0x5565fb6c9100] mmco: unref short failure
[h264 @ 0x55d9135d5f00] mmco: unref short failure
[h264 @ 0x55d9135d5f00] mmco: unref short failure
[h264 @ 0x55d9135d5f00] mmco: unref short failure
[h264 @ 0x55d9135d5f00] mmco: unref short failure
[h264 @ 0x55d9135d5f00] mmco: unref short failure
[h264 @ 0x55d9135d5f00] mmco: unref short failure
[h264 @ 0x5565f596eec0] mmco: unref short failure
  3%|▎         | 74/2910 [23:49<25:37:37, 32.53s/it]  3%|▎         | 75/2910 [23:54<19:04:44, 24.23s/it][h264 @ 0x5598dfe29200] mmco: unref short failure
[h264 @ 0x5598e5b8c7c0] mmco: unref short failure
[h264 @ 0x5598e5b8c7c0] mmco: unref short failure
  3%|▎         | 76/2910 [23:59<14:30:48, 18.44s/it][h264 @ 0x5598e703d500] mmco: unref short failure
[h264 @ 0x5598e703d500] mmco: unref short failure
[h264 @ 0x55d90cce4580] mmco: unref short failure
  3%|▎         | 77/2910 [24:08<12:26:39, 15.81s/it][h264 @ 0x5598e1718800] mmco: unref short failure
  3%|▎         | 78/2910 [24:14<10:03:39, 12.79s/it][h264 @ 0x5598e20ba200] mmco: unref short failure
[h264 @ 0x55d90cdfc900] mmco: unref short failure
[h264 @ 0x55d90cdfc900] mmco: unref short failure
  3%|▎         | 79/2910 [24:38<12:45:54, 16.23s/it][h264 @ 0x5565e8db2ec0] mmco: unref short failure
[h264 @ 0x5565e8db2ec0] mmco: unref short failure
  3%|▎         | 80/2910 [24:44<10:14:34, 13.03s/it]  3%|▎         | 81/2910 [24:49<8:22:11, 10.65s/it] [h264 @ 0x5598dc1aa800] mmco: unref short failure
[h264 @ 0x5598dc1aa800] mmco: unref short failure
[h264 @ 0x55d91632e740] mmco: unref short failure
[h264 @ 0x55d91632e740] mmco: unref short failure
[h264 @ 0x55d91632e740] mmco: unref short failure
[h264 @ 0x55d91632e740] mmco: unref short failure
[h264 @ 0x55d918f0f900] mmco: unref short failure
[h264 @ 0x5598ec3cf080] mmco: unref short failure
[h264 @ 0x5598ec3cf080] mmco: unref short failure
09/18/2024 15:30:49 - INFO - __main__ -   current idx cLYenAzoWwo.8 from finetune_area returns wrong image/video, use 12933 instead.
[h264 @ 0x55cb6728b740] mmco: unref short failure
[h264 @ 0x55d9072bfc80] mmco: unref short failure
[h264 @ 0x55d9072bfc80] mmco: unref short failure
[h264 @ 0x55d9072bfc80] mmco: unref short failure
09/18/2024 15:31:03 - INFO - __main__ -   current idx bhKtsohT-Xg.14 from finetune_area returns wrong image/video, use 106581 instead.
[h264 @ 0x55cb712f5380] mmco: unref short failure
[h264 @ 0x55cb65d5a240] mmco: unref short failure
[h264 @ 0x55d90e675a40] mmco: unref short failure
[h264 @ 0x55d90e675a40] mmco: unref short failure
[h264 @ 0x55cb71665140] mmco: unref short failure
[h264 @ 0x55cb71665140] mmco: unref short failure
[h264 @ 0x55cb5ed92500] mmco: unref short failure
  3%|▎         | 82/2910 [26:19<27:03:12, 34.44s/it][h264 @ 0x55cb67d43b80] mmco: unref short failure
[h264 @ 0x55cb67d43b80] mmco: unref short failure
  3%|▎         | 83/2910 [26:24<20:11:11, 25.71s/it][h264 @ 0x55d912d2e9c0] mmco: unref short failure
[h264 @ 0x55d912d2e9c0] mmco: unref short failure
  3%|▎         | 84/2910 [26:29<15:16:49, 19.47s/it][h264 @ 0x55d91a0cc480] mmco: unref short failure
[h264 @ 0x55d91a0cc480] mmco: unref short failure
[h264 @ 0x5565f753bec0] mmco: unref short failure
[h264 @ 0x55cb71a109c0] mmco: unref short failure
[h264 @ 0x55cb71a109c0] mmco: unref short failure
  3%|▎         | 85/2910 [26:46<14:37:23, 18.64s/it][mov,mp4,m4a,3gp,3g2,mj2 @ 0x55cb64a26ec0] moov atom not found
  3%|▎         | 86/2910 [26:51<11:30:31, 14.67s/it]  3%|▎         | 87/2910 [27:10<12:21:56, 15.77s/it]  3%|▎         | 88/2910 [27:15<9:52:40, 12.60s/it] [h264 @ 0x55d9120b5fc0] mmco: unref short failure
[h264 @ 0x55d9120b5fc0] mmco: unref short failure
[h264 @ 0x5565fcb29600] mmco: unref short failure
[h264 @ 0x5565fcb29600] mmco: unref short failure
  3%|▎         | 89/2910 [27:20<8:09:25, 10.41s/it][h264 @ 0x5565e5b56e00] mmco: unref short failure
[h264 @ 0x5565e5b56e00] mmco: unref short failure
[h264 @ 0x5565e5b56e00] mmco: unref short failure
[h264 @ 0x5565e5b56e00] mmco: unref short failure
[h264 @ 0x55cb62e81f00] mmco: unref short failure
[h264 @ 0x5598e38bd8c0] mmco: unref short failure
[h264 @ 0x5598e38bd8c0] mmco: unref short failure
[h264 @ 0x5598e8c025c0] mmco: unref short failure
[h264 @ 0x5598e8c025c0] mmco: unref short failure
[h264 @ 0x5565f71884c0] mmco: unref short failure
[h264 @ 0x5565f71884c0] mmco: unref short failure
[h264 @ 0x5565f71884c0] mmco: unref short failure
[h264 @ 0x5565f71884c0] mmco: unref short failure
[h264 @ 0x55cb72087440] mmco: unref short failure
[h264 @ 0x55cb72087440] mmco: unref short failure
[h264 @ 0x55d8fe4f5680] mmco: unref short failure
[h264 @ 0x55d91ad3c100] mmco: unref short failure
[h264 @ 0x55d91ad3c100] mmco: unref short failure
[h264 @ 0x55d91ad3c100] mmco: unref short failure
[h264 @ 0x55cb66f72700] mmco: unref short failure
[h264 @ 0x55cb66f72700] mmco: unref short failure
[h264 @ 0x55d8ffa4df40] mmco: unref short failure
  3%|▎         | 90/2910 [28:42<24:55:21, 31.82s/it][h264 @ 0x55cb5e431140] mmco: unref short failure
[h264 @ 0x55cb5e431140] mmco: unref short failure
  3%|▎         | 91/2910 [28:47<18:42:16, 23.89s/it]  3%|▎         | 92/2910 [28:52<14:19:26, 18.30s/it][h264 @ 0x55cb6f4aa180] mmco: unref short failure
[h264 @ 0x55cb6f4aa180] mmco: unref short failure
  3%|▎         | 93/2910 [29:04<12:43:12, 16.26s/it][h264 @ 0x5565e5ac8800] mmco: unref short failure
  3%|▎         | 94/2910 [29:19<12:25:56, 15.89s/it][h264 @ 0x5598f0f45300] mmco: unref short failure
[h264 @ 0x5598f0f45300] mmco: unref short failure
[h264 @ 0x5565fe776980] mmco: unref short failure
[h264 @ 0x5565fe776980] mmco: unref short failure
  3%|▎         | 95/2910 [29:40<13:42:24, 17.53s/it]  3%|▎         | 96/2910 [29:46<10:55:57, 13.99s/it]  3%|▎         | 97/2910 [29:52<8:57:34, 11.47s/it] [h264 @ 0x55d905f4d800] mmco: unref short failure
not have audios GAwav3sZcGw.4
[h264 @ 0x55d9134697c0] mmco: unref short failure
[h264 @ 0x5598e078ec80] mmco: unref short failure
[h264 @ 0x5598e078ec80] mmco: unref short failure
[h264 @ 0x5598e77df580] mmco: unref short failure
[h264 @ 0x5598e77df580] mmco: unref short failure
[h264 @ 0x5565f8aaab40] mmco: unref short failure
[h264 @ 0x5565f8aaab40] mmco: unref short failure
[h264 @ 0x55cb64767240] mmco: unref short failure
[h264 @ 0x55cb64767240] mmco: unref short failure
[h264 @ 0x55cb64767240] mmco: unref short failure
[h264 @ 0x55cb64767240] mmco: unref short failure
[h264 @ 0x55d917755300] mmco: unref short failure
[h264 @ 0x55d917755300] mmco: unref short failure
09/18/2024 15:35:27 - INFO - __main__ -   current idx JRTVXn0PfXQ.11 from finetune_area returns wrong image/video, use 109641 instead.
[h264 @ 0x55cb79b0b140] mmco: unref short failure
[h264 @ 0x5598fc5d9700] mmco: unref short failure
[h264 @ 0x5598fc5d9700] mmco: unref short failure
[h264 @ 0x5598fc5d9700] mmco: unref short failure
[h264 @ 0x5598fc5d9700] mmco: unref short failure
[h264 @ 0x5598f0f44c40] mmco: unref short failure
[h264 @ 0x5598f0f44c40] mmco: unref short failure
[h264 @ 0x5598f0f44c40] mmco: unref short failure
[h264 @ 0x5598f0f44c40] mmco: unref short failure
[h264 @ 0x55cb67bb9280] mmco: unref short failure
[h264 @ 0x55cb67bb9280] mmco: unref short failure
[h264 @ 0x5598f140e280] mmco: unref short failure
[h264 @ 0x55cb6e8865c0] mmco: unref short failure
[h264 @ 0x55cb6e8865c0] mmco: unref short failure
[h264 @ 0x5598e4063140] mmco: unref short failure
[h264 @ 0x5598f600db80] mmco: unref short failure
[h264 @ 0x5598f600db80] mmco: unref short failure
[h264 @ 0x5565e894b300] mmco: unref short failure
[h264 @ 0x55cb742a6180] mmco: unref short failure
[h264 @ 0x5598f691cd40] mmco: unref short failure
[h264 @ 0x55cb6bbcd900] mmco: unref short failure
[h264 @ 0x55cb6bbcd900] mmco: unref short failure
[h264 @ 0x5565ead7dcc0] mmco: unref short failure
[h264 @ 0x55cb64239440] mmco: unref short failure
[h264 @ 0x55cb68d18100] mmco: unref short failure
[h264 @ 0x55cb68d18100] mmco: unref short failure
[h264 @ 0x5598f3c49000] mmco: unref short failure
[h264 @ 0x5598f3c49000] mmco: unref short failure
[h264 @ 0x55d9072b6c80] mmco: unref short failure
[h264 @ 0x5598f5ac8080] mmco: unref short failure
[h264 @ 0x5598f5ac8080] mmco: unref short failure
  3%|▎         | 98/2910 [31:05<23:33:31, 30.16s/it][h264 @ 0x55d91ae9f880] mmco: unref short failure
[h264 @ 0x55d91ae9f880] mmco: unref short failure
[h264 @ 0x55d91ae9f880] mmco: unref short failure
[h264 @ 0x55d91ae9f880] mmco: unref short failure
[h264 @ 0x55d903299f80] mmco: unref short failure
  3%|▎         | 99/2910 [31:11<17:41:06, 22.65s/it]09/18/2024 15:36:20 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 15:36:20 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598eefd5300] mmco: unref short failure
[h264 @ 0x5598eefd5300] mmco: unref short failure
[h264 @ 0x5598eefd5300] mmco: unref short failure
[h264 @ 0x5598eefd5300] mmco: unref short failure
[h264 @ 0x5598ef01e400] mmco: unref short failure
[h264 @ 0x55cb69ad09c0] mmco: unref short failure
[h264 @ 0x55cb69ad09c0] mmco: unref short failure
[h264 @ 0x5598f40cbb40] mmco: unref short failure
[h264 @ 0x5598f40cbb40] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb79f2c600] mmco: unref short failure
[h264 @ 0x55cb79f2c600] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 15:37:29 - INFO - __main__ -   current idx AXNTPfihw4Y.36 from finetune_area returns wrong image/video, use 138393 instead.
[h264 @ 0x5565fb1a0d80] mmco: unref short failure
[h264 @ 0x5565fb1a0d80] mmco: unref short failure
[h264 @ 0x55d9134e28c0] mmco: unref short failure
[h264 @ 0x55d916e37800] mmco: unref short failure
[h264 @ 0x55d916e37800] mmco: unref short failure
[h264 @ 0x55cb70422280] mmco: unref short failure
[h264 @ 0x55cb70422280] mmco: unref short failure
[h264 @ 0x55cb70422280] mmco: unref short failure
[h264 @ 0x55cb70422280] mmco: unref short failure
[h264 @ 0x55cb70422280] mmco: unref short failure
[h264 @ 0x55cb6c1328c0] mmco: unref short failure
[h264 @ 0x55d914634380] mmco: unref short failure
[h264 @ 0x55d914634380] mmco: unref short failure
[h264 @ 0x5598f5e66e40] mmco: unref short failure
[h264 @ 0x5598f5e66e40] mmco: unref short failure
[h264 @ 0x5565f5147c40] mmco: unref short failure
[h264 @ 0x5598e7f232c0] mmco: unref short failure
[h264 @ 0x55d913410a40] mmco: unref short failure
[h264 @ 0x55d913410a40] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:04,  3.39it/s][A
  1%|          | 2/221 [00:00<01:26,  2.52it/s][A09/18/2024 15:38:43 - INFO - __main__ -   current idx dYEsNlBENWc.3 from finetune_area returns wrong image/video, use 115903 instead.

  1%|▏         | 3/221 [00:01<01:28,  2.47it/s][A
  2%|▏         | 4/221 [00:01<01:07,  3.21it/s][A
  2%|▏         | 5/221 [00:01<01:07,  3.20it/s][A
  3%|▎         | 6/221 [00:01<01:03,  3.41it/s][A
  3%|▎         | 7/221 [00:02<01:04,  3.31it/s][A
  4%|▎         | 8/221 [00:02<01:00,  3.54it/s][A
  4%|▍         | 9/221 [00:02<00:52,  4.05it/s][A
  5%|▍         | 10/221 [00:02<00:58,  3.63it/s][A
  5%|▍         | 11/221 [00:03<00:49,  4.27it/s][A
  5%|▌         | 12/221 [00:03<00:53,  3.88it/s][h264 @ 0x5598f3c48bc0] mmco: unref short failure
[h264 @ 0x5598f3c48bc0] mmco: unref short failure
[A
  6%|▌         | 13/221 [00:03<00:47,  4.39it/s][A
  6%|▋         | 14/221 [00:03<00:50,  4.12it/s][A
  7%|▋         | 15/221 [00:04<00:54,  3.79it/s][A
  7%|▋         | 16/221 [00:04<01:07,  3.02it/s][A[h264 @ 0x5565efafe240] mmco: unref short failure

  8%|▊         | 17/221 [00:05<01:27,  2.32it/s][A
  8%|▊         | 18/221 [00:05<01:22,  2.47it/s][A
  9%|▊         | 19/221 [00:05<01:11,  2.83it/s][A
  9%|▉         | 20/221 [00:06<00:59,  3.39it/s][A
 10%|▉         | 21/221 [00:06<00:59,  3.34it/s][A
 10%|▉         | 22/221 [00:06<01:13,  2.70it/s][A
 11%|█         | 24/221 [00:07<00:54,  3.63it/s][A[h264 @ 0x5598ef882400] mmco: unref short failure

 11%|█▏        | 25/221 [00:07<00:52,  3.71it/s][A[h264 @ 0x5598ef882400] mmco: unref short failure

 12%|█▏        | 26/221 [00:07<01:01,  3.18it/s][A
 12%|█▏        | 27/221 [00:08<00:50,  3.88it/s][A
 13%|█▎        | 28/221 [00:08<00:57,  3.37it/s][A
 13%|█▎        | 29/221 [00:08<00:56,  3.43it/s][A
 14%|█▎        | 30/221 [00:09<01:03,  3.01it/s][A
 14%|█▍        | 31/221 [00:09<01:05,  2.92it/s][A
 15%|█▍        | 33/221 [00:09<00:49,  3.83it/s][A
 16%|█▌        | 35/221 [00:10<00:37,  4.94it/s][A
 16%|█▋        | 36/221 [00:10<00:39,  4.66it/s][A
 17%|█▋        | 37/221 [00:10<00:45,  4.03it/s][A
 17%|█▋        | 38/221 [00:11<00:54,  3.34it/s][A
 18%|█▊        | 39/221 [00:11<00:49,  3.66it/s][A[h264 @ 0x5598f091ef80] mmco: unref short failure

 18%|█▊        | 40/221 [00:11<00:51,  3.49it/s][A
 19%|█▊        | 41/221 [00:11<00:45,  3.99it/s][A
 19%|█▉        | 42/221 [00:12<01:06,  2.67it/s][A
 19%|█▉        | 43/221 [00:12<00:54,  3.27it/s][A
 20%|█▉        | 44/221 [00:12<00:50,  3.50it/s][A
 20%|██        | 45/221 [00:13<01:06,  2.63it/s][A
 21%|██        | 46/221 [00:13<01:00,  2.88it/s][A
 21%|██▏       | 47/221 [00:15<02:35,  1.12it/s][A
 22%|██▏       | 48/221 [00:16<02:04,  1.39it/s][A
 22%|██▏       | 49/221 [00:16<01:38,  1.75it/s][A
 23%|██▎       | 50/221 [00:16<01:22,  2.07it/s][A
 23%|██▎       | 51/221 [00:16<01:05,  2.60it/s][A
 24%|██▎       | 52/221 [00:17<01:00,  2.78it/s][A
 24%|██▍       | 53/221 [00:17<00:49,  3.38it/s][A
 24%|██▍       | 54/221 [00:18<01:34,  1.77it/s][A
 25%|██▍       | 55/221 [00:19<01:29,  1.85it/s][A
 25%|██▌       | 56/221 [00:19<01:11,  2.32it/s][A
 26%|██▌       | 57/221 [00:19<01:16,  2.16it/s][A
 26%|██▌       | 58/221 [00:20<01:07,  2.42it/s][A
 27%|██▋       | 59/221 [00:20<00:59,  2.74it/s][A
 27%|██▋       | 60/221 [00:21<01:22,  1.96it/s][A
 28%|██▊       | 61/221 [00:21<01:07,  2.35it/s][A
 28%|██▊       | 62/221 [00:21<01:12,  2.20it/s][A
 29%|██▊       | 63/221 [00:22<01:00,  2.59it/s][A
 29%|██▉       | 64/221 [00:22<01:05,  2.38it/s][A
 29%|██▉       | 65/221 [00:22<00:56,  2.74it/s][A
 30%|██▉       | 66/221 [00:23<01:07,  2.29it/s][A
 30%|███       | 67/221 [00:23<01:00,  2.55it/s][A
 31%|███       | 68/221 [00:24<00:57,  2.65it/s][A
 31%|███       | 69/221 [00:24<01:19,  1.92it/s][A
 32%|███▏      | 70/221 [00:25<01:06,  2.29it/s][A
 32%|███▏      | 71/221 [00:25<00:54,  2.73it/s][A
 33%|███▎      | 72/221 [00:25<01:01,  2.41it/s][A
 33%|███▎      | 73/221 [00:26<01:07,  2.18it/s][A
 33%|███▎      | 74/221 [00:26<00:57,  2.58it/s][A
 34%|███▍      | 75/221 [00:27<00:53,  2.71it/s][A
 34%|███▍      | 76/221 [00:27<00:50,  2.87it/s][A
 35%|███▍      | 77/221 [00:27<00:55,  2.61it/s][A
 35%|███▌      | 78/221 [00:27<00:43,  3.28it/s][A
 36%|███▌      | 79/221 [00:28<00:49,  2.85it/s][A
 36%|███▌      | 80/221 [00:28<00:48,  2.93it/s][A
 37%|███▋      | 81/221 [00:29<00:52,  2.68it/s][A
 37%|███▋      | 82/221 [00:29<00:54,  2.55it/s][A
 38%|███▊      | 83/221 [00:29<00:51,  2.66it/s][A
 38%|███▊      | 84/221 [00:30<00:46,  2.92it/s][A
 38%|███▊      | 85/221 [00:30<00:38,  3.58it/s][A
 39%|███▉      | 86/221 [00:30<00:35,  3.82it/s][A
 39%|███▉      | 87/221 [00:31<01:04,  2.07it/s][A
 40%|███▉      | 88/221 [00:32<01:07,  1.96it/s][A
 40%|████      | 89/221 [00:32<00:54,  2.41it/s][A
 41%|████      | 90/221 [00:32<00:51,  2.55it/s][A
 41%|████      | 91/221 [00:32<00:44,  2.90it/s][h264 @ 0x556603c82880] mmco: unref short failure
[h264 @ 0x556603c82880] mmco: unref short failure
[A
 42%|████▏     | 92/221 [00:33<00:43,  2.93it/s][A
 42%|████▏     | 93/221 [00:33<00:44,  2.86it/s][A
 43%|████▎     | 94/221 [00:33<00:44,  2.85it/s][A
 43%|████▎     | 95/221 [00:34<00:43,  2.91it/s][A09/18/2024 15:39:17 - INFO - __main__ -   current idx c0na5aaBMBE.58 from finetune_area returns wrong image/video, use 26302 instead.

 43%|████▎     | 96/221 [00:35<01:03,  1.97it/s][A
 44%|████▍     | 97/221 [00:35<00:56,  2.20it/s][A09/18/2024 15:39:18 - INFO - __main__ -   current idx NK2f7abMxq4.56 from finetune_area returns wrong image/video, use 141039 instead.
[h264 @ 0x5565fdca2640] mmco: unref short failure
[h264 @ 0x5565fdca2640] mmco: unref short failure

 44%|████▍     | 98/221 [00:36<01:08,  1.80it/s][A[h264 @ 0x5565fdca2640] mmco: unref short failure
[h264 @ 0x5565fdca2640] mmco: unref short failure

 45%|████▍     | 99/221 [00:36<01:01,  1.98it/s][A
 45%|████▌     | 100/221 [00:37<00:55,  2.18it/s][A
 46%|████▌     | 101/221 [00:37<00:48,  2.49it/s][A
 46%|████▌     | 102/221 [00:37<00:49,  2.41it/s][A
 47%|████▋     | 103/221 [00:37<00:39,  2.98it/s][A
 47%|████▋     | 104/221 [00:38<00:32,  3.58it/s][A
 48%|████▊     | 105/221 [00:38<00:35,  3.28it/s][A
 48%|████▊     | 106/221 [00:39<00:53,  2.14it/s][A
 48%|████▊     | 107/221 [00:39<00:47,  2.42it/s][A
 49%|████▉     | 108/221 [00:39<00:40,  2.78it/s][A
 49%|████▉     | 109/221 [00:40<00:40,  2.77it/s][A
 50%|████▉     | 110/221 [00:40<00:32,  3.37it/s][A
 50%|█████     | 111/221 [00:40<00:35,  3.12it/s][h264 @ 0x5565f3dcf680] mmco: unref short failure
[h264 @ 0x5565f3dcf680] mmco: unref short failure
[A[h264 @ 0x5565f3dcf680] mmco: unref short failure
[h264 @ 0x5565f3dcf680] mmco: unref short failure
[h264 @ 0x5565f3dcf680] mmco: unref short failure
[h264 @ 0x5565f3dcf680] mmco: unref short failure

 51%|█████     | 112/221 [00:40<00:34,  3.17it/s][A
 51%|█████     | 113/221 [00:41<00:31,  3.45it/s][A
 52%|█████▏    | 114/221 [00:41<00:25,  4.26it/s][A
 52%|█████▏    | 115/221 [00:41<00:22,  4.63it/s][A
 52%|█████▏    | 116/221 [00:42<00:44,  2.38it/s][A
 53%|█████▎    | 117/221 [00:42<00:42,  2.43it/s][A
 53%|█████▎    | 118/221 [00:42<00:36,  2.85it/s][A
 54%|█████▍    | 119/221 [00:43<00:35,  2.85it/s][A
 54%|█████▍    | 120/221 [00:43<00:30,  3.33it/s][A
 55%|█████▍    | 121/221 [00:43<00:27,  3.59it/s][A
 55%|█████▌    | 122/221 [00:44<00:38,  2.55it/s][A09/18/2024 15:39:26 - INFO - __main__ -   current idx fGbJT-U6ymc.18 from finetune_area returns wrong image/video, use 96578 instead.

 56%|█████▌    | 123/221 [00:44<00:35,  2.76it/s][A
 56%|█████▌    | 124/221 [00:45<00:35,  2.70it/s][A
 57%|█████▋    | 125/221 [00:45<00:40,  2.36it/s][A
 57%|█████▋    | 126/221 [00:45<00:37,  2.52it/s][A
 57%|█████▋    | 127/221 [00:47<00:56,  1.66it/s][A
 58%|█████▊    | 128/221 [00:47<00:51,  1.82it/s][A
 58%|█████▊    | 129/221 [00:47<00:38,  2.38it/s][A
 59%|█████▉    | 130/221 [00:47<00:33,  2.73it/s][A[h264 @ 0x55660000a900] mmco: unref short failure

 59%|█████▉    | 131/221 [00:47<00:27,  3.26it/s][A[h264 @ 0x55cb6d76c500] mmco: unref short failure

 60%|█████▉    | 132/221 [00:49<00:47,  1.89it/s][A[h264 @ 0x5598f1c5e040] mmco: unref short failure
[h264 @ 0x5598f1c5e040] mmco: unref short failure

 60%|██████    | 133/221 [00:49<00:41,  2.11it/s][A
 61%|██████    | 134/221 [00:50<00:49,  1.75it/s][A
 61%|██████    | 135/221 [00:50<00:53,  1.62it/s][A
 62%|██████▏   | 136/221 [00:51<00:48,  1.74it/s][A[h264 @ 0x55cb7acbc300] mmco: unref short failure
[h264 @ 0x55cb7acbc300] mmco: unref short failure

 62%|██████▏   | 137/221 [00:51<00:45,  1.83it/s][A[h264 @ 0x5598de56da80] mmco: unref short failure

 62%|██████▏   | 138/221 [00:52<00:43,  1.92it/s][A
 63%|██████▎   | 139/221 [00:53<00:52,  1.55it/s][A
 63%|██████▎   | 140/221 [00:53<00:46,  1.75it/s][A
 64%|██████▍   | 141/221 [00:54<00:46,  1.71it/s][A
 64%|██████▍   | 142/221 [00:54<00:43,  1.81it/s][A
 65%|██████▍   | 143/221 [00:55<00:41,  1.86it/s][A
 65%|██████▌   | 144/221 [00:55<00:35,  2.18it/s][A
 66%|██████▌   | 145/221 [00:55<00:28,  2.69it/s][A
 66%|██████▌   | 146/221 [00:55<00:22,  3.39it/s][A
 67%|██████▋   | 147/221 [00:56<00:22,  3.32it/s][A
 67%|██████▋   | 148/221 [00:56<00:33,  2.18it/s][A
 67%|██████▋   | 149/221 [00:57<00:33,  2.18it/s][A
 68%|██████▊   | 150/221 [00:57<00:28,  2.49it/s][A
 68%|██████▊   | 151/221 [00:58<00:27,  2.57it/s][A
 69%|██████▉   | 152/221 [00:58<00:32,  2.12it/s][A
 69%|██████▉   | 153/221 [00:58<00:26,  2.52it/s][A
 70%|██████▉   | 154/221 [00:59<00:23,  2.89it/s][A
 70%|███████   | 155/221 [00:59<00:21,  3.05it/s][A
 71%|███████   | 156/221 [00:59<00:23,  2.73it/s][A
 71%|███████   | 157/221 [01:01<00:46,  1.38it/s][A
 71%|███████▏  | 158/221 [01:01<00:39,  1.60it/s][A
 72%|███████▏  | 159/221 [01:02<00:30,  2.04it/s][A[h264 @ 0x55d8ffca1b40] mmco: unref short failure
[h264 @ 0x55d8ffca1b40] mmco: unref short failure

 72%|███████▏  | 160/221 [01:02<00:24,  2.44it/s][A
 73%|███████▎  | 161/221 [01:02<00:19,  3.04it/s][A
 73%|███████▎  | 162/221 [01:02<00:15,  3.75it/s][A
 74%|███████▍  | 163/221 [01:02<00:17,  3.36it/s][A
 74%|███████▍  | 164/221 [01:03<00:16,  3.50it/s][A
 75%|███████▍  | 165/221 [01:03<00:15,  3.59it/s][A
 75%|███████▌  | 166/221 [01:04<00:32,  1.68it/s][A
 76%|███████▌  | 167/221 [01:04<00:26,  2.07it/s][A
 76%|███████▌  | 168/221 [01:06<00:38,  1.39it/s][A
 76%|███████▋  | 169/221 [01:06<00:28,  1.84it/s][A
 77%|███████▋  | 170/221 [01:06<00:23,  2.15it/s][A
 77%|███████▋  | 171/221 [01:07<00:24,  2.04it/s][A
 78%|███████▊  | 172/221 [01:07<00:20,  2.36it/s][A
 78%|███████▊  | 173/221 [01:07<00:21,  2.24it/s][A
 79%|███████▊  | 174/221 [01:08<00:16,  2.83it/s][A
 79%|███████▉  | 175/221 [01:08<00:15,  2.96it/s][A
 80%|███████▉  | 176/221 [01:08<00:12,  3.55it/s][A
 80%|████████  | 177/221 [01:08<00:11,  3.77it/s][A
 81%|████████  | 178/221 [01:09<00:20,  2.06it/s][A
 81%|████████  | 179/221 [01:10<00:21,  2.00it/s][A
 81%|████████▏ | 180/221 [01:10<00:16,  2.46it/s][A
 82%|████████▏ | 181/221 [01:10<00:13,  2.92it/s][A
 82%|████████▏ | 182/221 [01:10<00:12,  3.04it/s][A
 83%|████████▎ | 183/221 [01:11<00:13,  2.91it/s][A
 83%|████████▎ | 184/221 [01:11<00:13,  2.82it/s][A
 84%|████████▍ | 186/221 [01:12<00:13,  2.68it/s][A
 85%|████████▍ | 187/221 [01:12<00:12,  2.82it/s][A
 85%|████████▌ | 188/221 [01:13<00:11,  2.81it/s][A
 86%|████████▌ | 189/221 [01:13<00:11,  2.71it/s][A
 86%|████████▌ | 190/221 [01:13<00:11,  2.60it/s][A
 86%|████████▋ | 191/221 [01:14<00:09,  3.26it/s][A
 87%|████████▋ | 192/221 [01:14<00:08,  3.55it/s][A
 87%|████████▋ | 193/221 [01:14<00:06,  4.25it/s][A
 88%|████████▊ | 194/221 [01:16<00:20,  1.35it/s][A
 88%|████████▊ | 195/221 [01:16<00:15,  1.70it/s][A
 89%|████████▊ | 196/221 [01:16<00:11,  2.21it/s][A
 89%|████████▉ | 197/221 [01:17<00:09,  2.40it/s][A
 90%|████████▉ | 198/221 [01:17<00:07,  2.88it/s][A
 90%|█████████ | 199/221 [01:17<00:07,  2.90it/s][A
 90%|█████████ | 200/221 [01:17<00:05,  3.54it/s][A
 91%|█████████ | 201/221 [01:17<00:05,  3.84it/s][A
 91%|█████████▏| 202/221 [01:18<00:05,  3.74it/s][A
 92%|█████████▏| 203/221 [01:18<00:04,  3.71it/s][A
 92%|█████████▏| 204/221 [01:18<00:04,  4.00it/s][A
 93%|█████████▎| 205/221 [01:19<00:04,  3.85it/s][A
 93%|█████████▎| 206/221 [01:19<00:04,  3.22it/s][A
 94%|█████████▎| 207/221 [01:19<00:03,  4.01it/s][A
 94%|█████████▍| 208/221 [01:19<00:03,  3.33it/s][A
 95%|█████████▍| 209/221 [01:20<00:03,  3.57it/s][A
 95%|█████████▌| 211/221 [01:20<00:02,  3.82it/s][A
 96%|█████████▌| 212/221 [01:20<00:02,  3.74it/s][A
 96%|█████████▋| 213/221 [01:21<00:01,  4.09it/s][A
 97%|█████████▋| 214/221 [01:21<00:02,  3.36it/s][A
 97%|█████████▋| 215/221 [01:21<00:01,  3.99it/s][A
 98%|█████████▊| 216/221 [01:21<00:01,  3.83it/s][A
 98%|█████████▊| 217/221 [01:22<00:01,  2.64it/s][A
 99%|█████████▊| 218/221 [01:22<00:01,  2.95it/s][A
 99%|█████████▉| 219/221 [01:23<00:00,  2.95it/s][A
100%|█████████▉| 220/221 [01:24<00:00,  1.92it/s][A100%|██████████| 221/221 [01:24<00:00,  2.62it/s]
[h264 @ 0x5565ea2b7380] mmco: unref short failure
[h264 @ 0x55cb6263e740] mmco: unref short failure
[h264 @ 0x55cb6263e740] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.28it/s][A
  1%|          | 2/221 [00:00<01:06,  3.29it/s][A
  1%|▏         | 3/221 [00:00<01:07,  3.24it/s][A
  2%|▏         | 4/221 [00:01<01:08,  3.18it/s][A
  2%|▏         | 5/221 [00:01<01:07,  3.21it/s][A
  3%|▎         | 6/221 [00:01<01:07,  3.20it/s][A
  3%|▎         | 7/221 [00:02<01:06,  3.23it/s][A
  4%|▎         | 8/221 [00:02<01:05,  3.25it/s][A
  4%|▍         | 9/221 [00:02<01:05,  3.26it/s][A
  5%|▍         | 10/221 [00:03<01:05,  3.20it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.23it/s][A
  5%|▌         | 12/221 [00:03<01:04,  3.24it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.25it/s][A
  6%|▋         | 14/221 [00:04<01:03,  3.26it/s][A
  7%|▋         | 15/221 [00:04<01:04,  3.21it/s][A
  7%|▋         | 16/221 [00:04<01:03,  3.23it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.24it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.26it/s][A[h264 @ 0x5598e3785a40] mmco: unref short failure

  9%|▊         | 19/221 [00:05<01:01,  3.27it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.27it/s][A
 10%|▉         | 21/221 [00:06<01:01,  3.28it/s][A
 10%|▉         | 22/221 [00:06<01:00,  3.28it/s][A
 10%|█         | 23/221 [00:07<01:01,  3.22it/s][A
 11%|█         | 24/221 [00:07<01:01,  3.18it/s][A
 11%|█▏        | 25/221 [00:07<01:01,  3.21it/s][A
 12%|█▏        | 26/221 [00:08<01:00,  3.24it/s][A
 12%|█▏        | 27/221 [00:08<00:59,  3.25it/s][A[h264 @ 0x5598fbe33540] mmco: unref short failure
[h264 @ 0x5598fbe33540] mmco: unref short failure

 13%|█▎        | 28/221 [00:08<00:59,  3.27it/s][A
 13%|█▎        | 29/221 [00:08<00:59,  3.22it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.24it/s][A
 14%|█▍        | 31/221 [00:09<00:58,  3.26it/s][A
 14%|█▍        | 32/221 [00:09<00:58,  3.24it/s][A
 15%|█▍        | 33/221 [00:10<00:59,  3.18it/s][A
 15%|█▌        | 34/221 [00:10<00:58,  3.21it/s][A[h264 @ 0x5598f1ae0c80] mmco: unref short failure
[h264 @ 0x5598f1ae0c80] mmco: unref short failure

 16%|█▌        | 35/221 [00:10<00:57,  3.23it/s][A
 16%|█▋        | 36/221 [00:11<00:56,  3.25it/s][A
 17%|█▋        | 37/221 [00:11<00:57,  3.21it/s][A
 17%|█▋        | 38/221 [00:11<00:56,  3.24it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.21it/s][A09/18/2024 15:40:21 - INFO - __main__ -   current idx 1DPLzfs419g.3 from finetune_area returns wrong image/video, use 1432 instead.

 18%|█▊        | 40/221 [00:12<00:57,  3.14it/s][A
 19%|█▊        | 41/221 [00:12<00:57,  3.16it/s][A[h264 @ 0x55cb70386440] mmco: unref short failure

 19%|█▉        | 42/221 [00:13<00:56,  3.19it/s][A
 19%|█▉        | 43/221 [00:13<00:55,  3.21it/s][A
 20%|█▉        | 44/221 [00:13<00:55,  3.20it/s][A
 20%|██        | 45/221 [00:13<00:54,  3.21it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.20it/s][A
 21%|██▏       | 47/221 [00:14<00:53,  3.23it/s][A
 22%|██▏       | 48/221 [00:14<00:53,  3.22it/s][A
 22%|██▏       | 49/221 [00:15<00:53,  3.24it/s][A
 23%|██▎       | 50/221 [00:15<00:53,  3.21it/s][A
 23%|██▎       | 51/221 [00:15<00:53,  3.19it/s][A
 24%|██▎       | 52/221 [00:16<00:52,  3.22it/s][A
 24%|██▍       | 53/221 [00:16<00:51,  3.24it/s][A
 24%|██▍       | 54/221 [00:16<00:51,  3.22it/s][A
 25%|██▍       | 55/221 [00:17<00:51,  3.24it/s][A
 25%|██▌       | 56/221 [00:17<00:51,  3.21it/s][A
 26%|██▌       | 57/221 [00:17<00:50,  3.23it/s][A
 26%|██▌       | 58/221 [00:17<00:50,  3.23it/s][A
 27%|██▋       | 59/221 [00:18<00:50,  3.23it/s][A[h264 @ 0x55cb73dc3dc0] mmco: unref short failure

 27%|██▋       | 60/221 [00:18<00:49,  3.23it/s][A
 28%|██▊       | 61/221 [00:18<00:49,  3.24it/s][A
 28%|██▊       | 62/221 [00:19<00:48,  3.25it/s][A
 29%|██▊       | 63/221 [00:19<00:48,  3.26it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.27it/s][A
 29%|██▉       | 65/221 [00:20<00:47,  3.28it/s][A
 30%|██▉       | 66/221 [00:20<00:47,  3.28it/s][A
 30%|███       | 67/221 [00:20<00:47,  3.22it/s][A
 31%|███       | 68/221 [00:21<00:47,  3.24it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.25it/s][A
 32%|███▏      | 70/221 [00:21<00:46,  3.27it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.27it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.27it/s][A
 33%|███▎      | 73/221 [00:22<00:45,  3.25it/s][A
 33%|███▎      | 74/221 [00:22<00:45,  3.26it/s][A
 34%|███▍      | 75/221 [00:23<00:44,  3.27it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.28it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.28it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.29it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.29it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.29it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.30it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.30it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.30it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.30it/s][A
 38%|███▊      | 85/221 [00:26<00:41,  3.30it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.30it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.30it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.30it/s][A
 40%|████      | 89/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.31it/s][A
 42%|████▏     | 92/221 [00:28<00:38,  3.31it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.28it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:25,  8.77it/s][A
  1%|          | 2/221 [00:00<01:03,  3.47it/s][A
  1%|▏         | 3/221 [00:00<00:51,  4.22it/s][A
  2%|▏         | 4/221 [00:00<00:53,  4.05it/s][A
  2%|▏         | 5/221 [00:01<00:57,  3.75it/s][A
  3%|▎         | 7/221 [00:01<00:51,  4.19it/s][A
  4%|▎         | 8/221 [00:02<00:55,  3.81it/s][A
  4%|▍         | 9/221 [00:02<01:02,  3.38it/s][A
  5%|▍         | 10/221 [00:03<01:26,  2.44it/s][A
  5%|▍         | 11/221 [00:03<01:16,  2.73it/s][A
  5%|▌         | 12/221 [00:03<01:11,  2.93it/s][A
  6%|▌         | 13/221 [00:04<01:21,  2.56it/s][A
  6%|▋         | 14/221 [00:04<01:17,  2.67it/s][A
  7%|▋         | 15/221 [00:04<01:02,  3.30it/s][A
  7%|▋         | 16/221 [00:04<01:01,  3.34it/s][A
  8%|▊         | 17/221 [00:05<01:30,  2.26it/s][A
  8%|▊         | 18/221 [00:06<01:25,  2.37it/s][A
  9%|▊         | 19/221 [00:06<01:47,  1.88it/s][A
 10%|▉         | 21/221 [00:07<01:12,  2.77it/s][A
 10%|▉         | 22/221 [00:07<01:01,  3.24it/s][A
 10%|█         | 23/221 [00:07<00:52,  3.79it/s][A
 11%|█         | 24/221 [00:07<00:53,  3.69it/s][A
 11%|█▏        | 25/221 [00:08<00:54,  3.60it/s][A
 12%|█▏        | 26/221 [00:08<00:51,  3.79it/s][A
 12%|█▏        | 27/221 [00:08<00:52,  3.70it/s][A
 13%|█▎        | 28/221 [00:09<01:39,  1.94it/s][A
 13%|█▎        | 29/221 [00:10<01:53,  1.69it/s][A
 14%|█▎        | 30/221 [00:10<01:37,  1.95it/s][A
 14%|█▍        | 31/221 [00:11<01:23,  2.28it/s][A
 14%|█▍        | 32/221 [00:11<01:05,  2.87it/s][A
 15%|█▍        | 33/221 [00:11<00:57,  3.27it/s][A
 15%|█▌        | 34/221 [00:11<00:50,  3.69it/s][A
 16%|█▌        | 35/221 [00:11<00:43,  4.30it/s][A
 16%|█▋        | 36/221 [00:12<00:53,  3.48it/s][A
 17%|█▋        | 37/221 [00:12<00:56,  3.26it/s][A
 17%|█▋        | 38/221 [00:12<01:04,  2.82it/s][A
 18%|█▊        | 39/221 [00:13<01:03,  2.87it/s][A
 18%|█▊        | 40/221 [00:13<00:58,  3.07it/s][A
 19%|█▊        | 41/221 [00:13<00:47,  3.82it/s][A
 19%|█▉        | 42/221 [00:13<00:48,  3.68it/s][A
 19%|█▉        | 43/221 [00:14<00:49,  3.62it/s][A
 20%|██        | 45/221 [00:14<00:44,  3.98it/s][A
 21%|██        | 46/221 [00:14<00:43,  4.00it/s][A
 21%|██▏       | 47/221 [00:15<00:43,  3.97it/s][A
 22%|██▏       | 48/221 [00:15<00:39,  4.37it/s][A
 22%|██▏       | 49/221 [00:15<00:35,  4.91it/s][A
 23%|██▎       | 50/221 [00:16<00:59,  2.88it/s][A
 23%|██▎       | 51/221 [00:16<00:54,  3.14it/s][A
 24%|██▎       | 52/221 [00:16<00:43,  3.89it/s][A
 24%|██▍       | 53/221 [00:16<00:38,  4.41it/s][A
 24%|██▍       | 54/221 [00:17<00:46,  3.58it/s][A
 25%|██▍       | 55/221 [00:17<00:38,  4.29it/s][A
 25%|██▌       | 56/221 [00:17<00:33,  4.89it/s][A
 26%|██▌       | 57/221 [00:17<00:30,  5.33it/s][A
 26%|██▌       | 58/221 [00:17<00:43,  3.79it/s][A
 27%|██▋       | 59/221 [00:18<00:41,  3.89it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.32it/s][A
 28%|██▊       | 61/221 [00:18<00:44,  3.57it/s][A
 28%|██▊       | 62/221 [00:19<00:39,  3.98it/s][A
 29%|██▊       | 63/221 [00:19<00:43,  3.65it/s][A
 29%|██▉       | 64/221 [00:19<00:51,  3.06it/s][A
 29%|██▉       | 65/221 [00:20<01:11,  2.18it/s][A
 30%|██▉       | 66/221 [00:20<00:56,  2.76it/s][A
 30%|███       | 67/221 [00:21<00:55,  2.79it/s][A
 31%|███       | 68/221 [00:21<00:43,  3.56it/s][A
 32%|███▏      | 70/221 [00:21<00:31,  4.75it/s][A
 32%|███▏      | 71/221 [00:21<00:38,  3.90it/s][A
 33%|███▎      | 72/221 [00:22<00:41,  3.62it/s][A
 33%|███▎      | 73/221 [00:22<00:46,  3.18it/s][A
 33%|███▎      | 74/221 [00:22<00:40,  3.63it/s][A
 34%|███▍      | 75/221 [00:23<00:42,  3.46it/s][A
 34%|███▍      | 76/221 [00:23<00:37,  3.82it/s][A
 35%|███▍      | 77/221 [00:23<00:37,  3.82it/s][A
 35%|███▌      | 78/221 [00:23<00:36,  3.91it/s][A
 36%|███▌      | 79/221 [00:24<00:50,  2.82it/s][A
 36%|███▌      | 80/221 [00:24<00:40,  3.51it/s][A
 37%|███▋      | 81/221 [00:24<00:37,  3.78it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.27it/s][A
 38%|███▊      | 83/221 [00:25<00:43,  3.20it/s][A
 38%|███▊      | 84/221 [00:26<01:03,  2.16it/s][A
 38%|███▊      | 85/221 [00:26<00:54,  2.51it/s][A
 39%|███▉      | 86/221 [00:26<00:48,  2.80it/s][A
 39%|███▉      | 87/221 [00:26<00:44,  3.03it/s][A
 40%|███▉      | 88/221 [00:27<00:49,  2.68it/s][A
 40%|████      | 89/221 [00:27<00:48,  2.70it/s][A
 41%|████      | 90/221 [00:28<00:44,  2.96it/s][A
 41%|████      | 91/221 [00:28<00:36,  3.56it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.27it/s][A
 43%|████▎     | 94/221 [00:28<00:28,  4.43it/s][A
 43%|████▎     | 95/221 [00:29<00:32,  3.84it/s][A
 43%|████▎     | 96/221 [00:29<00:29,  4.18it/s][A
 44%|████▍     | 98/221 [00:30<00:39,  3.14it/s][A
 45%|████▍     | 99/221 [00:30<00:38,  3.20it/s][A
 45%|████▌     | 100/221 [00:30<00:38,  3.17it/s][A
 46%|████▌     | 101/221 [00:31<00:39,  3.02it/s][A
 46%|████▌     | 102/221 [00:31<00:32,  3.64it/s][A
 47%|████▋     | 103/221 [00:31<00:28,  4.15it/s][A
 47%|████▋     | 104/221 [00:31<00:31,  3.72it/s][A
 48%|████▊     | 105/221 [00:32<00:30,  3.75it/s][A
 48%|████▊     | 106/221 [00:32<00:29,  3.86it/s][A
 48%|████▊     | 107/221 [00:32<00:28,  4.06it/s][A
 49%|████▉     | 108/221 [00:32<00:27,  4.06it/s][A
 49%|████▉     | 109/221 [00:33<00:31,  3.51it/s][A
 50%|████▉     | 110/221 [00:33<00:36,  3.07it/s][A
 50%|█████     | 111/221 [00:34<00:45,  2.40it/s][A
 51%|█████     | 112/221 [00:34<00:35,  3.09it/s][A
 51%|█████     | 113/221 [00:34<00:30,  3.58it/s][A
 52%|█████▏    | 115/221 [00:34<00:20,  5.15it/s][A
 52%|█████▏    | 116/221 [00:34<00:20,  5.04it/s][A
 53%|█████▎    | 117/221 [00:35<00:27,  3.85it/s][A
 53%|█████▎    | 118/221 [00:35<00:29,  3.47it/s][A
 54%|█████▍    | 119/221 [00:36<00:35,  2.88it/s][A
 54%|█████▍    | 120/221 [00:36<00:34,  2.93it/s][A
 55%|█████▍    | 121/221 [00:36<00:29,  3.40it/s][A
 55%|█████▌    | 122/221 [00:37<00:28,  3.53it/s][A
 56%|█████▌    | 123/221 [00:37<00:25,  3.87it/s][A
 56%|█████▌    | 124/221 [00:37<00:30,  3.21it/s][A
 57%|█████▋    | 125/221 [00:38<00:33,  2.85it/s][A
 57%|█████▋    | 126/221 [00:38<00:27,  3.50it/s][A
 57%|█████▋    | 127/221 [00:38<00:30,  3.10it/s][A
 58%|█████▊    | 128/221 [00:38<00:26,  3.48it/s][A
 58%|█████▊    | 129/221 [00:38<00:22,  4.07it/s][A
 59%|█████▉    | 130/221 [00:39<00:29,  3.07it/s][A
 60%|█████▉    | 132/221 [00:39<00:19,  4.47it/s][A
 60%|██████    | 133/221 [00:39<00:19,  4.47it/s][A
 61%|██████    | 134/221 [00:40<00:23,  3.73it/s][A
 61%|██████    | 135/221 [00:40<00:25,  3.37it/s][A
 62%|██████▏   | 136/221 [00:40<00:25,  3.37it/s][A
 62%|██████▏   | 137/221 [00:41<00:22,  3.81it/s][A
 62%|██████▏   | 138/221 [00:41<00:22,  3.69it/s][A
 63%|██████▎   | 139/221 [00:41<00:25,  3.18it/s][A
 63%|██████▎   | 140/221 [00:42<00:27,  2.96it/s][A
 64%|██████▍   | 141/221 [00:42<00:27,  2.90it/s][A
 64%|██████▍   | 142/221 [00:43<00:28,  2.73it/s][A
 65%|██████▍   | 143/221 [00:43<00:28,  2.74it/s][A
 65%|██████▌   | 144/221 [00:43<00:22,  3.44it/s][A
 66%|██████▌   | 145/221 [00:43<00:19,  3.81it/s][A
 66%|██████▌   | 146/221 [00:43<00:19,  3.91it/s][A
 67%|██████▋   | 148/221 [00:44<00:14,  5.14it/s][A
 67%|██████▋   | 149/221 [00:44<00:13,  5.51it/s][A
 68%|██████▊   | 150/221 [00:44<00:13,  5.18it/s][A
 68%|██████▊   | 151/221 [00:44<00:13,  5.24it/s][A
 69%|██████▉   | 152/221 [00:45<00:20,  3.37it/s][A
 69%|██████▉   | 153/221 [00:45<00:20,  3.37it/s][A
 70%|██████▉   | 154/221 [00:46<00:22,  2.93it/s][A
 70%|███████   | 155/221 [00:46<00:22,  2.93it/s][A
 71%|███████   | 156/221 [00:47<00:29,  2.22it/s][A
 71%|███████   | 157/221 [00:47<00:24,  2.56it/s][A
 71%|███████▏  | 158/221 [00:47<00:20,  3.08it/s][A
 72%|███████▏  | 159/221 [00:47<00:18,  3.36it/s][A
 72%|███████▏  | 160/221 [00:47<00:15,  3.86it/s][A
 73%|███████▎  | 161/221 [00:48<00:13,  4.57it/s][A
 73%|███████▎  | 162/221 [00:48<00:10,  5.45it/s][A
 74%|███████▍  | 163/221 [00:48<00:11,  4.83it/s][A
 74%|███████▍  | 164/221 [00:48<00:16,  3.46it/s][A
 75%|███████▍  | 165/221 [00:49<00:14,  3.85it/s][A
 75%|███████▌  | 166/221 [00:49<00:19,  2.84it/s][A
 76%|███████▌  | 167/221 [00:49<00:17,  3.12it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.46it/s][A
 76%|███████▋  | 169/221 [00:50<00:14,  3.68it/s][A
 77%|███████▋  | 170/221 [00:50<00:16,  3.00it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.36it/s][A
 78%|███████▊  | 172/221 [00:51<00:12,  3.82it/s][A
 78%|███████▊  | 173/221 [00:51<00:12,  3.85it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.40it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.25it/s][A
 80%|███████▉  | 176/221 [00:52<00:12,  3.73it/s][A
 80%|████████  | 177/221 [00:52<00:12,  3.51it/s][A
 81%|████████  | 178/221 [00:53<00:14,  2.87it/s][A
 81%|████████  | 179/221 [00:53<00:12,  3.34it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.78it/s][A
 82%|████████▏ | 181/221 [00:53<00:10,  3.79it/s][A
 82%|████████▏ | 182/221 [00:54<00:09,  3.92it/s][A
 83%|████████▎ | 183/221 [00:54<00:08,  4.30it/s][A
 83%|████████▎ | 184/221 [00:54<00:11,  3.23it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.07it/s][A
 84%|████████▍ | 186/221 [00:55<00:09,  3.87it/s][A
 85%|████████▍ | 187/221 [00:55<00:07,  4.59it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.59it/s][A
 86%|████████▌ | 189/221 [00:56<00:10,  3.05it/s][A
 86%|████████▌ | 190/221 [00:56<00:11,  2.69it/s][A
 86%|████████▋ | 191/221 [00:56<00:09,  3.27it/s][A
 87%|████████▋ | 192/221 [00:57<00:08,  3.47it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.82it/s][A
 88%|████████▊ | 194/221 [00:57<00:10,  2.63it/s][A
 88%|████████▊ | 195/221 [00:58<00:11,  2.34it/s][A
 89%|████████▊ | 196/221 [00:58<00:10,  2.28it/s][A
 89%|████████▉ | 197/221 [00:59<00:09,  2.52it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  2.91it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.37it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.06it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.39it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  3.94it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.16it/s][A
 93%|█████████▎| 205/221 [01:01<00:03,  4.94it/s][A
 93%|█████████▎| 206/221 [01:01<00:03,  4.63it/s][A
 94%|█████████▎| 207/221 [01:01<00:02,  4.90it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  4.00it/s][A
 95%|█████████▍| 209/221 [01:02<00:03,  3.03it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.88it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.68it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  2.90it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.30it/s][A
 97%|█████████▋| 215/221 [01:04<00:01,  3.07it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  3.24it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.35it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.81it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  3.16it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:06<00:00,  2.27it/s][A100%|██████████| 221/221 [01:06<00:00,  3.33it/s]
09/18/2024 15:42:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 99--===========

09/18/2024 15:42:26 - INFO - __main__ -   {'area_r1': 41.0, 'area_recall': '41.0/68.7/79.1', 'area_ravg': 62.9}
09/18/2024 15:42:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 99--===========

09/18/2024 15:42:26 - INFO - __main__ -   {'forward_r1': 36.3, 'forward_recall': '36.3/64.6/75.1', 'forward_ravg': 58.7}
09/18/2024 15:42:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 99--===========

09/18/2024 15:42:26 - INFO - __main__ -   {'area_video_r1': 35.7, 'area_video_recall': '35.7/64.5/75.9', 'area_video_ravg': 58.7}
09/18/2024 15:42:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 49=======

09/18/2024 15:42:26 - INFO - __main__ -   {'area_video_r1': 36.5, 'area_video_recall': '36.5/67.1/77.8', 'area_video_ravg': 60.5}
09/18/2024 15:42:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 99--===========

09/18/2024 15:42:26 - INFO - __main__ -   {'area_video_r1': 51.4, 'area_video_recall': '51.4/73.6/82.2', 'area_video_ravg': 69.1, 'area_video_back_r1': 51.0, 'area_video_back_recall': '51.0/72.6/79.5', 'area_video_back_ravg': 67.7}
09/18/2024 15:42:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 49=======

09/18/2024 15:42:26 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 15:42:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 99--===========

09/18/2024 15:42:26 - INFO - __main__ -   {'video_r1': 31.8, 'video_recall': '31.8/57.5/68.4', 'video_ravg': 52.6}
09/18/2024 15:42:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 99=======

09/18/2024 15:42:26 - INFO - __main__ -   {'video_r1': 31.8, 'video_recall': '31.8/57.5/68.4', 'video_ravg': 52.6}
09/18/2024 15:42:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 99--===========

09/18/2024 15:42:26 - INFO - __main__ -   {'video_r1': 50.2, 'video_recall': '50.2/71.8/79.0', 'video_ravg': 67.0}
09/18/2024 15:42:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 15:42:26 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 15:42:50 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0016591264866292477, 'loss_ret%tv%ta--finetune_area/loss_area': 2.824154853820801, 'loss_ret%tv%ta--finetune_area/total_loss': 2.8258140087127686}
  3%|▎         | 100/2910 [37:43<104:13:23, 133.52s/it]  3%|▎         | 101/2910 [37:46<73:47:50, 94.58s/it]  [h264 @ 0x55cb780c1300] mmco: unref short failure
[h264 @ 0x55cb780c1300] mmco: unref short failure
  4%|▎         | 102/2910 [37:50<52:34:27, 67.40s/it]  4%|▎         | 103/2910 [37:55<37:55:40, 48.64s/it]  4%|▎         | 104/2910 [38:00<27:40:51, 35.51s/it]  4%|▎         | 105/2910 [38:06<20:37:31, 26.47s/it][h264 @ 0x5565f8348900] mmco: unref short failure
[h264 @ 0x5565f8348900] mmco: unref short failure
  4%|▎         | 106/2910 [38:11<15:48:16, 20.29s/it][h264 @ 0x5598e0fc35c0] mmco: unref short failure
[h264 @ 0x5598e0fc35c0] mmco: unref short failure
[h264 @ 0x5565fc67f200] mmco: unref short failure
[h264 @ 0x5565fc67f200] mmco: unref short failure
[h264 @ 0x55cb6f92a480] mmco: unref short failure
[h264 @ 0x55cb6f92a480] mmco: unref short failure
  4%|▎         | 107/2910 [38:16<12:13:42, 15.71s/it][h264 @ 0x55cb6c542600] mmco: unref short failure
[h264 @ 0x5598de580c00] mmco: unref short failure
  4%|▎         | 108/2910 [38:23<10:05:37, 12.97s/it]  4%|▎         | 109/2910 [38:28<8:15:53, 10.62s/it] [h264 @ 0x55cb6bdd2940] mmco: unref short failure
[h264 @ 0x55cb6bdd2940] mmco: unref short failure
[h264 @ 0x55cb7cdc3c00] mmco: unref short failure
[h264 @ 0x55cb7cdc3c00] mmco: unref short failure
[h264 @ 0x55cb7cdc3c00] mmco: unref short failure
[h264 @ 0x55cb7cdc3c00] mmco: unref short failure
  4%|▍         | 110/2910 [38:33<6:59:57,  9.00s/it]  4%|▍         | 111/2910 [38:39<6:05:48,  7.84s/it]  4%|▍         | 112/2910 [38:44<5:32:29,  7.13s/it]  4%|▍         | 113/2910 [38:49<5:03:18,  6.51s/it]  4%|▍         | 114/2910 [38:55<4:49:51,  6.22s/it][h264 @ 0x55d918952080] mmco: unref short failure
[h264 @ 0x55d918952080] mmco: unref short failure
  4%|▍         | 115/2910 [39:00<4:37:01,  5.95s/it]09/18/2024 15:44:14 - INFO - __main__ -   current idx BYJeo2oa0SU.13 from finetune_area returns wrong image/video, use 23897 instead.
[h264 @ 0x55d918a290c0] mmco: unref short failure
[h264 @ 0x55d918a290c0] mmco: unref short failure
[h264 @ 0x55d91fdd2180] mmco: unref short failure
[h264 @ 0x55d91fdd2180] mmco: unref short failure
[h264 @ 0x5565eab2d140] mmco: unref short failure
[h264 @ 0x55cb77a4a5c0] mmco: unref short failure
[h264 @ 0x556602f6f300] mmco: unref short failure
[h264 @ 0x556602f6f300] mmco: unref short failure
[h264 @ 0x55cb6d81dc40] mmco: unref short failure
[h264 @ 0x55d8ff9ab1c0] mmco: unref short failure
[h264 @ 0x5565ff690ec0] mmco: unref short failure
[h264 @ 0x5598e3f8ed00] mmco: unref short failure
[h264 @ 0x5598e3f8ed00] mmco: unref short failure
  4%|▍         | 116/2910 [39:47<14:13:19, 18.32s/it]  4%|▍         | 117/2910 [39:52<11:06:55, 14.33s/it][h264 @ 0x55cb6734f880] mmco: unref short failure
[h264 @ 0x55cb6734f880] mmco: unref short failure
[h264 @ 0x5598f9ce8600] mmco: unref short failure
[h264 @ 0x5598eefd50c0] mmco: unref short failure
[h264 @ 0x5598eefd50c0] mmco: unref short failure
  4%|▍         | 118/2910 [40:11<12:03:52, 15.56s/it][h264 @ 0x5598e082e680] mmco: unref short failure
[h264 @ 0x5598e082e680] mmco: unref short failure
[h264 @ 0x55d91fe02bc0] mmco: unref short failure
  4%|▍         | 119/2910 [40:31<13:12:30, 17.04s/it][h264 @ 0x5598f1199740] mmco: unref short failure
  4%|▍         | 120/2910 [40:36<10:24:34, 13.43s/it]  4%|▍         | 121/2910 [40:41<8:27:10, 10.91s/it]   4%|▍         | 122/2910 [40:46<7:08:49,  9.23s/it][h264 @ 0x5565ea2d6f40] mmco: unref short failure
[h264 @ 0x5565ea2d6f40] mmco: unref short failure
  4%|▍         | 123/2910 [41:01<8:22:27, 10.82s/it][h264 @ 0x5565e9f31940] mmco: unref short failure
09/18/2024 15:46:18 - INFO - __main__ -   current idx UqfcB6M8FA4.49 from finetune_area returns wrong image/video, use 7219 instead.
[h264 @ 0x55cb7c1c7900] mmco: unref short failure
[h264 @ 0x55d904705b00] mmco: unref short failure
[h264 @ 0x55d904705b00] mmco: unref short failure
[h264 @ 0x5598f24f2500] mmco: unref short failure
[h264 @ 0x5598f24f2500] mmco: unref short failure
[h264 @ 0x55d90eb3b5c0] mmco: unref short failure
[h264 @ 0x5565f5c893c0] mmco: unref short failure
[h264 @ 0x5598f784d740] mmco: unref short failure
[h264 @ 0x5598f784d740] mmco: unref short failure
[h264 @ 0x5565ee96fb80] mmco: unref short failure
[h264 @ 0x5565ee96fb80] mmco: unref short failure
[h264 @ 0x55d91926ffc0] mmco: unref short failure
[h264 @ 0x55d91926ffc0] mmco: unref short failure
[h264 @ 0x5598f9b74800] mmco: unref short failure
[h264 @ 0x55d90154a780] mmco: unref short failure
[h264 @ 0x55d90154a780] mmco: unref short failure
[h264 @ 0x55cb7552fe40] mmco: unref short failure
[h264 @ 0x55cb7552fe40] mmco: unref short failure
[h264 @ 0x5565ff690800] mmco: unref short failure
[h264 @ 0x5565ff690800] mmco: unref short failure
[h264 @ 0x5598ef10afc0] mmco: unref short failure
09/18/2024 15:47:32 - INFO - __main__ -   current idx M28z2aVJm9M.7 from finetune_area returns wrong image/video, use 148289 instead.
[h264 @ 0x5598e0c86380] mmco: unref short failure
[h264 @ 0x5598e0c86380] mmco: unref short failure
  4%|▍         | 124/2910 [42:32<27:05:10, 35.00s/it]  4%|▍         | 125/2910 [42:38<20:13:43, 26.15s/it][h264 @ 0x55cb71eff1c0] mmco: unref short failure
  4%|▍         | 126/2910 [42:44<15:40:59, 20.28s/it][h264 @ 0x5565f5c2aa40] mmco: unref short failure
[h264 @ 0x5565f5c2aa40] mmco: unref short failure
  4%|▍         | 127/2910 [42:57<13:54:04, 17.98s/it][h264 @ 0x55d9075ada40] mmco: unref short failure
[h264 @ 0x5598f4995380] mmco: unref short failure
  4%|▍         | 128/2910 [43:03<11:09:13, 14.43s/it][h264 @ 0x55cb66d755c0] mmco: unref short failure
[h264 @ 0x55cb654bec80] mmco: unref short failure
  4%|▍         | 129/2910 [43:09<9:02:33, 11.71s/it]   4%|▍         | 130/2910 [43:14<7:33:55,  9.80s/it][h264 @ 0x55cb6abfbb80] mmco: unref short failure
[h264 @ 0x55cb768283c0] mmco: unref short failure
[h264 @ 0x55cb768283c0] mmco: unref short failure
[h264 @ 0x5565f1933d40] mmco: unref short failure
[h264 @ 0x5565f1933d40] mmco: unref short failure
  5%|▍         | 131/2910 [43:25<7:52:55, 10.21s/it][h264 @ 0x55cb633e73c0] mmco: unref short failure
[h264 @ 0x5598e781a2c0] mmco: unref short failure
[h264 @ 0x5598e781a2c0] mmco: unref short failure
[h264 @ 0x55d9131a6d00] mmco: unref short failure
[h264 @ 0x55d9131a6d00] mmco: unref short failure
09/18/2024 15:49:04 - INFO - __main__ -   current idx 6_p3rXnYE3Y.137 from finetune_area returns wrong image/video, use 38831 instead.
[h264 @ 0x55d919c9a880] mmco: unref short failure
[h264 @ 0x55d919c9a880] mmco: unref short failure
[h264 @ 0x55d919c9a880] mmco: unref short failure
[h264 @ 0x55d919c9a880] mmco: unref short failure
[h264 @ 0x5598fabc5c00] mmco: unref short failure
[h264 @ 0x55d904bb1880] mmco: unref short failure
[h264 @ 0x55d904bb1880] mmco: unref short failure
[h264 @ 0x55d904bb1880] mmco: unref short failure
[h264 @ 0x5565f1933400] mmco: unref short failure
[h264 @ 0x5565f1933400] mmco: unref short failure
[h264 @ 0x55d8ffc804c0] mmco: unref short failure
[h264 @ 0x55d8ffc804c0] mmco: unref short failure
[h264 @ 0x55d91625e1c0] mmco: unref short failure
[h264 @ 0x55d91625e1c0] mmco: unref short failure
[h264 @ 0x5565ef931900] mmco: unref short failure
[h264 @ 0x5565ef931900] mmco: unref short failure
[h264 @ 0x55cb74c7d1c0] mmco: unref short failure
  5%|▍         | 132/2910 [44:58<27:03:29, 35.06s/it][h264 @ 0x55cb708a6000] mmco: unref short failure
[h264 @ 0x55cb708a6000] mmco: unref short failure
[h264 @ 0x5598f7f59300] mmco: unref short failure
[h264 @ 0x5598f7f59300] mmco: unref short failure
  5%|▍         | 133/2910 [45:03<20:06:30, 26.07s/it]  5%|▍         | 134/2910 [45:14<16:27:50, 21.35s/it][h264 @ 0x5598e2cb0200] mmco: unref short failure
[h264 @ 0x5598e2cb0200] mmco: unref short failure
[h264 @ 0x5598f5a82640] mmco: unref short failure
[h264 @ 0x55d9193076c0] mmco: unref short failure
[h264 @ 0x55d9193076c0] mmco: unref short failure
  5%|▍         | 135/2910 [45:25<14:13:48, 18.46s/it]  5%|▍         | 136/2910 [45:31<11:12:36, 14.55s/it][h264 @ 0x5566052f96c0] mmco: unref short failure
[h264 @ 0x5598ee096ec0] mmco: unref short failure
[h264 @ 0x5598ee096ec0] mmco: unref short failure
  5%|▍         | 137/2910 [45:36<9:08:23, 11.87s/it] [h264 @ 0x55cb678da140] mmco: unref short failure
  5%|▍         | 138/2910 [45:46<8:41:20, 11.28s/it][h264 @ 0x556603494a80] mmco: unref short failure
[h264 @ 0x556603494a80] mmco: unref short failure
[h264 @ 0x556603494a80] mmco: unref short failure
[h264 @ 0x556603494a80] mmco: unref short failure
[h264 @ 0x556603494a80] mmco: unref short failure
[h264 @ 0x556603494a80] mmco: unref short failure
[h264 @ 0x55d9165e9600] mmco: unref short failure
[h264 @ 0x55d9165e9600] mmco: unref short failure
  5%|▍         | 139/2910 [46:02<9:48:58, 12.75s/it][h264 @ 0x55d91af7c980] mmco: unref short failure
[h264 @ 0x55d91af7c980] mmco: unref short failure
[h264 @ 0x5565e61861c0] mmco: unref short failure
09/18/2024 15:51:29 - INFO - __main__ -   current idx fwdyHxHBek4.10 from finetune_area returns wrong image/video, use 51250 instead.
[h264 @ 0x5598f257a980] mmco: unref short failure
[h264 @ 0x5598f257a980] mmco: unref short failure
[h264 @ 0x55d91f4fe1c0] mmco: unref short failure
09/18/2024 15:51:54 - INFO - __main__ -   current idx Xlq5RVCHKWg.8 from finetune_area returns wrong image/video, use 58295 instead.
[h264 @ 0x5598e7193e80] mmco: unref short failure
09/18/2024 15:51:58 - INFO - __main__ -   current idx hvInlSH5o8c.6 from finetune_area returns wrong image/video, use 16063 instead.
[h264 @ 0x5598f2f42340] mmco: unref short failure
[h264 @ 0x55d91fc83300] mmco: unref short failure
[h264 @ 0x55d91fc83300] mmco: unref short failure
[h264 @ 0x55d91fc83300] mmco: unref short failure
[h264 @ 0x55d91fc83300] mmco: unref short failure
[h264 @ 0x55cb67834380] mmco: unref short failure
[h264 @ 0x55cb67834380] mmco: unref short failure
09/18/2024 15:52:07 - INFO - __main__ -   current idx MVC6nKCrOgI.12 from finetune_area returns wrong image/video, use 71575 instead.
[h264 @ 0x5598f365e200] mmco: unref short failure
[h264 @ 0x5598f365e200] mmco: unref short failure
[h264 @ 0x5598f365e200] mmco: unref short failure
[h264 @ 0x5598f365e200] mmco: unref short failure
[h264 @ 0x5598f265e000] mmco: unref short failure
[h264 @ 0x5598f265e000] mmco: unref short failure
[h264 @ 0x55cb65bb6c00] mmco: unref short failure
[h264 @ 0x55cb65bb6c00] mmco: unref short failure
[h264 @ 0x55cb70bd1000] mmco: unref short failure
[h264 @ 0x55cb70bd1000] mmco: unref short failure
  5%|▍         | 140/2910 [47:28<26:42:35, 34.71s/it][h264 @ 0x5565f323d800] mmco: unref short failure
[h264 @ 0x5565f323d800] mmco: unref short failure
[h264 @ 0x5565f323d800] mmco: unref short failure
[h264 @ 0x5565f323d800] mmco: unref short failure
  5%|▍         | 141/2910 [47:34<19:59:28, 25.99s/it][h264 @ 0x5598f2f42140] mmco: unref short failure
[h264 @ 0x5598f2f42140] mmco: unref short failure
[h264 @ 0x5598f2f42140] mmco: unref short failure
[h264 @ 0x5598f2f42140] mmco: unref short failure
  5%|▍         | 142/2910 [47:39<15:04:49, 19.61s/it][h264 @ 0x55d91b126f00] mmco: unref short failure
[h264 @ 0x55d91b126f00] mmco: unref short failure
[h264 @ 0x55d90b1b3e00] mmco: unref short failure
09/18/2024 15:52:58 - INFO - __main__ -   current idx dIAiCVZ5tEo.28 from finetune_area returns wrong image/video, use 21760 instead.
  5%|▍         | 143/2910 [47:50<13:03:45, 17.00s/it][h264 @ 0x5598e0835780] mmco: unref short failure
[h264 @ 0x5598e0835780] mmco: unref short failure
[h264 @ 0x55cb5f6c8b00] mmco: unref short failure
  5%|▍         | 144/2910 [47:55<10:23:32, 13.53s/it][h264 @ 0x5598f9b74800] mmco: unref short failure
  5%|▍         | 145/2910 [48:01<8:33:37, 11.15s/it] [h264 @ 0x55cb7c3cb2c0] mmco: unref short failure
  5%|▌         | 146/2910 [48:13<8:44:48, 11.39s/it][h264 @ 0x55d9212d6500] mmco: unref short failure
[h264 @ 0x55d9212d6500] mmco: unref short failure
[h264 @ 0x55d9212d6500] mmco: unref short failure
[h264 @ 0x55d90a73ad40] mmco: unref short failure
[h264 @ 0x55d90a73ad40] mmco: unref short failure
[h264 @ 0x5598f366bf00] mmco: unref short failure
[h264 @ 0x5598f366bf00] mmco: unref short failure
[h264 @ 0x55cb633d6280] mmco: unref short failure
[h264 @ 0x55cb7ea675c0] mmco: unref short failure
[h264 @ 0x55cb7ea675c0] mmco: unref short failure
[h264 @ 0x55d912cde140] mmco: unref short failure
  5%|▌         | 147/2910 [48:32<10:29:44, 13.68s/it][h264 @ 0x5598eb326180] mmco: unref short failure
[h264 @ 0x5566051bc8c0] mmco: unref short failure
[h264 @ 0x5566051bc8c0] mmco: unref short failure
[h264 @ 0x5566051bc8c0] mmco: unref short failure
[h264 @ 0x5566051bc8c0] mmco: unref short failure
[h264 @ 0x5565f82ba440] mmco: unref short failure
[h264 @ 0x5565f82ba440] mmco: unref short failure
[h264 @ 0x5565f82ba440] mmco: unref short failure
[h264 @ 0x5565f82ba440] mmco: unref short failure
[h264 @ 0x55cb77e45b40] mmco: unref short failure
[h264 @ 0x55cb77e45b40] mmco: unref short failure
09/18/2024 15:54:07 - INFO - __main__ -   current idx 7rNZakrnk74.48 from finetune_area returns wrong image/video, use 146023 instead.
[h264 @ 0x55cb7ea67840] mmco: unref short failure
[h264 @ 0x55cb7ea67840] mmco: unref short failure
[h264 @ 0x55cb7ea67840] mmco: unref short failure
[h264 @ 0x55cb7ea67840] mmco: unref short failure
[h264 @ 0x5598eb446580] mmco: unref short failure
09/18/2024 15:54:13 - INFO - __main__ -   current idx gtp43VGk4b0.59 from finetune_area returns wrong image/video, use 42619 instead.
[h264 @ 0x5598f7064840] mmco: unref short failure
[h264 @ 0x5598f7064840] mmco: unref short failure
[h264 @ 0x55d920964700] mmco: unref short failure
[h264 @ 0x55d920964700] mmco: unref short failure
[h264 @ 0x5565e5165a40] mmco: unref short failure
[h264 @ 0x55d90a6dc180] mmco: unref short failure
[h264 @ 0x55cb7ca97d80] mmco: unref short failure
[h264 @ 0x55cb7ca97d80] mmco: unref short failure
  5%|▌         | 148/2910 [50:01<28:00:49, 36.51s/it]  5%|▌         | 149/2910 [50:06<20:45:25, 27.06s/it]09/18/2024 15:55:16 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 15:55:16 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565e8b46680] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598f51debc0] mmco: unref short failure
[h264 @ 0x5598f51debc0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb692030c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x556601c572c0] mmco: unref short failure
[h264 @ 0x556601c572c0] mmco: unref short failure
[h264 @ 0x55cb7caae200] mmco: unref short failure
[h264 @ 0x5565edf19400] mmco: unref short failure
[h264 @ 0x55d921614ac0] mmco: unref short failure
[h264 @ 0x55d921614ac0] mmco: unref short failure
[h264 @ 0x55d921614ac0] mmco: unref short failure
[h264 @ 0x55cb6bdd2dc0] mmco: unref short failure
[h264 @ 0x55cb6bdd2dc0] mmco: unref short failure
[h264 @ 0x55d90ae74800] mmco: unref short failure
[h264 @ 0x55d90ae74800] mmco: unref short failure
[h264 @ 0x5565ed9e7dc0] mmco: unref short failure
[h264 @ 0x5565ed9e7dc0] mmco: unref short failure
[h264 @ 0x556606235e80] mmco: unref short failure
[h264 @ 0x55660040a000] mmco: unref short failure
[h264 @ 0x55660040a000] mmco: unref short failure
[h264 @ 0x55cb6c51b940] mmco: unref short failure
[h264 @ 0x55cb6c51b940] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:55,  1.91it/s][A
  1%|          | 2/221 [00:00<01:45,  2.07it/s][A
  1%|▏         | 3/221 [00:01<01:47,  2.03it/s][A
  2%|▏         | 4/221 [00:01<01:24,  2.58it/s][A
  2%|▏         | 5/221 [00:02<01:24,  2.57it/s][A
  3%|▎         | 6/221 [00:02<01:21,  2.64it/s][A
  3%|▎         | 7/221 [00:02<01:29,  2.38it/s][A
  4%|▎         | 8/221 [00:03<01:27,  2.42it/s][A
  4%|▍         | 9/221 [00:03<01:33,  2.28it/s][A[h264 @ 0x55cb5f5dfec0] mmco: unref short failure
[h264 @ 0x55cb5f5dfec0] mmco: unref short failure

  5%|▍         | 10/221 [00:04<01:41,  2.09it/s][A
  5%|▌         | 12/221 [00:04<01:11,  2.92it/s][A
  6%|▌         | 13/221 [00:05<01:03,  3.25it/s][A
  6%|▋         | 14/221 [00:05<01:00,  3.40it/s][A
  7%|▋         | 15/221 [00:05<00:58,  3.49it/s][A
  7%|▋         | 16/221 [00:05<01:07,  3.04it/s][A
  8%|▊         | 17/221 [00:06<01:26,  2.37it/s][A
  8%|▊         | 18/221 [00:06<01:19,  2.55it/s][A
  9%|▊         | 19/221 [00:07<01:08,  2.93it/s][A
  9%|▉         | 20/221 [00:07<00:59,  3.37it/s][A
 10%|▉         | 21/221 [00:07<00:54,  3.66it/s][A
 10%|▉         | 22/221 [00:07<01:01,  3.24it/s][A
 10%|█         | 23/221 [00:08<00:48,  4.06it/s][A
 11%|█         | 24/221 [00:08<00:44,  4.39it/s][A
 11%|█▏        | 25/221 [00:08<00:43,  4.47it/s][A
 12%|█▏        | 26/221 [00:08<00:57,  3.41it/s][A
 12%|█▏        | 27/221 [00:09<00:48,  3.99it/s][A
 13%|█▎        | 28/221 [00:09<01:15,  2.54it/s][A
 13%|█▎        | 29/221 [00:10<01:09,  2.75it/s][A
 14%|█▎        | 30/221 [00:10<01:12,  2.65it/s][A
 14%|█▍        | 31/221 [00:10<01:14,  2.53it/s][A
 14%|█▍        | 32/221 [00:11<01:04,  2.92it/s][A
 15%|█▍        | 33/221 [00:11<01:03,  2.95it/s][A
 15%|█▌        | 34/221 [00:11<00:51,  3.64it/s][A
 16%|█▌        | 35/221 [00:11<00:47,  3.92it/s][A
 16%|█▋        | 36/221 [00:12<00:54,  3.36it/s][A[h264 @ 0x5565e5b96c80] mmco: unref short failure

 17%|█▋        | 37/221 [00:12<01:00,  3.05it/s][A
 17%|█▋        | 38/221 [00:13<01:15,  2.44it/s][A
 18%|█▊        | 39/221 [00:13<00:58,  3.10it/s][A
 18%|█▊        | 40/221 [00:13<00:57,  3.13it/s][A
 19%|█▊        | 41/221 [00:13<00:55,  3.25it/s][A
 19%|█▉        | 42/221 [00:14<01:07,  2.64it/s][A
 19%|█▉        | 43/221 [00:14<00:58,  3.06it/s][A
 20%|█▉        | 44/221 [00:14<00:49,  3.60it/s][A
 20%|██        | 45/221 [00:15<01:03,  2.77it/s][A
 21%|██        | 46/221 [00:15<01:02,  2.82it/s][A[h264 @ 0x5598e3280580] mmco: unref short failure
[h264 @ 0x5598e3280580] mmco: unref short failure

 21%|██▏       | 47/221 [00:16<01:26,  2.00it/s][A
 22%|██▏       | 48/221 [00:16<01:07,  2.56it/s][A
 22%|██▏       | 49/221 [00:16<00:58,  2.92it/s][A
 23%|██▎       | 50/221 [00:17<00:54,  3.11it/s][A
 23%|██▎       | 51/221 [00:17<00:49,  3.43it/s][A
 24%|██▎       | 52/221 [00:17<00:51,  3.31it/s][A[h264 @ 0x556608171380] mmco: unref short failure
[h264 @ 0x556608171380] mmco: unref short failure

 24%|██▍       | 53/221 [00:17<00:47,  3.56it/s][A
 24%|██▍       | 54/221 [00:19<01:51,  1.50it/s][A
 25%|██▍       | 55/221 [00:20<01:40,  1.65it/s][A
 25%|██▌       | 56/221 [00:20<01:28,  1.86it/s][h264 @ 0x5598f8ef3b00] mmco: unref short failure
[h264 @ 0x5598f8ef3b00] mmco: unref short failure
[A[h264 @ 0x5598f8ef3b00] mmco: unref short failure
[h264 @ 0x5598f8ef3b00] mmco: unref short failure

 26%|██▌       | 57/221 [00:20<01:28,  1.85it/s][A
 26%|██▌       | 58/221 [00:21<01:14,  2.18it/s][A
 27%|██▋       | 59/221 [00:21<01:02,  2.58it/s][A09/18/2024 15:58:09 - INFO - __main__ -   current idx 3xMq89jLDYE.29 from finetune_area returns wrong image/video, use 48211 instead.

 27%|██▋       | 60/221 [00:22<01:32,  1.75it/s][A
 28%|██▊       | 61/221 [00:22<01:16,  2.08it/s][A
 28%|██▊       | 62/221 [00:23<01:12,  2.19it/s][A
 29%|██▊       | 63/221 [00:23<01:00,  2.63it/s][A
 29%|██▉       | 64/221 [00:23<01:09,  2.27it/s][A
 29%|██▉       | 65/221 [00:24<00:57,  2.71it/s][A
 30%|██▉       | 66/221 [00:24<01:02,  2.50it/s][A
 30%|███       | 67/221 [00:24<00:53,  2.88it/s][A
 31%|███       | 68/221 [00:24<00:45,  3.35it/s][A
 31%|███       | 69/221 [00:25<01:16,  2.00it/s][A
 32%|███▏      | 70/221 [00:26<01:10,  2.15it/s][A
 32%|███▏      | 71/221 [00:26<01:00,  2.47it/s][A
 33%|███▎      | 72/221 [00:27<01:05,  2.27it/s][A
 33%|███▎      | 73/221 [00:27<01:09,  2.12it/s][A[h264 @ 0x55cb7ff81440] mmco: unref short failure
[h264 @ 0x55cb7ff81440] mmco: unref short failure

 33%|███▎      | 74/221 [00:27<00:58,  2.53it/s][A09/18/2024 15:58:14 - INFO - __main__ -   current idx dMEgMG6TS3c.6 from finetune_area returns wrong image/video, use 15518 instead.

 34%|███▍      | 75/221 [00:28<01:04,  2.28it/s][A
 34%|███▍      | 76/221 [00:28<00:55,  2.63it/s][A
 35%|███▍      | 77/221 [00:29<00:56,  2.55it/s][A
 35%|███▌      | 78/221 [00:29<00:44,  3.21it/s][A[h264 @ 0x5598f6141240] mmco: unref short failure

 36%|███▌      | 79/221 [00:29<00:49,  2.88it/s][A
 36%|███▌      | 80/221 [00:30<00:49,  2.86it/s][A
 37%|███▋      | 81/221 [00:30<00:49,  2.83it/s][A
 37%|███▋      | 82/221 [00:30<00:51,  2.72it/s][A
 38%|███▊      | 83/221 [00:31<00:56,  2.45it/s][A
 38%|███▊      | 84/221 [00:31<00:49,  2.76it/s][A
 38%|███▊      | 85/221 [00:31<00:40,  3.38it/s][A
 39%|███▉      | 86/221 [00:31<00:35,  3.85it/s][A
 39%|███▉      | 87/221 [00:32<01:08,  1.96it/s][A
 40%|███▉      | 88/221 [00:33<01:17,  1.72it/s][A
 40%|████      | 89/221 [00:33<01:04,  2.06it/s][A
 41%|████      | 90/221 [00:34<00:56,  2.34it/s][A
 41%|████      | 91/221 [00:34<00:46,  2.83it/s][A
 42%|████▏     | 92/221 [00:34<00:41,  3.08it/s][A
 42%|████▏     | 93/221 [00:35<00:48,  2.66it/s][A
 43%|████▎     | 94/221 [00:35<00:44,  2.83it/s][A
 43%|████▎     | 95/221 [00:35<00:42,  2.94it/s][A
 43%|████▎     | 96/221 [00:36<00:55,  2.25it/s][A
 44%|████▍     | 97/221 [00:36<00:47,  2.62it/s][A[h264 @ 0x55cb5f912080] mmco: unref short failure

 44%|████▍     | 98/221 [00:37<01:02,  1.96it/s][A
 45%|████▍     | 99/221 [00:37<00:53,  2.28it/s][A
 45%|████▌     | 100/221 [00:38<00:47,  2.57it/s][A
 46%|████▌     | 101/221 [00:38<00:42,  2.83it/s][A
 46%|████▌     | 102/221 [00:38<00:47,  2.49it/s][A
 47%|████▋     | 104/221 [00:39<00:35,  3.34it/s][A
 48%|████▊     | 105/221 [00:39<00:34,  3.37it/s][A
 48%|████▊     | 106/221 [00:40<00:50,  2.28it/s][A
 48%|████▊     | 107/221 [00:40<00:46,  2.48it/s][A
 49%|████▉     | 108/221 [00:40<00:43,  2.63it/s][A
 49%|████▉     | 109/221 [00:41<00:39,  2.83it/s][A
 50%|████▉     | 110/221 [00:41<00:31,  3.50it/s][A
 50%|█████     | 111/221 [00:41<00:34,  3.20it/s][A
 51%|█████     | 112/221 [00:42<00:33,  3.22it/s][A
 51%|█████     | 113/221 [00:42<00:33,  3.22it/s][A
 52%|█████▏    | 115/221 [00:42<00:24,  4.26it/s][A
 52%|█████▏    | 116/221 [00:43<00:36,  2.91it/s][A
 53%|█████▎    | 117/221 [00:43<00:36,  2.88it/s][A
 53%|█████▎    | 118/221 [00:43<00:31,  3.27it/s][A
 54%|█████▍    | 119/221 [00:44<00:37,  2.75it/s][A
 54%|█████▍    | 120/221 [00:44<00:31,  3.22it/s][A
 55%|█████▍    | 121/221 [00:44<00:25,  3.90it/s][A
 55%|█████▌    | 122/221 [00:45<00:27,  3.63it/s][A
 56%|█████▌    | 123/221 [00:45<00:24,  4.01it/s][A
 56%|█████▌    | 124/221 [00:45<00:30,  3.23it/s][A
 57%|█████▋    | 125/221 [00:46<00:38,  2.51it/s][A
 57%|█████▋    | 126/221 [00:46<00:35,  2.65it/s][A
 57%|█████▋    | 127/221 [00:48<01:09,  1.36it/s][A
 58%|█████▊    | 128/221 [00:48<00:58,  1.59it/s][A
 59%|█████▉    | 130/221 [00:48<00:37,  2.43it/s][A
 59%|█████▉    | 131/221 [00:49<00:33,  2.73it/s][A
 60%|█████▉    | 132/221 [00:50<00:53,  1.65it/s][A
 60%|██████    | 133/221 [00:51<00:57,  1.54it/s][A
 61%|██████    | 134/221 [00:51<01:00,  1.43it/s][A
 61%|██████    | 135/221 [00:52<00:58,  1.48it/s][A
 62%|██████▏   | 136/221 [00:52<00:50,  1.69it/s][A
 62%|██████▏   | 137/221 [00:53<00:42,  1.99it/s][A
 62%|██████▏   | 138/221 [00:53<00:39,  2.10it/s][A
 63%|██████▎   | 139/221 [00:54<00:45,  1.82it/s][A
 63%|██████▎   | 140/221 [00:54<00:40,  2.01it/s][A
 64%|██████▍   | 141/221 [00:55<00:36,  2.20it/s][A
 64%|██████▍   | 142/221 [00:55<00:33,  2.37it/s][A
 65%|██████▍   | 143/221 [00:55<00:32,  2.37it/s][A
 65%|██████▌   | 144/221 [00:56<00:30,  2.53it/s][A
 66%|██████▌   | 145/221 [00:56<00:24,  3.14it/s][A
 66%|██████▌   | 146/221 [00:56<00:18,  3.95it/s][A
 67%|██████▋   | 147/221 [00:56<00:20,  3.64it/s][A
 67%|██████▋   | 148/221 [00:57<00:23,  3.10it/s][A
 67%|██████▋   | 149/221 [00:57<00:27,  2.66it/s][A
 68%|██████▊   | 150/221 [00:57<00:23,  3.04it/s][A
 68%|██████▊   | 151/221 [00:58<00:28,  2.48it/s][A
 69%|██████▉   | 152/221 [00:59<00:40,  1.69it/s][A
 69%|██████▉   | 153/221 [00:59<00:30,  2.24it/s][A
 70%|██████▉   | 154/221 [00:59<00:25,  2.60it/s][A
 70%|███████   | 155/221 [01:00<00:22,  2.93it/s][A
 71%|███████   | 156/221 [01:00<00:24,  2.69it/s][A
 71%|███████   | 157/221 [01:01<00:26,  2.39it/s][A
 71%|███████▏  | 158/221 [01:01<00:23,  2.67it/s][A
 72%|███████▏  | 159/221 [01:01<00:18,  3.27it/s][A
 72%|███████▏  | 160/221 [01:01<00:16,  3.74it/s][A
 73%|███████▎  | 161/221 [01:01<00:14,  4.24it/s][A
 73%|███████▎  | 162/221 [01:01<00:11,  5.01it/s][A[h264 @ 0x55d921f4e140] mmco: unref short failure

 74%|███████▍  | 163/221 [01:02<00:13,  4.37it/s][A
 74%|███████▍  | 164/221 [01:02<00:13,  4.21it/s][A
 75%|███████▍  | 165/221 [01:02<00:12,  4.38it/s][A
 75%|███████▌  | 166/221 [01:03<00:18,  2.91it/s][A
 76%|███████▌  | 167/221 [01:03<00:14,  3.64it/s][A
 76%|███████▌  | 168/221 [01:04<00:21,  2.52it/s][A
 76%|███████▋  | 169/221 [01:04<00:16,  3.22it/s][A
 77%|███████▋  | 170/221 [01:04<00:15,  3.35it/s][A
 77%|███████▋  | 171/221 [01:05<00:17,  2.81it/s][A
 78%|███████▊  | 172/221 [01:05<00:15,  3.24it/s][A
 78%|███████▊  | 173/221 [01:05<00:17,  2.71it/s][A
 79%|███████▊  | 174/221 [01:05<00:14,  3.14it/s][A
 79%|███████▉  | 175/221 [01:06<00:14,  3.07it/s][A
 80%|███████▉  | 176/221 [01:06<00:12,  3.67it/s][A
 80%|████████  | 177/221 [01:06<00:11,  3.93it/s][A
 81%|████████  | 178/221 [01:07<00:19,  2.25it/s][A
 81%|████████  | 179/221 [01:07<00:18,  2.31it/s][A
 81%|████████▏ | 180/221 [01:08<00:14,  2.89it/s][A
 82%|████████▏ | 182/221 [01:08<00:10,  3.76it/s][A[h264 @ 0x55d91403ab80] mmco: unref short failure
[h264 @ 0x55d91403ab80] mmco: unref short failure

 83%|████████▎ | 183/221 [01:08<00:10,  3.53it/s][A
 83%|████████▎ | 184/221 [01:09<00:11,  3.14it/s][A
 84%|████████▍ | 186/221 [01:09<00:11,  3.01it/s][A
 85%|████████▍ | 187/221 [01:10<00:10,  3.23it/s][A
 85%|████████▌ | 188/221 [01:10<00:09,  3.41it/s][A
 86%|████████▌ | 189/221 [01:10<00:09,  3.22it/s][A
 86%|████████▌ | 190/221 [01:11<00:10,  2.93it/s][A
 86%|████████▋ | 191/221 [01:11<00:08,  3.50it/s][A
 87%|████████▋ | 192/221 [01:11<00:07,  3.66it/s][A[h264 @ 0x55d916e4de00] mmco: unref short failure

 88%|████████▊ | 194/221 [01:13<00:15,  1.71it/s][A
 88%|████████▊ | 195/221 [01:13<00:12,  2.04it/s][A
 89%|████████▊ | 196/221 [01:13<00:10,  2.38it/s][A
 89%|████████▉ | 197/221 [01:14<00:09,  2.56it/s][A
 90%|████████▉ | 198/221 [01:14<00:07,  2.90it/s][A
 90%|█████████ | 199/221 [01:14<00:07,  3.04it/s][A
 90%|█████████ | 200/221 [01:14<00:06,  3.46it/s][A
 91%|█████████ | 201/221 [01:15<00:05,  3.93it/s][A
 91%|█████████▏| 202/221 [01:15<00:05,  3.66it/s][A
 92%|█████████▏| 203/221 [01:15<00:05,  3.56it/s][A
 92%|█████████▏| 204/221 [01:15<00:04,  3.74it/s][A
 93%|█████████▎| 205/221 [01:16<00:03,  4.25it/s][A
 93%|█████████▎| 206/221 [01:16<00:05,  2.96it/s][A
 94%|█████████▍| 208/221 [01:17<00:03,  3.40it/s][A
 95%|█████████▍| 209/221 [01:17<00:03,  3.45it/s][A
 95%|█████████▌| 211/221 [01:17<00:02,  3.79it/s][A
 96%|█████████▌| 212/221 [01:18<00:02,  3.71it/s][A
 96%|█████████▋| 213/221 [01:18<00:02,  3.98it/s][A
 97%|█████████▋| 214/221 [01:18<00:02,  2.79it/s][A
 97%|█████████▋| 215/221 [01:19<00:01,  3.23it/s][A
 98%|█████████▊| 216/221 [01:19<00:01,  3.18it/s][A
 98%|█████████▊| 217/221 [01:19<00:01,  2.84it/s][A
 99%|█████████▊| 218/221 [01:20<00:01,  2.89it/s][A
 99%|█████████▉| 219/221 [01:20<00:00,  2.91it/s][A09/18/2024 15:59:08 - INFO - __main__ -   current idx DYY8KovKO3M.52 from finetune_area returns wrong image/video, use 146096 instead.

100%|█████████▉| 220/221 [01:21<00:00,  2.08it/s][A
100%|██████████| 221/221 [01:21<00:00,  2.70it/s][A100%|██████████| 221/221 [01:21<00:00,  2.71it/s]
[h264 @ 0x5565f708f600] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:11,  3.06it/s][A
  1%|          | 2/221 [00:00<01:10,  3.12it/s][A
  1%|▏         | 3/221 [00:00<01:08,  3.20it/s][A
  2%|▏         | 4/221 [00:01<01:07,  3.23it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.24it/s][A
  3%|▎         | 6/221 [00:01<01:06,  3.26it/s][A
  3%|▎         | 7/221 [00:02<01:05,  3.26it/s][A
  4%|▎         | 8/221 [00:02<01:05,  3.23it/s][A
  4%|▍         | 9/221 [00:02<01:05,  3.25it/s][A
  5%|▍         | 10/221 [00:03<01:04,  3.26it/s][A
  5%|▍         | 11/221 [00:03<01:04,  3.25it/s][A
  5%|▌         | 12/221 [00:03<01:04,  3.26it/s][A
  6%|▌         | 13/221 [00:04<01:05,  3.18it/s][A
  6%|▋         | 14/221 [00:04<01:04,  3.21it/s][A
  7%|▋         | 15/221 [00:04<01:03,  3.24it/s][A
  7%|▋         | 16/221 [00:04<01:03,  3.25it/s][A
  8%|▊         | 17/221 [00:05<01:03,  3.22it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.23it/s][A
  9%|▊         | 19/221 [00:05<01:02,  3.25it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.26it/s][A
 10%|▉         | 21/221 [00:06<01:01,  3.23it/s][A
 10%|▉         | 22/221 [00:06<01:01,  3.25it/s][A
 10%|█         | 23/221 [00:07<01:02,  3.18it/s][A
 11%|█         | 24/221 [00:07<01:01,  3.21it/s][A
 11%|█▏        | 25/221 [00:07<01:01,  3.21it/s][A
 12%|█▏        | 26/221 [00:08<01:00,  3.20it/s][A
 12%|█▏        | 27/221 [00:08<01:00,  3.23it/s][A
 13%|█▎        | 28/221 [00:08<00:59,  3.25it/s][A
 13%|█▎        | 29/221 [00:08<00:58,  3.26it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.27it/s][A
 14%|█▍        | 31/221 [00:09<00:57,  3.28it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.28it/s][A
 15%|█▍        | 33/221 [00:10<00:57,  3.29it/s][A
 15%|█▌        | 34/221 [00:10<00:56,  3.29it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.29it/s][A
 16%|█▋        | 36/221 [00:11<00:56,  3.29it/s][A
 17%|█▋        | 37/221 [00:11<00:55,  3.30it/s][A
 17%|█▋        | 38/221 [00:11<00:55,  3.30it/s][A
 18%|█▊        | 39/221 [00:12<00:55,  3.30it/s][A
 18%|█▊        | 40/221 [00:12<00:54,  3.30it/s][A
 19%|█▊        | 41/221 [00:12<00:54,  3.30it/s][A
 19%|█▉        | 42/221 [00:12<00:54,  3.30it/s][A
 19%|█▉        | 43/221 [00:13<00:53,  3.30it/s][A
 20%|█▉        | 44/221 [00:13<00:53,  3.30it/s][A
 20%|██        | 45/221 [00:13<00:53,  3.30it/s][A
 21%|██        | 46/221 [00:14<00:53,  3.30it/s][A
 21%|██▏       | 47/221 [00:14<00:52,  3.30it/s][A
 22%|██▏       | 48/221 [00:14<00:52,  3.30it/s][A
 22%|██▏       | 49/221 [00:15<00:52,  3.30it/s][A
 23%|██▎       | 50/221 [00:15<00:51,  3.30it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.30it/s][A
 24%|██▎       | 52/221 [00:15<00:51,  3.30it/s][A
 24%|██▍       | 53/221 [00:16<00:50,  3.30it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.30it/s][A
 25%|██▍       | 55/221 [00:16<00:50,  3.30it/s][A
 25%|██▌       | 56/221 [00:17<00:49,  3.30it/s][A
 26%|██▌       | 57/221 [00:17<00:49,  3.30it/s][A
 26%|██▌       | 58/221 [00:17<00:49,  3.30it/s][A
 27%|██▋       | 59/221 [00:18<00:49,  3.30it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.30it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.30it/s][A
 28%|██▊       | 62/221 [00:18<00:48,  3.30it/s][A
 29%|██▊       | 63/221 [00:19<00:47,  3.30it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.30it/s][A
 29%|██▉       | 65/221 [00:19<00:47,  3.30it/s][A
 30%|██▉       | 66/221 [00:20<00:46,  3.30it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.30it/s][A
 31%|███       | 68/221 [00:20<00:46,  3.30it/s][A
 31%|███       | 69/221 [00:21<00:45,  3.31it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.31it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.31it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.31it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 76/221 [00:23<00:43,  3.31it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.31it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.31it/s][A
 36%|███▌      | 79/221 [00:24<00:42,  3.31it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 81/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.31it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 85/221 [00:25<00:41,  3.31it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.31it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.31it/s][A
 40%|███▉      | 88/221 [00:26<00:40,  3.31it/s][A
 40%|████      | 89/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 91/221 [00:27<00:39,  3.31it/s][A
 42%|████▏     | 92/221 [00:28<00:38,  3.31it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:29<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:33<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:35<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:36<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:39<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:41<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:43<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:46<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:47<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:49<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:03<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.30it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:39,  5.57it/s][A
  1%|          | 2/221 [00:00<01:08,  3.17it/s][A
  1%|▏         | 3/221 [00:00<01:02,  3.51it/s][A
  2%|▏         | 4/221 [00:01<00:57,  3.80it/s][A
  2%|▏         | 5/221 [00:01<00:57,  3.79it/s][A
  3%|▎         | 7/221 [00:01<00:47,  4.55it/s][A
  4%|▎         | 8/221 [00:01<00:49,  4.33it/s][A
  4%|▍         | 9/221 [00:02<00:57,  3.70it/s][A
  5%|▍         | 10/221 [00:03<01:28,  2.38it/s][A
  5%|▍         | 11/221 [00:03<01:15,  2.77it/s][A
  5%|▌         | 12/221 [00:03<01:07,  3.12it/s][A
  6%|▌         | 13/221 [00:04<01:21,  2.55it/s][A
  6%|▋         | 14/221 [00:04<01:17,  2.66it/s][A
  7%|▋         | 15/221 [00:04<01:05,  3.15it/s][A
  7%|▋         | 16/221 [00:04<01:02,  3.26it/s][A
  8%|▊         | 17/221 [00:05<01:34,  2.16it/s][A
  8%|▊         | 18/221 [00:06<01:27,  2.31it/s][A
  9%|▊         | 19/221 [00:06<01:35,  2.12it/s][A
 10%|▉         | 21/221 [00:06<01:05,  3.06it/s][A
 10%|▉         | 22/221 [00:07<00:57,  3.45it/s][A
 10%|█         | 23/221 [00:07<00:48,  4.06it/s][A
 11%|█         | 24/221 [00:07<00:46,  4.22it/s][A
 11%|█▏        | 25/221 [00:07<00:48,  4.06it/s][A
 12%|█▏        | 26/221 [00:07<00:46,  4.20it/s][A
 12%|█▏        | 27/221 [00:08<00:46,  4.16it/s][A
 13%|█▎        | 28/221 [00:09<01:47,  1.79it/s][A
 13%|█▎        | 29/221 [00:09<01:39,  1.92it/s][A
 14%|█▎        | 30/221 [00:10<01:30,  2.12it/s][A
 14%|█▍        | 31/221 [00:10<01:21,  2.33it/s][A
 14%|█▍        | 32/221 [00:10<01:04,  2.92it/s][A
 15%|█▍        | 33/221 [00:11<00:58,  3.22it/s][A
 15%|█▌        | 34/221 [00:11<00:50,  3.73it/s][A
 16%|█▌        | 35/221 [00:11<00:41,  4.45it/s][A
 16%|█▋        | 36/221 [00:11<00:52,  3.53it/s][A
 17%|█▋        | 37/221 [00:12<00:54,  3.37it/s][A
 17%|█▋        | 38/221 [00:12<01:02,  2.94it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.22it/s][A
 18%|█▊        | 40/221 [00:13<00:53,  3.36it/s][A
 19%|█▊        | 41/221 [00:13<00:45,  3.96it/s][A
 19%|█▉        | 42/221 [00:13<00:52,  3.43it/s][A
 19%|█▉        | 43/221 [00:13<00:58,  3.04it/s][A
 20%|█▉        | 44/221 [00:14<00:46,  3.77it/s][A
 20%|██        | 45/221 [00:14<00:55,  3.15it/s][A
 21%|██        | 46/221 [00:14<00:52,  3.34it/s][A
 21%|██▏       | 47/221 [00:15<00:51,  3.41it/s][A
 22%|██▏       | 48/221 [00:15<00:42,  4.03it/s][A
 22%|██▏       | 49/221 [00:15<00:37,  4.59it/s][A
 23%|██▎       | 50/221 [00:15<00:50,  3.39it/s][A
 23%|██▎       | 51/221 [00:16<00:46,  3.64it/s][A
 24%|██▎       | 52/221 [00:16<00:38,  4.42it/s][A
 24%|██▍       | 53/221 [00:16<00:37,  4.51it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.31it/s][A
 25%|██▍       | 55/221 [00:17<00:42,  3.88it/s][A
 25%|██▌       | 56/221 [00:17<00:36,  4.48it/s][A
 26%|██▌       | 57/221 [00:17<00:33,  4.94it/s][A
 26%|██▌       | 58/221 [00:17<00:39,  4.09it/s][A
 27%|██▋       | 59/221 [00:17<00:41,  3.87it/s][A
 27%|██▋       | 60/221 [00:18<00:54,  2.94it/s][A
 28%|██▊       | 61/221 [00:18<00:47,  3.35it/s][A
 28%|██▊       | 62/221 [00:18<00:41,  3.82it/s][A
 29%|██▊       | 63/221 [00:19<00:43,  3.60it/s][A
 29%|██▉       | 64/221 [00:19<00:44,  3.51it/s][A
 29%|██▉       | 65/221 [00:19<00:50,  3.07it/s][A
 30%|██▉       | 66/221 [00:20<00:40,  3.80it/s][A
 30%|███       | 67/221 [00:20<00:43,  3.55it/s][A
 31%|███       | 68/221 [00:20<00:35,  4.30it/s][A
 32%|███▏      | 70/221 [00:20<00:28,  5.38it/s][A
 32%|███▏      | 71/221 [00:21<00:36,  4.06it/s][A
 33%|███▎      | 72/221 [00:21<00:42,  3.53it/s][A
 33%|███▎      | 73/221 [00:21<00:46,  3.16it/s][A
 33%|███▎      | 74/221 [00:22<00:38,  3.81it/s][A
 34%|███▍      | 75/221 [00:22<00:40,  3.57it/s][A
 34%|███▍      | 76/221 [00:22<00:36,  3.96it/s][A
 35%|███▍      | 77/221 [00:22<00:39,  3.62it/s][A
 35%|███▌      | 78/221 [00:23<00:36,  3.87it/s][A
 36%|███▌      | 79/221 [00:23<00:46,  3.06it/s][A
 36%|███▌      | 80/221 [00:23<00:37,  3.77it/s][A
 37%|███▋      | 81/221 [00:23<00:34,  4.10it/s][A
 37%|███▋      | 82/221 [00:24<00:40,  3.43it/s][A
 38%|███▊      | 83/221 [00:24<00:40,  3.39it/s][A
 38%|███▊      | 84/221 [00:25<00:48,  2.84it/s][A
 38%|███▊      | 85/221 [00:25<00:41,  3.26it/s][A
 39%|███▉      | 86/221 [00:25<00:38,  3.50it/s][A
 39%|███▉      | 87/221 [00:26<00:45,  2.95it/s][A
 40%|███▉      | 88/221 [00:26<00:47,  2.83it/s][A
 40%|████      | 89/221 [00:26<00:48,  2.73it/s][A
 41%|████      | 90/221 [00:27<00:46,  2.85it/s][A
 41%|████      | 91/221 [00:27<00:40,  3.23it/s][A
 42%|████▏     | 92/221 [00:27<00:40,  3.19it/s][A
 43%|████▎     | 94/221 [00:27<00:29,  4.34it/s][A
 43%|████▎     | 95/221 [00:28<00:32,  3.87it/s][A
 43%|████▎     | 96/221 [00:28<00:31,  3.92it/s][A
 44%|████▍     | 98/221 [00:29<00:31,  3.92it/s][A
 45%|████▍     | 99/221 [00:29<00:34,  3.56it/s][A
 45%|████▌     | 100/221 [00:29<00:34,  3.50it/s][A
 46%|████▌     | 101/221 [00:30<00:38,  3.14it/s][A
 46%|████▌     | 102/221 [00:30<00:33,  3.55it/s][A
 47%|████▋     | 103/221 [00:30<00:28,  4.12it/s][A
 47%|████▋     | 104/221 [00:30<00:31,  3.66it/s][A
 48%|████▊     | 105/221 [00:31<00:30,  3.86it/s][A
 48%|████▊     | 106/221 [00:31<00:27,  4.11it/s][A
 48%|████▊     | 107/221 [00:31<00:27,  4.13it/s][A
 49%|████▉     | 108/221 [00:31<00:25,  4.49it/s][A
 49%|████▉     | 109/221 [00:32<00:29,  3.80it/s][A
 50%|████▉     | 110/221 [00:32<00:33,  3.27it/s][A
 50%|█████     | 111/221 [00:33<00:44,  2.49it/s][A
 51%|█████     | 113/221 [00:33<00:31,  3.47it/s][A
 52%|█████▏    | 115/221 [00:33<00:22,  4.79it/s][A
 52%|█████▏    | 116/221 [00:33<00:21,  4.86it/s][A
 53%|█████▎    | 117/221 [00:34<00:25,  4.01it/s][A
 53%|█████▎    | 118/221 [00:34<00:29,  3.51it/s][A
 54%|█████▍    | 119/221 [00:35<00:36,  2.80it/s][A
 54%|█████▍    | 120/221 [00:35<00:33,  2.97it/s][A
 55%|█████▍    | 121/221 [00:35<00:29,  3.39it/s][A
 55%|█████▌    | 122/221 [00:35<00:28,  3.48it/s][A
 56%|█████▌    | 123/221 [00:35<00:25,  3.77it/s][A
 56%|█████▌    | 124/221 [00:36<00:31,  3.06it/s][A
 57%|█████▋    | 125/221 [00:36<00:32,  2.93it/s][A
 57%|█████▋    | 126/221 [00:37<00:27,  3.50it/s][A
 57%|█████▋    | 127/221 [00:37<00:33,  2.81it/s][A
 58%|█████▊    | 128/221 [00:37<00:29,  3.11it/s][A
 58%|█████▊    | 129/221 [00:37<00:26,  3.50it/s][A
 59%|█████▉    | 130/221 [00:38<00:28,  3.20it/s][A
 60%|█████▉    | 132/221 [00:38<00:19,  4.60it/s][A
 60%|██████    | 133/221 [00:38<00:20,  4.33it/s][A
 61%|██████    | 134/221 [00:39<00:22,  3.89it/s][A
 61%|██████    | 135/221 [00:39<00:24,  3.49it/s][A
 62%|██████▏   | 136/221 [00:39<00:25,  3.37it/s][A
 62%|██████▏   | 137/221 [00:39<00:21,  3.93it/s][A
 62%|██████▏   | 138/221 [00:40<00:22,  3.61it/s][A
 63%|██████▎   | 139/221 [00:40<00:27,  2.93it/s][A
 63%|██████▎   | 140/221 [00:41<00:28,  2.80it/s][A
 64%|██████▍   | 141/221 [00:41<00:26,  2.99it/s][A
 64%|██████▍   | 142/221 [00:41<00:28,  2.80it/s][A
 65%|██████▍   | 143/221 [00:42<00:29,  2.61it/s][A
 65%|██████▌   | 144/221 [00:42<00:23,  3.29it/s][A
 66%|██████▌   | 145/221 [00:42<00:20,  3.80it/s][A
 66%|██████▌   | 146/221 [00:42<00:19,  3.87it/s][A
 67%|██████▋   | 147/221 [00:42<00:15,  4.73it/s][A
 67%|██████▋   | 148/221 [00:43<00:15,  4.70it/s][A
 67%|██████▋   | 149/221 [00:43<00:13,  5.30it/s][A
 68%|██████▊   | 150/221 [00:43<00:14,  4.93it/s][A
 68%|██████▊   | 151/221 [00:43<00:14,  4.92it/s][A
 69%|██████▉   | 152/221 [00:44<00:24,  2.81it/s][A
 69%|██████▉   | 153/221 [00:44<00:24,  2.76it/s][A
 70%|██████▉   | 154/221 [00:45<00:26,  2.50it/s][A
 70%|███████   | 155/221 [00:45<00:27,  2.44it/s][A
 71%|███████   | 156/221 [00:46<00:35,  1.85it/s][A
 71%|███████   | 157/221 [00:46<00:29,  2.16it/s][A
 71%|███████▏  | 158/221 [00:47<00:23,  2.63it/s][A
 72%|███████▏  | 159/221 [00:47<00:21,  2.86it/s][A
 72%|███████▏  | 160/221 [00:47<00:18,  3.36it/s][A
 73%|███████▎  | 161/221 [00:47<00:14,  4.15it/s][A
 73%|███████▎  | 162/221 [00:47<00:11,  4.97it/s][A
 74%|███████▍  | 163/221 [00:48<00:13,  4.38it/s][A
 74%|███████▍  | 164/221 [00:48<00:16,  3.56it/s][A
 75%|███████▍  | 165/221 [00:48<00:13,  4.01it/s][A
 75%|███████▌  | 166/221 [00:49<00:18,  2.97it/s][A
 76%|███████▌  | 167/221 [00:49<00:16,  3.30it/s][A
 76%|███████▌  | 168/221 [00:49<00:15,  3.49it/s][A
 76%|███████▋  | 169/221 [00:49<00:13,  3.86it/s][A
 77%|███████▋  | 170/221 [00:50<00:16,  3.04it/s][A
 77%|███████▋  | 171/221 [00:50<00:15,  3.18it/s][A
 78%|███████▊  | 172/221 [00:50<00:12,  3.81it/s][A
 78%|███████▊  | 173/221 [00:51<00:12,  3.83it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.40it/s][A
 79%|███████▉  | 175/221 [00:51<00:13,  3.39it/s][A
 80%|███████▉  | 176/221 [00:51<00:12,  3.65it/s][A
 80%|████████  | 177/221 [00:52<00:12,  3.49it/s][A
 81%|████████  | 178/221 [00:52<00:15,  2.79it/s][A
 81%|████████  | 179/221 [00:52<00:13,  3.14it/s][A
 81%|████████▏ | 180/221 [00:53<00:11,  3.59it/s][A
 82%|████████▏ | 181/221 [00:53<00:11,  3.62it/s][A
 82%|████████▏ | 182/221 [00:53<00:10,  3.81it/s][A
 83%|████████▎ | 183/221 [00:53<00:09,  4.13it/s][A
 83%|████████▎ | 184/221 [00:54<00:11,  3.11it/s][A
 84%|████████▎ | 185/221 [00:54<00:11,  3.17it/s][A
 85%|████████▍ | 187/221 [00:54<00:07,  4.72it/s][A
 85%|████████▌ | 188/221 [00:55<00:08,  4.02it/s][A
 86%|████████▌ | 189/221 [00:55<00:09,  3.25it/s][A
 86%|████████▌ | 190/221 [00:56<00:11,  2.69it/s][A
 86%|████████▋ | 191/221 [00:56<00:09,  3.06it/s][A
 87%|████████▋ | 192/221 [00:56<00:08,  3.23it/s][A
 87%|████████▋ | 193/221 [00:56<00:07,  3.85it/s][A
 88%|████████▊ | 194/221 [00:57<00:10,  2.59it/s][A
 88%|████████▊ | 195/221 [00:58<00:10,  2.42it/s][A
 89%|████████▊ | 196/221 [00:58<00:10,  2.30it/s][A
 89%|████████▉ | 197/221 [00:58<00:09,  2.47it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  2.96it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  3.72it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.18it/s][A
 91%|█████████ | 201/221 [00:59<00:06,  3.07it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.49it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.14it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.06it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.88it/s][A
 93%|█████████▎| 206/221 [01:00<00:03,  4.06it/s][A
 94%|█████████▎| 207/221 [01:01<00:03,  4.65it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  4.07it/s][A
 95%|█████████▍| 209/221 [01:02<00:04,  2.76it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.64it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.58it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  2.80it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.07it/s][A
 97%|█████████▋| 215/221 [01:03<00:02,  2.86it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  3.16it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.22it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.55it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  3.73it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.74it/s][A
100%|██████████| 221/221 [01:05<00:00,  2.90it/s][A100%|██████████| 221/221 [01:05<00:00,  3.37it/s]
09/18/2024 16:01:27 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 149--===========

09/18/2024 16:01:27 - INFO - __main__ -   {'area_r1': 42.4, 'area_recall': '42.4/69.8/79.8', 'area_ravg': 64.0}
09/18/2024 16:01:27 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 149--===========

09/18/2024 16:01:27 - INFO - __main__ -   {'forward_r1': 38.5, 'forward_recall': '38.5/65.3/75.9', 'forward_ravg': 59.9}
09/18/2024 16:01:27 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 149--===========

09/18/2024 16:01:27 - INFO - __main__ -   {'area_video_r1': 38.0, 'area_video_recall': '38.0/65.8/76.0', 'area_video_ravg': 60.0}
09/18/2024 16:01:27 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 149=======

09/18/2024 16:01:27 - INFO - __main__ -   {'area_video_r1': 38.0, 'area_video_recall': '38.0/65.8/76.0', 'area_video_ravg': 60.0}
09/18/2024 16:01:27 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 149--===========

09/18/2024 16:01:27 - INFO - __main__ -   {'area_video_r1': 49.7, 'area_video_recall': '49.7/72.7/82.0', 'area_video_ravg': 68.1, 'area_video_back_r1': 46.7, 'area_video_back_recall': '46.7/70.1/78.1', 'area_video_back_ravg': 65.0}
09/18/2024 16:01:27 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 49=======

09/18/2024 16:01:27 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 16:01:27 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 149--===========

09/18/2024 16:01:27 - INFO - __main__ -   {'video_r1': 31.8, 'video_recall': '31.8/58.6/68.8', 'video_ravg': 53.1}
09/18/2024 16:01:27 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 149=======

09/18/2024 16:01:27 - INFO - __main__ -   {'video_r1': 31.8, 'video_recall': '31.8/58.6/68.8', 'video_ravg': 53.1}
09/18/2024 16:01:27 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 149--===========

09/18/2024 16:01:27 - INFO - __main__ -   {'video_r1': 49.1, 'video_recall': '49.1/70.9/79.2', 'video_ravg': 66.4}
09/18/2024 16:01:27 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 16:01:27 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 16:01:57 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0007816599681973457, 'loss_ret%tv%ta--finetune_area/loss_area': 2.5701022148132324, 'loss_ret%tv%ta--finetune_area/total_loss': 2.5708839893341064}
[h264 @ 0x55d8ffd01800] mmco: unref short failure
[h264 @ 0x55d8ffd01800] mmco: unref short failure
  5%|▌         | 150/2910 [56:50<107:17:55, 139.95s/it][h264 @ 0x55d8fe97ae40] mmco: unref short failure
[h264 @ 0x55d8fe97ae40] mmco: unref short failure
[h264 @ 0x55d8fe97ae40] mmco: unref short failure
[h264 @ 0x55d8fe97ae40] mmco: unref short failure
[h264 @ 0x55d8fe97ae40] mmco: unref short failure
[h264 @ 0x55d90f30cfc0] mmco: unref short failure
09/18/2024 16:02:02 - INFO - __main__ -   current idx rGthtRZl8B0.21 from finetune_area returns wrong image/video, use 30520 instead.
  5%|▌         | 151/2910 [56:53<75:53:57, 99.04s/it]    5%|▌         | 152/2910 [56:57<53:59:28, 70.47s/it][h264 @ 0x5565e70bc980] mmco: unref short failure
  5%|▌         | 153/2910 [57:01<38:43:50, 50.57s/it]  5%|▌         | 154/2910 [57:06<28:10:11, 36.80s/it]  5%|▌         | 155/2910 [57:11<20:46:19, 27.14s/it][h264 @ 0x5565e5a62d00] mmco: unref short failure
  5%|▌         | 156/2910 [57:16<15:45:07, 20.59s/it][h264 @ 0x5598f49ed4c0] mmco: unref short failure
  5%|▌         | 157/2910 [57:22<12:26:00, 16.26s/it][h264 @ 0x5598e1faf740] mmco: unref short failure
  5%|▌         | 158/2910 [57:28<10:00:24, 13.09s/it][h264 @ 0x55d917d4cb80] mmco: unref short failure
[h264 @ 0x55d917d4cb80] mmco: unref short failure
[h264 @ 0x55d917d4cb80] mmco: unref short failure
[h264 @ 0x55d917d4cb80] mmco: unref short failure
[h264 @ 0x55cb6271ca80] mmco: unref short failure
[h264 @ 0x55cb6271ca80] mmco: unref short failure
  5%|▌         | 159/2910 [57:33<8:13:14, 10.76s/it]   5%|▌         | 160/2910 [57:38<6:47:16,  8.89s/it][h264 @ 0x55cb74fab300] mmco: unref short failure
[h264 @ 0x55cb74fab300] mmco: unref short failure
  6%|▌         | 161/2910 [57:43<5:56:38,  7.78s/it]  6%|▌         | 162/2910 [57:48<5:19:31,  6.98s/it]  6%|▌         | 163/2910 [57:53<4:59:14,  6.54s/it]  6%|▌         | 164/2910 [57:59<4:48:50,  6.31s/it][h264 @ 0x55d91c3ef380] mmco: unref short failure
  6%|▌         | 165/2910 [58:04<4:31:10,  5.93s/it][h264 @ 0x5598dc1b7800] mmco: unref short failure
[h264 @ 0x5598dc1b7800] mmco: unref short failure
[h264 @ 0x55d8ff453880] mmco: unref short failure
[h264 @ 0x55d8ff453880] mmco: unref short failure
[h264 @ 0x5565eda94bc0] mmco: unref short failure
[h264 @ 0x5565eda94bc0] mmco: unref short failure
[h264 @ 0x5598fbf58140] mmco: unref short failure
[h264 @ 0x5598fbf58140] mmco: unref short failure
[h264 @ 0x5565ea11c5c0] mmco: unref short failure
[h264 @ 0x5565ea11c5c0] mmco: unref short failure
[h264 @ 0x5598e3df1340] mmco: unref short failure
[h264 @ 0x5598e3df1340] mmco: unref short failure
[h264 @ 0x5565e5a184c0] mmco: unref short failure
[h264 @ 0x5598e044c6c0] mmco: unref short failure
[h264 @ 0x5598e044c6c0] mmco: unref short failure
[h264 @ 0x5598e1148ec0] mmco: unref short failure
[h264 @ 0x5598e1148ec0] mmco: unref short failure
[h264 @ 0x5598e1148ec0] mmco: unref short failure
[h264 @ 0x5598e1148ec0] mmco: unref short failure
[h264 @ 0x5565e5f2e080] mmco: unref short failure
[h264 @ 0x5565e5f2e080] mmco: unref short failure
[h264 @ 0x5598dcfb4740] mmco: unref short failure
[h264 @ 0x5598dcfb4740] mmco: unref short failure
[h264 @ 0x5598dcfb4740] mmco: unref short failure
[h264 @ 0x5598dcfb4740] mmco: unref short failure
[h264 @ 0x5565ebfbfe00] mmco: unref short failure
  6%|▌         | 166/2910 [58:55<14:42:24, 19.29s/it][h264 @ 0x55cb63fc65c0] mmco: unref short failure
[h264 @ 0x55cb5fb34540] mmco: unref short failure
[h264 @ 0x55cb5fb34540] mmco: unref short failure
  6%|▌         | 167/2910 [59:07<13:00:35, 17.07s/it][h264 @ 0x556601f13800] mmco: unref short failure
[h264 @ 0x55d9095cccc0] mmco: unref short failure
[h264 @ 0x55d9095cccc0] mmco: unref short failure
09/18/2024 16:04:27 - INFO - __main__ -   current idx 6wN4IYAiKIg.70 from finetune_area returns wrong image/video, use 15062 instead.
[h264 @ 0x55cb67746580] mmco: unref short failure
[h264 @ 0x55cb67746580] mmco: unref short failure
[h264 @ 0x55cb7287db80] mmco: unref short failure
[h264 @ 0x55d90955c8c0] mmco: unref short failure
  6%|▌         | 168/2910 [59:29<14:19:47, 18.81s/it][h264 @ 0x55d8ff6693c0] mmco: unref short failure
[h264 @ 0x55d8ff6693c0] mmco: unref short failure
  6%|▌         | 169/2910 [59:35<11:14:41, 14.77s/it][h264 @ 0x55cb60cd8fc0] mmco: unref short failure
[h264 @ 0x55cb60cd8fc0] mmco: unref short failure
[h264 @ 0x5565f1bd4a40] mmco: unref short failure
  6%|▌         | 170/2910 [59:50<11:23:00, 14.96s/it][h264 @ 0x55cb5f470400] mmco: unref short failure
[h264 @ 0x55cb5f470400] mmco: unref short failure
  6%|▌         | 171/2910 [1:00:00<10:15:04, 13.47s/it][h264 @ 0x55d9047d0380] mmco: unref short failure
09/18/2024 16:05:13 - INFO - __main__ -   current idx nCOpzvA0wgE.12 from finetune_area returns wrong image/video, use 6319 instead.
  6%|▌         | 172/2910 [1:00:06<8:35:35, 11.30s/it] [h264 @ 0x55d9006c9e00] mmco: unref short failure
[h264 @ 0x55d9006c9e00] mmco: unref short failure
  6%|▌         | 173/2910 [1:00:13<7:30:11,  9.87s/it][h264 @ 0x55d8ff7ccf80] mmco: unref short failure
[h264 @ 0x5565ece6e800] mmco: unref short failure
[h264 @ 0x5565ece6e800] mmco: unref short failure
[h264 @ 0x55cb79218d80] mmco: unref short failure
[h264 @ 0x55cb79218d80] mmco: unref short failure
[h264 @ 0x55cb6317e5c0] mmco: unref short failure
[h264 @ 0x55cb6317e5c0] mmco: unref short failure
[h264 @ 0x55d8fec73ac0] mmco: unref short failure
[h264 @ 0x55cb5ee0e840] mmco: unref short failure
[h264 @ 0x55cb5ee0e840] mmco: unref short failure
[h264 @ 0x5598dc38f440] mmco: unref short failure
[h264 @ 0x5598dc38f440] mmco: unref short failure
[h264 @ 0x55d902569780] mmco: unref short failure
[h264 @ 0x5598fa25fcc0] mmco: unref short failure
[h264 @ 0x5598fa25fcc0] mmco: unref short failure
09/18/2024 16:05:56 - INFO - __main__ -   current idx 7OpYRTupJJ0.44 from finetune_area returns wrong image/video, use 70154 instead.
[h264 @ 0x5565eeb0b700] mmco: unref short failure
[h264 @ 0x5565eeb0b700] mmco: unref short failure
[h264 @ 0x55d8ffd03040] mmco: unref short failure
[h264 @ 0x55d8ffd03040] mmco: unref short failure
[h264 @ 0x55d8ffd03040] mmco: unref short failure
[h264 @ 0x55cb5f26cdc0] mmco: unref short failure
[h264 @ 0x55cb5f26cdc0] mmco: unref short failure
[h264 @ 0x55cb74c7d3c0] mmco: unref short failure
[h264 @ 0x55d913c6a680] mmco: unref short failure
09/18/2024 16:06:40 - INFO - __main__ -   current idx Yy-gLEskjRc.46 from finetune_area returns wrong image/video, use 78565 instead.
  6%|▌         | 174/2910 [1:01:31<23:00:50, 30.28s/it][h264 @ 0x55d902849480] mmco: unref short failure
[h264 @ 0x5598f87298c0] mmco: unref short failure
[h264 @ 0x5598f87298c0] mmco: unref short failure
[h264 @ 0x55d8fe590d80] mmco: unref short failure
[h264 @ 0x55d8fe590d80] mmco: unref short failure
  6%|▌         | 175/2910 [1:01:42<18:38:42, 24.54s/it][h264 @ 0x55cb60cd9480] mmco: unref short failure
[h264 @ 0x55cb60cd9480] mmco: unref short failure
[h264 @ 0x55cb60cd9480] mmco: unref short failure
[h264 @ 0x5598dcb65140] mmco: unref short failure
[h264 @ 0x5598dcb65140] mmco: unref short failure
09/18/2024 16:07:05 - INFO - __main__ -   current idx cdG0MaKc36M.46 from finetune_area returns wrong image/video, use 148488 instead.
[h264 @ 0x55d9188db180] mmco: unref short failure
[h264 @ 0x55d9188db180] mmco: unref short failure
  6%|▌         | 176/2910 [1:02:03<17:48:47, 23.46s/it][h264 @ 0x55cb7287d900] mmco: unref short failure
[h264 @ 0x55cb7287d900] mmco: unref short failure
  6%|▌         | 177/2910 [1:02:08<13:42:14, 18.05s/it]  6%|▌         | 178/2910 [1:02:21<12:26:45, 16.40s/it][h264 @ 0x5598dc351700] mmco: unref short failure
[h264 @ 0x5598dc351700] mmco: unref short failure
  6%|▌         | 179/2910 [1:02:26<9:57:13, 13.12s/it] [h264 @ 0x5565f323dec0] mmco: unref short failure
  6%|▌         | 180/2910 [1:02:32<8:13:58, 10.86s/it][h264 @ 0x5598fb526000] mmco: unref short failure
[h264 @ 0x5598fb526000] mmco: unref short failure
[h264 @ 0x5565e8b46680] mmco: unref short failure
  6%|▌         | 181/2910 [1:02:37<6:58:31,  9.20s/it][h264 @ 0x55d8fe9908c0] mmco: unref short failure
[h264 @ 0x55d8fe9908c0] mmco: unref short failure
[h264 @ 0x5565fed69a80] mmco: unref short failure
[h264 @ 0x5565fed69a80] mmco: unref short failure
[h264 @ 0x55cb648a6e80] mmco: unref short failure
[h264 @ 0x55cb648a6e80] mmco: unref short failure
[h264 @ 0x5598dc9473c0] mmco: unref short failure
[h264 @ 0x55cb77860200] mmco: unref short failure
[h264 @ 0x5598f8c6c980] mmco: unref short failure
[h264 @ 0x5598f8c6c980] mmco: unref short failure
[h264 @ 0x5598e75fdf80] mmco: unref short failure
[h264 @ 0x5598e75fdf80] mmco: unref short failure
[h264 @ 0x5598f3353e80] mmco: unref short failure
[h264 @ 0x5598e143eb00] mmco: unref short failure
[h264 @ 0x5598e143eb00] mmco: unref short failure
[h264 @ 0x5598dfe4ac80] mmco: unref short failure
[h264 @ 0x5598e6148a80] mmco: unref short failure
[h264 @ 0x55d907df6d00] mmco: unref short failure
[h264 @ 0x55d907df6d00] mmco: unref short failure
[h264 @ 0x559900599a80] mmco: unref short failure
[h264 @ 0x559900599a80] mmco: unref short failure
[h264 @ 0x55cb79219680] mmco: unref short failure
[h264 @ 0x55d8fea27780] mmco: unref short failure
[h264 @ 0x55d8fea27780] mmco: unref short failure
[h264 @ 0x55d903cb7180] mmco: unref short failure
[h264 @ 0x55d903cb7180] mmco: unref short failure
  6%|▋         | 182/2910 [1:03:55<22:30:09, 29.70s/it][h264 @ 0x55cb5ef86680] mmco: unref short failure
[h264 @ 0x55d91494d2c0] mmco: unref short failure
[h264 @ 0x55d91494d2c0] mmco: unref short failure
not have audios 8-qwaveiHMM.3
[h264 @ 0x5598dfe4aa00] mmco: unref short failure
  6%|▋         | 183/2910 [1:04:13<19:55:28, 26.30s/it][h264 @ 0x55cb7c29f6c0] mmco: unref short failure
  6%|▋         | 184/2910 [1:04:19<15:12:45, 20.09s/it][h264 @ 0x55d903257800] mmco: unref short failure
[h264 @ 0x55d903257800] mmco: unref short failure
[h264 @ 0x5598e7db00c0] mmco: unref short failure
[h264 @ 0x5598e7db00c0] mmco: unref short failure
  6%|▋         | 185/2910 [1:04:28<12:40:13, 16.74s/it][h264 @ 0x5598e10bfd00] mmco: unref short failure
[h264 @ 0x5598e10bfd00] mmco: unref short failure
[h264 @ 0x5598f25da180] mmco: unref short failure
[h264 @ 0x5598f25da180] mmco: unref short failure
[h264 @ 0x55d907df7600] mmco: unref short failure
[h264 @ 0x55d907df7600] mmco: unref short failure
[h264 @ 0x5565e6233700] mmco: unref short failure
[h264 @ 0x5565e6233700] mmco: unref short failure
[h264 @ 0x55cb7c29f8c0] mmco: unref short failure
[h264 @ 0x5598e7819e40] mmco: unref short failure
[h264 @ 0x5598e7819e40] mmco: unref short failure
  6%|▋         | 186/2910 [1:04:39<11:27:38, 15.15s/it]  6%|▋         | 187/2910 [1:04:50<10:26:04, 13.80s/it][h264 @ 0x5565e9109780] mmco: unref short failure
[h264 @ 0x5565e9109780] mmco: unref short failure
  6%|▋         | 188/2910 [1:04:57<9:02:19, 11.95s/it] [h264 @ 0x5598deff5740] mmco: unref short failure
[h264 @ 0x55d90e4b6880] mmco: unref short failure
[h264 @ 0x55d90e4b6880] mmco: unref short failure
  6%|▋         | 189/2910 [1:05:04<7:52:47, 10.43s/it][h264 @ 0x5565ef9a39c0] mmco: unref short failure
09/18/2024 16:10:20 - INFO - __main__ -   current idx eYV2bd-oyoQ.13 from finetune_area returns wrong image/video, use 132972 instead.
[h264 @ 0x55cb7d577380] mmco: unref short failure
[h264 @ 0x55cb7d577380] mmco: unref short failure
[h264 @ 0x5598ddfc95c0] mmco: unref short failure
[h264 @ 0x5598ddfc95c0] mmco: unref short failure
[h264 @ 0x5598fdb12780] mmco: unref short failure
[h264 @ 0x5598fdb12780] mmco: unref short failure
[h264 @ 0x5598ef22ca40] mmco: unref short failure
[h264 @ 0x5598ef22ca40] mmco: unref short failure
[h264 @ 0x5565fa3ec480] mmco: unref short failure
[h264 @ 0x5565fa3ec480] mmco: unref short failure
[h264 @ 0x5565fa3ec480] mmco: unref short failure
[h264 @ 0x5565fa3ec480] mmco: unref short failure
[h264 @ 0x5598e5945ac0] mmco: unref short failure
[h264 @ 0x5598e5945ac0] mmco: unref short failure
[h264 @ 0x55cb64683a00] mmco: unref short failure
[h264 @ 0x55cb64683a00] mmco: unref short failure
[h264 @ 0x55cb64683a00] mmco: unref short failure
[h264 @ 0x55cb64683a00] mmco: unref short failure
[h264 @ 0x5565f1682980] mmco: unref short failure
[h264 @ 0x55d9082ca780] mmco: unref short failure
[h264 @ 0x55d9082ca780] mmco: unref short failure
[h264 @ 0x55cb61ef1980] mmco: unref short failure
[h264 @ 0x55cb61ef1980] mmco: unref short failure
[h264 @ 0x55cb5e409180] mmco: unref short failure
[h264 @ 0x55cb5e409180] mmco: unref short failure
  7%|▋         | 190/2910 [1:06:35<26:08:05, 34.59s/it]  7%|▋         | 191/2910 [1:06:46<20:45:39, 27.49s/it][h264 @ 0x55d9048e7e00] mmco: unref short failure
[h264 @ 0x55d9048e7e00] mmco: unref short failure
[h264 @ 0x55d9048e7e00] mmco: unref short failure
[h264 @ 0x55d9048e7e00] mmco: unref short failure
[h264 @ 0x55d9048e7e00] mmco: unref short failure
[h264 @ 0x55d9048e7e00] mmco: unref short failure
[h264 @ 0x5565ea11cb40] mmco: unref short failure
[h264 @ 0x5565ea11cb40] mmco: unref short failure
  7%|▋         | 192/2910 [1:06:52<15:50:29, 20.98s/it]09/18/2024 16:12:09 - INFO - __main__ -   current idx 6oFC5O_u-SI.107 from finetune_area returns wrong image/video, use 102237 instead.
  7%|▋         | 193/2910 [1:07:01<13:01:22, 17.26s/it][h264 @ 0x55cb69515e40] mmco: unref short failure
[h264 @ 0x55cb69515e40] mmco: unref short failure
[h264 @ 0x55cb65069e40] mmco: unref short failure
  7%|▋         | 194/2910 [1:07:07<10:27:47, 13.87s/it][h264 @ 0x5598f95f46c0] mmco: unref short failure
[h264 @ 0x5565f1b0fa40] mmco: unref short failure
  7%|▋         | 195/2910 [1:07:12<8:30:25, 11.28s/it] [h264 @ 0x55d91d650b00] mmco: unref short failure
[h264 @ 0x55d91d650b00] mmco: unref short failure
[h264 @ 0x55d91a2cee40] mmco: unref short failure
[h264 @ 0x55cb67e41d40] mmco: unref short failure
[h264 @ 0x55cb61ef22c0] mmco: unref short failure
[h264 @ 0x5598dfa31a00] mmco: unref short failure
[h264 @ 0x5598dfa31a00] mmco: unref short failure
  7%|▋         | 196/2910 [1:07:30<10:07:01, 13.42s/it][h264 @ 0x5598e75fe840] mmco: unref short failure
[h264 @ 0x55cb67e41d40] mmco: unref short failure
  7%|▋         | 197/2910 [1:07:35<8:09:46, 10.83s/it] [h264 @ 0x55cb6817ebc0] mmco: unref short failure
[h264 @ 0x55cb60a19840] mmco: unref short failure
[h264 @ 0x55d8fea67040] mmco: unref short failure
09/18/2024 16:12:57 - INFO - __main__ -   current idx 988Ksd7axRY.8 from finetune_area returns wrong image/video, use 141681 instead.
09/18/2024 16:13:05 - INFO - __main__ -   current idx 9W2zPcc4Kl8.5 from finetune_area returns wrong image/video, use 35737 instead.
[h264 @ 0x55d90510e9c0] mmco: unref short failure
[h264 @ 0x55d90510e9c0] mmco: unref short failure
[h264 @ 0x55cb7b3a6d80] mmco: unref short failure
[h264 @ 0x55cb7b3a6d80] mmco: unref short failure
[h264 @ 0x5598f36f8380] mmco: unref short failure
[h264 @ 0x5598f36f8380] mmco: unref short failure
[h264 @ 0x55d90ac81000] mmco: unref short failure
[h264 @ 0x55d90ac81000] mmco: unref short failure
[h264 @ 0x5565e65968c0] mmco: unref short failure
[h264 @ 0x55d9127301c0] mmco: unref short failure
[h264 @ 0x55d9078aa700] mmco: unref short failure
[h264 @ 0x55cb7c4375c0] mmco: unref short failure
[h264 @ 0x55cb7c4375c0] mmco: unref short failure
[h264 @ 0x55d8ffce2240] mmco: unref short failure
[h264 @ 0x55d8ffce2240] mmco: unref short failure
[h264 @ 0x5598f783bb00] mmco: unref short failure
[h264 @ 0x55cb5fd6fd00] mmco: unref short failure
[h264 @ 0x55cb5fd6fd00] mmco: unref short failure
[h264 @ 0x5565e86bb2c0] mmco: unref short failure
[h264 @ 0x55cb78b53080] mmco: unref short failure
[h264 @ 0x55cb78b53080] mmco: unref short failure
  7%|▋         | 198/2910 [1:08:59<24:45:05, 32.86s/it][h264 @ 0x55660ab9a880] mmco: unref short failure
[h264 @ 0x55660ab9a880] mmco: unref short failure
  7%|▋         | 199/2910 [1:09:14<20:43:18, 27.52s/it]09/18/2024 16:14:24 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 16:14:24 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x5565efe055c0] mmco: unref short failure
[h264 @ 0x5565efe055c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598e46d8600] mmco: unref short failure
[h264 @ 0x55d900bf1ec0] mmco: unref short failure
[h264 @ 0x55d900bf1ec0] mmco: unref short failure
[h264 @ 0x55cb703c8200] mmco: unref short failure
[h264 @ 0x5598f4995800] mmco: unref short failure
[h264 @ 0x5598f4995800] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598df578b00] mmco: unref short failure
[h264 @ 0x5598df578b00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598e8b4ae00] mmco: unref short failure
[h264 @ 0x5598e8b4ae00] mmco: unref short failure
[h264 @ 0x55d8ff7c80c0] mmco: unref short failure
[h264 @ 0x55d8ff7c80c0] mmco: unref short failure
[h264 @ 0x55d8ff7c80c0] mmco: unref short failure
[h264 @ 0x55d901bc0640] mmco: unref short failure
[h264 @ 0x55d901bc0640] mmco: unref short failure
[h264 @ 0x55d907226040] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:32,  2.37it/s][A
  1%|          | 2/221 [00:00<01:51,  1.96it/s][A
  1%|▏         | 3/221 [00:01<01:45,  2.06it/s][A
  2%|▏         | 4/221 [00:01<01:20,  2.70it/s][A
  2%|▏         | 5/221 [00:01<01:02,  3.48it/s][A
  3%|▎         | 6/221 [00:01<00:48,  4.44it/s][A
  3%|▎         | 7/221 [00:02<00:45,  4.68it/s][A
  4%|▎         | 8/221 [00:02<00:44,  4.78it/s][A
  4%|▍         | 9/221 [00:02<00:52,  4.01it/s][A
  5%|▍         | 10/221 [00:03<01:19,  2.66it/s][A
  5%|▍         | 11/221 [00:03<01:11,  2.92it/s][A
  5%|▌         | 12/221 [00:03<01:08,  3.03it/s][A
  6%|▌         | 13/221 [00:04<01:01,  3.38it/s][A
  6%|▋         | 14/221 [00:04<00:55,  3.74it/s][A
  7%|▋         | 15/221 [00:04<00:50,  4.09it/s][A
  7%|▋         | 16/221 [00:04<01:05,  3.11it/s][A
  8%|▊         | 17/221 [00:05<01:31,  2.24it/s][A
  8%|▊         | 18/221 [00:06<01:26,  2.35it/s][A
  9%|▊         | 19/221 [00:06<01:17,  2.59it/s][A[h264 @ 0x55cb5f729a80] mmco: unref short failure

  9%|▉         | 20/221 [00:06<01:03,  3.17it/s][A
 10%|▉         | 21/221 [00:06<00:55,  3.61it/s][A
 10%|▉         | 22/221 [00:07<01:07,  2.93it/s][A
 10%|█         | 23/221 [00:07<00:57,  3.46it/s][A
 11%|█         | 24/221 [00:07<00:52,  3.77it/s][A
 11%|█▏        | 25/221 [00:07<00:53,  3.69it/s][A[h264 @ 0x5598f1495880] mmco: unref short failure

 12%|█▏        | 26/221 [00:08<01:07,  2.89it/s][A
 13%|█▎        | 28/221 [00:09<01:19,  2.42it/s][A
 13%|█▎        | 29/221 [00:09<01:08,  2.82it/s][A
 14%|█▎        | 30/221 [00:09<01:05,  2.92it/s][A
 14%|█▍        | 31/221 [00:10<01:03,  2.98it/s][A
 14%|█▍        | 32/221 [00:10<00:53,  3.53it/s][A
 15%|█▍        | 33/221 [00:10<00:49,  3.82it/s][A
 15%|█▌        | 34/221 [00:10<00:48,  3.89it/s][A
 16%|█▌        | 35/221 [00:11<00:50,  3.70it/s][A
 16%|█▋        | 36/221 [00:11<01:00,  3.07it/s][A
 17%|█▋        | 37/221 [00:11<01:05,  2.81it/s][A
 17%|█▋        | 38/221 [00:12<01:09,  2.65it/s][A
 18%|█▊        | 39/221 [00:12<00:55,  3.31it/s][A
 18%|█▊        | 40/221 [00:12<00:52,  3.43it/s][A
 19%|█▊        | 41/221 [00:12<00:47,  3.81it/s][A
 19%|█▉        | 42/221 [00:13<00:58,  3.06it/s][A
 19%|█▉        | 43/221 [00:13<00:50,  3.55it/s][A
 20%|█▉        | 44/221 [00:13<00:42,  4.16it/s][A[h264 @ 0x55d8ff7c82c0] mmco: unref short failure
[h264 @ 0x55d8ff7c82c0] mmco: unref short failure

 20%|██        | 45/221 [00:14<01:02,  2.82it/s][A
 21%|██        | 46/221 [00:14<01:01,  2.83it/s][A
 21%|██▏       | 47/221 [00:15<01:34,  1.85it/s][A
 22%|██▏       | 48/221 [00:15<01:12,  2.40it/s][A
 22%|██▏       | 49/221 [00:16<01:03,  2.72it/s][A
 23%|██▎       | 50/221 [00:16<00:57,  2.99it/s][A
 23%|██▎       | 51/221 [00:16<00:56,  3.03it/s][A
 24%|██▎       | 52/221 [00:17<01:00,  2.77it/s][A
 24%|██▍       | 53/221 [00:17<00:53,  3.13it/s][A[h264 @ 0x5565f4c7bd40] mmco: unref short failure
[h264 @ 0x5565f4c7bd40] mmco: unref short failure

 24%|██▍       | 54/221 [00:19<02:18,  1.21it/s][A
 25%|██▍       | 55/221 [00:19<02:03,  1.34it/s][A
 25%|██▌       | 56/221 [00:20<01:33,  1.76it/s][A
 26%|██▌       | 57/221 [00:20<01:16,  2.14it/s][A
 26%|██▌       | 58/221 [00:20<01:04,  2.53it/s][A
 27%|██▋       | 59/221 [00:20<00:55,  2.94it/s][A[h264 @ 0x5565eac72ec0] mmco: unref short failure
[h264 @ 0x5565eac72ec0] mmco: unref short failure

 27%|██▋       | 60/221 [00:21<01:16,  2.09it/s][A
 28%|██▊       | 61/221 [00:21<01:04,  2.49it/s][A
 28%|██▊       | 62/221 [00:22<01:00,  2.62it/s][A
 29%|██▊       | 63/221 [00:22<00:50,  3.11it/s][A
 29%|██▉       | 64/221 [00:22<00:52,  3.01it/s][A
 29%|██▉       | 65/221 [00:22<00:44,  3.49it/s][A
 30%|██▉       | 66/221 [00:23<01:01,  2.51it/s][A
 30%|███       | 67/221 [00:23<00:54,  2.82it/s][A
 31%|███       | 68/221 [00:23<00:44,  3.41it/s][A
 31%|███       | 69/221 [00:24<01:18,  1.94it/s][A
 32%|███▏      | 70/221 [00:25<01:11,  2.10it/s][A
 32%|███▏      | 71/221 [00:25<01:02,  2.39it/s][A
 33%|███▎      | 72/221 [00:25<01:00,  2.48it/s][A
 33%|███▎      | 73/221 [00:26<01:02,  2.37it/s][A
 33%|███▎      | 74/221 [00:26<00:51,  2.84it/s][A09/18/2024 16:17:08 - INFO - __main__ -   current idx bdHJRIWkgos.39 from finetune_area returns wrong image/video, use 8538 instead.

 34%|███▍      | 75/221 [00:27<00:57,  2.54it/s][A
 34%|███▍      | 76/221 [00:27<00:48,  2.98it/s][A
 35%|███▍      | 77/221 [00:27<00:57,  2.52it/s][A
 35%|███▌      | 78/221 [00:27<00:44,  3.20it/s][A
 36%|███▌      | 79/221 [00:28<00:48,  2.95it/s][A
 36%|███▌      | 80/221 [00:28<00:44,  3.20it/s][A
 37%|███▋      | 81/221 [00:28<00:43,  3.23it/s][A
 37%|███▋      | 82/221 [00:29<00:55,  2.52it/s][A
 38%|███▊      | 83/221 [00:29<00:57,  2.41it/s][A
 38%|███▊      | 84/221 [00:30<00:59,  2.29it/s][A
 38%|███▊      | 85/221 [00:30<00:52,  2.61it/s][A
 39%|███▉      | 86/221 [00:30<00:44,  3.02it/s][A
 39%|███▉      | 87/221 [00:31<01:14,  1.80it/s][A
 40%|███▉      | 88/221 [00:32<01:19,  1.67it/s][A
 40%|████      | 89/221 [00:32<01:04,  2.03it/s][A
 41%|████      | 90/221 [00:33<00:56,  2.32it/s][A
 41%|████      | 91/221 [00:33<00:46,  2.81it/s][A
 42%|████▏     | 92/221 [00:33<00:45,  2.83it/s][A
 42%|████▏     | 93/221 [00:34<00:51,  2.46it/s][A
 43%|████▎     | 94/221 [00:34<00:46,  2.75it/s][A
 43%|████▎     | 95/221 [00:34<00:42,  2.97it/s][A
 43%|████▎     | 96/221 [00:35<00:59,  2.11it/s][A
 44%|████▍     | 97/221 [00:35<00:50,  2.46it/s][A
 44%|████▍     | 98/221 [00:36<01:02,  1.96it/s][A
 45%|████▍     | 99/221 [00:36<00:53,  2.28it/s][A
 45%|████▌     | 100/221 [00:37<00:46,  2.58it/s][A
 46%|████▌     | 101/221 [00:37<00:40,  2.94it/s][A
 46%|████▌     | 102/221 [00:38<00:50,  2.37it/s][A
 47%|████▋     | 103/221 [00:38<00:44,  2.67it/s][A
 47%|████▋     | 104/221 [00:38<00:41,  2.83it/s][A
 48%|████▊     | 105/221 [00:38<00:39,  2.90it/s][A
 48%|████▊     | 106/221 [00:39<00:56,  2.03it/s][A
 48%|████▊     | 107/221 [00:40<00:49,  2.30it/s][A
 49%|████▉     | 108/221 [00:40<00:53,  2.12it/s][A
 49%|████▉     | 109/221 [00:40<00:48,  2.32it/s][A
 50%|████▉     | 110/221 [00:41<00:45,  2.45it/s][A
 50%|█████     | 111/221 [00:41<00:51,  2.16it/s][A
 51%|█████     | 112/221 [00:42<00:50,  2.16it/s][A
 51%|█████     | 113/221 [00:42<00:44,  2.41it/s][A
 52%|█████▏    | 114/221 [00:42<00:34,  3.08it/s][A
 52%|█████▏    | 115/221 [00:42<00:28,  3.77it/s][A
 52%|█████▏    | 116/221 [00:43<00:44,  2.36it/s][A
 53%|█████▎    | 117/221 [00:44<00:43,  2.40it/s][A
 53%|█████▎    | 118/221 [00:44<00:38,  2.69it/s][A
 54%|█████▍    | 119/221 [00:44<00:41,  2.49it/s][A
 54%|█████▍    | 120/221 [00:45<00:35,  2.86it/s][A
 55%|█████▍    | 121/221 [00:45<00:28,  3.54it/s][A
 55%|█████▌    | 122/221 [00:45<00:29,  3.41it/s][A
 56%|█████▌    | 123/221 [00:45<00:25,  3.84it/s][A
 56%|█████▌    | 124/221 [00:46<00:28,  3.37it/s][A[h264 @ 0x55d8fe938280] mmco: unref short failure
[h264 @ 0x55d8fe938280] mmco: unref short failure

 57%|█████▋    | 125/221 [00:46<00:37,  2.59it/s][A
 57%|█████▋    | 126/221 [00:47<00:36,  2.60it/s][A
 57%|█████▋    | 127/221 [00:48<01:19,  1.18it/s][A
 58%|█████▊    | 128/221 [00:49<01:06,  1.40it/s][A
 58%|█████▊    | 129/221 [00:49<00:49,  1.86it/s][A
 59%|█████▉    | 130/221 [00:49<00:40,  2.26it/s][A
 59%|█████▉    | 131/221 [00:49<00:35,  2.52it/s][A
not have audios 7wavFXW3AFw.7
 60%|█████▉    | 132/221 [00:51<00:58,  1.51it/s][A
 60%|██████    | 133/221 [00:51<00:52,  1.67it/s][A[h264 @ 0x55d91cb4c600] mmco: unref short failure
[h264 @ 0x55d91cb4c600] mmco: unref short failure

 61%|██████    | 134/221 [00:52<00:58,  1.49it/s][A
 61%|██████    | 135/221 [00:53<00:54,  1.58it/s][A
 62%|██████▏   | 136/221 [00:53<00:48,  1.76it/s][A
 62%|██████▏   | 137/221 [00:53<00:41,  2.02it/s][A
 62%|██████▏   | 138/221 [00:54<00:39,  2.08it/s][A
 63%|██████▎   | 139/221 [00:55<00:49,  1.66it/s][A
 63%|██████▎   | 140/221 [00:55<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [00:55<00:37,  2.16it/s][A
 64%|██████▍   | 142/221 [00:56<00:45,  1.72it/s][A
 65%|██████▍   | 143/221 [00:57<00:43,  1.78it/s][A
 65%|██████▌   | 144/221 [00:57<00:38,  2.00it/s][A
 66%|██████▌   | 145/221 [00:57<00:28,  2.63it/s][A
 66%|██████▌   | 146/221 [00:57<00:22,  3.35it/s][A[h264 @ 0x5565e9115a00] mmco: unref short failure
[h264 @ 0x5565e9115a00] mmco: unref short failure

 67%|██████▋   | 147/221 [00:58<00:21,  3.38it/s][A
 67%|██████▋   | 148/221 [00:58<00:25,  2.86it/s][A
 67%|██████▋   | 149/221 [00:59<00:28,  2.53it/s][A
 68%|██████▊   | 150/221 [00:59<00:23,  2.97it/s][A
 68%|██████▊   | 151/221 [00:59<00:32,  2.18it/s][A
 69%|██████▉   | 152/221 [01:01<00:57,  1.21it/s][A
 69%|██████▉   | 153/221 [01:01<00:42,  1.59it/s][A
 70%|██████▉   | 154/221 [01:02<00:35,  1.91it/s][A
 70%|███████   | 155/221 [01:02<00:30,  2.14it/s][A
 71%|███████   | 156/221 [01:02<00:31,  2.06it/s][A
 71%|███████   | 157/221 [01:03<00:36,  1.78it/s][A
 71%|███████▏  | 158/221 [01:04<00:30,  2.05it/s][A
 72%|███████▏  | 159/221 [01:04<00:23,  2.61it/s][A
 72%|███████▏  | 160/221 [01:04<00:21,  2.79it/s][A
 73%|███████▎  | 161/221 [01:04<00:18,  3.19it/s][A
 73%|███████▎  | 162/221 [01:04<00:15,  3.78it/s][A
 74%|███████▍  | 163/221 [01:05<00:16,  3.60it/s][A
 74%|███████▍  | 164/221 [01:05<00:16,  3.52it/s][A
 75%|███████▍  | 165/221 [01:05<00:13,  4.11it/s][A
 75%|███████▌  | 166/221 [01:06<00:17,  3.16it/s][A
 76%|███████▌  | 167/221 [01:06<00:15,  3.59it/s][A[h264 @ 0x5565f0b13480] mmco: unref short failure
[h264 @ 0x5565f0b13480] mmco: unref short failure

 76%|███████▌  | 168/221 [01:07<00:23,  2.22it/s][A
 76%|███████▋  | 169/221 [01:07<00:20,  2.49it/s][A
 77%|███████▋  | 170/221 [01:07<00:18,  2.71it/s][A[h264 @ 0x55d91afa4040] mmco: unref short failure
[h264 @ 0x55d91afa4040] mmco: unref short failure

 77%|███████▋  | 171/221 [01:08<00:18,  2.69it/s][A
 78%|███████▊  | 172/221 [01:08<00:16,  3.03it/s][A
 78%|███████▊  | 173/221 [01:08<00:17,  2.80it/s][A
 79%|███████▊  | 174/221 [01:08<00:14,  3.19it/s][A
 79%|███████▉  | 175/221 [01:09<00:14,  3.15it/s][A
 80%|███████▉  | 176/221 [01:09<00:11,  3.92it/s][A
 80%|████████  | 177/221 [01:09<00:10,  4.22it/s][A
 81%|████████  | 178/221 [01:10<00:17,  2.52it/s][A
 81%|████████  | 179/221 [01:10<00:17,  2.45it/s][A
 81%|████████▏ | 180/221 [01:10<00:13,  3.04it/s][A
 82%|████████▏ | 182/221 [01:11<00:09,  4.11it/s][A
 83%|████████▎ | 183/221 [01:11<00:09,  3.91it/s][A
 83%|████████▎ | 184/221 [01:11<00:11,  3.35it/s][A
 84%|████████▍ | 186/221 [01:12<00:10,  3.45it/s][A
 85%|████████▍ | 187/221 [01:12<00:09,  3.46it/s][A
 85%|████████▌ | 188/221 [01:12<00:08,  3.67it/s][A
 86%|████████▌ | 189/221 [01:13<00:09,  3.43it/s][A
 86%|████████▌ | 190/221 [01:13<00:10,  3.00it/s][A
 86%|████████▋ | 191/221 [01:13<00:08,  3.53it/s][A
 87%|████████▋ | 192/221 [01:14<00:08,  3.61it/s][A
 88%|████████▊ | 194/221 [01:15<00:15,  1.80it/s][A
 88%|████████▊ | 195/221 [01:16<00:12,  2.15it/s][A
 89%|████████▊ | 196/221 [01:16<00:10,  2.45it/s][A
 89%|████████▉ | 197/221 [01:16<00:08,  2.74it/s][A
 90%|████████▉ | 198/221 [01:16<00:07,  3.05it/s][A
 90%|█████████ | 199/221 [01:17<00:06,  3.20it/s][A
 90%|█████████ | 200/221 [01:17<00:05,  3.79it/s][A
 91%|█████████ | 201/221 [01:17<00:04,  4.06it/s][A
 91%|█████████▏| 202/221 [01:17<00:04,  3.88it/s][A
 92%|█████████▏| 203/221 [01:18<00:04,  3.82it/s][A
 92%|█████████▏| 204/221 [01:18<00:04,  4.11it/s][A
 93%|█████████▎| 205/221 [01:18<00:03,  4.51it/s][A[h264 @ 0x55d8ff3816c0] mmco: unref short failure

 93%|█████████▎| 206/221 [01:19<00:05,  2.85it/s][A
 94%|█████████▎| 207/221 [01:19<00:03,  3.53it/s][A
 94%|█████████▍| 208/221 [01:19<00:04,  3.13it/s][A
 95%|█████████▍| 209/221 [01:19<00:03,  3.60it/s][A
 95%|█████████▌| 211/221 [01:20<00:02,  3.91it/s][A
 96%|█████████▌| 212/221 [01:20<00:02,  3.83it/s][A
 96%|█████████▋| 213/221 [01:20<00:01,  4.08it/s][A
 97%|█████████▋| 214/221 [01:21<00:02,  2.66it/s][A
 97%|█████████▋| 215/221 [01:21<00:01,  3.07it/s][A
 98%|█████████▊| 216/221 [01:21<00:01,  3.09it/s][A
 98%|█████████▊| 217/221 [01:22<00:01,  2.76it/s][A
 99%|█████████▊| 218/221 [01:22<00:01,  2.96it/s][A
 99%|█████████▉| 219/221 [01:23<00:00,  3.03it/s][A[h264 @ 0x55d913c6a280] mmco: unref short failure

100%|█████████▉| 220/221 [01:23<00:00,  2.02it/s][A
100%|██████████| 221/221 [01:24<00:00,  2.61it/s][A100%|██████████| 221/221 [01:24<00:00,  2.63it/s]
[h264 @ 0x55d910c70680] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.30it/s][A
  1%|          | 2/221 [00:00<01:06,  3.30it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.29it/s][A[h264 @ 0x55cb5fd703c0] mmco: unref short failure
[h264 @ 0x55cb5fd703c0] mmco: unref short failure

  2%|▏         | 4/221 [00:01<01:05,  3.29it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.24it/s][A
  3%|▎         | 6/221 [00:01<01:06,  3.25it/s][A
  3%|▎         | 7/221 [00:02<01:05,  3.27it/s][A
  4%|▎         | 8/221 [00:02<01:05,  3.23it/s][A
  4%|▍         | 9/221 [00:02<01:05,  3.24it/s][A
  5%|▍         | 10/221 [00:03<01:06,  3.17it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.21it/s][A
  5%|▌         | 12/221 [00:03<01:04,  3.22it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.25it/s][A
  6%|▋         | 14/221 [00:04<01:08,  3.04it/s][A
  7%|▋         | 15/221 [00:04<01:06,  3.11it/s][A
  7%|▋         | 16/221 [00:04<01:04,  3.17it/s][A
  8%|▊         | 17/221 [00:05<01:03,  3.20it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.23it/s][A
  9%|▊         | 19/221 [00:05<01:03,  3.20it/s][A
  9%|▉         | 20/221 [00:06<01:03,  3.16it/s][A
 10%|▉         | 21/221 [00:06<01:03,  3.17it/s][A
 10%|▉         | 22/221 [00:06<01:02,  3.18it/s][A
 10%|█         | 23/221 [00:07<01:01,  3.21it/s][A
 11%|█         | 24/221 [00:07<01:01,  3.20it/s][A09/18/2024 16:18:15 - INFO - __main__ -   current idx 0GZSfBuhf6Y.63 from finetune_area returns wrong image/video, use 98921 instead.

 11%|█▏        | 25/221 [00:07<01:00,  3.22it/s][A
 12%|█▏        | 26/221 [00:08<01:00,  3.20it/s][A
 12%|█▏        | 27/221 [00:08<01:01,  3.16it/s][A
 13%|█▎        | 28/221 [00:08<01:00,  3.17it/s][A
 13%|█▎        | 29/221 [00:09<01:02,  3.08it/s][A
 14%|█▎        | 30/221 [00:09<01:01,  3.12it/s][A
 14%|█▍        | 31/221 [00:09<01:00,  3.16it/s][A
 14%|█▍        | 32/221 [00:10<01:00,  3.13it/s][A
 15%|█▍        | 33/221 [00:10<01:00,  3.10it/s][A
 15%|█▌        | 34/221 [00:10<01:00,  3.12it/s][A
 16%|█▌        | 35/221 [00:10<00:58,  3.15it/s][A
 16%|█▋        | 36/221 [00:11<00:59,  3.09it/s][A
 17%|█▋        | 37/221 [00:11<00:58,  3.15it/s][A
 17%|█▋        | 38/221 [00:11<00:57,  3.19it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.20it/s][A
 18%|█▊        | 40/221 [00:12<00:56,  3.21it/s][A
 19%|█▊        | 41/221 [00:12<00:55,  3.22it/s][A
 19%|█▉        | 42/221 [00:13<00:55,  3.24it/s][A
 19%|█▉        | 43/221 [00:13<00:54,  3.26it/s][A
 20%|█▉        | 44/221 [00:13<00:54,  3.25it/s][A
 20%|██        | 45/221 [00:14<00:53,  3.27it/s][A
 21%|██        | 46/221 [00:14<00:53,  3.27it/s][A
 21%|██▏       | 47/221 [00:14<00:53,  3.28it/s][A[h264 @ 0x5598e04482c0] mmco: unref short failure

 22%|██▏       | 48/221 [00:14<00:52,  3.28it/s][A
 22%|██▏       | 49/221 [00:15<00:52,  3.26it/s][A
 23%|██▎       | 50/221 [00:15<00:52,  3.27it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.27it/s][A
 24%|██▎       | 52/221 [00:16<00:51,  3.28it/s][A
 24%|██▍       | 53/221 [00:16<00:51,  3.28it/s][A[h264 @ 0x5565ebc292c0] mmco: unref short failure

 24%|██▍       | 54/221 [00:16<00:50,  3.29it/s][A
 25%|██▍       | 55/221 [00:17<00:50,  3.29it/s][A
 25%|██▌       | 56/221 [00:17<00:50,  3.29it/s][A
 26%|██▌       | 57/221 [00:17<00:49,  3.29it/s][A
 26%|██▌       | 58/221 [00:18<00:49,  3.29it/s][A
 27%|██▋       | 59/221 [00:18<00:49,  3.29it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.29it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.29it/s][A
 28%|██▊       | 62/221 [00:19<00:48,  3.29it/s][A
 29%|██▊       | 63/221 [00:19<00:48,  3.29it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.29it/s][A
 29%|██▉       | 65/221 [00:20<00:47,  3.27it/s][A
 30%|██▉       | 66/221 [00:20<00:47,  3.28it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.28it/s][A
 31%|███       | 68/221 [00:21<00:46,  3.28it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.29it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.29it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.29it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.29it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.29it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.29it/s][A[h264 @ 0x5598ebb189c0] mmco: unref short failure

 34%|███▍      | 75/221 [00:23<00:44,  3.29it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.29it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.29it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.29it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.29it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.29it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.29it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.29it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.29it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.29it/s][A
 38%|███▊      | 85/221 [00:26<00:41,  3.30it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.30it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.30it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.30it/s][A
 40%|████      | 89/221 [00:27<00:40,  3.30it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.30it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:48<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:51<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:01<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.28it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:42,  5.13it/s][A
  1%|          | 2/221 [00:00<01:14,  2.95it/s][A
  1%|▏         | 3/221 [00:00<01:10,  3.11it/s][A
  2%|▏         | 4/221 [00:01<01:00,  3.57it/s][A
  2%|▏         | 5/221 [00:01<00:53,  4.01it/s][A
  3%|▎         | 7/221 [00:01<00:48,  4.45it/s][A
  4%|▎         | 8/221 [00:02<00:51,  4.15it/s][A
  4%|▍         | 9/221 [00:02<00:59,  3.57it/s][A
  5%|▍         | 10/221 [00:03<01:37,  2.16it/s][A
  5%|▍         | 11/221 [00:03<01:23,  2.52it/s][A
  5%|▌         | 12/221 [00:03<01:11,  2.93it/s][A
  6%|▌         | 13/221 [00:04<01:23,  2.50it/s][A
  6%|▋         | 14/221 [00:04<01:17,  2.66it/s][A
  7%|▋         | 15/221 [00:04<01:05,  3.15it/s][A
  7%|▋         | 16/221 [00:05<01:08,  3.00it/s][A
  8%|▊         | 17/221 [00:05<01:36,  2.11it/s][A
  8%|▊         | 18/221 [00:06<01:28,  2.28it/s][A
  9%|▊         | 19/221 [00:06<01:36,  2.10it/s][A
 10%|▉         | 21/221 [00:07<01:06,  3.00it/s][A
 10%|▉         | 22/221 [00:07<00:59,  3.35it/s][A
 10%|█         | 23/221 [00:07<00:50,  3.90it/s][A
 11%|█         | 24/221 [00:07<00:46,  4.19it/s][A
 11%|█▏        | 25/221 [00:08<00:51,  3.84it/s][A
 12%|█▏        | 26/221 [00:08<00:47,  4.14it/s][A
 12%|█▏        | 27/221 [00:08<00:47,  4.04it/s][A
 13%|█▎        | 28/221 [00:09<01:42,  1.88it/s][A
 13%|█▎        | 29/221 [00:10<01:30,  2.11it/s][A
 14%|█▎        | 30/221 [00:10<01:23,  2.30it/s][A
 14%|█▍        | 31/221 [00:10<01:16,  2.49it/s][A
 14%|█▍        | 32/221 [00:10<01:01,  3.08it/s][A
 15%|█▍        | 33/221 [00:11<00:52,  3.57it/s][A
 15%|█▌        | 34/221 [00:11<00:45,  4.12it/s][A
 16%|█▌        | 35/221 [00:11<00:39,  4.70it/s][A
 16%|█▋        | 36/221 [00:11<00:46,  3.95it/s][A
 17%|█▋        | 37/221 [00:12<00:49,  3.73it/s][A
 17%|█▋        | 38/221 [00:12<01:00,  3.04it/s][A
 18%|█▊        | 39/221 [00:12<00:53,  3.40it/s][A
 18%|█▊        | 40/221 [00:12<00:52,  3.45it/s][A
 19%|█▊        | 41/221 [00:13<00:47,  3.83it/s][A
 19%|█▉        | 42/221 [00:13<00:55,  3.24it/s][A
 19%|█▉        | 43/221 [00:14<01:08,  2.59it/s][A
 20%|█▉        | 44/221 [00:14<00:57,  3.09it/s][A
 20%|██        | 45/221 [00:14<01:01,  2.88it/s][A
 21%|██        | 46/221 [00:15<00:58,  3.01it/s][A
 21%|██▏       | 47/221 [00:15<00:56,  3.10it/s][A
 22%|██▏       | 48/221 [00:15<00:46,  3.72it/s][A
 22%|██▏       | 49/221 [00:15<00:38,  4.47it/s][A
 23%|██▎       | 50/221 [00:15<00:41,  4.15it/s][A
 23%|██▎       | 51/221 [00:16<00:39,  4.35it/s][A
 24%|██▎       | 52/221 [00:16<00:34,  4.91it/s][A
 24%|██▍       | 53/221 [00:16<00:35,  4.69it/s][A
 24%|██▍       | 54/221 [00:16<00:51,  3.27it/s][A
 25%|██▍       | 55/221 [00:17<00:43,  3.78it/s][A
 25%|██▌       | 56/221 [00:17<00:39,  4.16it/s][A
 26%|██▌       | 57/221 [00:17<00:34,  4.74it/s][A
 26%|██▌       | 58/221 [00:17<00:39,  4.12it/s][A
 27%|██▋       | 59/221 [00:18<00:40,  3.99it/s][A
 27%|██▋       | 60/221 [00:18<00:54,  2.96it/s][A
 28%|██▊       | 61/221 [00:18<00:47,  3.34it/s][A
 28%|██▊       | 62/221 [00:19<00:42,  3.73it/s][A
 29%|██▊       | 63/221 [00:19<00:45,  3.44it/s][A
 29%|██▉       | 64/221 [00:19<00:45,  3.48it/s][A
 29%|██▉       | 65/221 [00:20<00:53,  2.94it/s][A
 30%|██▉       | 66/221 [00:20<00:42,  3.67it/s][A
 30%|███       | 67/221 [00:20<00:45,  3.37it/s][A
 31%|███       | 68/221 [00:20<00:37,  4.11it/s][A
 32%|███▏      | 70/221 [00:20<00:29,  5.14it/s][A
 32%|███▏      | 71/221 [00:21<00:40,  3.67it/s][A
 33%|███▎      | 72/221 [00:21<00:45,  3.26it/s][A
 33%|███▎      | 73/221 [00:22<00:49,  3.00it/s][A
 33%|███▎      | 74/221 [00:22<00:40,  3.64it/s][A
 34%|███▍      | 75/221 [00:22<00:43,  3.35it/s][A
 34%|███▍      | 76/221 [00:22<00:38,  3.76it/s][A
 35%|███▍      | 77/221 [00:23<00:41,  3.45it/s][A
 35%|███▌      | 78/221 [00:23<00:39,  3.60it/s][A
 36%|███▌      | 79/221 [00:24<00:47,  2.98it/s][A
 36%|███▌      | 80/221 [00:24<00:40,  3.44it/s][A
 37%|███▋      | 81/221 [00:24<00:34,  4.11it/s][A
 37%|███▋      | 82/221 [00:24<00:36,  3.86it/s][A
 38%|███▊      | 83/221 [00:24<00:36,  3.74it/s][A
 38%|███▊      | 84/221 [00:25<00:42,  3.21it/s][A
 38%|███▊      | 85/221 [00:25<00:37,  3.62it/s][A
 39%|███▉      | 86/221 [00:25<00:35,  3.84it/s][A
 39%|███▉      | 87/221 [00:26<00:44,  3.01it/s][A
 40%|███▉      | 88/221 [00:26<00:44,  2.99it/s][A
 40%|████      | 89/221 [00:27<00:47,  2.80it/s][A
 41%|████      | 90/221 [00:27<00:47,  2.78it/s][A
 41%|████      | 91/221 [00:27<00:38,  3.36it/s][A
 42%|████▏     | 92/221 [00:27<00:37,  3.40it/s][A
 43%|████▎     | 94/221 [00:28<00:28,  4.48it/s][A
 43%|████▎     | 95/221 [00:28<00:30,  4.19it/s][A
 43%|████▎     | 96/221 [00:28<00:31,  4.00it/s][A
 44%|████▍     | 97/221 [00:28<00:26,  4.70it/s][A
 44%|████▍     | 98/221 [00:29<00:30,  4.06it/s][A
 45%|████▍     | 99/221 [00:29<00:32,  3.72it/s][A
 45%|████▌     | 100/221 [00:29<00:33,  3.64it/s][A
 46%|████▌     | 101/221 [00:30<00:42,  2.83it/s][A
 46%|████▌     | 102/221 [00:30<00:38,  3.05it/s][A
 47%|████▋     | 103/221 [00:30<00:33,  3.57it/s][A
 47%|████▋     | 104/221 [00:31<00:34,  3.36it/s][A
 48%|████▊     | 105/221 [00:31<00:32,  3.60it/s][A
 48%|████▊     | 106/221 [00:31<00:29,  3.86it/s][A
 48%|████▊     | 107/221 [00:31<00:28,  3.97it/s][A
 49%|████▉     | 108/221 [00:31<00:25,  4.49it/s][A
 49%|████▉     | 109/221 [00:32<00:29,  3.77it/s][A
 50%|████▉     | 110/221 [00:32<00:31,  3.50it/s][A
 50%|█████     | 111/221 [00:33<00:42,  2.61it/s][A
 51%|█████     | 113/221 [00:33<00:30,  3.50it/s][A
 52%|█████▏    | 115/221 [00:33<00:22,  4.72it/s][A
 52%|█████▏    | 116/221 [00:33<00:21,  4.89it/s][A
 53%|█████▎    | 117/221 [00:34<00:25,  4.12it/s][A
 53%|█████▎    | 118/221 [00:34<00:30,  3.38it/s][A
 54%|█████▍    | 119/221 [00:35<00:39,  2.57it/s][A
 54%|█████▍    | 120/221 [00:35<00:36,  2.76it/s][A
 55%|█████▍    | 121/221 [00:35<00:31,  3.18it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.34it/s][A
 56%|█████▌    | 123/221 [00:36<00:27,  3.59it/s][A
 56%|█████▌    | 124/221 [00:36<00:34,  2.85it/s][A
 57%|█████▋    | 125/221 [00:37<00:33,  2.84it/s][A
 57%|█████▋    | 126/221 [00:37<00:28,  3.36it/s][A
 57%|█████▋    | 127/221 [00:37<00:34,  2.72it/s][A
 58%|█████▊    | 128/221 [00:38<00:31,  2.99it/s][A
 58%|█████▊    | 129/221 [00:38<00:27,  3.39it/s][A
 59%|█████▉    | 130/221 [00:38<00:28,  3.24it/s][A
 59%|█████▉    | 131/221 [00:38<00:22,  4.06it/s][A
 60%|█████▉    | 132/221 [00:38<00:18,  4.81it/s][A
 60%|██████    | 133/221 [00:39<00:20,  4.23it/s][A
 61%|██████    | 134/221 [00:39<00:22,  3.91it/s][A
 61%|██████    | 135/221 [00:39<00:24,  3.55it/s][A
 62%|██████▏   | 136/221 [00:40<00:24,  3.47it/s][A
 62%|██████▏   | 137/221 [00:40<00:20,  4.07it/s][A
 62%|██████▏   | 138/221 [00:40<00:22,  3.65it/s][A
 63%|██████▎   | 139/221 [00:41<00:28,  2.84it/s][A
 63%|██████▎   | 140/221 [00:41<00:29,  2.72it/s][A
 64%|██████▍   | 141/221 [00:41<00:26,  3.05it/s][A
 64%|██████▍   | 142/221 [00:42<00:27,  2.84it/s][A
 65%|██████▍   | 143/221 [00:42<00:29,  2.62it/s][A
 65%|██████▌   | 144/221 [00:42<00:23,  3.33it/s][A
 66%|██████▌   | 145/221 [00:42<00:19,  3.81it/s][A
 66%|██████▌   | 146/221 [00:43<00:19,  3.79it/s][A
 67%|██████▋   | 147/221 [00:43<00:16,  4.54it/s][A
 67%|██████▋   | 148/221 [00:43<00:15,  4.60it/s][A
 67%|██████▋   | 149/221 [00:43<00:13,  5.41it/s][A
 68%|██████▊   | 150/221 [00:43<00:14,  4.96it/s][A
 68%|██████▊   | 151/221 [00:44<00:14,  4.89it/s][A
 69%|██████▉   | 152/221 [00:44<00:25,  2.68it/s][A
 69%|██████▉   | 153/221 [00:45<00:26,  2.60it/s][A
 70%|██████▉   | 154/221 [00:45<00:29,  2.26it/s][A
 70%|███████   | 155/221 [00:46<00:31,  2.09it/s][A
 71%|███████   | 156/221 [00:47<00:38,  1.71it/s][A
 71%|███████   | 157/221 [00:47<00:31,  2.02it/s][A
 71%|███████▏  | 158/221 [00:47<00:25,  2.50it/s][A
 72%|███████▏  | 159/221 [00:48<00:22,  2.79it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.21it/s][A
 73%|███████▎  | 161/221 [00:48<00:14,  4.04it/s][A
 74%|███████▍  | 163/221 [00:48<00:12,  4.56it/s][A
 74%|███████▍  | 164/221 [00:49<00:14,  3.93it/s][A
 75%|███████▍  | 165/221 [00:49<00:12,  4.32it/s][A
 75%|███████▌  | 166/221 [00:49<00:16,  3.26it/s][A
 76%|███████▌  | 167/221 [00:49<00:14,  3.63it/s][A
 76%|███████▌  | 168/221 [00:50<00:14,  3.69it/s][A
 76%|███████▋  | 169/221 [00:50<00:13,  3.97it/s][A
 77%|███████▋  | 170/221 [00:50<00:15,  3.26it/s][A
 77%|███████▋  | 171/221 [00:51<00:14,  3.48it/s][A
 78%|███████▊  | 172/221 [00:51<00:11,  4.24it/s][A
 78%|███████▊  | 173/221 [00:51<00:11,  4.30it/s][A
 79%|███████▊  | 174/221 [00:51<00:11,  4.10it/s][A
 79%|███████▉  | 175/221 [00:51<00:11,  4.08it/s][A
 80%|███████▉  | 176/221 [00:52<00:10,  4.23it/s][A
 80%|████████  | 177/221 [00:52<00:11,  3.84it/s][A
 81%|████████  | 178/221 [00:53<00:14,  2.87it/s][A
 81%|████████  | 179/221 [00:53<00:13,  3.15it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.75it/s][A
 82%|████████▏ | 181/221 [00:53<00:10,  3.83it/s][A
 82%|████████▏ | 182/221 [00:53<00:10,  3.81it/s][A
 83%|████████▎ | 183/221 [00:54<00:09,  4.04it/s][A
 83%|████████▎ | 184/221 [00:54<00:11,  3.12it/s][A
 84%|████████▎ | 185/221 [00:54<00:11,  3.01it/s][A
 85%|████████▍ | 187/221 [00:55<00:07,  4.54it/s][A
 85%|████████▌ | 188/221 [00:55<00:08,  3.80it/s][A
 86%|████████▌ | 189/221 [00:56<00:10,  3.06it/s][A
 86%|████████▌ | 190/221 [00:56<00:12,  2.51it/s][A
 86%|████████▋ | 191/221 [00:56<00:10,  2.85it/s][A
 87%|████████▋ | 192/221 [00:57<00:09,  3.13it/s][A
 87%|████████▋ | 193/221 [00:57<00:07,  3.83it/s][A
 88%|████████▊ | 194/221 [00:57<00:10,  2.63it/s][A
 88%|████████▊ | 195/221 [00:58<00:10,  2.39it/s][A
 89%|████████▊ | 196/221 [00:58<00:10,  2.43it/s][A
 89%|████████▉ | 197/221 [00:59<00:08,  2.77it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  3.18it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  3.91it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.22it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.14it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.55it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.32it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.02it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.80it/s][A
 93%|█████████▎| 206/221 [01:01<00:04,  3.59it/s][A
 94%|█████████▎| 207/221 [01:01<00:03,  4.23it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.60it/s][A
 95%|█████████▍| 209/221 [01:02<00:04,  2.74it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.62it/s][A
 96%|█████████▌| 212/221 [01:03<00:02,  3.37it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  2.73it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.00it/s][A
 97%|█████████▋| 215/221 [01:04<00:02,  2.77it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  3.04it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  3.07it/s][A
 99%|█████████▊| 218/221 [01:05<00:00,  3.58it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  3.83it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  3.83it/s][A
100%|██████████| 221/221 [01:06<00:00,  2.92it/s][A100%|██████████| 221/221 [01:06<00:00,  3.34it/s]
09/18/2024 16:20:25 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 199--===========

09/18/2024 16:20:25 - INFO - __main__ -   {'area_r1': 42.1, 'area_recall': '42.1/69.5/79.3', 'area_ravg': 63.6}
09/18/2024 16:20:25 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 199--===========

09/18/2024 16:20:25 - INFO - __main__ -   {'forward_r1': 38.6, 'forward_recall': '38.6/65.4/77.7', 'forward_ravg': 60.6}
09/18/2024 16:20:25 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 199--===========

09/18/2024 16:20:25 - INFO - __main__ -   {'area_video_r1': 38.7, 'area_video_recall': '38.7/66.1/77.8', 'area_video_ravg': 60.9}
09/18/2024 16:20:25 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 199=======

09/18/2024 16:20:25 - INFO - __main__ -   {'area_video_r1': 38.7, 'area_video_recall': '38.7/66.1/77.8', 'area_video_ravg': 60.9}
09/18/2024 16:20:25 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 199--===========

09/18/2024 16:20:25 - INFO - __main__ -   {'area_video_r1': 49.8, 'area_video_recall': '49.8/71.2/80.7', 'area_video_ravg': 67.2, 'area_video_back_r1': 46.6, 'area_video_back_recall': '46.6/68.8/78.2', 'area_video_back_ravg': 64.5}
09/18/2024 16:20:25 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 49=======

09/18/2024 16:20:25 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 16:20:25 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 199--===========

09/18/2024 16:20:25 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 16:20:25 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 16:20:25 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 16:20:25 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 199--===========

09/18/2024 16:20:25 - INFO - __main__ -   {'video_r1': 49.1, 'video_recall': '49.1/70.4/78.7', 'video_ravg': 66.1}
09/18/2024 16:20:25 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 16:20:25 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 16:20:59 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.00019661494297906756, 'loss_ret%tv%ta--finetune_area/loss_area': 2.212324619293213, 'loss_ret%tv%ta--finetune_area/total_loss': 2.2125213146209717}
  7%|▋         | 200/2910 [1:15:52<104:22:39, 138.66s/it]  7%|▋         | 201/2910 [1:15:56<73:50:39, 98.13s/it]    7%|▋         | 202/2910 [1:16:00<52:32:34, 69.85s/it][h264 @ 0x55d901410040] mmco: unref short failure
[h264 @ 0x55d901410040] mmco: unref short failure
[h264 @ 0x5565f4942d00] mmco: unref short failure
[h264 @ 0x5565f4942d00] mmco: unref short failure
  7%|▋         | 203/2910 [1:16:04<37:45:00, 50.20s/it][h264 @ 0x55cb70328fc0] mmco: unref short failure
[h264 @ 0x55d8fee239c0] mmco: unref short failure
  7%|▋         | 204/2910 [1:16:08<27:24:37, 36.47s/it]  7%|▋         | 205/2910 [1:16:13<20:18:11, 27.02s/it][h264 @ 0x55d903767e80] mmco: unref short failure
[h264 @ 0x55cb5f337980] mmco: unref short failure
  7%|▋         | 206/2910 [1:16:19<15:21:38, 20.45s/it]  7%|▋         | 207/2910 [1:16:24<11:53:01, 15.83s/it][h264 @ 0x556604f39d00] mmco: unref short failure
[h264 @ 0x556604f39d00] mmco: unref short failure
  7%|▋         | 208/2910 [1:16:30<9:38:57, 12.86s/it]   7%|▋         | 209/2910 [1:16:35<7:52:54, 10.51s/it][h264 @ 0x55cb63e76e40] mmco: unref short failure
[h264 @ 0x5565edf19d00] mmco: unref short failure
[h264 @ 0x5565edf19d00] mmco: unref short failure
  7%|▋         | 210/2910 [1:16:40<6:39:11,  8.87s/it][h264 @ 0x5598e8bb06c0] mmco: unref short failure
[h264 @ 0x5598e8bb06c0] mmco: unref short failure
  7%|▋         | 211/2910 [1:16:45<5:55:17,  7.90s/it][h264 @ 0x5565e571cd00] mmco: unref short failure
[h264 @ 0x5565e571cd00] mmco: unref short failure
  7%|▋         | 212/2910 [1:16:51<5:24:14,  7.21s/it]  7%|▋         | 213/2910 [1:16:56<5:01:25,  6.71s/it]  7%|▋         | 214/2910 [1:17:02<4:45:38,  6.36s/it]  7%|▋         | 215/2910 [1:17:07<4:26:17,  5.93s/it][h264 @ 0x55d8fee237c0] mmco: unref short failure
[h264 @ 0x5598ebb18e40] mmco: unref short failure
[h264 @ 0x5598ebb18e40] mmco: unref short failure
[h264 @ 0x55d8ff564700] mmco: unref short failure
[h264 @ 0x55d8ff564700] mmco: unref short failure
[h264 @ 0x5566041bc340] mmco: unref short failure
[h264 @ 0x55cb7f88b040] mmco: unref short failure
[h264 @ 0x5598e67f7480] mmco: unref short failure
[h264 @ 0x5598e67f7480] mmco: unref short failure
[h264 @ 0x55d90ac952c0] mmco: unref short failure
  7%|▋         | 216/2910 [1:18:00<14:59:33, 20.03s/it][h264 @ 0x55d9028dd6c0] mmco: unref short failure
[h264 @ 0x55cb5fb7bf00] mmco: unref short failure
09/18/2024 16:23:14 - INFO - __main__ -   current idx bYqphpDyeRQ.22 from finetune_area returns wrong image/video, use 122375 instead.
[h264 @ 0x55660bd930c0] mmco: unref short failure
[h264 @ 0x55660bd930c0] mmco: unref short failure
  7%|▋         | 217/2910 [1:18:15<13:57:28, 18.66s/it][h264 @ 0x5598edebeb80] mmco: unref short failure
[h264 @ 0x5598f623f6c0] mmco: unref short failure
[h264 @ 0x5598f623f6c0] mmco: unref short failure
  7%|▋         | 218/2910 [1:18:21<11:00:26, 14.72s/it][h264 @ 0x5565e83bde40] mmco: unref short failure
[h264 @ 0x5565e83bde40] mmco: unref short failure
[h264 @ 0x55d8ff0e2380] mmco: unref short failure
[h264 @ 0x55d8ff0e2380] mmco: unref short failure
[h264 @ 0x55d9030ff5c0] mmco: unref short failure
[h264 @ 0x55cb5ee0d600] mmco: unref short failure
[h264 @ 0x55cb5ee0d600] mmco: unref short failure
  8%|▊         | 219/2910 [1:18:34<10:41:47, 14.31s/it]  8%|▊         | 220/2910 [1:18:41<8:58:42, 12.02s/it] [h264 @ 0x55d8ffc32d40] mmco: unref short failure
[h264 @ 0x5598e187a240] mmco: unref short failure
[h264 @ 0x5598e187a240] mmco: unref short failure
  8%|▊         | 221/2910 [1:18:49<8:02:25, 10.76s/it][h264 @ 0x55d90c0923c0] mmco: unref short failure
[h264 @ 0x55d90c0923c0] mmco: unref short failure
  8%|▊         | 222/2910 [1:18:54<6:43:09,  9.00s/it][h264 @ 0x55cb64a25c40] mmco: unref short failure
[h264 @ 0x55cb64a25c40] mmco: unref short failure
  8%|▊         | 223/2910 [1:18:59<5:57:11,  7.98s/it]09/18/2024 16:24:09 - INFO - __main__ -   current idx TXPcr1HyKkY.54 from finetune_area returns wrong image/video, use 68487 instead.
[h264 @ 0x55cb61f0b140] mmco: unref short failure
[h264 @ 0x55cb61f0b140] mmco: unref short failure
[h264 @ 0x556601f07080] mmco: unref short failure
[h264 @ 0x556601f07080] mmco: unref short failure
[h264 @ 0x5598f602f3c0] mmco: unref short failure
[h264 @ 0x5598f602f3c0] mmco: unref short failure
[h264 @ 0x55cb5fd6d440] mmco: unref short failure
[h264 @ 0x5598e19631c0] mmco: unref short failure
[h264 @ 0x5598e19631c0] mmco: unref short failure
[h264 @ 0x5598e4f8af00] mmco: unref short failure
[h264 @ 0x55cb62a30180] mmco: unref short failure
[h264 @ 0x5565fbfceb80] mmco: unref short failure
[h264 @ 0x5598e1fe7480] mmco: unref short failure
[h264 @ 0x5598e1fe7480] mmco: unref short failure
[h264 @ 0x55d9133a4100] mmco: unref short failure
[h264 @ 0x55d9133a4100] mmco: unref short failure
[h264 @ 0x55d9133a4100] mmco: unref short failure
[h264 @ 0x55d9032b54c0] mmco: unref short failure
[h264 @ 0x5565fa1b3fc0] mmco: unref short failure
[h264 @ 0x5565f3608ac0] mmco: unref short failure
09/18/2024 16:25:11 - INFO - __main__ -   current idx yYJt_XCcAsY.15 from finetune_area returns wrong image/video, use 10302 instead.
[h264 @ 0x5598db9576c0] mmco: unref short failure
[h264 @ 0x55cb5e5303c0] mmco: unref short failure
[h264 @ 0x55cb5e5303c0] mmco: unref short failure
09/18/2024 16:25:29 - INFO - __main__ -   current idx 1wUrnDB1U9c.8 from finetune_area returns wrong image/video, use 27324 instead.
[h264 @ 0x55d903579b40] mmco: unref short failure
[h264 @ 0x55d903579b40] mmco: unref short failure
[h264 @ 0x55cb755db200] mmco: unref short failure
[h264 @ 0x5565fed69c80] mmco: unref short failure
[h264 @ 0x5565fed69c80] mmco: unref short failure
[h264 @ 0x5598e5998400] mmco: unref short failure
[h264 @ 0x5598e5998400] mmco: unref short failure
[h264 @ 0x55d9175bf200] mmco: unref short failure
[h264 @ 0x55660a227a40] mmco: unref short failure
[h264 @ 0x55660a227a40] mmco: unref short failure
[h264 @ 0x55cb5f4b6480] mmco: unref short failure
[h264 @ 0x55cb5f4b6480] mmco: unref short failure
  8%|▊         | 224/2910 [1:20:37<26:00:58, 34.87s/it][h264 @ 0x5565f7cc6b40] mmco: unref short failure
[h264 @ 0x5565f7cc6b40] mmco: unref short failure
[h264 @ 0x5565fffe4a80] mmco: unref short failure
[h264 @ 0x5565fffe4a80] mmco: unref short failure
[h264 @ 0x5598dc166280] mmco: unref short failure
[h264 @ 0x5565e520c040] mmco: unref short failure
[h264 @ 0x5565e520c040] mmco: unref short failure
  8%|▊         | 225/2910 [1:20:52<21:43:41, 29.13s/it]  8%|▊         | 226/2910 [1:21:00<16:53:24, 22.65s/it][h264 @ 0x55cb697c6380] mmco: unref short failure
[h264 @ 0x55cb697c6380] mmco: unref short failure
[h264 @ 0x55cb71d48080] mmco: unref short failure
  8%|▊         | 227/2910 [1:21:08<13:31:11, 18.14s/it]  8%|▊         | 228/2910 [1:21:13<10:45:59, 14.45s/it][h264 @ 0x55d916f65140] mmco: unref short failure
  8%|▊         | 229/2910 [1:21:18<8:36:26, 11.56s/it] [h264 @ 0x55d913710180] mmco: unref short failure
[h264 @ 0x5598e5998200] mmco: unref short failure
[h264 @ 0x55d9047d05c0] mmco: unref short failure
09/18/2024 16:26:34 - INFO - __main__ -   current idx Zsx1-4aJ4ys.28 from finetune_area returns wrong image/video, use 101535 instead.
[h264 @ 0x55d901105480] mmco: unref short failure
[h264 @ 0x55d901105480] mmco: unref short failure
[h264 @ 0x55cb5eebfb40] mmco: unref short failure
[h264 @ 0x55cb5eebfb40] mmco: unref short failure
[h264 @ 0x55cb5eebfb40] mmco: unref short failure
[h264 @ 0x55cb5eebfb40] mmco: unref short failure
  8%|▊         | 230/2910 [1:21:29<8:29:28, 11.41s/it]  8%|▊         | 231/2910 [1:21:35<7:09:54,  9.63s/it][h264 @ 0x55d908c31fc0] mmco: unref short failure
[h264 @ 0x55d908c31fc0] mmco: unref short failure
[h264 @ 0x5565f7fd8940] mmco: unref short failure
[h264 @ 0x5565f7fd8940] mmco: unref short failure
[h264 @ 0x5598dc351040] mmco: unref short failure
[h264 @ 0x5598dc351040] mmco: unref short failure
[h264 @ 0x55d91279cdc0] mmco: unref short failure
[h264 @ 0x55d913468a40] mmco: unref short failure
[h264 @ 0x55d913468a40] mmco: unref short failure
[h264 @ 0x55d913468a40] mmco: unref short failure
[h264 @ 0x55d8ff861280] mmco: unref short failure
[h264 @ 0x55d8ff861280] mmco: unref short failure
[h264 @ 0x55d91c6ca280] mmco: unref short failure
[h264 @ 0x55d91c6ca280] mmco: unref short failure
[h264 @ 0x55cb5e9fa840] mmco: unref short failure
[h264 @ 0x55cb5e9fa840] mmco: unref short failure
[h264 @ 0x5598f2c6cfc0] mmco: unref short failure
[h264 @ 0x5598f2c6cfc0] mmco: unref short failure
[h264 @ 0x55cb6e82f340] mmco: unref short failure
[h264 @ 0x55cb6286da40] mmco: unref short failure
[h264 @ 0x55d90cdd5140] mmco: unref short failure
[h264 @ 0x55d90cdd5140] mmco: unref short failure
[h264 @ 0x5598f37eac40] mmco: unref short failure
[h264 @ 0x5598f37eac40] mmco: unref short failure
09/18/2024 16:27:41 - INFO - __main__ -   current idx gtt8O0yuAiA.7 from finetune_area returns wrong image/video, use 92865 instead.
[h264 @ 0x55d9030ff840] mmco: unref short failure
[h264 @ 0x55d9030ff840] mmco: unref short failure
[h264 @ 0x5598f21e6d80] mmco: unref short failure
[h264 @ 0x5598f21e6d80] mmco: unref short failure
[h264 @ 0x5598f21e6d80] mmco: unref short failure
[h264 @ 0x5598f21e6d80] mmco: unref short failure
  8%|▊         | 232/2910 [1:23:01<24:13:15, 32.56s/it]09/18/2024 16:28:11 - INFO - __main__ -   current idx Q_Kn3yzSJ5w.69 from finetune_area returns wrong image/video, use 43269 instead.
[h264 @ 0x5598dcffc580] mmco: unref short failure
[h264 @ 0x5598dcffc580] mmco: unref short failure
[h264 @ 0x55d903257600] mmco: unref short failure
[h264 @ 0x55d8feb404c0] mmco: unref short failure
  8%|▊         | 233/2910 [1:23:15<20:02:01, 26.94s/it][h264 @ 0x55cb5e612dc0] mmco: unref short failure
[h264 @ 0x55cb5e612dc0] mmco: unref short failure
[h264 @ 0x5598f9cf4dc0] mmco: unref short failure
[h264 @ 0x55cb61611540] mmco: unref short failure
[h264 @ 0x55cb61611540] mmco: unref short failure
  8%|▊         | 234/2910 [1:23:26<16:31:38, 22.23s/it][h264 @ 0x5598f60b7a40] mmco: unref short failure
  8%|▊         | 235/2910 [1:23:31<12:45:20, 17.17s/it][h264 @ 0x5598f9cf4dc0] mmco: unref short failure
[h264 @ 0x5598f9cf4dc0] mmco: unref short failure
[h264 @ 0x5598f9cf4dc0] mmco: unref short failure
[h264 @ 0x5598f9cf4dc0] mmco: unref short failure
[h264 @ 0x5598f28df440] mmco: unref short failure
[h264 @ 0x55d9012c7640] mmco: unref short failure
  8%|▊         | 236/2910 [1:23:44<11:40:41, 15.72s/it]  8%|▊         | 237/2910 [1:23:50<9:29:49, 12.79s/it] [h264 @ 0x55cb7b9a3000] mmco: unref short failure
[h264 @ 0x55cb7b9a3000] mmco: unref short failure
  8%|▊         | 238/2910 [1:23:58<8:36:04, 11.59s/it][h264 @ 0x5565e5b04b80] mmco: unref short failure
  8%|▊         | 239/2910 [1:24:03<7:08:32,  9.63s/it][h264 @ 0x5565f0c88940] mmco: unref short failure
[h264 @ 0x5565f0c88940] mmco: unref short failure
[h264 @ 0x5565efa07740] mmco: unref short failure
[h264 @ 0x5565efa07740] mmco: unref short failure
[h264 @ 0x55d904a7c100] mmco: unref short failure
[h264 @ 0x55d904a7c100] mmco: unref short failure
[h264 @ 0x5565e7c41d40] mmco: unref short failure
09/18/2024 16:29:43 - INFO - __main__ -   current idx kZsPqzB36mk.13 from finetune_area returns wrong image/video, use 109316 instead.
[h264 @ 0x55cb5e4f07c0] mmco: unref short failure
[h264 @ 0x55cb81470440] mmco: unref short failure
[h264 @ 0x55cb81470440] mmco: unref short failure
[h264 @ 0x5598ebb189c0] mmco: unref short failure
[h264 @ 0x5598ebb189c0] mmco: unref short failure
[h264 @ 0x5565f069e080] mmco: unref short failure
[h264 @ 0x55d9176bbe40] mmco: unref short failure
[h264 @ 0x55cb742151c0] mmco: unref short failure
[h264 @ 0x55cb742151c0] mmco: unref short failure
[h264 @ 0x55d908c47b40] mmco: unref short failure
[h264 @ 0x55d908c47b40] mmco: unref short failure
[h264 @ 0x5598f1796100] mmco: unref short failure
[h264 @ 0x5598f1796100] mmco: unref short failure
[h264 @ 0x5598f1796100] mmco: unref short failure
[h264 @ 0x5598f1796100] mmco: unref short failure
[h264 @ 0x5598ea6d9680] mmco: unref short failure
[h264 @ 0x5598ef608380] mmco: unref short failure
[h264 @ 0x5598ef608380] mmco: unref short failure
[h264 @ 0x5598ef608380] mmco: unref short failure
[h264 @ 0x5598ef608380] mmco: unref short failure
[h264 @ 0x5565e7bd3880] mmco: unref short failure
[h264 @ 0x5565e7bd3880] mmco: unref short failure
  8%|▊         | 240/2910 [1:25:22<22:32:30, 30.39s/it][h264 @ 0x55d917540bc0] mmco: unref short failure
[h264 @ 0x55d901bc0840] mmco: unref short failure
[h264 @ 0x55d901bc0840] mmco: unref short failure
[h264 @ 0x55d8ff8d15c0] mmco: unref short failure
[h264 @ 0x5598e03a7cc0] mmco: unref short failure
  8%|▊         | 241/2910 [1:25:48<21:34:21, 29.10s/it]  8%|▊         | 242/2910 [1:25:57<17:04:46, 23.05s/it]  8%|▊         | 243/2910 [1:26:03<13:19:09, 17.98s/it][h264 @ 0x5565f1933400] mmco: unref short failure
[h264 @ 0x5565f1933400] mmco: unref short failure
[h264 @ 0x5565f1933400] mmco: unref short failure
[h264 @ 0x5565f1933400] mmco: unref short failure
[h264 @ 0x55d8ff0a7d40] mmco: unref short failure
[h264 @ 0x55d8ff0a7d40] mmco: unref short failure
[h264 @ 0x55d8ff0a7d40] mmco: unref short failure
[h264 @ 0x55d8ff0a7d40] mmco: unref short failure
[h264 @ 0x55cb73fee240] mmco: unref short failure
[h264 @ 0x55cb73fee240] mmco: unref short failure
[h264 @ 0x5565f7095c80] mmco: unref short failure
[h264 @ 0x5565f7095c80] mmco: unref short failure
[h264 @ 0x55cb719ca0c0] mmco: unref short failure
[h264 @ 0x5598e1161e40] mmco: unref short failure
[h264 @ 0x5598e1161e40] mmco: unref short failure
[h264 @ 0x55cb6e6ca400] mmco: unref short failure
[h264 @ 0x55cb6e6ca400] mmco: unref short failure
09/18/2024 16:31:34 - INFO - __main__ -   current idx 4DqZp71UU3o.53 from finetune_area returns wrong image/video, use 33976 instead.
  8%|▊         | 244/2910 [1:26:25<14:07:36, 19.08s/it]  8%|▊         | 245/2910 [1:26:30<11:00:17, 14.87s/it]  8%|▊         | 246/2910 [1:26:35<8:51:26, 11.97s/it] [h264 @ 0x5598ebf76e40] mmco: unref short failure
  8%|▊         | 247/2910 [1:26:42<7:47:31, 10.53s/it][h264 @ 0x55cb6f5ac540] mmco: unref short failure
[h264 @ 0x55cb6f5ac540] mmco: unref short failure
[h264 @ 0x5598ddfc9a80] mmco: unref short failure
[h264 @ 0x55660683f600] mmco: unref short failure
[h264 @ 0x55660683f600] mmco: unref short failure
[h264 @ 0x5598f389bac0] mmco: unref short failure
[h264 @ 0x5565f66f5fc0] mmco: unref short failure
[h264 @ 0x5565f66f5fc0] mmco: unref short failure
[h264 @ 0x5565e52a9440] mmco: unref short failure
[h264 @ 0x55d905691ec0] mmco: unref short failure
[h264 @ 0x5565e49d6ac0] mmco: unref short failure
[h264 @ 0x5565e49d6ac0] mmco: unref short failure
[h264 @ 0x5565e64ed340] mmco: unref short failure
[h264 @ 0x5565e64ed340] mmco: unref short failure
[h264 @ 0x5598e51cfdc0] mmco: unref short failure
[h264 @ 0x55660260bc00] mmco: unref short failure
[h264 @ 0x55660260bc00] mmco: unref short failure
09/18/2024 16:33:07 - INFO - __main__ -   current idx 66BbxK8sJjM.10 from finetune_area returns wrong image/video, use 130037 instead.
  9%|▊         | 248/2910 [1:28:02<23:09:49, 31.33s/it][h264 @ 0x5565f5d33500] mmco: unref short failure
[h264 @ 0x5565f5d33500] mmco: unref short failure
  9%|▊         | 249/2910 [1:28:11<18:09:36, 24.57s/it]09/18/2024 16:33:21 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 16:33:21 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x5598e22f26c0] mmco: unref short failure
[h264 @ 0x5598e51cf740] mmco: unref short failure
[h264 @ 0x5598e51cf740] mmco: unref short failure
[h264 @ 0x55cb74214680] mmco: unref short failure
[h264 @ 0x55cb74214680] mmco: unref short failure
[h264 @ 0x55cb74214680] mmco: unref short failure
[h264 @ 0x55cb74214680] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb60089800] mmco: unref short failure
[h264 @ 0x55cb60089800] mmco: unref short failure
[h264 @ 0x55cb60089800] mmco: unref short failure
[h264 @ 0x55cb60089800] mmco: unref short failure
[h264 @ 0x55d8ff1a2800] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598e1161e40] mmco: unref short failure
[h264 @ 0x5598e1161e40] mmco: unref short failure
[h264 @ 0x5598e1161e40] mmco: unref short failure
[h264 @ 0x5598e1161e40] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55d91a292800] mmco: unref short failure
[h264 @ 0x5598e4792480] mmco: unref short failure
[h264 @ 0x55660171e100] mmco: unref short failure
[h264 @ 0x55660171e100] mmco: unref short failure
[h264 @ 0x55cb69adf940] mmco: unref short failure
[h264 @ 0x5598e71940c0] mmco: unref short failure
[h264 @ 0x5598e71940c0] mmco: unref short failure
[h264 @ 0x5598e31b4780] mmco: unref short failure
[h264 @ 0x5598efd66340] mmco: unref short failure
[h264 @ 0x55d9027ef100] mmco: unref short failure
[h264 @ 0x55d9027ef100] mmco: unref short failure
09/18/2024 16:35:13 - INFO - __main__ -   current idx G_BESfJVLFg.1 from finetune_area returns wrong image/video, use 4065 instead.
[h264 @ 0x55d908763c80] mmco: unref short failure
[h264 @ 0x55d908763c80] mmco: unref short failure
[h264 @ 0x55d908763c80] mmco: unref short failure
[h264 @ 0x55d908763c80] mmco: unref short failure
[h264 @ 0x556603c85c40] mmco: unref short failure
[h264 @ 0x556603c85c40] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:40,  2.19it/s][A
  1%|          | 2/221 [00:00<01:48,  2.01it/s][A
  1%|▏         | 3/221 [00:01<01:44,  2.09it/s][A
  2%|▏         | 4/221 [00:01<01:25,  2.55it/s][A
  2%|▏         | 5/221 [00:01<01:05,  3.30it/s][A
  3%|▎         | 6/221 [00:02<00:54,  3.95it/s][A
  3%|▎         | 7/221 [00:02<00:46,  4.59it/s][A
  4%|▎         | 8/221 [00:02<00:42,  5.01it/s][A
  4%|▍         | 9/221 [00:02<00:50,  4.23it/s][A
  5%|▍         | 10/221 [00:03<01:15,  2.80it/s][A
  5%|▍         | 11/221 [00:03<00:59,  3.51it/s][A
  5%|▌         | 12/221 [00:03<01:08,  3.04it/s][A
  6%|▌         | 13/221 [00:04<01:03,  3.28it/s][A
  6%|▋         | 14/221 [00:04<01:01,  3.38it/s][A
  7%|▋         | 15/221 [00:04<01:09,  2.95it/s][A
  7%|▋         | 16/221 [00:05<01:23,  2.45it/s][A
  8%|▊         | 17/221 [00:06<01:54,  1.78it/s][A
  8%|▊         | 18/221 [00:06<01:47,  1.90it/s][A
  9%|▊         | 19/221 [00:06<01:29,  2.27it/s][A
  9%|▉         | 20/221 [00:07<01:10,  2.84it/s][A
 10%|▉         | 21/221 [00:07<01:06,  3.00it/s][A
 10%|▉         | 22/221 [00:07<01:06,  2.98it/s][A
 10%|█         | 23/221 [00:07<00:53,  3.71it/s][A
 11%|█         | 24/221 [00:08<00:48,  4.03it/s][A
 11%|█▏        | 25/221 [00:08<00:50,  3.89it/s][A
 12%|█▏        | 26/221 [00:08<00:59,  3.28it/s][A
 12%|█▏        | 27/221 [00:08<00:47,  4.11it/s][A[h264 @ 0x5598f9774b40] mmco: unref short failure
[h264 @ 0x5598f9774b40] mmco: unref short failure

 13%|█▎        | 28/221 [00:09<01:25,  2.26it/s][A
 13%|█▎        | 29/221 [00:10<01:15,  2.53it/s][A
 14%|█▎        | 30/221 [00:10<01:05,  2.90it/s][A
 14%|█▍        | 31/221 [00:10<01:07,  2.81it/s][A
 14%|█▍        | 32/221 [00:10<00:54,  3.46it/s][A
 15%|█▍        | 33/221 [00:10<00:47,  3.99it/s][A
 16%|█▌        | 35/221 [00:11<00:43,  4.30it/s][A
 16%|█▋        | 36/221 [00:11<00:52,  3.49it/s][A
 17%|█▋        | 37/221 [00:12<01:03,  2.91it/s][A
 17%|█▋        | 38/221 [00:12<01:13,  2.51it/s][A
 18%|█▊        | 39/221 [00:13<01:00,  3.00it/s][A
 18%|█▊        | 40/221 [00:13<00:58,  3.07it/s][A
 19%|█▊        | 41/221 [00:13<00:53,  3.38it/s][A
 19%|█▉        | 42/221 [00:14<01:18,  2.28it/s][A[h264 @ 0x556600cdad40] mmco: unref short failure
[h264 @ 0x556600cdad40] mmco: unref short failure

 19%|█▉        | 43/221 [00:14<01:08,  2.60it/s][A
 20%|█▉        | 44/221 [00:14<00:54,  3.27it/s][A
 20%|██        | 45/221 [00:15<01:21,  2.16it/s][A
 21%|██        | 46/221 [00:16<01:24,  2.07it/s][A
 21%|██▏       | 47/221 [00:16<01:41,  1.71it/s][A
 22%|██▏       | 48/221 [00:17<01:18,  2.21it/s][A
 22%|██▏       | 49/221 [00:17<01:07,  2.56it/s][A
 23%|██▎       | 50/221 [00:17<00:56,  3.02it/s][A
 23%|██▎       | 51/221 [00:17<00:46,  3.62it/s][A
 24%|██▎       | 52/221 [00:17<00:43,  3.86it/s][A
 24%|██▍       | 53/221 [00:18<00:48,  3.48it/s][A
 24%|██▍       | 54/221 [00:19<01:47,  1.55it/s][A
 25%|██▍       | 55/221 [00:20<01:47,  1.54it/s][A
 25%|██▌       | 56/221 [00:20<01:26,  1.90it/s][A
 26%|██▌       | 57/221 [00:20<01:13,  2.23it/s][A
 26%|██▌       | 58/221 [00:20<00:57,  2.83it/s][A
 27%|██▋       | 59/221 [00:21<00:50,  3.20it/s][A
 27%|██▋       | 60/221 [00:22<01:23,  1.93it/s][A
 28%|██▊       | 61/221 [00:22<01:10,  2.26it/s][A
 28%|██▊       | 62/221 [00:22<01:05,  2.43it/s][A
 29%|██▊       | 63/221 [00:22<00:52,  3.01it/s][A
 29%|██▉       | 64/221 [00:23<00:48,  3.27it/s][A
 29%|██▉       | 65/221 [00:23<00:42,  3.66it/s][A
 30%|██▉       | 66/221 [00:23<00:53,  2.88it/s][A
 30%|███       | 67/221 [00:24<00:54,  2.82it/s][A[h264 @ 0x5598efef1740] mmco: unref short failure
[h264 @ 0x5598efef1740] mmco: unref short failure

 31%|███       | 68/221 [00:24<00:44,  3.46it/s][A
 31%|███       | 69/221 [00:25<01:03,  2.40it/s][A
 32%|███▏      | 70/221 [00:25<00:51,  2.91it/s][A
 32%|███▏      | 71/221 [00:25<00:47,  3.19it/s][A[h264 @ 0x5598f24a72c0] mmco: unref short failure
[h264 @ 0x5598f24a72c0] mmco: unref short failure

 33%|███▎      | 72/221 [00:26<00:56,  2.63it/s][h264 @ 0x55d909405d80] mmco: unref short failure
[A
 33%|███▎      | 73/221 [00:26<01:02,  2.36it/s][A
 34%|███▍      | 75/221 [00:27<00:51,  2.85it/s][A
 34%|███▍      | 76/221 [00:27<00:49,  2.96it/s][A
 35%|███▍      | 77/221 [00:27<00:56,  2.55it/s][A
 35%|███▌      | 78/221 [00:28<00:46,  3.08it/s][A
 36%|███▌      | 79/221 [00:28<00:55,  2.58it/s][A
 36%|███▌      | 80/221 [00:28<00:47,  2.97it/s][A
 37%|███▋      | 81/221 [00:29<00:48,  2.89it/s][A
 37%|███▋      | 82/221 [00:29<01:02,  2.23it/s][A
 38%|███▊      | 83/221 [00:30<00:58,  2.37it/s][A
 38%|███▊      | 84/221 [00:30<00:52,  2.62it/s][A
 38%|███▊      | 85/221 [00:30<00:43,  3.14it/s][A
 39%|███▉      | 86/221 [00:31<00:43,  3.12it/s][A
 39%|███▉      | 87/221 [00:32<01:15,  1.78it/s][A
 40%|███▉      | 88/221 [00:33<01:27,  1.52it/s][A
 40%|████      | 89/221 [00:33<01:19,  1.67it/s][A
 41%|████      | 90/221 [00:33<01:10,  1.85it/s][A
 41%|████      | 91/221 [00:34<00:55,  2.34it/s][A
 42%|████▏     | 92/221 [00:34<00:50,  2.57it/s][A
 42%|████▏     | 93/221 [00:34<00:50,  2.56it/s][A
 43%|████▎     | 94/221 [00:35<00:44,  2.87it/s][A
 43%|████▎     | 95/221 [00:35<00:42,  2.99it/s][A[h264 @ 0x5565fce2be80] mmco: unref short failure

 43%|████▎     | 96/221 [00:35<00:52,  2.39it/s][A
 44%|████▍     | 97/221 [00:36<00:42,  2.94it/s][A
 44%|████▍     | 98/221 [00:36<00:41,  2.98it/s][A
 45%|████▍     | 99/221 [00:36<00:36,  3.37it/s][A
 45%|████▌     | 100/221 [00:36<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:37<00:34,  3.49it/s][A
 46%|████▌     | 102/221 [00:37<00:42,  2.83it/s][A
 47%|████▋     | 103/221 [00:37<00:34,  3.46it/s][A
 47%|████▋     | 104/221 [00:38<00:36,  3.22it/s][A
 48%|████▊     | 105/221 [00:38<00:37,  3.08it/s][A
 48%|████▊     | 106/221 [00:39<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:39<00:47,  2.38it/s][A
 49%|████▉     | 108/221 [00:40<00:42,  2.65it/s][A
 49%|████▉     | 109/221 [00:40<00:38,  2.88it/s][A
 50%|████▉     | 110/221 [00:40<00:34,  3.20it/s][A
 50%|█████     | 111/221 [00:41<00:42,  2.58it/s][A
 51%|█████     | 112/221 [00:41<00:35,  3.04it/s][A
 51%|█████     | 113/221 [00:41<00:36,  2.95it/s][A
 52%|█████▏    | 114/221 [00:41<00:30,  3.48it/s][A
 52%|█████▏    | 115/221 [00:42<00:27,  3.92it/s][A
 52%|█████▏    | 116/221 [00:42<00:48,  2.18it/s][A
 53%|█████▎    | 117/221 [00:43<00:44,  2.34it/s][A
 53%|█████▎    | 118/221 [00:43<00:37,  2.72it/s][A[h264 @ 0x5598eb36d640] mmco: unref short failure
[h264 @ 0x5598eb36d640] mmco: unref short failure
[h264 @ 0x5598eb36d640] mmco: unref short failure
[h264 @ 0x5598eb36d640] mmco: unref short failure

 54%|█████▍    | 119/221 [00:44<00:44,  2.32it/s][A[h264 @ 0x5565f512c100] mmco: unref short failure
[h264 @ 0x5565f512c100] mmco: unref short failure

 54%|█████▍    | 120/221 [00:44<00:43,  2.34it/s][A
 55%|█████▍    | 121/221 [00:44<00:33,  3.00it/s][A
 55%|█████▌    | 122/221 [00:44<00:31,  3.13it/s][A
 56%|█████▌    | 123/221 [00:45<00:27,  3.62it/s][A
 56%|█████▌    | 124/221 [00:45<00:28,  3.43it/s][A
 57%|█████▋    | 125/221 [00:46<00:38,  2.50it/s][A
 57%|█████▋    | 126/221 [00:46<00:32,  2.92it/s][A
 57%|█████▋    | 127/221 [00:48<01:11,  1.32it/s][A
 58%|█████▊    | 128/221 [00:48<00:59,  1.56it/s][A
 58%|█████▊    | 129/221 [00:48<00:46,  2.00it/s][A[h264 @ 0x5565f2aae680] mmco: unref short failure

 59%|█████▉    | 130/221 [00:48<00:42,  2.12it/s][A
 59%|█████▉    | 131/221 [00:49<00:36,  2.44it/s][A
 60%|█████▉    | 132/221 [00:49<00:43,  2.07it/s][A
 60%|██████    | 133/221 [00:50<00:42,  2.06it/s][A
 61%|██████    | 134/221 [00:51<00:48,  1.80it/s][A
 61%|██████    | 135/221 [00:51<00:43,  1.98it/s][A
 62%|██████▏   | 136/221 [00:51<00:39,  2.17it/s][A
 62%|██████▏   | 137/221 [00:52<00:33,  2.49it/s][A
 62%|██████▏   | 138/221 [00:52<00:34,  2.38it/s][A
 63%|██████▎   | 139/221 [00:53<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [00:53<00:39,  2.04it/s][A
 64%|██████▍   | 141/221 [00:54<00:35,  2.28it/s][A
 64%|██████▍   | 142/221 [00:54<00:32,  2.45it/s][A
 65%|██████▍   | 143/221 [00:54<00:30,  2.59it/s][A
 65%|██████▌   | 144/221 [00:55<00:28,  2.70it/s][A
 66%|██████▌   | 146/221 [00:55<00:19,  3.91it/s][A
 67%|██████▋   | 147/221 [00:55<00:18,  3.95it/s][A
 67%|██████▋   | 148/221 [00:55<00:17,  4.10it/s][A
 67%|██████▋   | 149/221 [00:56<00:19,  3.73it/s][A
 68%|██████▊   | 150/221 [00:56<00:17,  4.14it/s][A
 68%|██████▊   | 151/221 [00:57<00:35,  1.99it/s][A
 69%|██████▉   | 152/221 [00:58<00:51,  1.33it/s][A
 69%|██████▉   | 153/221 [00:59<00:43,  1.57it/s][A
 70%|██████▉   | 154/221 [00:59<00:35,  1.91it/s][A
 70%|███████   | 155/221 [00:59<00:30,  2.15it/s][A
 71%|███████   | 156/221 [01:00<00:33,  1.91it/s][A
 71%|███████   | 157/221 [01:01<00:39,  1.60it/s][A
 71%|███████▏  | 158/221 [01:01<00:31,  1.98it/s][A
 72%|███████▏  | 159/221 [01:01<00:24,  2.48it/s][A
 72%|███████▏  | 160/221 [01:01<00:22,  2.66it/s][A
 73%|███████▎  | 162/221 [01:02<00:14,  4.12it/s][A
 74%|███████▍  | 163/221 [01:02<00:14,  3.97it/s][A
 74%|███████▍  | 164/221 [01:02<00:15,  3.70it/s][A
 75%|███████▍  | 165/221 [01:02<00:13,  4.11it/s][A[h264 @ 0x55d91d482b80] mmco: unref short failure
[h264 @ 0x55d91d482b80] mmco: unref short failure

 75%|███████▌  | 166/221 [01:03<00:17,  3.08it/s][A
 76%|███████▌  | 167/221 [01:03<00:16,  3.34it/s][A
 76%|███████▌  | 168/221 [01:04<00:30,  1.74it/s][A
 76%|███████▋  | 169/221 [01:05<00:23,  2.21it/s][A[h264 @ 0x5598ed527b40] mmco: unref short failure
[h264 @ 0x5598ed527b40] mmco: unref short failure

 77%|███████▋  | 170/221 [01:05<00:21,  2.42it/s][A
 77%|███████▋  | 171/221 [01:05<00:21,  2.28it/s][A
 78%|███████▊  | 172/221 [01:06<00:18,  2.65it/s][A
 78%|███████▊  | 173/221 [01:06<00:18,  2.53it/s][A
 79%|███████▊  | 174/221 [01:06<00:15,  3.12it/s][A
 79%|███████▉  | 175/221 [01:07<00:14,  3.21it/s][A
 80%|███████▉  | 176/221 [01:07<00:13,  3.46it/s][A
 80%|████████  | 177/221 [01:07<00:10,  4.04it/s][A
 81%|████████  | 178/221 [01:07<00:14,  2.88it/s][A
 81%|████████  | 179/221 [01:08<00:15,  2.68it/s][A
 81%|████████▏ | 180/221 [01:08<00:13,  2.97it/s][A
 82%|████████▏ | 181/221 [01:08<00:11,  3.59it/s][A
 82%|████████▏ | 182/221 [01:09<00:10,  3.79it/s][A
 83%|████████▎ | 183/221 [01:09<00:09,  3.99it/s][A
 83%|████████▎ | 184/221 [01:09<00:09,  3.91it/s][A
 84%|████████▍ | 186/221 [01:09<00:07,  4.46it/s][A
 85%|████████▍ | 187/221 [01:10<00:09,  3.75it/s][A
 85%|████████▌ | 188/221 [01:10<00:08,  3.77it/s][A
 86%|████████▌ | 189/221 [01:10<00:09,  3.31it/s][A[h264 @ 0x55cb5f713b40] mmco: unref short failure

 86%|████████▌ | 190/221 [01:11<00:11,  2.80it/s][A
 86%|████████▋ | 191/221 [01:11<00:08,  3.35it/s][A
 87%|████████▋ | 192/221 [01:11<00:09,  3.12it/s][A
 87%|████████▋ | 193/221 [01:12<00:07,  3.82it/s][A
 88%|████████▊ | 194/221 [01:13<00:16,  1.62it/s][A
 88%|████████▊ | 195/221 [01:13<00:13,  1.98it/s][A09/18/2024 16:36:55 - INFO - __main__ -   current idx 9Ralave0cFg.16 from finetune_area returns wrong image/video, use 87295 instead.

 89%|████████▊ | 196/221 [01:14<00:11,  2.23it/s][A
 89%|████████▉ | 197/221 [01:14<00:09,  2.57it/s][A
 90%|████████▉ | 198/221 [01:14<00:07,  2.96it/s][A
 90%|█████████ | 199/221 [01:14<00:06,  3.53it/s][A
 90%|█████████ | 200/221 [01:14<00:05,  3.89it/s][A
 91%|█████████ | 201/221 [01:15<00:04,  4.16it/s][A
 91%|█████████▏| 202/221 [01:15<00:04,  3.93it/s][A
 92%|█████████▏| 203/221 [01:15<00:05,  3.55it/s][A
 92%|█████████▏| 204/221 [01:15<00:04,  3.81it/s][A
 93%|█████████▎| 205/221 [01:16<00:03,  4.32it/s][A09/18/2024 16:36:57 - INFO - __main__ -   current idx uFCptSipOAs.53 from finetune_area returns wrong image/video, use 109667 instead.

 93%|█████████▎| 206/221 [01:16<00:05,  2.51it/s][A
 94%|█████████▍| 208/221 [01:17<00:03,  3.55it/s][A
 95%|█████████▍| 209/221 [01:17<00:03,  3.77it/s][A
 95%|█████████▌| 211/221 [01:17<00:02,  3.98it/s][A[h264 @ 0x5598fbb0b600] mmco: unref short failure

 96%|█████████▌| 212/221 [01:18<00:02,  3.95it/s][A[h264 @ 0x556607890e40] mmco: unref short failure

 96%|█████████▋| 213/221 [01:18<00:01,  4.09it/s][A
 97%|█████████▋| 214/221 [01:18<00:02,  2.97it/s][A
 97%|█████████▋| 215/221 [01:19<00:01,  3.17it/s][A
 98%|█████████▊| 216/221 [01:19<00:01,  3.04it/s][A
 98%|█████████▊| 217/221 [01:20<00:01,  2.62it/s][A
 99%|█████████▊| 218/221 [01:20<00:01,  2.73it/s][A
 99%|█████████▉| 219/221 [01:20<00:00,  2.71it/s][A
100%|█████████▉| 220/221 [01:21<00:00,  1.72it/s][A
100%|██████████| 221/221 [01:22<00:00,  2.20it/s][A100%|██████████| 221/221 [01:22<00:00,  2.69it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:12,  3.04it/s][A
  1%|          | 2/221 [00:00<01:09,  3.16it/s][A
  1%|▏         | 3/221 [00:00<01:07,  3.22it/s][A
  2%|▏         | 4/221 [00:01<01:07,  3.22it/s][A
  2%|▏         | 5/221 [00:01<01:07,  3.22it/s][A
  3%|▎         | 6/221 [00:01<01:06,  3.24it/s][A
  3%|▎         | 7/221 [00:02<01:06,  3.24it/s][A
  4%|▎         | 8/221 [00:02<01:07,  3.15it/s][A
  4%|▍         | 9/221 [00:02<01:07,  3.15it/s][A
  5%|▍         | 10/221 [00:03<01:06,  3.20it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.21it/s][A[h264 @ 0x556609c48d00] mmco: unref short failure
[h264 @ 0x556609c48d00] mmco: unref short failure
[h264 @ 0x556609c48d00] mmco: unref short failure
[h264 @ 0x556609c48d00] mmco: unref short failure

  5%|▌         | 12/221 [00:03<01:05,  3.20it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.22it/s][A
  6%|▋         | 14/221 [00:04<01:03,  3.24it/s][A
  7%|▋         | 15/221 [00:04<01:03,  3.25it/s][A
  7%|▋         | 16/221 [00:04<01:03,  3.24it/s][A
  8%|▊         | 17/221 [00:05<01:03,  3.24it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.25it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.26it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.27it/s][A
 10%|▉         | 21/221 [00:06<01:01,  3.27it/s][A
 10%|▉         | 22/221 [00:06<01:02,  3.19it/s][A
 10%|█         | 23/221 [00:07<01:01,  3.22it/s][A
 11%|█         | 24/221 [00:07<01:00,  3.24it/s][A
 11%|█▏        | 25/221 [00:07<01:00,  3.24it/s][A
 12%|█▏        | 26/221 [00:08<00:59,  3.25it/s][A
 12%|█▏        | 27/221 [00:08<01:00,  3.22it/s][A
 13%|█▎        | 28/221 [00:08<00:59,  3.25it/s][A
 13%|█▎        | 29/221 [00:08<00:59,  3.23it/s][A
 14%|█▎        | 30/221 [00:09<01:00,  3.18it/s][A
 14%|█▍        | 31/221 [00:09<01:01,  3.10it/s][A
 14%|█▍        | 32/221 [00:09<00:59,  3.15it/s][A
 15%|█▍        | 33/221 [00:10<00:58,  3.19it/s][A
 15%|█▌        | 34/221 [00:10<00:58,  3.22it/s][A
 16%|█▌        | 35/221 [00:10<00:57,  3.24it/s][A
 16%|█▋        | 36/221 [00:11<00:56,  3.26it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.25it/s][A
 17%|█▋        | 38/221 [00:11<00:56,  3.26it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.21it/s][A
 18%|█▊        | 40/221 [00:12<00:57,  3.17it/s][A
 19%|█▊        | 41/221 [00:12<00:56,  3.19it/s][A
 19%|█▉        | 42/221 [00:13<00:56,  3.17it/s][A
 19%|█▉        | 43/221 [00:13<00:55,  3.19it/s][A
 20%|█▉        | 44/221 [00:13<00:54,  3.22it/s][A
 20%|██        | 45/221 [00:14<00:54,  3.20it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.23it/s][A
 21%|██▏       | 47/221 [00:14<00:53,  3.25it/s][A
 22%|██▏       | 48/221 [00:14<00:53,  3.24it/s][A
 22%|██▏       | 49/221 [00:15<00:52,  3.25it/s][A
 23%|██▎       | 50/221 [00:15<00:52,  3.26it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.27it/s][A
 24%|██▎       | 52/221 [00:16<00:52,  3.24it/s][A
 24%|██▍       | 53/221 [00:16<00:51,  3.25it/s][A
 24%|██▍       | 54/221 [00:16<00:51,  3.26it/s][A
 25%|██▍       | 55/221 [00:17<00:50,  3.27it/s][A
 25%|██▌       | 56/221 [00:17<00:50,  3.26it/s][A
 26%|██▌       | 57/221 [00:17<00:50,  3.27it/s][A
 26%|██▌       | 58/221 [00:17<00:49,  3.28it/s][A
 27%|██▋       | 59/221 [00:18<00:49,  3.25it/s][A
 27%|██▋       | 60/221 [00:18<00:49,  3.25it/s][A
 28%|██▊       | 61/221 [00:18<00:49,  3.22it/s][A
 28%|██▊       | 62/221 [00:19<00:49,  3.23it/s][A
 29%|██▊       | 63/221 [00:19<00:48,  3.23it/s][A
 29%|██▉       | 64/221 [00:19<00:49,  3.20it/s][A
 29%|██▉       | 65/221 [00:20<00:48,  3.22it/s][A
 30%|██▉       | 66/221 [00:20<00:47,  3.24it/s][A
 30%|███       | 67/221 [00:20<00:47,  3.26it/s][A
 31%|███       | 68/221 [00:21<00:46,  3.27it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.27it/s][A
 32%|███▏      | 70/221 [00:21<00:46,  3.28it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.27it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.28it/s][A
 33%|███▎      | 73/221 [00:22<00:45,  3.28it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.29it/s][A
 34%|███▍      | 75/221 [00:23<00:44,  3.29it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.26it/s][A[h264 @ 0x5598ecc9b3c0] mmco: unref short failure

 35%|███▍      | 77/221 [00:23<00:43,  3.27it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.28it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.28it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.28it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.29it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.29it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.29it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.29it/s][A
 38%|███▊      | 85/221 [00:26<00:41,  3.29it/s][A
 39%|███▉      | 86/221 [00:26<00:41,  3.29it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.29it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.30it/s][A
 40%|████      | 89/221 [00:27<00:40,  3.30it/s][A[h264 @ 0x55d909384200] mmco: unref short failure

 41%|████      | 90/221 [00:27<00:39,  3.30it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.30it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:48<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:51<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:01<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.28it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:54,  4.02it/s][A
  1%|          | 2/221 [00:00<01:20,  2.72it/s][A
  1%|▏         | 3/221 [00:00<01:04,  3.38it/s][A
  2%|▏         | 4/221 [00:01<00:58,  3.73it/s][A
  2%|▏         | 5/221 [00:01<00:50,  4.24it/s][A
  3%|▎         | 7/221 [00:01<00:46,  4.59it/s][A
  4%|▎         | 8/221 [00:02<00:52,  4.05it/s][A
  4%|▍         | 9/221 [00:02<01:00,  3.51it/s][A
  5%|▍         | 10/221 [00:02<01:16,  2.74it/s][A
  5%|▍         | 11/221 [00:03<01:06,  3.17it/s][A
  5%|▌         | 12/221 [00:03<01:01,  3.40it/s][A
  6%|▌         | 13/221 [00:03<01:14,  2.77it/s][A
  6%|▋         | 14/221 [00:04<01:01,  3.34it/s][A
  7%|▋         | 15/221 [00:04<00:51,  3.98it/s][A
  7%|▋         | 16/221 [00:04<01:05,  3.13it/s][A
  8%|▊         | 17/221 [00:05<01:27,  2.34it/s][A
  8%|▊         | 18/221 [00:05<01:22,  2.47it/s][A
  9%|▊         | 19/221 [00:06<01:30,  2.22it/s][A
 10%|▉         | 21/221 [00:06<01:03,  3.16it/s][A
 10%|▉         | 22/221 [00:06<01:02,  3.19it/s][A
 10%|█         | 23/221 [00:07<00:52,  3.74it/s][A
 11%|█         | 24/221 [00:07<00:49,  3.99it/s][A
 11%|█▏        | 25/221 [00:07<00:56,  3.47it/s][A
 12%|█▏        | 26/221 [00:07<00:50,  3.84it/s][A
 12%|█▏        | 27/221 [00:08<00:46,  4.16it/s][A
 13%|█▎        | 28/221 [00:09<01:27,  2.19it/s][A
 13%|█▎        | 29/221 [00:09<01:13,  2.60it/s][A
 14%|█▎        | 30/221 [00:09<01:08,  2.78it/s][A
 14%|█▍        | 31/221 [00:09<01:08,  2.78it/s][A
 14%|█▍        | 32/221 [00:10<00:54,  3.49it/s][A
 15%|█▍        | 33/221 [00:10<00:47,  3.95it/s][A
 15%|█▌        | 34/221 [00:10<00:39,  4.73it/s][A
 16%|█▌        | 35/221 [00:10<00:33,  5.49it/s][A
 16%|█▋        | 36/221 [00:10<00:45,  4.10it/s][A
 17%|█▋        | 37/221 [00:11<00:46,  3.99it/s][A
 17%|█▋        | 38/221 [00:11<00:58,  3.11it/s][A
 18%|█▊        | 39/221 [00:11<00:48,  3.73it/s][A
 18%|█▊        | 40/221 [00:11<00:49,  3.62it/s][A
 19%|█▊        | 41/221 [00:12<00:45,  3.92it/s][A
 19%|█▉        | 42/221 [00:12<01:02,  2.87it/s][A
 19%|█▉        | 43/221 [00:13<01:14,  2.38it/s][A
 20%|█▉        | 44/221 [00:13<01:07,  2.61it/s][A
 20%|██        | 45/221 [00:14<01:10,  2.50it/s][A
 21%|██        | 46/221 [00:14<01:03,  2.75it/s][A
 21%|██▏       | 47/221 [00:14<01:00,  2.86it/s][A
 22%|██▏       | 48/221 [00:14<00:50,  3.46it/s][A
 23%|██▎       | 50/221 [00:15<00:38,  4.49it/s][A
 23%|██▎       | 51/221 [00:15<00:34,  4.86it/s][A
 24%|██▎       | 52/221 [00:15<00:33,  5.05it/s][A
 24%|██▍       | 53/221 [00:15<00:40,  4.13it/s][A
 24%|██▍       | 54/221 [00:16<00:48,  3.48it/s][A
 25%|██▍       | 55/221 [00:16<00:43,  3.79it/s][A
 25%|██▌       | 56/221 [00:16<00:42,  3.91it/s][A
 26%|██▌       | 57/221 [00:16<00:37,  4.35it/s][A
 26%|██▌       | 58/221 [00:17<00:39,  4.13it/s][A
 27%|██▋       | 59/221 [00:17<00:38,  4.16it/s][A
 27%|██▋       | 60/221 [00:17<00:54,  2.98it/s][A
 28%|██▊       | 61/221 [00:18<00:50,  3.16it/s][A
 28%|██▊       | 62/221 [00:18<00:45,  3.49it/s][A
 29%|██▊       | 63/221 [00:18<00:52,  3.01it/s][A
 29%|██▉       | 64/221 [00:19<00:48,  3.26it/s][A
 29%|██▉       | 65/221 [00:19<01:01,  2.54it/s][A
 30%|██▉       | 66/221 [00:19<00:48,  3.17it/s][A
 30%|███       | 67/221 [00:20<00:50,  3.04it/s][A
 31%|███       | 68/221 [00:20<00:40,  3.80it/s][A
 31%|███       | 69/221 [00:20<00:32,  4.66it/s][A
 32%|███▏      | 70/221 [00:20<00:32,  4.60it/s][A
 32%|███▏      | 71/221 [00:21<00:42,  3.50it/s][A
 33%|███▎      | 72/221 [00:21<00:53,  2.78it/s][A
 33%|███▎      | 73/221 [00:21<00:54,  2.70it/s][A
 33%|███▎      | 74/221 [00:22<00:47,  3.12it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.28it/s][A
 34%|███▍      | 76/221 [00:22<00:37,  3.89it/s][A
 35%|███▍      | 77/221 [00:22<00:40,  3.55it/s][A
 35%|███▌      | 78/221 [00:23<00:39,  3.63it/s][A
 36%|███▌      | 79/221 [00:23<00:45,  3.11it/s][A
 36%|███▌      | 80/221 [00:23<00:43,  3.23it/s][A
 37%|███▋      | 82/221 [00:24<00:34,  3.99it/s][A
 38%|███▊      | 83/221 [00:24<00:39,  3.49it/s][A
 38%|███▊      | 84/221 [00:25<00:42,  3.23it/s][A
 38%|███▊      | 85/221 [00:25<00:39,  3.44it/s][A
 39%|███▉      | 86/221 [00:25<00:35,  3.81it/s][A
 39%|███▉      | 87/221 [00:26<00:54,  2.46it/s][A
 40%|███▉      | 88/221 [00:26<00:53,  2.50it/s][A
 40%|████      | 89/221 [00:26<00:50,  2.61it/s][A
 41%|████      | 90/221 [00:27<00:48,  2.72it/s][A
 41%|████      | 91/221 [00:27<00:43,  3.02it/s][A
 42%|████▏     | 92/221 [00:27<00:41,  3.14it/s][A
 43%|████▎     | 94/221 [00:28<00:29,  4.23it/s][A
 43%|████▎     | 95/221 [00:28<00:31,  3.96it/s][A
 43%|████▎     | 96/221 [00:28<00:34,  3.68it/s][A
 44%|████▍     | 97/221 [00:28<00:30,  4.06it/s][A
 44%|████▍     | 98/221 [00:29<00:29,  4.14it/s][A
 45%|████▍     | 99/221 [00:29<00:28,  4.23it/s][A
 45%|████▌     | 100/221 [00:29<00:28,  4.21it/s][A
 46%|████▌     | 101/221 [00:30<00:35,  3.36it/s][A
 46%|████▌     | 102/221 [00:30<00:36,  3.26it/s][A
 47%|████▋     | 103/221 [00:30<00:33,  3.52it/s][A
 47%|████▋     | 104/221 [00:30<00:33,  3.53it/s][A
 48%|████▊     | 105/221 [00:31<00:30,  3.77it/s][A
 48%|████▊     | 106/221 [00:31<00:31,  3.70it/s][A
 48%|████▊     | 107/221 [00:31<00:29,  3.85it/s][A
 49%|████▉     | 108/221 [00:31<00:25,  4.51it/s][A
 49%|████▉     | 109/221 [00:32<00:34,  3.24it/s][A
 50%|████▉     | 110/221 [00:32<00:33,  3.30it/s][A
 50%|█████     | 111/221 [00:33<00:41,  2.66it/s][A
 51%|█████     | 112/221 [00:33<00:32,  3.37it/s][A
 51%|█████     | 113/221 [00:33<00:32,  3.37it/s][A
 52%|█████▏    | 115/221 [00:33<00:21,  4.85it/s][A
 52%|█████▏    | 116/221 [00:33<00:20,  5.22it/s][A
 53%|█████▎    | 117/221 [00:34<00:24,  4.16it/s][A
 53%|█████▎    | 118/221 [00:34<00:34,  3.01it/s][A
 54%|█████▍    | 119/221 [00:35<00:45,  2.23it/s][A
 54%|█████▍    | 120/221 [00:35<00:41,  2.42it/s][A
 55%|█████▍    | 121/221 [00:36<00:34,  2.91it/s][A
 55%|█████▌    | 122/221 [00:36<00:31,  3.10it/s][A
 56%|█████▌    | 123/221 [00:36<00:29,  3.28it/s][A
 56%|█████▌    | 124/221 [00:37<00:34,  2.80it/s][A
 57%|█████▋    | 125/221 [00:37<00:34,  2.82it/s][A
 57%|█████▋    | 126/221 [00:37<00:30,  3.17it/s][A
 57%|█████▋    | 127/221 [00:38<00:32,  2.92it/s][A
 58%|█████▊    | 128/221 [00:38<00:30,  3.05it/s][A
 58%|█████▊    | 129/221 [00:38<00:27,  3.33it/s][A
 59%|█████▉    | 130/221 [00:38<00:25,  3.52it/s][A
 59%|█████▉    | 131/221 [00:38<00:21,  4.26it/s][A
 60%|█████▉    | 132/221 [00:39<00:18,  4.90it/s][A
 60%|██████    | 133/221 [00:39<00:23,  3.76it/s][A
 61%|██████    | 134/221 [00:39<00:25,  3.36it/s][A
 61%|██████    | 135/221 [00:40<00:24,  3.53it/s][A
 62%|██████▏   | 136/221 [00:40<00:23,  3.54it/s][A
 62%|██████▏   | 137/221 [00:40<00:20,  4.10it/s][A
 62%|██████▏   | 138/221 [00:40<00:22,  3.62it/s][A
 63%|██████▎   | 139/221 [00:41<00:28,  2.92it/s][A
 63%|██████▎   | 140/221 [00:41<00:29,  2.77it/s][A
 64%|██████▍   | 141/221 [00:41<00:23,  3.42it/s][A
 64%|██████▍   | 142/221 [00:42<00:25,  3.05it/s][A
 65%|██████▍   | 143/221 [00:42<00:25,  3.01it/s][A
 65%|██████▌   | 144/221 [00:42<00:21,  3.63it/s][A
 66%|██████▌   | 145/221 [00:43<00:18,  4.00it/s][A
 66%|██████▌   | 146/221 [00:43<00:19,  3.92it/s][A
 67%|██████▋   | 147/221 [00:43<00:15,  4.67it/s][A
 67%|██████▋   | 148/221 [00:43<00:16,  4.48it/s][A
 67%|██████▋   | 149/221 [00:43<00:15,  4.76it/s][A
 68%|██████▊   | 150/221 [00:44<00:17,  4.05it/s][A
 68%|██████▊   | 151/221 [00:44<00:17,  3.97it/s][A
 69%|██████▉   | 152/221 [00:45<00:25,  2.76it/s][A
 69%|██████▉   | 153/221 [00:45<00:26,  2.58it/s][A
 70%|██████▉   | 154/221 [00:45<00:27,  2.41it/s][A
 70%|███████   | 155/221 [00:46<00:32,  2.04it/s][A
 71%|███████   | 156/221 [00:47<00:34,  1.86it/s][A
 71%|███████   | 157/221 [00:47<00:30,  2.12it/s][A
 71%|███████▏  | 158/221 [00:47<00:24,  2.53it/s][A
 72%|███████▏  | 159/221 [00:48<00:23,  2.66it/s][A
 72%|███████▏  | 160/221 [00:48<00:20,  2.92it/s][A
 73%|███████▎  | 161/221 [00:48<00:16,  3.58it/s][A
 74%|███████▍  | 163/221 [00:48<00:14,  3.96it/s][A
 74%|███████▍  | 164/221 [00:49<00:15,  3.78it/s][A
 75%|███████▍  | 165/221 [00:49<00:12,  4.32it/s][A
 75%|███████▌  | 166/221 [00:49<00:17,  3.15it/s][A
 76%|███████▌  | 167/221 [00:50<00:14,  3.84it/s][A
 76%|███████▌  | 168/221 [00:50<00:13,  3.82it/s][A
 76%|███████▋  | 169/221 [00:50<00:13,  3.74it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.23it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.27it/s][A
 78%|███████▊  | 172/221 [00:51<00:12,  3.97it/s][A
 78%|███████▊  | 173/221 [00:51<00:11,  4.14it/s][A
 79%|███████▊  | 174/221 [00:51<00:10,  4.45it/s][A
 79%|███████▉  | 175/221 [00:52<00:09,  4.66it/s][A
 80%|███████▉  | 176/221 [00:52<00:10,  4.31it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.01it/s][A
 81%|████████  | 178/221 [00:53<00:15,  2.85it/s][A
 81%|████████  | 179/221 [00:53<00:13,  3.12it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.74it/s][A
 82%|████████▏ | 181/221 [00:53<00:10,  3.74it/s][A
 82%|████████▏ | 182/221 [00:54<00:10,  3.75it/s][A
 83%|████████▎ | 183/221 [00:54<00:09,  4.02it/s][A
 83%|████████▎ | 184/221 [00:54<00:11,  3.33it/s][A
 84%|████████▎ | 185/221 [00:54<00:10,  3.56it/s][A
 85%|████████▍ | 187/221 [00:55<00:06,  4.94it/s][A
 85%|████████▌ | 188/221 [00:55<00:09,  3.63it/s][A
 86%|████████▌ | 189/221 [00:56<00:10,  3.13it/s][A
 86%|████████▌ | 190/221 [00:56<00:11,  2.67it/s][A
 86%|████████▋ | 191/221 [00:56<00:10,  2.79it/s][A
 87%|████████▋ | 192/221 [00:57<00:10,  2.73it/s][A
 88%|████████▊ | 194/221 [00:58<00:10,  2.60it/s][A
 88%|████████▊ | 195/221 [00:58<00:11,  2.33it/s][A
 89%|████████▊ | 196/221 [00:59<00:09,  2.60it/s][A
 89%|████████▉ | 197/221 [00:59<00:08,  2.89it/s][A
 90%|████████▉ | 198/221 [00:59<00:06,  3.34it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  4.00it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.28it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.70it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.30it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.98it/s][A
 93%|█████████▎| 205/221 [01:01<00:03,  4.72it/s][A
 93%|█████████▎| 206/221 [01:01<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:02<00:03,  3.46it/s][A
 95%|█████████▍| 209/221 [01:02<00:04,  2.85it/s][A
 95%|█████████▌| 211/221 [01:03<00:02,  3.54it/s][A
 96%|█████████▌| 212/221 [01:03<00:02,  3.33it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  2.86it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  3.30it/s][A
 97%|█████████▋| 215/221 [01:04<00:02,  2.97it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  3.05it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  3.05it/s][A
 99%|█████████▊| 218/221 [01:05<00:00,  3.63it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  3.97it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  4.14it/s][A
100%|██████████| 221/221 [01:06<00:00,  3.20it/s][A100%|██████████| 221/221 [01:06<00:00,  3.34it/s]
09/18/2024 16:39:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 249--===========

09/18/2024 16:39:26 - INFO - __main__ -   {'area_r1': 41.1, 'area_recall': '41.1/68.2/78.2', 'area_ravg': 62.5}
09/18/2024 16:39:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 249--===========

09/18/2024 16:39:26 - INFO - __main__ -   {'forward_r1': 38.3, 'forward_recall': '38.3/65.0/74.7', 'forward_ravg': 59.4}
09/18/2024 16:39:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 249--===========

09/18/2024 16:39:26 - INFO - __main__ -   {'area_video_r1': 39.3, 'area_video_recall': '39.3/67.0/78.6', 'area_video_ravg': 61.6}
09/18/2024 16:39:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 249=======

09/18/2024 16:39:26 - INFO - __main__ -   {'area_video_r1': 39.3, 'area_video_recall': '39.3/67.0/78.6', 'area_video_ravg': 61.6}
09/18/2024 16:39:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 249--===========

09/18/2024 16:39:26 - INFO - __main__ -   {'area_video_r1': 50.8, 'area_video_recall': '50.8/71.7/80.0', 'area_video_ravg': 67.5, 'area_video_back_r1': 46.6, 'area_video_back_recall': '46.6/68.3/78.2', 'area_video_back_ravg': 64.4}
09/18/2024 16:39:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 49=======

09/18/2024 16:39:26 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 16:39:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 249--===========

09/18/2024 16:39:26 - INFO - __main__ -   {'video_r1': 31.7, 'video_recall': '31.7/56.9/68.7', 'video_ravg': 52.4}
09/18/2024 16:39:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 16:39:26 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 16:39:26 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 249--===========

09/18/2024 16:39:26 - INFO - __main__ -   {'video_r1': 50.0, 'video_recall': '50.0/69.7/77.5', 'video_ravg': 65.7}
09/18/2024 16:39:26 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 16:39:26 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 16:39:52 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.001033347798511386, 'loss_ret%tv%ta--finetune_area/loss_area': 1.9526406526565552, 'loss_ret%tv%ta--finetune_area/total_loss': 1.9536739587783813}
  9%|▊         | 250/2910 [1:34:45<99:56:45, 135.27s/it]  9%|▊         | 251/2910 [1:34:48<70:46:39, 95.83s/it] [h264 @ 0x5598dce601c0] mmco: unref short failure
09/18/2024 16:40:00 - INFO - __main__ -   current idx hvDXjC-_7uo.11 from finetune_area returns wrong image/video, use 17857 instead.
[h264 @ 0x55d900792500] mmco: unref short failure
  9%|▊         | 252/2910 [1:34:52<50:22:37, 68.23s/it][h264 @ 0x5598e730d8c0] mmco: unref short failure
[h264 @ 0x5598e730d8c0] mmco: unref short failure
09/18/2024 16:40:06 - INFO - __main__ -   current idx 3B1z5s6SZbQ.15 from finetune_area returns wrong image/video, use 145948 instead.
  9%|▊         | 253/2910 [1:34:57<36:11:50, 49.04s/it]09/18/2024 16:40:06 - INFO - __main__ -   current idx 5cCEw9Tageg.3 from finetune_area returns wrong image/video, use 141833 instead.
[h264 @ 0x55cb78c3c880] mmco: unref short failure
  9%|▊         | 254/2910 [1:35:01<26:15:31, 35.59s/it]  9%|▉         | 255/2910 [1:35:05<19:24:12, 26.31s/it]  9%|▉         | 256/2910 [1:35:10<14:38:53, 19.87s/it][h264 @ 0x556604104500] mmco: unref short failure
[h264 @ 0x556604104500] mmco: unref short failure
  9%|▉         | 257/2910 [1:35:16<11:31:12, 15.63s/it]  9%|▉         | 258/2910 [1:35:22<9:19:22, 12.66s/it]   9%|▉         | 259/2910 [1:35:27<7:42:48, 10.47s/it][h264 @ 0x55d920379a00] mmco: unref short failure
[h264 @ 0x55d920379a00] mmco: unref short failure
  9%|▉         | 260/2910 [1:35:33<6:36:15,  8.97s/it][h264 @ 0x55cb7b358500] mmco: unref short failure
[h264 @ 0x55cb7b358500] mmco: unref short failure
[h264 @ 0x5565ec67f500] mmco: unref short failure
[h264 @ 0x5565ec67f500] mmco: unref short failure
[h264 @ 0x5598eedf03c0] mmco: unref short failure
[h264 @ 0x5598eedf03c0] mmco: unref short failure
[h264 @ 0x5598eedf03c0] mmco: unref short failure
[h264 @ 0x5598eedf03c0] mmco: unref short failure
[h264 @ 0x556601f72100] mmco: unref short failure
[h264 @ 0x556601f72100] mmco: unref short failure
  9%|▉         | 261/2910 [1:35:38<5:44:05,  7.79s/it][h264 @ 0x55cb611cfe00] mmco: unref short failure
[h264 @ 0x55d911055e80] mmco: unref short failure
[h264 @ 0x55d911055e80] mmco: unref short failure
  9%|▉         | 262/2910 [1:35:43<5:08:04,  6.98s/it][h264 @ 0x55d90d813780] mmco: unref short failure
  9%|▉         | 263/2910 [1:35:48<4:44:47,  6.46s/it]  9%|▉         | 264/2910 [1:35:53<4:29:21,  6.11s/it][h264 @ 0x5598ec2ccc00] mmco: unref short failure
[h264 @ 0x55d8fefb3300] mmco: unref short failure
[h264 @ 0x55d8fefb3300] mmco: unref short failure
  9%|▉         | 265/2910 [1:35:59<4:25:03,  6.01s/it][h264 @ 0x5598dc77d980] mmco: unref short failure
[h264 @ 0x5598dc77d980] mmco: unref short failure
09/18/2024 16:41:23 - INFO - __main__ -   current idx 8XNjcDFNZzM.6 from finetune_area returns wrong image/video, use 85484 instead.
[h264 @ 0x5598fb1d9140] mmco: unref short failure
[h264 @ 0x5565e5ee7300] mmco: unref short failure
[h264 @ 0x55660339c180] mmco: unref short failure
[h264 @ 0x55660339c180] mmco: unref short failure
[h264 @ 0x5598e0a42fc0] mmco: unref short failure
[h264 @ 0x5598e0a42fc0] mmco: unref short failure
[h264 @ 0x5565ec409b00] mmco: unref short failure
[h264 @ 0x5565ec409b00] mmco: unref short failure
[h264 @ 0x5565e60503c0] mmco: unref short failure
[h264 @ 0x5565f0eec680] mmco: unref short failure
[h264 @ 0x5565f0eec680] mmco: unref short failure
[h264 @ 0x5565f61b0cc0] mmco: unref short failure
[h264 @ 0x5565f61b0cc0] mmco: unref short failure
[h264 @ 0x55d905ff8600] mmco: unref short failure
[h264 @ 0x55d905ff8600] mmco: unref short failure
[h264 @ 0x55d905ff8600] mmco: unref short failure
  9%|▉         | 266/2910 [1:36:46<13:21:52, 18.20s/it][h264 @ 0x5598fa8cbac0] mmco: unref short failure
[h264 @ 0x5598fa8cbac0] mmco: unref short failure
[h264 @ 0x55d9219e3140] mmco: unref short failure
  9%|▉         | 267/2910 [1:36:58<12:09:43, 16.57s/it][h264 @ 0x55cb6ebc6540] mmco: unref short failure
[h264 @ 0x55cb6ebc6540] mmco: unref short failure
  9%|▉         | 268/2910 [1:37:16<12:19:38, 16.80s/it][h264 @ 0x55d9046cc300] mmco: unref short failure
[h264 @ 0x55d90c1ace80] mmco: unref short failure
  9%|▉         | 269/2910 [1:37:24<10:31:48, 14.35s/it]  9%|▉         | 270/2910 [1:37:31<8:50:12, 12.05s/it] [h264 @ 0x55cb6b083d40] mmco: unref short failure
[h264 @ 0x55cb6b083d40] mmco: unref short failure
  9%|▉         | 271/2910 [1:37:37<7:22:19, 10.06s/it]  9%|▉         | 272/2910 [1:37:45<7:03:04,  9.62s/it][h264 @ 0x55d8ffbd2a40] mmco: unref short failure
  9%|▉         | 273/2910 [1:37:51<6:09:57,  8.42s/it][h264 @ 0x5598def101c0] mmco: unref short failure
[h264 @ 0x55d8fe7cb600] mmco: unref short failure
[h264 @ 0x55d8fe7cb600] mmco: unref short failure
[h264 @ 0x55cb75a2ae00] mmco: unref short failure
[h264 @ 0x55cb75a2ae00] mmco: unref short failure
[h264 @ 0x5598dab43680] mmco: unref short failure
[h264 @ 0x5598dab43680] mmco: unref short failure
[h264 @ 0x55d91afa4040] mmco: unref short failure
[h264 @ 0x5598db96ae00] mmco: unref short failure
not have audios xrsV4ybwavc.40
[h264 @ 0x55cb6c281100] mmco: unref short failure
[h264 @ 0x55cb6c281100] mmco: unref short failure
[h264 @ 0x556600d29d00] mmco: unref short failure
[h264 @ 0x556600d29d00] mmco: unref short failure
[h264 @ 0x55cb60089a80] mmco: unref short failure
[h264 @ 0x55cb60089a80] mmco: unref short failure
[h264 @ 0x55cb7165ce80] mmco: unref short failure
[h264 @ 0x55d90825bf40] mmco: unref short failure
09/18/2024 16:43:59 - INFO - __main__ -   current idx TXiTVBVNPyE.67 from finetune_area returns wrong image/video, use 54888 instead.
[h264 @ 0x5598e2369b00] mmco: unref short failure
[h264 @ 0x55d90c478400] mmco: unref short failure
[h264 @ 0x5565f71cdd40] mmco: unref short failure
[h264 @ 0x5565f71cdd40] mmco: unref short failure
[h264 @ 0x5565f71cdd40] mmco: unref short failure
[h264 @ 0x5565f71cdd40] mmco: unref short failure
  9%|▉         | 274/2910 [1:39:14<22:39:26, 30.94s/it][h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x5565f2b67240] mmco: unref short failure
[h264 @ 0x55cb6d3ac740] mmco: unref short failure
  9%|▉         | 275/2910 [1:39:36<20:43:41, 28.32s/it][h264 @ 0x55cb835d9e40] mmco: unref short failure
[h264 @ 0x55cb835d9e40] mmco: unref short failure
  9%|▉         | 276/2910 [1:39:42<15:37:32, 21.36s/it][h264 @ 0x5565e63f5b80] mmco: unref short failure
[h264 @ 0x5565e63f5b80] mmco: unref short failure
[h264 @ 0x5565e63f5b80] mmco: unref short failure
[h264 @ 0x5565e63f5b80] mmco: unref short failure
[h264 @ 0x5565e63f5b80] mmco: unref short failure
[h264 @ 0x55d901227780] mmco: unref short failure
[h264 @ 0x5598eba868c0] mmco: unref short failure
[h264 @ 0x55cb6ebc69c0] mmco: unref short failure
 10%|▉         | 277/2910 [1:40:00<14:56:21, 20.43s/it][h264 @ 0x5598eaf09280] mmco: unref short failure
[h264 @ 0x5598eaf09280] mmco: unref short failure
 10%|▉         | 278/2910 [1:40:05<11:36:34, 15.88s/it] 10%|▉         | 279/2910 [1:40:11<9:22:36, 12.83s/it]  10%|▉         | 280/2910 [1:40:17<7:49:16, 10.71s/it][h264 @ 0x5565f43204c0] mmco: unref short failure
[h264 @ 0x5565f43204c0] mmco: unref short failure
 10%|▉         | 281/2910 [1:40:22<6:42:22,  9.18s/it][h264 @ 0x5565e870c400] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x5565fcfa5c80] mmco: unref short failure
[h264 @ 0x55d90a2a9200] mmco: unref short failure
[h264 @ 0x55d90a2a9200] mmco: unref short failure
[h264 @ 0x55d90a2a9200] mmco: unref short failure
[h264 @ 0x55d90a2a9200] mmco: unref short failure
[h264 @ 0x55d90bd7b700] mmco: unref short failure
[h264 @ 0x55cb663bac40] mmco: unref short failure
[h264 @ 0x55cb663bac40] mmco: unref short failure
[h264 @ 0x5565f1a58000] mmco: unref short failure
09/18/2024 16:45:48 - INFO - __main__ -   current idx 4VH9UPwnHCQ.1 from finetune_area returns wrong image/video, use 64300 instead.
[h264 @ 0x55d90795c800] mmco: unref short failure
[h264 @ 0x55cb6be88840] mmco: unref short failure
[h264 @ 0x55cb6be88840] mmco: unref short failure
[h264 @ 0x55cb7d27fcc0] mmco: unref short failure
[h264 @ 0x55cb6198e500] mmco: unref short failure
[h264 @ 0x55cb6198e500] mmco: unref short failure
[h264 @ 0x55cb6198e500] mmco: unref short failure
[h264 @ 0x55cb6198e500] mmco: unref short failure
[h264 @ 0x5565f65a8440] mmco: unref short failure
[h264 @ 0x55cb636cb140] mmco: unref short failure
[h264 @ 0x55cb636cb140] mmco: unref short failure
[h264 @ 0x55d907ca5580] mmco: unref short failure
[h264 @ 0x55d907ca5580] mmco: unref short failure
[h264 @ 0x55d8fea58440] mmco: unref short failure
[h264 @ 0x55d8fea58440] mmco: unref short failure
[h264 @ 0x55d8fea58440] mmco: unref short failure
[h264 @ 0x55d8fea58440] mmco: unref short failure
09/18/2024 16:46:27 - INFO - __main__ -   current idx aKaIctkC1IU.24 from finetune_area returns wrong image/video, use 39049 instead.
[h264 @ 0x55cb6ebc6740] mmco: unref short failure
[h264 @ 0x55d919bf1f40] mmco: unref short failure
[h264 @ 0x55cb6b4b47c0] mmco: unref short failure
[h264 @ 0x55cb6b4b47c0] mmco: unref short failure
[h264 @ 0x556609c48f80] mmco: unref short failure
[h264 @ 0x556609c48f80] mmco: unref short failure
[h264 @ 0x5598e8082fc0] mmco: unref short failure
[h264 @ 0x55cb7b35d1c0] mmco: unref short failure
 10%|▉         | 282/2910 [1:41:50<23:57:21, 32.82s/it][h264 @ 0x55cb676d6a00] mmco: unref short failure
[h264 @ 0x55cb676d6a00] mmco: unref short failure
[h264 @ 0x5565fc8c8f80] mmco: unref short failure
[h264 @ 0x5566002e5800] mmco: unref short failure
[h264 @ 0x5566002e5800] mmco: unref short failure
[h264 @ 0x5565f55c2b40] mmco: unref short failure
[h264 @ 0x5565f55c2b40] mmco: unref short failure
[h264 @ 0x55cb636cb140] mmco: unref short failure
[h264 @ 0x55cb636cb140] mmco: unref short failure
 10%|▉         | 283/2910 [1:42:08<20:43:22, 28.40s/it][h264 @ 0x556601eb2500] mmco: unref short failure
[h264 @ 0x556601eb2500] mmco: unref short failure
 10%|▉         | 284/2910 [1:42:15<15:53:31, 21.79s/it][h264 @ 0x5598dfad5c80] mmco: unref short failure
[h264 @ 0x5598dfad5c80] mmco: unref short failure
[h264 @ 0x55cb7933d380] mmco: unref short failure
 10%|▉         | 285/2910 [1:42:27<13:49:38, 18.96s/it][h264 @ 0x55cb793588c0] mmco: unref short failure
 10%|▉         | 286/2910 [1:42:32<10:49:48, 14.86s/it] 10%|▉         | 287/2910 [1:42:38<8:45:46, 12.03s/it] [h264 @ 0x55d9000edac0] mmco: unref short failure
[h264 @ 0x55d9000edac0] mmco: unref short failure
09/18/2024 16:47:54 - INFO - __main__ -   current idx edpFNTpJidw.6 from finetune_area returns wrong image/video, use 33504 instead.
[h264 @ 0x5598ecd8b3c0] mmco: unref short failure
[h264 @ 0x5598ecd8b3c0] mmco: unref short failure
 10%|▉         | 288/2910 [1:42:47<8:11:01, 11.24s/it][h264 @ 0x55cb5ef25e80] mmco: unref short failure
[h264 @ 0x55cb5ef25e80] mmco: unref short failure
 10%|▉         | 289/2910 [1:42:53<6:58:21,  9.58s/it][h264 @ 0x55d902799e00] mmco: unref short failure
[h264 @ 0x5598f32dc540] mmco: unref short failure
[h264 @ 0x5598f32dc540] mmco: unref short failure
[h264 @ 0x5565e661ea00] mmco: unref short failure
[h264 @ 0x5565e661ea00] mmco: unref short failure
[h264 @ 0x55d90511d740] mmco: unref short failure
[h264 @ 0x55d8fedb9700] mmco: unref short failure
[h264 @ 0x5598fbe0e180] mmco: unref short failure
[h264 @ 0x5598fbe0e180] mmco: unref short failure
[h264 @ 0x5598ebd99740] mmco: unref short failure
[h264 @ 0x55cb70bda840] mmco: unref short failure
[h264 @ 0x55cb70bda840] mmco: unref short failure
[h264 @ 0x55cb792a2540] mmco: unref short failure
[h264 @ 0x55cb792a2540] mmco: unref short failure
[h264 @ 0x55d916549ac0] mmco: unref short failure
[h264 @ 0x556602be8240] mmco: unref short failure
[h264 @ 0x55cb60990540] mmco: unref short failure
[h264 @ 0x55cb70840280] mmco: unref short failure
[h264 @ 0x5566024b9800] mmco: unref short failure
[h264 @ 0x55d9075dda00] mmco: unref short failure
[h264 @ 0x55d9075dda00] mmco: unref short failure
[h264 @ 0x55d9012c0e80] mmco: unref short failure
[h264 @ 0x55d9012c0e80] mmco: unref short failure
[h264 @ 0x5598ff6ff980] mmco: unref short failure
[h264 @ 0x5598df853200] mmco: unref short failure
[h264 @ 0x5598df853200] mmco: unref short failure
[h264 @ 0x5565f1125340] mmco: unref short failure
[h264 @ 0x5565f1125340] mmco: unref short failure
[h264 @ 0x55d90c1aca00] mmco: unref short failure
[h264 @ 0x55d90c1aca00] mmco: unref short failure
[h264 @ 0x55cb70bdc8c0] mmco: unref short failure
[h264 @ 0x55cb70bdc8c0] mmco: unref short failure
09/18/2024 16:49:27 - INFO - __main__ -   current idx Wirilsgk0qk.51 from finetune_area returns wrong image/video, use 128368 instead.
 10%|▉         | 290/2910 [1:44:21<24:02:37, 33.04s/it][h264 @ 0x55cb796da3c0] mmco: unref short failure
[h264 @ 0x5565f109d000] mmco: unref short failure
[h264 @ 0x5565f109d000] mmco: unref short failure
 10%|█         | 291/2910 [1:44:35<20:01:46, 27.53s/it] 10%|█         | 292/2910 [1:44:41<15:12:35, 20.92s/it] 10%|█         | 293/2910 [1:44:53<13:14:58, 18.23s/it][h264 @ 0x55cb63d1afc0] mmco: unref short failure
[h264 @ 0x55cb63d1afc0] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
[h264 @ 0x5566086f8580] mmco: unref short failure
 10%|█         | 294/2910 [1:44:59<10:36:58, 14.61s/it][h264 @ 0x55d9075ddc00] mmco: unref short failure
 10%|█         | 295/2910 [1:45:04<8:28:30, 11.67s/it] [h264 @ 0x5598e77dc500] mmco: unref short failure
[h264 @ 0x55d90a2a9000] mmco: unref short failure
[h264 @ 0x55d90a2a9000] mmco: unref short failure
[h264 @ 0x55cb728dcb00] mmco: unref short failure
[h264 @ 0x5565e63f6000] mmco: unref short failure
[h264 @ 0x55d91d4ac980] mmco: unref short failure
[h264 @ 0x55d91d4ac980] mmco: unref short failure
 10%|█         | 296/2910 [1:45:22<9:56:43, 13.70s/it] 10%|█         | 297/2910 [1:45:28<8:10:58, 11.27s/it][h264 @ 0x55cb6352a900] mmco: unref short failure
[h264 @ 0x5565ee5c3b80] mmco: unref short failure
[h264 @ 0x5565ee5c3b80] mmco: unref short failure
[h264 @ 0x55d8fefb2e80] mmco: unref short failure
09/18/2024 16:50:46 - INFO - __main__ -   current idx -H9yXYU9VP4.53 from finetune_area returns wrong image/video, use 143939 instead.
[h264 @ 0x5598dc67f600] mmco: unref short failure
[h264 @ 0x5565f3608ac0] mmco: unref short failure
[h264 @ 0x5565f3608ac0] mmco: unref short failure
[h264 @ 0x5565f282ed80] mmco: unref short failure
[h264 @ 0x5598e7e5cec0] mmco: unref short failure
[h264 @ 0x5598e7e5cec0] mmco: unref short failure
[h264 @ 0x5598e7e5cec0] mmco: unref short failure
[h264 @ 0x5598e7e5cec0] mmco: unref short failure
[h264 @ 0x5598e31aa940] mmco: unref short failure
[h264 @ 0x5598e31aa940] mmco: unref short failure
[h264 @ 0x55cb5e406c40] mmco: unref short failure
[h264 @ 0x55cb5e406c40] mmco: unref short failure
[h264 @ 0x55cb5e406c40] mmco: unref short failure
[h264 @ 0x55cb5e406c40] mmco: unref short failure
[h264 @ 0x55d90b292e00] mmco: unref short failure
[h264 @ 0x55d90b292e00] mmco: unref short failure
[h264 @ 0x55cb62d1d8c0] mmco: unref short failure
[h264 @ 0x55cb74f52c00] mmco: unref short failure
[h264 @ 0x5565e6125740] mmco: unref short failure
[h264 @ 0x5598dc1c3780] mmco: unref short failure
[h264 @ 0x5565f36088c0] mmco: unref short failure
[h264 @ 0x5565f36088c0] mmco: unref short failure
[h264 @ 0x55d91dc01c40] mmco: unref short failure
[h264 @ 0x55d91dc01c40] mmco: unref short failure
 10%|█         | 298/2910 [1:46:45<22:33:05, 31.08s/it][h264 @ 0x55cb6c6af380] mmco: unref short failure
[h264 @ 0x55cb6c6af380] mmco: unref short failure
[h264 @ 0x5598f3937a80] mmco: unref short failure
[h264 @ 0x5598f3937a80] mmco: unref short failure
[h264 @ 0x5565f4333840] mmco: unref short failure
[h264 @ 0x5565f4333840] mmco: unref short failure
 10%|█         | 299/2910 [1:46:59<18:46:36, 25.89s/it]09/18/2024 16:52:08 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 16:52:08 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb70c56c80] mmco: unref short failure
[h264 @ 0x55cb70c56c80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55d916e3dcc0] mmco: unref short failure
[h264 @ 0x55d916e3dcc0] mmco: unref short failure
[h264 @ 0x55660202f540] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565ea293200] mmco: unref short failure
[h264 @ 0x5565ea293200] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 16:52:47 - INFO - __main__ -   current idx -Gh2S5bmJFk.26 from finetune_area returns wrong image/video, use 133852 instead.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598ef0c1d80] mmco: unref short failure
[h264 @ 0x5598ef0c1d80] mmco: unref short failure
[h264 @ 0x55cb70c56e80] mmco: unref short failure
[h264 @ 0x55660a3489c0] mmco: unref short failure
[h264 @ 0x55660a3489c0] mmco: unref short failure
[h264 @ 0x55d908aa8340] mmco: unref short failure
[h264 @ 0x55d908aa8340] mmco: unref short failure
[h264 @ 0x5598e30f8200] mmco: unref short failure
09/18/2024 16:53:42 - INFO - __main__ -   current idx GdCloC04v0E.24 from finetune_area returns wrong image/video, use 109565 instead.
[h264 @ 0x55cb66a86780] mmco: unref short failure
[h264 @ 0x55cb66a86780] mmco: unref short failure
[h264 @ 0x55cb66a86780] mmco: unref short failure
[h264 @ 0x55cb66a86780] mmco: unref short failure
09/18/2024 16:53:50 - INFO - __main__ -   current idx 1Ubj2kIiKMw.71 from finetune_area returns wrong image/video, use 105471 instead.
[h264 @ 0x5598e8904440] mmco: unref short failure
[h264 @ 0x5598e8904440] mmco: unref short failure
[h264 @ 0x5565e51d4000] mmco: unref short failure
[h264 @ 0x5565e51d4000] mmco: unref short failure
[h264 @ 0x55cb733bbe40] mmco: unref short failure
[h264 @ 0x556607bd8c80] mmco: unref short failure
[h264 @ 0x55d91cdf3d00] mmco: unref short failure
[h264 @ 0x55cb62bc2780] mmco: unref short failure
[h264 @ 0x55660339a480] mmco: unref short failure
[h264 @ 0x55660339a480] mmco: unref short failure
[h264 @ 0x5598f64e41c0] mmco: unref short failure
[h264 @ 0x5598f64e41c0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<02:19,  1.58it/s][A
  1%|          | 2/221 [00:01<02:21,  1.54it/s][A
  1%|▏         | 3/221 [00:01<01:54,  1.90it/s][A
  2%|▏         | 4/221 [00:01<01:23,  2.60it/s][A
  2%|▏         | 5/221 [00:02<01:07,  3.20it/s][A
  3%|▎         | 6/221 [00:02<00:57,  3.74it/s][A
  3%|▎         | 7/221 [00:02<00:56,  3.80it/s][A
  4%|▎         | 8/221 [00:02<00:57,  3.73it/s][A
  4%|▍         | 9/221 [00:03<01:06,  3.19it/s][A
  5%|▍         | 10/221 [00:03<01:22,  2.55it/s][A
  5%|▍         | 11/221 [00:03<01:08,  3.06it/s][A
  5%|▌         | 12/221 [00:04<01:26,  2.41it/s][A
  6%|▌         | 13/221 [00:04<01:21,  2.55it/s][A
  6%|▋         | 14/221 [00:05<01:29,  2.30it/s][A
  7%|▋         | 15/221 [00:05<01:20,  2.57it/s][A
  7%|▋         | 16/221 [00:06<01:31,  2.23it/s][A
  8%|▊         | 17/221 [00:06<01:31,  2.22it/s][A
  8%|▊         | 18/221 [00:07<01:26,  2.34it/s][A[h264 @ 0x55cb642e1cc0] mmco: unref short failure

  9%|▊         | 19/221 [00:07<01:13,  2.74it/s][A
  9%|▉         | 20/221 [00:07<01:01,  3.25it/s][A
 10%|▉         | 21/221 [00:07<00:56,  3.55it/s][A
 10%|▉         | 22/221 [00:07<00:54,  3.66it/s][A
 10%|█         | 23/221 [00:08<00:45,  4.32it/s][A
 11%|█         | 24/221 [00:08<00:43,  4.56it/s][A
 11%|█▏        | 25/221 [00:08<00:47,  4.10it/s][A
 12%|█▏        | 26/221 [00:09<00:59,  3.29it/s][A
 12%|█▏        | 27/221 [00:09<00:48,  4.02it/s][A
 13%|█▎        | 28/221 [00:10<01:25,  2.27it/s][A
 13%|█▎        | 29/221 [00:10<01:13,  2.63it/s][A
 14%|█▎        | 30/221 [00:10<01:07,  2.82it/s][A
 14%|█▍        | 31/221 [00:11<01:12,  2.62it/s][A
 14%|█▍        | 32/221 [00:11<00:57,  3.31it/s][A
 15%|█▍        | 33/221 [00:11<00:50,  3.73it/s][A
 15%|█▌        | 34/221 [00:11<00:41,  4.49it/s][A
 16%|█▌        | 35/221 [00:11<00:46,  4.00it/s][A
 16%|█▋        | 36/221 [00:12<01:03,  2.92it/s][A
 17%|█▋        | 37/221 [00:12<01:13,  2.49it/s][A
 17%|█▋        | 38/221 [00:13<01:19,  2.30it/s][A
 18%|█▊        | 39/221 [00:13<01:04,  2.83it/s][A
 18%|█▊        | 40/221 [00:13<01:04,  2.79it/s][A
 19%|█▊        | 41/221 [00:14<00:56,  3.19it/s][A
 19%|█▉        | 42/221 [00:14<01:22,  2.16it/s][A
 19%|█▉        | 43/221 [00:15<01:07,  2.63it/s][A
 20%|██        | 45/221 [00:15<01:05,  2.68it/s][A
 21%|██        | 46/221 [00:16<01:06,  2.61it/s][A
 21%|██▏       | 47/221 [00:16<01:10,  2.47it/s][A
 22%|██▏       | 48/221 [00:16<00:59,  2.91it/s][A
 22%|██▏       | 49/221 [00:17<01:06,  2.60it/s][A
 23%|██▎       | 50/221 [00:17<01:00,  2.82it/s][A
 23%|██▎       | 51/221 [00:17<00:54,  3.12it/s][A
 24%|██▎       | 52/221 [00:18<00:51,  3.31it/s][A
 24%|██▍       | 53/221 [00:18<00:47,  3.51it/s][A
 24%|██▍       | 54/221 [00:19<01:13,  2.27it/s][A
 25%|██▍       | 55/221 [00:19<01:10,  2.36it/s][A
 25%|██▌       | 56/221 [00:19<01:00,  2.74it/s][A
 26%|██▌       | 57/221 [00:20<00:53,  3.06it/s][A
 26%|██▌       | 58/221 [00:20<00:44,  3.70it/s][A
 27%|██▋       | 59/221 [00:20<00:44,  3.61it/s][A
 27%|██▋       | 60/221 [00:21<01:40,  1.61it/s][A
 28%|██▊       | 61/221 [00:22<01:22,  1.93it/s][A
 28%|██▊       | 62/221 [00:22<01:15,  2.10it/s][A
 29%|██▊       | 63/221 [00:22<01:03,  2.49it/s][A
 29%|██▉       | 64/221 [00:23<00:57,  2.71it/s][A
 29%|██▉       | 65/221 [00:23<00:55,  2.80it/s][A
 30%|██▉       | 66/221 [00:23<00:55,  2.79it/s][A
 30%|███       | 67/221 [00:24<01:04,  2.38it/s][A[h264 @ 0x55d91af30680] mmco: unref short failure
[h264 @ 0x55d91af30680] mmco: unref short failure
[h264 @ 0x55d91af30680] mmco: unref short failure

 31%|███       | 68/221 [00:24<00:54,  2.83it/s][A[h264 @ 0x55cb6d6731c0] mmco: unref short failure

 31%|███       | 69/221 [00:25<01:05,  2.34it/s][A
 32%|███▏      | 70/221 [00:25<00:56,  2.66it/s][A
 32%|███▏      | 71/221 [00:25<00:53,  2.79it/s][A
 33%|███▎      | 72/221 [00:26<01:06,  2.24it/s][A
 33%|███▎      | 73/221 [00:26<01:08,  2.15it/s][A
 33%|███▎      | 74/221 [00:27<00:56,  2.59it/s][A
 34%|███▍      | 75/221 [00:27<00:55,  2.62it/s][A
 34%|███▍      | 76/221 [00:27<00:47,  3.07it/s][A
 35%|███▍      | 77/221 [00:27<00:45,  3.16it/s][A
 35%|███▌      | 78/221 [00:28<00:38,  3.67it/s][A
 36%|███▌      | 79/221 [00:28<01:02,  2.27it/s][A
 36%|███▌      | 80/221 [00:29<00:57,  2.43it/s][A
 37%|███▋      | 81/221 [00:29<00:50,  2.78it/s][A
 37%|███▋      | 82/221 [00:30<01:02,  2.22it/s][A
 38%|███▊      | 83/221 [00:30<01:00,  2.28it/s][A[h264 @ 0x55cb6ed6bb80] mmco: unref short failure

 38%|███▊      | 84/221 [00:30<00:54,  2.52it/s][A
 38%|███▊      | 85/221 [00:31<00:42,  3.20it/s][A
 39%|███▉      | 86/221 [00:31<00:37,  3.58it/s][A
 39%|███▉      | 87/221 [00:32<01:06,  2.02it/s][A[h264 @ 0x55cb6d66b0c0] mmco: unref short failure

 40%|███▉      | 88/221 [00:33<01:23,  1.60it/s][A
 40%|████      | 89/221 [00:33<01:08,  1.92it/s][A
 41%|████      | 90/221 [00:33<00:57,  2.28it/s][A
 41%|████      | 91/221 [00:33<00:46,  2.81it/s][A
 42%|████▏     | 92/221 [00:34<00:49,  2.62it/s][A
 42%|████▏     | 93/221 [00:34<00:49,  2.57it/s][A
 43%|████▎     | 94/221 [00:34<00:44,  2.86it/s][A
 43%|████▎     | 95/221 [00:35<00:42,  2.94it/s][A
 43%|████▎     | 96/221 [00:35<00:45,  2.73it/s][A
 44%|████▍     | 97/221 [00:36<00:46,  2.66it/s][A
 44%|████▍     | 98/221 [00:36<00:43,  2.85it/s][A
 45%|████▍     | 99/221 [00:36<00:36,  3.36it/s][A
 45%|████▌     | 100/221 [00:36<00:35,  3.41it/s][A
 46%|████▌     | 101/221 [00:37<00:33,  3.64it/s][A
 46%|████▌     | 102/221 [00:37<00:34,  3.46it/s][A
 47%|████▋     | 103/221 [00:37<00:30,  3.89it/s][A
 47%|████▋     | 104/221 [00:37<00:25,  4.52it/s][A
 48%|████▊     | 105/221 [00:37<00:28,  4.06it/s][A
 48%|████▊     | 106/221 [00:39<00:59,  1.93it/s][A
 48%|████▊     | 107/221 [00:39<00:46,  2.44it/s][A
 49%|████▉     | 108/221 [00:39<00:41,  2.69it/s][A
 49%|████▉     | 109/221 [00:39<00:40,  2.75it/s][A
 50%|████▉     | 110/221 [00:40<00:43,  2.53it/s][A[h264 @ 0x55cb7a4c1cc0] mmco: unref short failure

 50%|█████     | 111/221 [00:40<00:49,  2.23it/s][A
 51%|█████     | 112/221 [00:41<00:40,  2.66it/s][A
 51%|█████     | 113/221 [00:41<00:41,  2.59it/s][A
 52%|█████▏    | 114/221 [00:41<00:32,  3.30it/s][A[h264 @ 0x55660477a480] mmco: unref short failure
[h264 @ 0x55660477a480] mmco: unref short failure

 52%|█████▏    | 115/221 [00:41<00:25,  4.12it/s][A
 52%|█████▏    | 116/221 [00:42<00:37,  2.77it/s][A
 53%|█████▎    | 117/221 [00:42<00:39,  2.62it/s][A09/18/2024 16:55:13 - INFO - __main__ -   current idx 0mdFhko5-lk.77 from finetune_area returns wrong image/video, use 30674 instead.

 53%|█████▎    | 118/221 [00:43<00:38,  2.69it/s][A
 54%|█████▍    | 119/221 [00:43<00:43,  2.36it/s][A
 54%|█████▍    | 120/221 [00:44<00:38,  2.60it/s][A
 55%|█████▍    | 121/221 [00:44<00:31,  3.13it/s][A
 55%|█████▌    | 122/221 [00:44<00:30,  3.21it/s][A
 56%|█████▌    | 123/221 [00:44<00:28,  3.43it/s][A
 56%|█████▌    | 124/221 [00:45<00:29,  3.32it/s][A
 57%|█████▋    | 125/221 [00:45<00:37,  2.54it/s][A
 57%|█████▋    | 126/221 [00:45<00:31,  2.99it/s][A
 57%|█████▋    | 127/221 [00:46<00:44,  2.10it/s][A
 58%|█████▊    | 128/221 [00:47<00:42,  2.18it/s][A
 58%|█████▊    | 129/221 [00:47<00:35,  2.59it/s][A
 59%|█████▉    | 130/221 [00:47<00:33,  2.73it/s][A
 59%|█████▉    | 131/221 [00:47<00:29,  3.08it/s][A
 60%|█████▉    | 132/221 [00:48<00:26,  3.40it/s][A[h264 @ 0x55d9197b86c0] mmco: unref short failure
[h264 @ 0x55d9197b86c0] mmco: unref short failure
[h264 @ 0x55d9197b86c0] mmco: unref short failure
[h264 @ 0x55d9197b86c0] mmco: unref short failure

 60%|██████    | 133/221 [00:48<00:35,  2.47it/s][A
 61%|██████    | 134/221 [00:49<00:32,  2.69it/s][A
 61%|██████    | 135/221 [00:49<00:28,  3.01it/s][A
 62%|██████▏   | 136/221 [00:49<00:30,  2.82it/s][A
 62%|██████▏   | 137/221 [00:49<00:25,  3.27it/s][A
 62%|██████▏   | 138/221 [00:50<00:27,  2.97it/s][A
 63%|██████▎   | 139/221 [00:50<00:32,  2.54it/s][A
 63%|██████▎   | 140/221 [00:51<00:31,  2.58it/s][A
 64%|██████▍   | 141/221 [00:51<00:27,  2.91it/s][A
 64%|██████▍   | 142/221 [00:51<00:28,  2.79it/s][A
 65%|██████▍   | 143/221 [00:52<00:29,  2.62it/s][A
 65%|██████▌   | 144/221 [00:52<00:28,  2.66it/s][A
 66%|██████▌   | 145/221 [00:52<00:23,  3.24it/s][A
 66%|██████▌   | 146/221 [00:52<00:19,  3.80it/s][A
 67%|██████▋   | 147/221 [00:53<00:19,  3.77it/s][A[h264 @ 0x5565e7d144c0] mmco: unref short failure

 67%|██████▋   | 148/221 [00:53<00:23,  3.13it/s][A
 67%|██████▋   | 149/221 [00:53<00:22,  3.21it/s][A
 68%|██████▊   | 150/221 [00:54<00:20,  3.48it/s][A
 68%|██████▊   | 151/221 [00:54<00:26,  2.66it/s][A
 69%|██████▉   | 152/221 [00:55<00:37,  1.83it/s][A
 69%|██████▉   | 153/221 [00:56<00:33,  2.01it/s][A
 70%|██████▉   | 154/221 [00:56<00:29,  2.25it/s][A
 70%|███████   | 155/221 [00:56<00:25,  2.55it/s][A
 71%|███████   | 156/221 [00:57<00:27,  2.39it/s][A
 71%|███████   | 157/221 [00:57<00:29,  2.14it/s][A
 71%|███████▏  | 158/221 [00:57<00:24,  2.52it/s][A
 72%|███████▏  | 159/221 [00:58<00:21,  2.94it/s][A
 72%|███████▏  | 160/221 [00:58<00:28,  2.17it/s][A
 73%|███████▎  | 161/221 [00:59<00:21,  2.79it/s][A
 73%|███████▎  | 162/221 [00:59<00:18,  3.17it/s][A
 74%|███████▍  | 163/221 [00:59<00:19,  3.04it/s][A
 74%|███████▍  | 164/221 [00:59<00:18,  3.01it/s][A
 75%|███████▍  | 165/221 [01:00<00:16,  3.32it/s][A
 75%|███████▌  | 166/221 [01:00<00:22,  2.43it/s][A
 76%|███████▌  | 167/221 [01:01<00:18,  2.84it/s][A[h264 @ 0x55d91684e9c0] mmco: unref short failure
[h264 @ 0x55d91684e9c0] mmco: unref short failure
[h264 @ 0x55d91684e9c0] mmco: unref short failure
[h264 @ 0x55d91684e9c0] mmco: unref short failure

 76%|███████▌  | 168/221 [01:01<00:23,  2.22it/s][A
 76%|███████▋  | 169/221 [01:01<00:19,  2.61it/s][A
 77%|███████▋  | 170/221 [01:02<00:19,  2.68it/s][A
 77%|███████▋  | 171/221 [01:02<00:22,  2.20it/s][A
 78%|███████▊  | 172/221 [01:03<00:18,  2.69it/s][A
 78%|███████▊  | 173/221 [01:03<00:17,  2.68it/s][A
 79%|███████▊  | 174/221 [01:03<00:15,  3.05it/s][A
 79%|███████▉  | 175/221 [01:04<00:16,  2.85it/s][A
 80%|███████▉  | 176/221 [01:04<00:14,  3.12it/s][A
[h264 @ 0x55cb76e68040] mmco: unref short failure
 80%|████████  | 177/221 [01:04<00:12,  3.50it/s][A
 81%|████████  | 178/221 [01:04<00:12,  3.32it/s][A
 81%|████████  | 179/221 [01:05<00:13,  3.02it/s][A
 81%|████████▏ | 180/221 [01:05<00:11,  3.54it/s][A
 82%|████████▏ | 181/221 [01:05<00:12,  3.29it/s][A
 82%|████████▏ | 182/221 [01:06<00:10,  3.62it/s][A
 83%|████████▎ | 183/221 [01:06<00:09,  3.91it/s][A
 83%|████████▎ | 184/221 [01:06<00:10,  3.46it/s][A
 84%|████████▎ | 185/221 [01:06<00:08,  4.20it/s][A
 84%|████████▍ | 186/221 [01:07<00:08,  3.90it/s][A
 85%|████████▍ | 187/221 [01:07<00:10,  3.37it/s][A
 85%|████████▌ | 188/221 [01:07<00:09,  3.41it/s][A
 86%|████████▌ | 189/221 [01:08<00:09,  3.22it/s][A
 86%|████████▌ | 190/221 [01:08<00:12,  2.55it/s][A
 86%|████████▋ | 191/221 [01:08<00:10,  2.97it/s][A
 87%|████████▋ | 192/221 [01:09<00:10,  2.85it/s][A
 87%|████████▋ | 193/221 [01:09<00:07,  3.60it/s][A[h264 @ 0x55cb75a1f700] mmco: unref short failure
[h264 @ 0x55cb75a1f700] mmco: unref short failure
[h264 @ 0x55cb75a1f700] mmco: unref short failure
[h264 @ 0x55cb75a1f700] mmco: unref short failure

 88%|████████▊ | 194/221 [01:10<00:13,  1.96it/s][A
 88%|████████▊ | 195/221 [01:10<00:11,  2.17it/s][A
 89%|████████▊ | 196/221 [01:11<00:11,  2.24it/s][A
 89%|████████▉ | 197/221 [01:11<00:09,  2.61it/s][A
 90%|████████▉ | 198/221 [01:11<00:07,  2.98it/s][A
 90%|█████████ | 199/221 [01:11<00:05,  3.69it/s][A
 90%|█████████ | 200/221 [01:12<00:06,  3.36it/s][A
 91%|█████████ | 201/221 [01:12<00:05,  3.49it/s][A
 91%|█████████▏| 202/221 [01:12<00:05,  3.36it/s][A
 92%|█████████▏| 203/221 [01:13<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:13<00:04,  3.45it/s][A
 93%|█████████▎| 205/221 [01:13<00:04,  3.60it/s][A
 93%|█████████▎| 206/221 [01:14<00:06,  2.37it/s][A
 94%|█████████▎| 207/221 [01:14<00:04,  3.05it/s][A
 94%|█████████▍| 208/221 [01:14<00:03,  3.56it/s][A[h264 @ 0x55cb63d1b1c0] mmco: unref short failure

 95%|█████████▍| 209/221 [01:14<00:03,  3.70it/s][A
 95%|█████████▌| 210/221 [01:14<00:02,  4.29it/s][A
 95%|█████████▌| 211/221 [01:15<00:02,  3.44it/s][A
 96%|█████████▌| 212/221 [01:15<00:02,  3.73it/s][A
 96%|█████████▋| 213/221 [01:15<00:02,  3.79it/s][A
 97%|█████████▋| 214/221 [01:16<00:01,  3.81it/s][A[h264 @ 0x5565f5da1f00] mmco: unref short failure

 97%|█████████▋| 215/221 [01:16<00:01,  3.37it/s][A
 98%|█████████▊| 216/221 [01:16<00:01,  3.07it/s][A
 98%|█████████▊| 217/221 [01:17<00:01,  2.76it/s][A
 99%|█████████▊| 218/221 [01:17<00:01,  2.68it/s][A
 99%|█████████▉| 219/221 [01:18<00:00,  2.77it/s][A
100%|█████████▉| 220/221 [01:19<00:00,  1.59it/s][A
100%|██████████| 221/221 [01:19<00:00,  1.96it/s][A100%|██████████| 221/221 [01:19<00:00,  2.78it/s]
[h264 @ 0x55d90dab3080] mmco: unref short failure
[h264 @ 0x5565ea32efc0] mmco: unref short failure
[h264 @ 0x5565ea32efc0] mmco: unref short failure
[h264 @ 0x5598eb60f600] mmco: unref short failure
[h264 @ 0x5598eb60f600] mmco: unref short failure
[h264 @ 0x5598eb60f600] mmco: unref short failure
[h264 @ 0x5598eb60f600] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:10,  3.13it/s][A
  1%|          | 2/221 [00:00<01:09,  3.15it/s][A
  1%|▏         | 3/221 [00:00<01:07,  3.21it/s][A
  2%|▏         | 4/221 [00:01<01:09,  3.13it/s][A
  2%|▏         | 5/221 [00:01<01:09,  3.12it/s][A
  3%|▎         | 6/221 [00:01<01:09,  3.09it/s][A
  3%|▎         | 7/221 [00:02<01:11,  2.98it/s][A
  4%|▎         | 8/221 [00:02<01:10,  3.01it/s][A
  4%|▍         | 9/221 [00:02<01:08,  3.09it/s][A
  5%|▍         | 10/221 [00:03<01:06,  3.15it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.19it/s][A
  5%|▌         | 12/221 [00:03<01:04,  3.22it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.22it/s][A
  6%|▋         | 14/221 [00:04<01:04,  3.20it/s][A
  7%|▋         | 15/221 [00:04<01:04,  3.21it/s][A
  7%|▋         | 16/221 [00:05<01:03,  3.23it/s][A
  8%|▊         | 17/221 [00:05<01:03,  3.20it/s][A
  8%|▊         | 18/221 [00:05<01:03,  3.20it/s][A
  9%|▊         | 19/221 [00:06<01:02,  3.23it/s][A
  9%|▉         | 20/221 [00:06<01:02,  3.20it/s][A
 10%|▉         | 21/221 [00:06<01:02,  3.22it/s][A
 10%|▉         | 22/221 [00:06<01:01,  3.21it/s][A
 10%|█         | 23/221 [00:07<01:02,  3.19it/s][A
 11%|█         | 24/221 [00:07<01:02,  3.17it/s][A
 11%|█▏        | 25/221 [00:07<01:01,  3.21it/s][A
 12%|█▏        | 26/221 [00:08<01:00,  3.23it/s][A
 12%|█▏        | 27/221 [00:08<00:59,  3.25it/s][A
 13%|█▎        | 28/221 [00:08<01:01,  3.12it/s][A
 13%|█▎        | 29/221 [00:09<01:00,  3.15it/s][A
 14%|█▎        | 30/221 [00:09<01:00,  3.17it/s][A
 14%|█▍        | 31/221 [00:09<00:59,  3.18it/s][A
 14%|█▍        | 32/221 [00:10<00:58,  3.21it/s][A
 15%|█▍        | 33/221 [00:10<00:58,  3.23it/s][A
 15%|█▌        | 34/221 [00:10<00:58,  3.19it/s][A
 16%|█▌        | 35/221 [00:11<00:57,  3.22it/s][A
 16%|█▋        | 36/221 [00:11<00:57,  3.24it/s][A
 17%|█▋        | 37/221 [00:11<00:58,  3.16it/s][A
 17%|█▋        | 38/221 [00:11<00:57,  3.20it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.23it/s][A09/18/2024 16:56:08 - INFO - __main__ -   current idx hDAA1fIvxDY.2 from finetune_area returns wrong image/video, use 96774 instead.

 18%|█▊        | 40/221 [00:12<00:56,  3.20it/s][A
 19%|█▊        | 41/221 [00:12<00:55,  3.23it/s][A
 19%|█▉        | 42/221 [00:13<00:55,  3.21it/s][A
 19%|█▉        | 43/221 [00:13<00:55,  3.24it/s][A
 20%|█▉        | 44/221 [00:13<00:56,  3.15it/s][A
 20%|██        | 45/221 [00:14<00:55,  3.19it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.22it/s][A
 21%|██▏       | 47/221 [00:14<00:54,  3.22it/s][A
 22%|██▏       | 48/221 [00:15<00:54,  3.20it/s][A
 22%|██▏       | 49/221 [00:15<00:53,  3.21it/s][A[h264 @ 0x55cb5f545980] mmco: unref short failure
[h264 @ 0x55cb5f545980] mmco: unref short failure
[h264 @ 0x55cb5f545980] mmco: unref short failure
[h264 @ 0x55cb5f545980] mmco: unref short failure

 23%|██▎       | 50/221 [00:15<00:52,  3.24it/s][A
 23%|██▎       | 51/221 [00:15<00:52,  3.25it/s][A
 24%|██▎       | 52/221 [00:16<00:51,  3.26it/s][A
 24%|██▍       | 53/221 [00:16<00:52,  3.22it/s][A
 24%|██▍       | 54/221 [00:16<00:52,  3.18it/s][A
 25%|██▍       | 55/221 [00:17<00:52,  3.19it/s][A
 25%|██▌       | 56/221 [00:17<00:51,  3.22it/s][A
 26%|██▌       | 57/221 [00:17<00:50,  3.24it/s][A
 26%|██▌       | 58/221 [00:18<00:50,  3.25it/s][A
 27%|██▋       | 59/221 [00:18<00:50,  3.23it/s][A
 27%|██▋       | 60/221 [00:18<00:50,  3.21it/s][A
 28%|██▊       | 61/221 [00:19<00:49,  3.23it/s][A
 28%|██▊       | 62/221 [00:19<00:50,  3.15it/s][A
 29%|██▊       | 63/221 [00:19<00:51,  3.05it/s][A
 29%|██▉       | 64/221 [00:20<00:50,  3.10it/s][A
 29%|██▉       | 65/221 [00:20<00:50,  3.11it/s][A
 30%|██▉       | 66/221 [00:20<00:49,  3.12it/s][A
 30%|███       | 67/221 [00:21<00:48,  3.15it/s][A
 31%|███       | 68/221 [00:21<00:48,  3.15it/s][A
 31%|███       | 69/221 [00:21<00:47,  3.18it/s][A[h264 @ 0x5598e9ba3b80] mmco: unref short failure
[h264 @ 0x5598e9ba3b80] mmco: unref short failure
[h264 @ 0x5598e9ba3b80] mmco: unref short failure
[h264 @ 0x5598e9ba3b80] mmco: unref short failure

 32%|███▏      | 70/221 [00:21<00:47,  3.20it/s][A
 32%|███▏      | 71/221 [00:22<00:46,  3.22it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.24it/s][A
 33%|███▎      | 73/221 [00:22<00:45,  3.26it/s][A
 33%|███▎      | 74/221 [00:23<00:45,  3.26it/s][A
 34%|███▍      | 75/221 [00:23<00:45,  3.24it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.25it/s][A
 35%|███▍      | 77/221 [00:24<00:44,  3.26it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.27it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.27it/s][A
 36%|███▌      | 80/221 [00:25<00:42,  3.28it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.28it/s][A
 37%|███▋      | 82/221 [00:25<00:43,  3.17it/s][A
 38%|███▊      | 83/221 [00:25<00:43,  3.20it/s][A
 38%|███▊      | 84/221 [00:26<00:43,  3.18it/s][A
 38%|███▊      | 85/221 [00:26<00:43,  3.16it/s][A
 39%|███▉      | 86/221 [00:26<00:42,  3.20it/s][A
 39%|███▉      | 87/221 [00:27<00:41,  3.22it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.24it/s][A
 40%|████      | 89/221 [00:27<00:40,  3.26it/s][A
 41%|████      | 90/221 [00:28<00:40,  3.27it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.28it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.27it/s][A
 42%|████▏     | 93/221 [00:29<00:39,  3.28it/s][A
 43%|████▎     | 94/221 [00:29<00:38,  3.28it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.28it/s][A
 43%|████▎     | 96/221 [00:29<00:38,  3.29it/s][A
 44%|████▍     | 97/221 [00:30<00:37,  3.29it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.29it/s][A
 45%|████▍     | 99/221 [00:30<00:37,  3.29it/s][A
 45%|████▌     | 100/221 [00:31<00:36,  3.29it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.29it/s][A
 46%|████▌     | 102/221 [00:31<00:36,  3.29it/s][A
 47%|████▋     | 103/221 [00:32<00:35,  3.29it/s][A
 47%|████▋     | 104/221 [00:32<00:35,  3.29it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.29it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.30it/s][A
 48%|████▊     | 107/221 [00:33<00:34,  3.30it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.30it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.30it/s][A
 50%|████▉     | 110/221 [00:34<00:33,  3.30it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.30it/s][A
 51%|█████     | 112/221 [00:34<00:33,  3.30it/s][A
 51%|█████     | 113/221 [00:35<00:32,  3.30it/s][A
 52%|█████▏    | 114/221 [00:35<00:32,  3.30it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.30it/s][A
 52%|█████▏    | 116/221 [00:36<00:31,  3.30it/s][A
 53%|█████▎    | 117/221 [00:36<00:31,  3.30it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.30it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.30it/s][A
 54%|█████▍    | 120/221 [00:37<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:38<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:39<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:40<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:42<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:42<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:44<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:46<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:47<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:48<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:48<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:49<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:50<00:23,  2.56it/s][A
 74%|███████▍  | 163/221 [00:50<00:21,  2.75it/s][A
 74%|███████▍  | 164/221 [00:50<00:19,  2.90it/s][A
 75%|███████▍  | 165/221 [00:51<00:18,  3.01it/s][A
 75%|███████▌  | 166/221 [00:51<00:17,  3.09it/s][A
 76%|███████▌  | 167/221 [00:51<00:17,  3.16it/s][A
 76%|███████▌  | 168/221 [00:52<00:16,  3.20it/s][A
 76%|███████▋  | 169/221 [00:52<00:16,  3.23it/s][A
 77%|███████▋  | 170/221 [00:52<00:15,  3.26it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.27it/s][A
 78%|███████▊  | 172/221 [00:53<00:14,  3.28it/s][A
 78%|███████▊  | 173/221 [00:53<00:14,  3.29it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.30it/s][A
 79%|███████▉  | 175/221 [00:54<00:13,  3.30it/s][A
 80%|███████▉  | 176/221 [00:54<00:13,  3.30it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:55<00:13,  3.31it/s][A
 81%|████████  | 179/221 [00:55<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:56<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:56<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:57<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:57<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:58<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:58<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:58<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:59<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [01:00<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [01:00<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:01<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:01<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:01<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:02<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:02<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:03<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:05<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:05<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:06<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:06<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:07<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:07<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:08<00:00,  3.31it/s][A100%|██████████| 221/221 [01:08<00:00,  3.25it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:01,  3.55it/s][A
  1%|          | 2/221 [00:01<01:59,  1.83it/s][A
  2%|▏         | 4/221 [00:01<01:08,  3.18it/s][A
  2%|▏         | 5/221 [00:01<01:01,  3.52it/s][A
  3%|▎         | 7/221 [00:01<00:53,  4.03it/s][A
  4%|▎         | 8/221 [00:02<01:01,  3.47it/s][A
  4%|▍         | 9/221 [00:02<01:12,  2.93it/s][A
  5%|▍         | 10/221 [00:03<01:22,  2.55it/s][A
  5%|▍         | 11/221 [00:03<01:11,  2.92it/s][A
  5%|▌         | 12/221 [00:03<01:10,  2.95it/s][A
  6%|▌         | 13/221 [00:04<01:42,  2.03it/s][A
  7%|▋         | 15/221 [00:05<01:04,  3.17it/s][A
  7%|▋         | 16/221 [00:05<01:07,  3.06it/s][A
  8%|▊         | 17/221 [00:06<01:40,  2.03it/s][A
  8%|▊         | 18/221 [00:06<01:33,  2.17it/s][A
  9%|▊         | 19/221 [00:07<01:26,  2.32it/s][A
  9%|▉         | 20/221 [00:07<01:08,  2.93it/s][A
 10%|▉         | 21/221 [00:07<00:54,  3.65it/s][A
 10%|▉         | 22/221 [00:07<00:50,  3.96it/s][A
 10%|█         | 23/221 [00:07<00:43,  4.54it/s][A
 11%|█         | 24/221 [00:07<00:38,  5.11it/s][A
 11%|█▏        | 25/221 [00:08<00:45,  4.32it/s][A
 12%|█▏        | 26/221 [00:08<00:47,  4.11it/s][A
 13%|█▎        | 28/221 [00:09<01:11,  2.68it/s][A
 13%|█▎        | 29/221 [00:09<01:04,  2.99it/s][A
 14%|█▎        | 30/221 [00:10<01:06,  2.86it/s][A
 14%|█▍        | 31/221 [00:10<01:12,  2.61it/s][A
 14%|█▍        | 32/221 [00:10<00:58,  3.23it/s][A
 15%|█▍        | 33/221 [00:10<00:52,  3.59it/s][A
 15%|█▌        | 34/221 [00:10<00:42,  4.36it/s][A
 16%|█▌        | 35/221 [00:11<00:41,  4.44it/s][A
 16%|█▋        | 36/221 [00:11<00:50,  3.66it/s][A
 17%|█▋        | 37/221 [00:11<00:59,  3.10it/s][A
 17%|█▋        | 38/221 [00:12<01:07,  2.69it/s][A
 18%|█▊        | 39/221 [00:12<00:54,  3.31it/s][A
 18%|█▊        | 40/221 [00:13<01:05,  2.78it/s][A
 19%|█▊        | 41/221 [00:13<00:55,  3.22it/s][A
 19%|█▉        | 42/221 [00:13<00:49,  3.59it/s][A
 19%|█▉        | 43/221 [00:13<00:40,  4.44it/s][A
 20%|█▉        | 44/221 [00:13<00:33,  5.32it/s][A
 20%|██        | 45/221 [00:14<00:43,  4.03it/s][A
 21%|██        | 46/221 [00:14<00:39,  4.42it/s][A
 21%|██▏       | 47/221 [00:14<00:42,  4.07it/s][A
 22%|██▏       | 48/221 [00:14<00:37,  4.58it/s][A
 22%|██▏       | 49/221 [00:14<00:33,  5.08it/s][A
 23%|██▎       | 50/221 [00:15<00:31,  5.49it/s][A
 24%|██▎       | 52/221 [00:15<00:26,  6.45it/s][A
 24%|██▍       | 53/221 [00:15<00:29,  5.62it/s][A
 24%|██▍       | 54/221 [00:15<00:36,  4.54it/s][A
 25%|██▍       | 55/221 [00:16<00:34,  4.78it/s][A
 25%|██▌       | 56/221 [00:16<00:35,  4.62it/s][A
 26%|██▌       | 57/221 [00:16<00:33,  4.86it/s][A
 26%|██▌       | 58/221 [00:16<00:39,  4.17it/s][A
 27%|██▋       | 59/221 [00:16<00:37,  4.38it/s][A
 27%|██▋       | 60/221 [00:17<00:49,  3.27it/s][A
 28%|██▊       | 61/221 [00:17<00:47,  3.38it/s][A
 28%|██▊       | 62/221 [00:17<00:44,  3.56it/s][A
 29%|██▊       | 63/221 [00:18<00:53,  2.96it/s][A
 29%|██▉       | 64/221 [00:18<00:46,  3.37it/s][A
 29%|██▉       | 65/221 [00:19<01:01,  2.56it/s][A
 30%|██▉       | 66/221 [00:19<00:48,  3.21it/s][A
 30%|███       | 67/221 [00:19<01:00,  2.56it/s][A
 31%|███       | 68/221 [00:20<00:48,  3.16it/s][A
 31%|███       | 69/221 [00:20<00:42,  3.61it/s][A
 32%|███▏      | 70/221 [00:20<00:40,  3.74it/s][A
 32%|███▏      | 71/221 [00:20<00:39,  3.76it/s][A
 33%|███▎      | 72/221 [00:21<00:49,  3.01it/s][A
 33%|███▎      | 73/221 [00:21<00:54,  2.72it/s][A
 33%|███▎      | 74/221 [00:22<00:50,  2.93it/s][A
 34%|███▍      | 75/221 [00:22<00:49,  2.97it/s][A
 34%|███▍      | 76/221 [00:22<00:40,  3.60it/s][A
 35%|███▍      | 77/221 [00:22<00:36,  3.91it/s][A
 35%|███▌      | 78/221 [00:22<00:37,  3.86it/s][A
 36%|███▌      | 79/221 [00:23<00:46,  3.04it/s][A
 36%|███▌      | 80/221 [00:23<00:42,  3.31it/s][A
 37%|███▋      | 81/221 [00:23<00:37,  3.77it/s][A
 37%|███▋      | 82/221 [00:24<00:42,  3.28it/s][A
 38%|███▊      | 83/221 [00:24<00:56,  2.45it/s][A
 38%|███▊      | 84/221 [00:25<00:54,  2.52it/s][A
 38%|███▊      | 85/221 [00:25<00:52,  2.58it/s][A
 39%|███▉      | 86/221 [00:25<00:45,  2.99it/s][A
 39%|███▉      | 87/221 [00:26<01:13,  1.83it/s][A
 40%|███▉      | 88/221 [00:27<01:05,  2.03it/s][A
 40%|████      | 89/221 [00:27<00:54,  2.43it/s][A
 41%|████      | 90/221 [00:27<00:47,  2.74it/s][A
 41%|████      | 91/221 [00:27<00:40,  3.20it/s][A
 42%|████▏     | 92/221 [00:28<00:38,  3.33it/s][A
 42%|████▏     | 93/221 [00:28<00:32,  4.00it/s][A
 43%|████▎     | 94/221 [00:28<00:29,  4.25it/s][A
 43%|████▎     | 95/221 [00:28<00:35,  3.60it/s][A
 43%|████▎     | 96/221 [00:29<00:29,  4.27it/s][A
 44%|████▍     | 97/221 [00:29<00:27,  4.50it/s][A
 44%|████▍     | 98/221 [00:29<00:28,  4.36it/s][A
 45%|████▍     | 99/221 [00:29<00:23,  5.18it/s][A
 45%|████▌     | 100/221 [00:29<00:23,  5.25it/s][A
 46%|████▌     | 101/221 [00:30<00:24,  4.98it/s][A
 46%|████▌     | 102/221 [00:30<00:25,  4.73it/s][A
 47%|████▋     | 103/221 [00:30<00:24,  4.85it/s][A
 47%|████▋     | 104/221 [00:30<00:20,  5.60it/s][A
 48%|████▊     | 105/221 [00:30<00:21,  5.40it/s][A
 48%|████▊     | 106/221 [00:31<00:27,  4.21it/s][A
 48%|████▊     | 107/221 [00:31<00:25,  4.42it/s][A
 49%|████▉     | 108/221 [00:31<00:27,  4.17it/s][A
 49%|████▉     | 109/221 [00:32<00:42,  2.61it/s][A
 50%|████▉     | 110/221 [00:32<00:45,  2.43it/s][A
 50%|█████     | 111/221 [00:33<00:45,  2.43it/s][A
 51%|█████     | 112/221 [00:33<00:38,  2.87it/s][A
 51%|█████     | 113/221 [00:33<00:36,  2.93it/s][A
 52%|█████▏    | 115/221 [00:33<00:24,  4.40it/s][A
 53%|█████▎    | 117/221 [00:34<00:22,  4.53it/s][A
 53%|█████▎    | 118/221 [00:34<00:29,  3.52it/s][A
 54%|█████▍    | 119/221 [00:35<00:36,  2.79it/s][A
 54%|█████▍    | 120/221 [00:35<00:36,  2.80it/s][A
 55%|█████▍    | 121/221 [00:35<00:29,  3.35it/s][A
 55%|█████▌    | 122/221 [00:36<00:27,  3.54it/s][A
 56%|█████▌    | 123/221 [00:36<00:29,  3.37it/s][A
 56%|█████▌    | 124/221 [00:36<00:31,  3.10it/s][A
 57%|█████▋    | 125/221 [00:37<00:33,  2.85it/s][A
 57%|█████▋    | 126/221 [00:37<00:31,  2.99it/s][A
 57%|█████▋    | 127/221 [00:37<00:28,  3.26it/s][A
 58%|█████▊    | 128/221 [00:38<00:29,  3.20it/s][A
 58%|█████▊    | 129/221 [00:38<00:24,  3.77it/s][A
 59%|█████▉    | 130/221 [00:38<00:24,  3.65it/s][A
 59%|█████▉    | 131/221 [00:38<00:22,  4.07it/s][A
 60%|█████▉    | 132/221 [00:38<00:18,  4.89it/s][A
 60%|██████    | 133/221 [00:39<00:30,  2.93it/s][A
 61%|██████    | 134/221 [00:39<00:24,  3.55it/s][A
 61%|██████    | 135/221 [00:39<00:24,  3.52it/s][A
 62%|██████▏   | 136/221 [00:40<00:25,  3.35it/s][A
 62%|██████▏   | 137/221 [00:40<00:23,  3.61it/s][A
 62%|██████▏   | 138/221 [00:40<00:24,  3.35it/s][A
 63%|██████▎   | 139/221 [00:41<00:22,  3.58it/s][A
 63%|██████▎   | 140/221 [00:41<00:21,  3.70it/s][A
 64%|██████▍   | 142/221 [00:41<00:19,  4.07it/s][A
 65%|██████▌   | 144/221 [00:42<00:16,  4.61it/s][A
 66%|██████▌   | 145/221 [00:42<00:15,  4.85it/s][A
 66%|██████▌   | 146/221 [00:42<00:16,  4.44it/s][A
 67%|██████▋   | 147/221 [00:42<00:14,  5.08it/s][A
 67%|██████▋   | 148/221 [00:43<00:18,  3.85it/s][A
 67%|██████▋   | 149/221 [00:43<00:15,  4.55it/s][A
 68%|██████▊   | 150/221 [00:43<00:20,  3.51it/s][A
 68%|██████▊   | 151/221 [00:44<00:20,  3.49it/s][A
 69%|██████▉   | 152/221 [00:44<00:33,  2.08it/s][A
 69%|██████▉   | 153/221 [00:45<00:32,  2.08it/s][A
 70%|██████▉   | 154/221 [00:45<00:31,  2.16it/s][A
 70%|███████   | 155/221 [00:46<00:27,  2.37it/s][A
 71%|███████   | 156/221 [00:46<00:27,  2.36it/s][A
 71%|███████   | 157/221 [00:46<00:24,  2.59it/s][A
 71%|███████▏  | 158/221 [00:47<00:22,  2.80it/s][A
 72%|███████▏  | 159/221 [00:47<00:22,  2.78it/s][A
 72%|███████▏  | 160/221 [00:47<00:21,  2.86it/s][A
 73%|███████▎  | 161/221 [00:48<00:16,  3.54it/s][A
 74%|███████▍  | 163/221 [00:48<00:13,  4.26it/s][A
 74%|███████▍  | 164/221 [00:48<00:15,  3.64it/s][A
 75%|███████▌  | 166/221 [00:49<00:16,  3.26it/s][A
 76%|███████▌  | 167/221 [00:49<00:14,  3.82it/s][A
 76%|███████▌  | 168/221 [00:49<00:13,  3.87it/s][A
 76%|███████▋  | 169/221 [00:50<00:12,  4.25it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.51it/s][A
 77%|███████▋  | 171/221 [00:50<00:14,  3.35it/s][A
 78%|███████▊  | 173/221 [00:50<00:09,  4.90it/s][A
 79%|███████▊  | 174/221 [00:51<00:11,  4.21it/s][A
 79%|███████▉  | 175/221 [00:51<00:11,  4.02it/s][A
 80%|███████▉  | 176/221 [00:51<00:12,  3.65it/s][A
 80%|████████  | 177/221 [00:52<00:11,  3.88it/s][A
 81%|████████  | 178/221 [00:52<00:09,  4.30it/s][A
 81%|████████  | 179/221 [00:52<00:11,  3.76it/s][A
 81%|████████▏ | 180/221 [00:52<00:09,  4.17it/s][A
 82%|████████▏ | 181/221 [00:52<00:08,  4.73it/s][A
 82%|████████▏ | 182/221 [00:53<00:08,  4.61it/s][A
 83%|████████▎ | 183/221 [00:53<00:07,  5.09it/s][A
 83%|████████▎ | 184/221 [00:53<00:11,  3.36it/s][A
 84%|████████▎ | 185/221 [00:54<00:09,  3.60it/s][A
 84%|████████▍ | 186/221 [00:54<00:08,  4.21it/s][A
 85%|████████▍ | 187/221 [00:54<00:07,  4.70it/s][A
 85%|████████▌ | 188/221 [00:54<00:10,  3.09it/s][A
 86%|████████▌ | 189/221 [00:55<00:12,  2.59it/s][A
 86%|████████▌ | 190/221 [00:56<00:13,  2.25it/s][A
 86%|████████▋ | 191/221 [00:56<00:11,  2.54it/s][A
 87%|████████▋ | 192/221 [00:56<00:10,  2.83it/s][A
 88%|████████▊ | 194/221 [00:57<00:10,  2.48it/s][A
 88%|████████▊ | 195/221 [00:58<00:11,  2.22it/s][A
 89%|████████▊ | 196/221 [00:58<00:10,  2.31it/s][A
 89%|████████▉ | 197/221 [00:58<00:08,  2.67it/s][A
 90%|████████▉ | 198/221 [00:58<00:06,  3.33it/s][A
 90%|█████████ | 199/221 [00:58<00:05,  4.11it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.05it/s][A
 91%|█████████ | 201/221 [00:59<00:06,  3.29it/s][A
 91%|█████████▏| 202/221 [00:59<00:04,  3.85it/s][A
 92%|█████████▏| 203/221 [01:00<00:03,  4.61it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  3.67it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.14it/s][A
 93%|█████████▎| 206/221 [01:00<00:04,  3.48it/s][A
 94%|█████████▎| 207/221 [01:01<00:03,  4.27it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.37it/s][A
 95%|█████████▍| 209/221 [01:02<00:04,  2.80it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.40it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.51it/s][A
 96%|█████████▋| 213/221 [01:03<00:02,  2.76it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  3.33it/s][A
 97%|█████████▋| 215/221 [01:03<00:02,  2.85it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  3.02it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  2.97it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.33it/s][A
 99%|█████████▉| 219/221 [01:04<00:00,  3.92it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  4.13it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.17it/s][A100%|██████████| 221/221 [01:05<00:00,  3.37it/s]
09/18/2024 16:58:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 299--===========

09/18/2024 16:58:14 - INFO - __main__ -   {'area_r1': 40.3, 'area_recall': '40.3/67.6/76.1', 'area_ravg': 61.3}
09/18/2024 16:58:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 299--===========

09/18/2024 16:58:14 - INFO - __main__ -   {'forward_r1': 36.4, 'forward_recall': '36.4/62.2/73.1', 'forward_ravg': 57.2}
09/18/2024 16:58:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 299--===========

09/18/2024 16:58:14 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 16:58:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 16:58:14 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 16:58:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 299--===========

09/18/2024 16:58:14 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/71.4/79.6', 'area_video_ravg': 67.0, 'area_video_back_r1': 48.2, 'area_video_back_recall': '48.2/68.8/76.5', 'area_video_back_ravg': 64.5}
09/18/2024 16:58:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 49=======

09/18/2024 16:58:14 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.8/77.0', 'area_video_ravg': 67.0, 'area_video_back_r1': 51.7, 'area_video_back_recall': '51.7/71.8/80.3', 'area_video_back_ravg': 67.9}
09/18/2024 16:58:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 299--===========

09/18/2024 16:58:14 - INFO - __main__ -   {'video_r1': 29.0, 'video_recall': '29.0/55.1/65.6', 'video_ravg': 49.9}
09/18/2024 16:58:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 16:58:14 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 16:58:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 299--===========

09/18/2024 16:58:14 - INFO - __main__ -   {'video_r1': 48.9, 'video_recall': '48.9/68.8/76.7', 'video_ravg': 64.8}
09/18/2024 16:58:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 16:58:14 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 16:58:41 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.000533795915544033, 'loss_ret%tv%ta--finetune_area/loss_area': 1.6987297534942627, 'loss_ret%tv%ta--finetune_area/total_loss': 1.699263572692871}
 10%|█         | 300/2910 [1:53:34<99:05:07, 136.67s/it][h264 @ 0x5598dc165e40] mmco: unref short failure
[h264 @ 0x5598dc165e40] mmco: unref short failure
[h264 @ 0x5598dc165e40] mmco: unref short failure
[h264 @ 0x5598dc165e40] mmco: unref short failure
 10%|█         | 301/2910 [1:53:38<70:08:20, 96.78s/it] [h264 @ 0x5598f5dcde40] mmco: unref short failure
 10%|█         | 302/2910 [1:53:41<49:54:34, 68.89s/it][h264 @ 0x5565e9db5100] mmco: unref short failure
[h264 @ 0x5565e9db5100] mmco: unref short failure
[h264 @ 0x5598e60cfc00] mmco: unref short failure
 10%|█         | 303/2910 [1:53:46<35:51:32, 49.52s/it][h264 @ 0x55cb73249fc0] mmco: unref short failure
[h264 @ 0x55cb73249fc0] mmco: unref short failure
 10%|█         | 304/2910 [1:53:50<25:59:16, 35.90s/it] 10%|█         | 305/2910 [1:53:55<19:13:30, 26.57s/it][h264 @ 0x55cb73c3c100] mmco: unref short failure
[h264 @ 0x55cb78a2d400] mmco: unref short failure
[h264 @ 0x55cb78a2d400] mmco: unref short failure
[h264 @ 0x55cb78a2d400] mmco: unref short failure
[h264 @ 0x55cb78a2d400] mmco: unref short failure
 11%|█         | 306/2910 [1:54:00<14:40:28, 20.29s/it][h264 @ 0x55d90ebc2fc0] mmco: unref short failure
[h264 @ 0x55d908a70dc0] mmco: unref short failure
[h264 @ 0x55d908a70dc0] mmco: unref short failure
 11%|█         | 307/2910 [1:54:06<11:28:40, 15.87s/it]09/18/2024 16:59:19 - INFO - __main__ -   current idx KPOxRziYDzs.2 from finetune_area returns wrong image/video, use 119381 instead.
[h264 @ 0x556604b6c0c0] mmco: unref short failure
[h264 @ 0x556604b6c0c0] mmco: unref short failure
 11%|█         | 308/2910 [1:54:11<9:15:07, 12.80s/it] [h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5565fa414040] mmco: unref short failure
[h264 @ 0x5598f02f5380] mmco: unref short failure
[h264 @ 0x5598f02f5380] mmco: unref short failure
 11%|█         | 309/2910 [1:54:17<7:34:56, 10.49s/it] 11%|█         | 310/2910 [1:54:22<6:33:28,  9.08s/it][h264 @ 0x5598dcb7e900] mmco: unref short failure
 11%|█         | 311/2910 [1:54:28<5:42:03,  7.90s/it] 11%|█         | 312/2910 [1:54:33<5:11:35,  7.20s/it] 11%|█         | 313/2910 [1:54:38<4:43:43,  6.55s/it] 11%|█         | 314/2910 [1:54:44<4:33:01,  6.31s/it][h264 @ 0x5598de590140] mmco: unref short failure
[h264 @ 0x5598de590140] mmco: unref short failure
[h264 @ 0x5598f2c46fc0] mmco: unref short failure
 11%|█         | 315/2910 [1:54:49<4:20:28,  6.02s/it][h264 @ 0x5565fc319400] mmco: unref short failure
[h264 @ 0x5565fc319400] mmco: unref short failure
[h264 @ 0x556603f0acc0] mmco: unref short failure
[h264 @ 0x556603f0acc0] mmco: unref short failure
[h264 @ 0x556603f0acc0] mmco: unref short failure
[h264 @ 0x556603f0acc0] mmco: unref short failure
[h264 @ 0x5598ea4a1d80] mmco: unref short failure
[h264 @ 0x5598ea4a1d80] mmco: unref short failure
[h264 @ 0x5565ef1607c0] mmco: unref short failure
[h264 @ 0x5565e697fb80] mmco: unref short failure
[h264 @ 0x5565e697fb80] mmco: unref short failure
[h264 @ 0x55d90ae34700] mmco: unref short failure
[h264 @ 0x55d90ae34700] mmco: unref short failure
[h264 @ 0x55cb6ba2a400] mmco: unref short failure
[h264 @ 0x55cb6ba2a400] mmco: unref short failure
[h264 @ 0x5598e7f0b100] mmco: unref short failure
[h264 @ 0x5598e7f0b100] mmco: unref short failure
[h264 @ 0x55cb601e7ec0] mmco: unref short failure
[h264 @ 0x55cb601e7ec0] mmco: unref short failure
[h264 @ 0x55cb601e7ec0] mmco: unref short failure
[h264 @ 0x55cb601e7ec0] mmco: unref short failure
 11%|█         | 316/2910 [1:55:44<14:59:08, 20.80s/it][h264 @ 0x5565e73e7b80] mmco: unref short failure
 11%|█         | 317/2910 [1:55:56<12:52:39, 17.88s/it]09/18/2024 17:01:10 - INFO - __main__ -   current idx QRr-nFmU9s8.3 from finetune_area returns wrong image/video, use 113549 instead.
[h264 @ 0x55cb6d3e5000] mmco: unref short failure
[h264 @ 0x55cb6d3e5000] mmco: unref short failure
 11%|█         | 318/2910 [1:56:01<10:10:27, 14.13s/it][h264 @ 0x55cb79d3d6c0] mmco: unref short failure
[h264 @ 0x55cb79d3d6c0] mmco: unref short failure
 11%|█         | 319/2910 [1:56:10<9:04:26, 12.61s/it] [h264 @ 0x5565fc778680] mmco: unref short failure
[h264 @ 0x5565fc778680] mmco: unref short failure
[h264 @ 0x5565ffdc9380] mmco: unref short failure
[h264 @ 0x5565ffdc9380] mmco: unref short failure
 11%|█         | 320/2910 [1:56:26<9:43:52, 13.53s/it][h264 @ 0x55cb78dfa100] mmco: unref short failure
09/18/2024 17:01:39 - INFO - __main__ -   current idx 9lm2wYzoyd4.69 from finetune_area returns wrong image/video, use 76286 instead.
[h264 @ 0x5598e22f30c0] mmco: unref short failure
[h264 @ 0x5598e22f30c0] mmco: unref short failure
[h264 @ 0x5598e22f30c0] mmco: unref short failure
[h264 @ 0x5598e22f30c0] mmco: unref short failure
[h264 @ 0x5598e22f30c0] mmco: unref short failure
[h264 @ 0x5598e22f30c0] mmco: unref short failure
 11%|█         | 321/2910 [1:56:36<9:01:54, 12.56s/it][h264 @ 0x556600a59780] mmco: unref short failure
[h264 @ 0x556600a59780] mmco: unref short failure
[h264 @ 0x5598ebbad440] mmco: unref short failure
[h264 @ 0x5598ebbad440] mmco: unref short failure
[h264 @ 0x55d91818c340] mmco: unref short failure
[h264 @ 0x55d91818c340] mmco: unref short failure
[h264 @ 0x5565f11d0600] mmco: unref short failure
 11%|█         | 322/2910 [1:56:41<7:30:02, 10.43s/it][h264 @ 0x5598e0463a40] mmco: unref short failure
 11%|█         | 323/2910 [1:56:49<6:52:21,  9.56s/it][h264 @ 0x5598f981b480] mmco: unref short failure
09/18/2024 17:02:00 - INFO - __main__ -   current idx U2DBfiveIQ0.26 from finetune_area returns wrong image/video, use 128011 instead.
[h264 @ 0x55cb5eed5cc0] mmco: unref short failure
[h264 @ 0x55cb5eed5cc0] mmco: unref short failure
[h264 @ 0x5565f2df3200] mmco: unref short failure
[h264 @ 0x5565f2df3200] mmco: unref short failure
[h264 @ 0x5565f2df3200] mmco: unref short failure
[h264 @ 0x5565f2df3200] mmco: unref short failure
[h264 @ 0x55d90d7e0080] mmco: unref short failure
[h264 @ 0x55d90d7e0080] mmco: unref short failure
[h264 @ 0x55d90d7e0080] mmco: unref short failure
[h264 @ 0x55d90d7e0080] mmco: unref short failure
[h264 @ 0x55d90d7e0080] mmco: unref short failure
[h264 @ 0x55d90d7e0080] mmco: unref short failure
[h264 @ 0x5598dea8f700] mmco: unref short failure
[h264 @ 0x55d91f018f40] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
[h264 @ 0x55cb6ac36d00] mmco: unref short failure
09/18/2024 17:02:46 - INFO - __main__ -   current idx gVhwSbu4UfU.1 from finetune_area returns wrong image/video, use 132935 instead.
[h264 @ 0x5598eaf351c0] mmco: unref short failure
[h264 @ 0x5598eaf351c0] mmco: unref short failure
[h264 @ 0x55cb7b4f45c0] mmco: unref short failure
[h264 @ 0x55cb73c3c300] mmco: unref short failure
[h264 @ 0x55d91cae1600] mmco: unref short failure
[h264 @ 0x55cb78dfa300] mmco: unref short failure
[h264 @ 0x55cb78dfa300] mmco: unref short failure
[h264 @ 0x55cb78dfa300] mmco: unref short failure
[h264 @ 0x55cb78dfa300] mmco: unref short failure
[h264 @ 0x5598e9f12c00] mmco: unref short failure
[h264 @ 0x5598e9f12c00] mmco: unref short failure
[h264 @ 0x5598ec7a2140] mmco: unref short failure
 11%|█         | 324/2910 [1:58:09<21:59:01, 30.60s/it][h264 @ 0x5598ebf4c980] mmco: unref short failure
[h264 @ 0x5598ebf4c980] mmco: unref short failure
[h264 @ 0x5598f1982200] mmco: unref short failure
[h264 @ 0x5598f1982200] mmco: unref short failure
[h264 @ 0x55cb6be01300] mmco: unref short failure
[h264 @ 0x55cb6be01300] mmco: unref short failure
 11%|█         | 325/2910 [1:58:23<18:21:40, 25.57s/it][h264 @ 0x5598dfae4a40] mmco: unref short failure
[h264 @ 0x5598e702dc40] mmco: unref short failure
[h264 @ 0x5598e702dc40] mmco: unref short failure
[h264 @ 0x55d912d180c0] mmco: unref short failure
[h264 @ 0x55d912d180c0] mmco: unref short failure
[h264 @ 0x55d912d180c0] mmco: unref short failure
[h264 @ 0x55d912d180c0] mmco: unref short failure
[h264 @ 0x5598df6ded00] mmco: unref short failure
[h264 @ 0x5598df6ded00] mmco: unref short failure
 11%|█         | 326/2910 [1:58:37<15:58:20, 22.25s/it][h264 @ 0x5565ef5b8d00] mmco: unref short failure
[h264 @ 0x55d91cae1600] mmco: unref short failure
[h264 @ 0x55cb7bd16e00] mmco: unref short failure
 11%|█         | 327/2910 [1:59:04<16:59:02, 23.67s/it]09/18/2024 17:04:19 - INFO - __main__ -   current idx _4-lJ7UF30Y.22 from finetune_area returns wrong image/video, use 148252 instead.
 11%|█▏        | 328/2910 [1:59:09<13:01:54, 18.17s/it][h264 @ 0x5598f02f5380] mmco: unref short failure
[h264 @ 0x5598f02f5380] mmco: unref short failure
[h264 @ 0x55d911478880] mmco: unref short failure
[h264 @ 0x55d911478880] mmco: unref short failure
[h264 @ 0x55d911478880] mmco: unref short failure
[h264 @ 0x55d911478880] mmco: unref short failure
[h264 @ 0x55d911478880] mmco: unref short failure
[h264 @ 0x55d911478880] mmco: unref short failure
[h264 @ 0x5565e5a41180] mmco: unref short failure
[h264 @ 0x5565e5a41180] mmco: unref short failure
 11%|█▏        | 329/2910 [1:59:14<10:10:58, 14.20s/it][h264 @ 0x55cb793244c0] mmco: unref short failure
[h264 @ 0x55cb793244c0] mmco: unref short failure
[h264 @ 0x55cb793244c0] mmco: unref short failure
[h264 @ 0x55cb793244c0] mmco: unref short failure
 11%|█▏        | 330/2910 [1:59:21<8:34:13, 11.96s/it] [h264 @ 0x55cb71f38980] mmco: unref short failure
[h264 @ 0x55cb71f38980] mmco: unref short failure
[h264 @ 0x55cb71f38980] mmco: unref short failure
[h264 @ 0x55cb71f38980] mmco: unref short failure
 11%|█▏        | 331/2910 [1:59:27<7:12:14, 10.06s/it][h264 @ 0x5598ed1924c0] mmco: unref short failure
[h264 @ 0x5598ed1924c0] mmco: unref short failure
[h264 @ 0x5565f7052480] mmco: unref short failure
[h264 @ 0x5565f7052480] mmco: unref short failure
[h264 @ 0x5598dc8b7bc0] mmco: unref short failure
[h264 @ 0x5598dc8b7bc0] mmco: unref short failure
[h264 @ 0x5598f2976f40] mmco: unref short failure
[h264 @ 0x55cb83152200] mmco: unref short failure
[h264 @ 0x55d91212bbc0] mmco: unref short failure
[h264 @ 0x55d91212bbc0] mmco: unref short failure
[h264 @ 0x55d905ce23c0] mmco: unref short failure
[h264 @ 0x55d905ce23c0] mmco: unref short failure
[h264 @ 0x5565e6997d00] mmco: unref short failure
[h264 @ 0x5598de0fc280] mmco: unref short failure
[h264 @ 0x5598de0fc280] mmco: unref short failure
[h264 @ 0x556600a59780] mmco: unref short failure
[h264 @ 0x556600a59780] mmco: unref short failure
09/18/2024 17:05:25 - INFO - __main__ -   current idx MuqZDGESIMY.30 from finetune_area returns wrong image/video, use 101214 instead.
09/18/2024 17:05:29 - INFO - __main__ -   current idx NplowkZTvc0.7 from finetune_area returns wrong image/video, use 60065 instead.
[h264 @ 0x55cb7b4f45c0] mmco: unref short failure
[h264 @ 0x55cb7b4f45c0] mmco: unref short failure
[h264 @ 0x556601c33040] mmco: unref short failure
[h264 @ 0x556601c33040] mmco: unref short failure
[h264 @ 0x556601c33040] mmco: unref short failure
[h264 @ 0x556601c33040] mmco: unref short failure
[h264 @ 0x55d8fe8db3c0] mmco: unref short failure
 11%|█▏        | 332/2910 [2:00:48<22:30:33, 31.43s/it] 11%|█▏        | 333/2910 [2:00:53<16:55:41, 23.65s/it][h264 @ 0x5598f83ad800] mmco: unref short failure
[h264 @ 0x5598f83ad800] mmco: unref short failure
[h264 @ 0x55cb625a8c00] mmco: unref short failure
[h264 @ 0x55cb625a8c00] mmco: unref short failure
[h264 @ 0x5565f11b7380] mmco: unref short failure
[h264 @ 0x5565f11b7380] mmco: unref short failure
[h264 @ 0x55d904700e80] mmco: unref short failure
[h264 @ 0x55d904700e80] mmco: unref short failure
[h264 @ 0x55cb61990a40] mmco: unref short failure
[h264 @ 0x55cb6a298340] mmco: unref short failure
[h264 @ 0x55cb6a298340] mmco: unref short failure
 11%|█▏        | 334/2910 [2:01:13<16:08:09, 22.55s/it][h264 @ 0x556604b13880] mmco: unref short failure
[h264 @ 0x556604b13880] mmco: unref short failure
[h264 @ 0x55cb63f080c0] mmco: unref short failure
[h264 @ 0x55cb63f080c0] mmco: unref short failure
[h264 @ 0x55cb7d2a66c0] mmco: unref short failure
 12%|█▏        | 335/2910 [2:01:33<15:31:20, 21.70s/it][h264 @ 0x5598e060db40] mmco: unref short failure
 12%|█▏        | 336/2910 [2:01:38<11:58:33, 16.75s/it] 12%|█▏        | 337/2910 [2:01:44<9:29:38, 13.28s/it] [h264 @ 0x5598fbe0b4c0] mmco: unref short failure
[h264 @ 0x5598fbe0b4c0] mmco: unref short failure
[h264 @ 0x55d9021f9b80] mmco: unref short failure
 12%|█▏        | 338/2910 [2:01:50<7:59:19, 11.18s/it][h264 @ 0x5565fb2dfdc0] mmco: unref short failure
[h264 @ 0x55cb614fb400] mmco: unref short failure
[h264 @ 0x55cb614fb400] mmco: unref short failure
 12%|█▏        | 339/2910 [2:02:01<7:56:58, 11.13s/it][h264 @ 0x55cb625a8c00] mmco: unref short failure
[h264 @ 0x55cb625a8c00] mmco: unref short failure
[h264 @ 0x55cb7e5bcb80] mmco: unref short failure
[h264 @ 0x5565ffdbfec0] mmco: unref short failure
[h264 @ 0x55d8ffc39e80] mmco: unref short failure
[h264 @ 0x556604b13880] mmco: unref short failure
[h264 @ 0x556604b13880] mmco: unref short failure
[h264 @ 0x55cb68ba3400] mmco: unref short failure
[h264 @ 0x5598f45ef6c0] mmco: unref short failure
[h264 @ 0x5598f45ef6c0] mmco: unref short failure
09/18/2024 17:07:48 - INFO - __main__ -   current idx DTaJjznuY74.20 from finetune_area returns wrong image/video, use 138921 instead.
[h264 @ 0x55cb7bcf2680] mmco: unref short failure
[h264 @ 0x55cb616159c0] mmco: unref short failure
[h264 @ 0x55cb616159c0] mmco: unref short failure
[h264 @ 0x55cb616159c0] mmco: unref short failure
[h264 @ 0x55cb616159c0] mmco: unref short failure
[h264 @ 0x55cb61d6cac0] mmco: unref short failure
[h264 @ 0x55cb61d6cac0] mmco: unref short failure
[h264 @ 0x55cb61d6cac0] mmco: unref short failure
[h264 @ 0x55cb61d6cac0] mmco: unref short failure
[h264 @ 0x55660459c680] mmco: unref short failure
[h264 @ 0x5598f29eaa80] mmco: unref short failure
[h264 @ 0x55d90ddd4a80] mmco: unref short failure
[h264 @ 0x55d90ddd4a80] mmco: unref short failure
[h264 @ 0x5565e5322a40] mmco: unref short failure
 12%|█▏        | 340/2910 [2:03:14<21:15:26, 29.78s/it][h264 @ 0x55cb6faf9340] mmco: unref short failure
[h264 @ 0x5598eaa3bfc0] mmco: unref short failure
[h264 @ 0x55cb76c7ba80] mmco: unref short failure
[h264 @ 0x55cb76c7ba80] mmco: unref short failure
 12%|█▏        | 341/2910 [2:03:28<17:56:22, 25.14s/it][h264 @ 0x55cb8091c240] mmco: unref short failure
[h264 @ 0x55cb8091c240] mmco: unref short failure
09/18/2024 17:08:52 - INFO - __main__ -   current idx vMl-g-GJ1Ac.24 from finetune_area returns wrong image/video, use 12696 instead.
[h264 @ 0x55d908d98dc0] mmco: unref short failure
[h264 @ 0x55d908d98dc0] mmco: unref short failure
 12%|█▏        | 342/2910 [2:03:52<17:37:01, 24.70s/it][h264 @ 0x55cb6c56bb80] mmco: unref short failure
[h264 @ 0x5598eaf2e5c0] mmco: unref short failure
[h264 @ 0x55cb8091c240] mmco: unref short failure
 12%|█▏        | 343/2910 [2:04:06<15:12:38, 21.33s/it][h264 @ 0x5598f59cea80] mmco: unref short failure
[h264 @ 0x5598f59cea80] mmco: unref short failure
[h264 @ 0x5565ea160700] mmco: unref short failure
[h264 @ 0x5565ea160700] mmco: unref short failure
 12%|█▏        | 344/2910 [2:04:11<11:47:40, 16.55s/it][h264 @ 0x55d9220ee540] mmco: unref short failure
 12%|█▏        | 345/2910 [2:04:16<9:24:38, 13.21s/it]  12%|█▏        | 346/2910 [2:04:23<7:56:22, 11.15s/it][h264 @ 0x5598fa26f380] mmco: unref short failure
[h264 @ 0x5565ea52bd00] mmco: unref short failure
[h264 @ 0x5565ea52bd00] mmco: unref short failure
[h264 @ 0x5598eaf2e5c0] mmco: unref short failure
[h264 @ 0x5598eaf2e5c0] mmco: unref short failure
09/18/2024 17:09:38 - INFO - __main__ -   current idx 1HAnFYVSFkk.3 from finetune_area returns wrong image/video, use 35169 instead.
 12%|█▏        | 347/2910 [2:04:29<6:51:36,  9.64s/it][h264 @ 0x5598f55721c0] mmco: unref short failure
[h264 @ 0x5565ef2f8500] mmco: unref short failure
[h264 @ 0x5598dfaa29c0] mmco: unref short failure
[h264 @ 0x5598dfaa29c0] mmco: unref short failure
[h264 @ 0x5598eb9e6180] mmco: unref short failure
[h264 @ 0x5598eb9e6180] mmco: unref short failure
[h264 @ 0x5598eb9e6180] mmco: unref short failure
[h264 @ 0x5598eb9e6180] mmco: unref short failure
[h264 @ 0x5598e15e1200] mmco: unref short failure
[h264 @ 0x55d90e723c00] mmco: unref short failure
[h264 @ 0x55d90e723c00] mmco: unref short failure
[h264 @ 0x55d90e723c00] mmco: unref short failure
[h264 @ 0x55d90e723c00] mmco: unref short failure
[h264 @ 0x55cb74f74240] mmco: unref short failure
[h264 @ 0x55cb74f74240] mmco: unref short failure
[h264 @ 0x55cb74f74240] mmco: unref short failure
[h264 @ 0x55cb74f74240] mmco: unref short failure
 12%|█▏        | 348/2910 [2:05:38<19:34:16, 27.50s/it] 12%|█▏        | 349/2910 [2:05:51<16:31:10, 23.22s/it]09/18/2024 17:11:01 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 17:11:01 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x55d90f0dff00] mmco: unref short failure
[h264 @ 0x55d90f0dff00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565eb97f640] mmco: unref short failure
[h264 @ 0x55d918446740] mmco: unref short failure
[h264 @ 0x5565f56df880] mmco: unref short failure
[h264 @ 0x5565f56df880] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565e9ca6f00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5565e59aee00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb67666040] mmco: unref short failure
[h264 @ 0x55cb5ec7fec0] mmco: unref short failure
[h264 @ 0x55cb5ec7fec0] mmco: unref short failure
[h264 @ 0x5565e6ecc9c0] mmco: unref short failure
[h264 @ 0x5565e6ecc9c0] mmco: unref short failure
[h264 @ 0x5598e93a0780] mmco: unref short failure
[h264 @ 0x5598e93a0780] mmco: unref short failure
[h264 @ 0x5565fa0ab840] mmco: unref short failure
[h264 @ 0x556600b72380] mmco: unref short failure
[h264 @ 0x556600b72380] mmco: unref short failure
[h264 @ 0x55cb6cb5d300] mmco: unref short failure
[h264 @ 0x55cb6cb5d300] mmco: unref short failure
[h264 @ 0x5598dc787140] mmco: unref short failure
[h264 @ 0x5598dc787140] mmco: unref short failure
09/18/2024 17:13:15 - INFO - __main__ -   current idx T9SsQafdxnY.19 from finetune_area returns wrong image/video, use 67620 instead.

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<02:43,  1.35it/s][A
  1%|          | 2/221 [00:01<02:32,  1.44it/s][A
  1%|▏         | 3/221 [00:01<01:54,  1.90it/s][A
  2%|▏         | 4/221 [00:01<01:24,  2.56it/s][A
  2%|▏         | 5/221 [00:02<01:13,  2.93it/s][A
  3%|▎         | 6/221 [00:02<01:00,  3.55it/s][A
  3%|▎         | 7/221 [00:02<00:55,  3.86it/s][A
  4%|▎         | 8/221 [00:03<01:09,  3.06it/s][A
  4%|▍         | 9/221 [00:03<01:12,  2.93it/s][A
  5%|▍         | 10/221 [00:04<01:37,  2.17it/s][A
  5%|▍         | 11/221 [00:04<01:23,  2.52it/s][A
  5%|▌         | 12/221 [00:05<01:48,  1.92it/s][A
  6%|▌         | 13/221 [00:05<01:41,  2.04it/s][A
  6%|▋         | 14/221 [00:06<02:03,  1.67it/s][A[h264 @ 0x55cb60af0180] mmco: unref short failure

  7%|▋         | 15/221 [00:06<01:40,  2.05it/s][A
  7%|▋         | 16/221 [00:07<01:44,  1.96it/s][A
  8%|▊         | 17/221 [00:07<01:53,  1.80it/s][A
  8%|▊         | 18/221 [00:08<01:38,  2.05it/s][A
  9%|▊         | 19/221 [00:08<01:19,  2.55it/s][A
  9%|▉         | 20/221 [00:08<01:18,  2.55it/s][A
 10%|▉         | 21/221 [00:09<01:11,  2.78it/s][A[h264 @ 0x5598dc73af00] mmco: unref short failure
[h264 @ 0x5598dc73af00] mmco: unref short failure

 10%|▉         | 22/221 [00:09<01:05,  3.05it/s][A
 11%|█         | 24/221 [00:09<00:47,  4.14it/s][A
 11%|█▏        | 25/221 [00:09<00:51,  3.83it/s][A[h264 @ 0x55d9154a7580] mmco: unref short failure
[h264 @ 0x55d9154a7580] mmco: unref short failure

 12%|█▏        | 26/221 [00:10<01:07,  2.87it/s][A
 12%|█▏        | 27/221 [00:10<00:55,  3.50it/s][A
 13%|█▎        | 28/221 [00:11<01:24,  2.28it/s][A[h264 @ 0x5565fc137500] mmco: unref short failure
[h264 @ 0x5565fc137500] mmco: unref short failure
[h264 @ 0x5565fc137500] mmco: unref short failure
[h264 @ 0x5565fc137500] mmco: unref short failure

 13%|█▎        | 29/221 [00:11<01:24,  2.28it/s][A
 14%|█▎        | 30/221 [00:12<01:16,  2.49it/s][A[h264 @ 0x55cb5e69a280] mmco: unref short failure
[h264 @ 0x55cb5e69a280] mmco: unref short failure

 14%|█▍        | 31/221 [00:12<01:12,  2.62it/s][A
 14%|█▍        | 32/221 [00:12<00:57,  3.26it/s][A
 15%|█▍        | 33/221 [00:12<00:51,  3.65it/s][A
 15%|█▌        | 34/221 [00:13<00:47,  3.97it/s][A
 16%|█▌        | 35/221 [00:13<00:54,  3.42it/s][A
 16%|█▋        | 36/221 [00:13<01:06,  2.79it/s][A
 17%|█▋        | 37/221 [00:14<01:34,  1.95it/s][A
 17%|█▋        | 38/221 [00:15<01:29,  2.06it/s][A
 18%|█▊        | 39/221 [00:15<01:19,  2.29it/s][A
 18%|█▊        | 40/221 [00:15<01:14,  2.43it/s][A
 19%|█▊        | 41/221 [00:16<00:59,  3.02it/s][A
 19%|█▉        | 42/221 [00:16<01:08,  2.61it/s][A
 19%|█▉        | 43/221 [00:16<00:53,  3.33it/s][A
 20%|██        | 45/221 [00:17<01:06,  2.65it/s][A
 21%|██        | 46/221 [00:18<01:06,  2.65it/s][A
 21%|██▏       | 47/221 [00:18<01:07,  2.57it/s][A
 22%|██▏       | 48/221 [00:18<00:55,  3.11it/s][A
 22%|██▏       | 49/221 [00:19<01:10,  2.45it/s][A
 23%|██▎       | 50/221 [00:19<01:09,  2.45it/s][A
 23%|██▎       | 51/221 [00:19<00:58,  2.91it/s][A
 24%|██▎       | 52/221 [00:19<00:50,  3.34it/s][A
 24%|██▍       | 53/221 [00:20<00:48,  3.47it/s][A
 24%|██▍       | 54/221 [00:21<01:53,  1.47it/s][A
 25%|██▍       | 55/221 [00:22<01:38,  1.68it/s][A[h264 @ 0x55d917dccbc0] mmco: unref short failure

 25%|██▌       | 56/221 [00:22<01:21,  2.03it/s][A
 26%|██▌       | 57/221 [00:22<01:06,  2.47it/s][A
 26%|██▌       | 58/221 [00:22<00:54,  2.98it/s][A
 27%|██▋       | 59/221 [00:23<00:50,  3.20it/s][A
 27%|██▋       | 60/221 [00:24<01:35,  1.69it/s][A
 28%|██▊       | 61/221 [00:24<01:18,  2.05it/s][A
 28%|██▊       | 62/221 [00:24<01:10,  2.27it/s][A
 29%|██▊       | 63/221 [00:25<01:03,  2.49it/s][A
 29%|██▉       | 64/221 [00:25<00:55,  2.81it/s][A
 29%|██▉       | 65/221 [00:25<00:50,  3.06it/s][A
 30%|██▉       | 66/221 [00:26<00:54,  2.83it/s][A
 30%|███       | 67/221 [00:27<01:15,  2.03it/s][A
 31%|███       | 68/221 [00:27<01:04,  2.37it/s][A[h264 @ 0x55cb81085100] mmco: unref short failure

 31%|███       | 69/221 [00:28<01:32,  1.65it/s][A
 32%|███▏      | 70/221 [00:28<01:12,  2.08it/s][A
 32%|███▏      | 71/221 [00:28<01:11,  2.10it/s][A
 33%|███▎      | 72/221 [00:29<01:07,  2.21it/s][A
 33%|███▎      | 73/221 [00:29<01:05,  2.26it/s][A
 33%|███▎      | 74/221 [00:29<00:52,  2.78it/s][A
 34%|███▍      | 75/221 [00:30<00:51,  2.83it/s][A
 34%|███▍      | 76/221 [00:30<00:44,  3.29it/s][A
 35%|███▍      | 77/221 [00:30<00:39,  3.68it/s][A
 35%|███▌      | 78/221 [00:30<00:36,  3.90it/s][A[h264 @ 0x55cb78652ec0] mmco: unref short failure

 36%|███▌      | 79/221 [00:31<00:58,  2.42it/s][A
 36%|███▌      | 80/221 [00:31<00:51,  2.71it/s][A
 37%|███▋      | 81/221 [00:32<00:46,  2.98it/s][A
 37%|███▋      | 82/221 [00:32<01:02,  2.23it/s][A
 38%|███▊      | 83/221 [00:33<01:02,  2.21it/s][A
 38%|███▊      | 84/221 [00:33<00:54,  2.50it/s][A
 38%|███▊      | 85/221 [00:33<00:45,  3.02it/s][A
 39%|███▉      | 86/221 [00:34<00:45,  2.97it/s][A
 39%|███▉      | 87/221 [00:35<01:05,  2.04it/s][A[h264 @ 0x5565e7620c40] mmco: unref short failure

 40%|███▉      | 88/221 [00:35<01:13,  1.80it/s][A
 40%|████      | 89/221 [00:36<01:09,  1.90it/s][A
 41%|████      | 90/221 [00:36<00:58,  2.23it/s][A
 41%|████      | 91/221 [00:36<00:49,  2.62it/s][A
 42%|████▏     | 92/221 [00:37<00:50,  2.58it/s][A
 42%|████▏     | 93/221 [00:37<00:53,  2.40it/s][A
 43%|████▎     | 94/221 [00:37<00:43,  2.91it/s][A
 43%|████▎     | 95/221 [00:38<00:41,  3.00it/s][A
 43%|████▎     | 96/221 [00:38<00:42,  2.97it/s][A
 44%|████▍     | 97/221 [00:38<00:35,  3.48it/s][A
 44%|████▍     | 98/221 [00:38<00:35,  3.46it/s][A
 45%|████▍     | 99/221 [00:39<00:30,  4.05it/s][A
 45%|████▌     | 100/221 [00:39<00:28,  4.24it/s][A
 46%|████▌     | 101/221 [00:39<00:25,  4.72it/s][A
 46%|████▌     | 102/221 [00:39<00:26,  4.53it/s][A
 47%|████▋     | 103/221 [00:39<00:30,  3.91it/s][A
 47%|████▋     | 104/221 [00:40<00:26,  4.44it/s][A
 48%|████▊     | 105/221 [00:40<00:27,  4.27it/s][A
 48%|████▊     | 106/221 [00:41<00:51,  2.23it/s][A
 48%|████▊     | 107/221 [00:41<00:42,  2.68it/s][A
 49%|████▉     | 108/221 [00:41<00:38,  2.96it/s][A
 49%|████▉     | 109/221 [00:42<00:38,  2.90it/s][A
 50%|████▉     | 110/221 [00:42<00:46,  2.38it/s][A
 50%|█████     | 111/221 [00:43<00:53,  2.06it/s][A
 51%|█████     | 112/221 [00:43<00:44,  2.45it/s][A
 51%|█████     | 113/221 [00:43<00:43,  2.47it/s][A
 52%|█████▏    | 114/221 [00:44<00:34,  3.14it/s][A
 52%|█████▏    | 115/221 [00:44<00:30,  3.44it/s][A
 52%|█████▏    | 116/221 [00:45<00:59,  1.75it/s][A
 53%|█████▎    | 117/221 [00:45<00:52,  1.97it/s][A
 53%|█████▎    | 118/221 [00:46<00:46,  2.20it/s][A
 54%|█████▍    | 119/221 [00:46<00:45,  2.25it/s][A
 54%|█████▍    | 120/221 [00:46<00:41,  2.43it/s][A
 55%|█████▍    | 121/221 [00:47<00:33,  3.03it/s][A
 55%|█████▌    | 122/221 [00:47<00:29,  3.40it/s][A
 56%|█████▌    | 123/221 [00:47<00:26,  3.76it/s][A
09/18/2024 17:14:15 - INFO - __main__ -   current idx DhOkpvuskU4.32 from finetune_area returns wrong image/video, use 112773 instead.
 56%|█████▌    | 124/221 [00:47<00:26,  3.71it/s][A[h264 @ 0x5598fa6c7540] mmco: unref short failure

 57%|█████▋    | 125/221 [00:48<00:32,  2.95it/s][A
 57%|█████▋    | 126/221 [00:48<00:29,  3.27it/s][A
 57%|█████▋    | 127/221 [00:49<00:37,  2.54it/s][A[h264 @ 0x55cb6b59f500] mmco: unref short failure
[h264 @ 0x55cb6b59f500] mmco: unref short failure

 58%|█████▊    | 128/221 [00:49<00:38,  2.39it/s][A
 58%|█████▊    | 129/221 [00:49<00:33,  2.77it/s][A
 59%|█████▉    | 130/221 [00:50<00:31,  2.88it/s][A
 59%|█████▉    | 131/221 [00:50<00:31,  2.89it/s][A
 60%|█████▉    | 132/221 [00:50<00:27,  3.25it/s][A
 60%|██████    | 133/221 [00:51<00:33,  2.62it/s][A
 61%|██████    | 134/221 [00:51<00:28,  3.01it/s][A
 61%|██████    | 135/221 [00:51<00:26,  3.22it/s][A
 62%|██████▏   | 136/221 [00:52<00:27,  3.05it/s][A
 62%|██████▏   | 137/221 [00:52<00:24,  3.47it/s][A[h264 @ 0x55660393fa00] mmco: unref short failure

 62%|██████▏   | 138/221 [00:52<00:26,  3.12it/s][A
 63%|██████▎   | 139/221 [00:53<00:30,  2.73it/s][A
 63%|██████▎   | 140/221 [00:53<00:29,  2.77it/s][A
 64%|██████▍   | 141/221 [00:53<00:25,  3.17it/s][A
 64%|██████▍   | 142/221 [00:54<00:25,  3.14it/s][A
 65%|██████▍   | 143/221 [00:54<00:25,  3.06it/s][A
 65%|██████▌   | 144/221 [00:54<00:25,  3.00it/s][A
 66%|██████▌   | 146/221 [00:55<00:17,  4.22it/s][A
 67%|██████▋   | 147/221 [00:55<00:18,  3.99it/s][A
[h264 @ 0x55d916842a00] mmco: unref short failure
[h264 @ 0x55d916842a00] mmco: unref short failure
 67%|██████▋   | 148/221 [00:55<00:20,  3.62it/s][A
 67%|██████▋   | 149/221 [00:55<00:18,  3.88it/s][A
 68%|██████▊   | 150/221 [00:56<00:20,  3.53it/s][A
 68%|██████▊   | 151/221 [00:56<00:28,  2.42it/s][A
 69%|██████▉   | 152/221 [00:58<00:47,  1.44it/s][A
 69%|██████▉   | 153/221 [00:58<00:41,  1.64it/s][A
 70%|██████▉   | 154/221 [00:58<00:33,  1.99it/s][A[h264 @ 0x55cb5ec7fec0] mmco: unref short failure
[h264 @ 0x55cb5ec7fec0] mmco: unref short failure

 70%|███████   | 155/221 [00:59<00:27,  2.36it/s][A
 71%|███████   | 156/221 [00:59<00:25,  2.54it/s][A[h264 @ 0x55d8fedaad00] mmco: unref short failure
[h264 @ 0x55d8fedaad00] mmco: unref short failure

 71%|███████   | 157/221 [01:00<00:27,  2.36it/s][A
 71%|███████▏  | 158/221 [01:00<00:24,  2.59it/s][A
 72%|███████▏  | 159/221 [01:00<00:19,  3.15it/s][A
 72%|███████▏  | 160/221 [01:00<00:20,  3.04it/s][A
 73%|███████▎  | 161/221 [01:01<00:16,  3.65it/s][A
 73%|███████▎  | 162/221 [01:01<00:19,  3.08it/s][A
 74%|███████▍  | 163/221 [01:01<00:19,  3.03it/s][A
 74%|███████▍  | 164/221 [01:02<00:20,  2.84it/s][A
 75%|███████▍  | 165/221 [01:02<00:16,  3.41it/s][A
 75%|███████▌  | 166/221 [01:02<00:21,  2.61it/s][A
 76%|███████▌  | 167/221 [01:03<00:17,  3.09it/s][A
 76%|███████▌  | 168/221 [01:03<00:19,  2.67it/s][A
 76%|███████▋  | 169/221 [01:03<00:16,  3.10it/s][h264 @ 0x55d906102cc0] mmco: unref short failure
[h264 @ 0x55d906102cc0] mmco: unref short failure
[A
 77%|███████▋  | 170/221 [01:04<00:21,  2.38it/s][A
 77%|███████▋  | 171/221 [01:04<00:22,  2.24it/s][A[h264 @ 0x5565f94b8e00] mmco: unref short failure

 78%|███████▊  | 172/221 [01:05<00:19,  2.53it/s][A
 78%|███████▊  | 173/221 [01:05<00:17,  2.80it/s][A
 79%|███████▊  | 174/221 [01:05<00:15,  3.05it/s][A
 79%|███████▉  | 175/221 [01:06<00:16,  2.77it/s][A
 80%|███████▉  | 176/221 [01:06<00:15,  2.90it/s][A
 80%|████████  | 177/221 [01:06<00:13,  3.29it/s][A
 81%|████████  | 178/221 [01:07<00:12,  3.44it/s][A
 81%|████████  | 179/221 [01:07<00:13,  3.14it/s][A
 81%|████████▏ | 180/221 [01:07<00:10,  3.93it/s][A
 82%|████████▏ | 181/221 [01:07<00:11,  3.51it/s][A
 82%|████████▏ | 182/221 [01:08<00:09,  4.02it/s][A
 83%|████████▎ | 183/221 [01:08<00:09,  4.15it/s][A
 83%|████████▎ | 184/221 [01:08<00:10,  3.62it/s][A
 84%|████████▎ | 185/221 [01:08<00:08,  4.40it/s][A
 84%|████████▍ | 186/221 [01:09<00:09,  3.65it/s][A
 85%|████████▍ | 187/221 [01:09<00:08,  3.91it/s][A
 85%|████████▌ | 188/221 [01:09<00:08,  3.85it/s][A
 86%|████████▌ | 189/221 [01:09<00:09,  3.30it/s][A
 86%|████████▌ | 190/221 [01:10<00:10,  2.92it/s][A
 86%|████████▋ | 191/221 [01:10<00:08,  3.40it/s][A
 87%|████████▋ | 192/221 [01:10<00:08,  3.41it/s][A
 87%|████████▋ | 193/221 [01:10<00:06,  4.16it/s][A
 88%|████████▊ | 194/221 [01:11<00:12,  2.19it/s][A
 88%|████████▊ | 195/221 [01:12<00:11,  2.31it/s][A
 89%|████████▊ | 196/221 [01:12<00:12,  2.03it/s][A
 89%|████████▉ | 197/221 [01:13<00:09,  2.51it/s][A
 90%|████████▉ | 198/221 [01:13<00:08,  2.84it/s][A
 90%|█████████ | 199/221 [01:13<00:06,  3.49it/s][A
 90%|█████████ | 200/221 [01:13<00:06,  3.23it/s][A
 91%|█████████ | 201/221 [01:14<00:06,  3.25it/s][A
 91%|█████████▏| 202/221 [01:14<00:05,  3.40it/s][A
 92%|█████████▏| 203/221 [01:14<00:04,  3.89it/s][A
 92%|█████████▏| 204/221 [01:14<00:04,  3.77it/s][A
 93%|█████████▎| 205/221 [01:15<00:03,  4.43it/s][A
 93%|█████████▎| 206/221 [01:15<00:05,  2.59it/s][A
 94%|█████████▎| 207/221 [01:15<00:04,  3.24it/s][A
 94%|█████████▍| 208/221 [01:16<00:03,  3.71it/s][A
 95%|█████████▍| 209/221 [01:16<00:03,  3.89it/s][A
 95%|█████████▌| 210/221 [01:16<00:02,  4.52it/s][A
 95%|█████████▌| 211/221 [01:16<00:02,  3.60it/s][A
 96%|█████████▌| 212/221 [01:17<00:02,  4.15it/s][A
 96%|█████████▋| 213/221 [01:17<00:02,  3.96it/s][A
 97%|█████████▋| 214/221 [01:17<00:01,  3.52it/s][A
 97%|█████████▋| 215/221 [01:17<00:01,  3.45it/s][A
 98%|█████████▊| 216/221 [01:18<00:01,  3.24it/s][A
 98%|█████████▊| 217/221 [01:18<00:01,  3.01it/s][A
 99%|█████████▊| 218/221 [01:19<00:01,  2.98it/s][A
 99%|█████████▉| 219/221 [01:19<00:00,  3.38it/s][A
100%|█████████▉| 220/221 [01:20<00:00,  1.39it/s][A
100%|██████████| 221/221 [01:21<00:00,  1.72it/s][A100%|██████████| 221/221 [01:21<00:00,  2.72it/s]
[h264 @ 0x55d90ce4b6c0] mmco: unref short failure
[h264 @ 0x55d90ce4b6c0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A[h264 @ 0x5598e8044480] mmco: unref short failure
[h264 @ 0x5598e8044480] mmco: unref short failure

  0%|          | 1/221 [00:00<01:08,  3.20it/s][A
  1%|          | 2/221 [00:00<01:07,  3.25it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.27it/s][A
  2%|▏         | 4/221 [00:01<01:07,  3.23it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.23it/s][A
  3%|▎         | 6/221 [00:01<01:06,  3.21it/s][A
  3%|▎         | 7/221 [00:02<01:06,  3.22it/s][A
  4%|▎         | 8/221 [00:02<01:06,  3.22it/s][A
  4%|▍         | 9/221 [00:02<01:06,  3.21it/s][A
  5%|▍         | 10/221 [00:03<01:06,  3.18it/s][A
  5%|▍         | 11/221 [00:03<01:06,  3.18it/s][A
  5%|▌         | 12/221 [00:03<01:05,  3.19it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.22it/s][A
  6%|▋         | 14/221 [00:04<01:03,  3.24it/s][A
  7%|▋         | 15/221 [00:04<01:03,  3.24it/s][A
  7%|▋         | 16/221 [00:04<01:03,  3.24it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.25it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.23it/s][A
  9%|▊         | 19/221 [00:05<01:02,  3.21it/s][A
  9%|▉         | 20/221 [00:06<01:02,  3.23it/s][A
 10%|▉         | 21/221 [00:06<01:02,  3.22it/s][A
 10%|▉         | 22/221 [00:06<01:04,  3.08it/s][A09/18/2024 17:15:01 - INFO - __main__ -   current idx R8HHCsDQ1cs.27 from finetune_area returns wrong image/video, use 15213 instead.

 10%|█         | 23/221 [00:07<01:05,  3.03it/s][A
 11%|█         | 24/221 [00:07<01:06,  2.97it/s][A
 11%|█▏        | 25/221 [00:07<01:04,  3.04it/s][A
 12%|█▏        | 26/221 [00:08<01:02,  3.11it/s][A
 12%|█▏        | 27/221 [00:08<01:01,  3.16it/s][A
 13%|█▎        | 28/221 [00:08<01:00,  3.20it/s][A
 13%|█▎        | 29/221 [00:09<01:00,  3.18it/s][A
 14%|█▎        | 30/221 [00:09<00:59,  3.21it/s][A
 14%|█▍        | 31/221 [00:09<00:59,  3.17it/s][A
 14%|█▍        | 32/221 [00:10<00:58,  3.20it/s][A
 15%|█▍        | 33/221 [00:10<00:59,  3.15it/s][A
 15%|█▌        | 34/221 [00:10<00:59,  3.15it/s][A
 16%|█▌        | 35/221 [00:11<00:58,  3.18it/s][A
 16%|█▋        | 36/221 [00:11<00:57,  3.21it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.23it/s][A
 17%|█▋        | 38/221 [00:11<00:57,  3.20it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.21it/s][A
 18%|█▊        | 40/221 [00:12<00:56,  3.19it/s][A
 19%|█▊        | 41/221 [00:12<00:55,  3.22it/s][A
 19%|█▉        | 42/221 [00:13<00:55,  3.24it/s][A
 19%|█▉        | 43/221 [00:13<00:54,  3.25it/s][A
 20%|█▉        | 44/221 [00:13<00:54,  3.27it/s][A
 20%|██        | 45/221 [00:14<00:54,  3.23it/s][A
 21%|██        | 46/221 [00:14<00:53,  3.24it/s][A
 21%|██▏       | 47/221 [00:14<00:56,  3.08it/s][A
 22%|██▏       | 48/221 [00:15<00:55,  3.13it/s][A
 22%|██▏       | 49/221 [00:15<00:55,  3.12it/s][A
 23%|██▎       | 50/221 [00:15<00:54,  3.16it/s][A
 23%|██▎       | 51/221 [00:16<00:53,  3.17it/s][A
 24%|██▎       | 52/221 [00:16<00:52,  3.21it/s][A
 24%|██▍       | 53/221 [00:16<00:51,  3.23it/s][A
 24%|██▍       | 54/221 [00:16<00:52,  3.21it/s][A
 25%|██▍       | 55/221 [00:17<00:51,  3.20it/s][A
 25%|██▌       | 56/221 [00:17<00:54,  3.00it/s][A
 26%|██▌       | 57/221 [00:17<00:55,  2.95it/s][A
 26%|██▌       | 58/221 [00:18<00:53,  3.04it/s][A[h264 @ 0x55cb6ec86a40] mmco: unref short failure
[h264 @ 0x55cb6ec86a40] mmco: unref short failure

 27%|██▋       | 59/221 [00:18<00:52,  3.11it/s][A
 27%|██▋       | 60/221 [00:18<00:51,  3.14it/s][A
 28%|██▊       | 61/221 [00:19<00:50,  3.18it/s][A
 28%|██▊       | 62/221 [00:19<00:49,  3.22it/s][A
 29%|██▊       | 63/221 [00:19<00:48,  3.24it/s][A
 29%|██▉       | 64/221 [00:20<00:48,  3.25it/s][A
 29%|██▉       | 65/221 [00:20<00:47,  3.27it/s][A
 30%|██▉       | 66/221 [00:20<00:47,  3.27it/s][A
 30%|███       | 67/221 [00:21<00:47,  3.27it/s][A
 31%|███       | 68/221 [00:21<00:46,  3.28it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.28it/s][A
 32%|███▏      | 70/221 [00:21<00:46,  3.25it/s][A
 32%|███▏      | 71/221 [00:22<00:46,  3.26it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.25it/s][A
 33%|███▎      | 73/221 [00:22<00:45,  3.27it/s][A
 33%|███▎      | 74/221 [00:23<00:44,  3.27it/s][A
 34%|███▍      | 75/221 [00:23<00:44,  3.28it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.28it/s][A
 35%|███▍      | 77/221 [00:24<00:44,  3.27it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.28it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.29it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.29it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.29it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.29it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.29it/s][A
 38%|███▊      | 84/221 [00:26<00:41,  3.30it/s][A
 38%|███▊      | 85/221 [00:26<00:41,  3.30it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.30it/s][A
 39%|███▉      | 87/221 [00:27<00:40,  3.30it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.30it/s][A
 40%|████      | 89/221 [00:27<00:40,  3.30it/s][A
 41%|████      | 90/221 [00:28<00:39,  3.30it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 94/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.30it/s][A
 44%|████▍     | 97/221 [00:30<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:34<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:36<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:37<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:38<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:40<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:42<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:44<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:46<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:47<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:48<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:50<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:51<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:52<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:53<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:54<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:56<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:57<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:58<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [01:00<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:01<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:02<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:06<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:07<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.27it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:49,  4.44it/s][A
  1%|          | 2/221 [00:00<01:52,  1.95it/s][A
  2%|▏         | 4/221 [00:01<01:09,  3.14it/s][A
  2%|▏         | 5/221 [00:01<01:08,  3.15it/s][A
  3%|▎         | 6/221 [00:01<00:55,  3.90it/s][A
  3%|▎         | 7/221 [00:01<00:52,  4.09it/s][A
  4%|▎         | 8/221 [00:02<01:07,  3.15it/s][A
  4%|▍         | 9/221 [00:02<01:08,  3.07it/s][A
  5%|▍         | 10/221 [00:03<01:37,  2.17it/s][A
  5%|▍         | 11/221 [00:03<01:20,  2.62it/s][A
  5%|▌         | 12/221 [00:04<01:16,  2.75it/s][A
  6%|▌         | 13/221 [00:04<01:43,  2.01it/s][A
  6%|▋         | 14/221 [00:05<01:24,  2.46it/s][A
  7%|▋         | 15/221 [00:05<01:05,  3.15it/s][A
  7%|▋         | 16/221 [00:05<01:09,  2.94it/s][A
  8%|▊         | 17/221 [00:06<02:05,  1.63it/s][A
  8%|▊         | 18/221 [00:07<01:48,  1.87it/s][A
  9%|▊         | 19/221 [00:07<01:32,  2.17it/s][A
  9%|▉         | 20/221 [00:07<01:14,  2.72it/s][A
 10%|▉         | 21/221 [00:07<00:58,  3.39it/s][A
 10%|▉         | 22/221 [00:07<00:53,  3.75it/s][A
 10%|█         | 23/221 [00:08<00:44,  4.42it/s][A
 11%|█         | 24/221 [00:08<00:39,  5.00it/s][A
 11%|█▏        | 25/221 [00:08<00:47,  4.12it/s][A
 12%|█▏        | 26/221 [00:08<00:49,  3.91it/s][A
 13%|█▎        | 28/221 [00:10<01:17,  2.48it/s][A
 13%|█▎        | 29/221 [00:10<01:08,  2.82it/s][A
 14%|█▎        | 30/221 [00:10<01:09,  2.74it/s][A
 14%|█▍        | 31/221 [00:11<01:11,  2.66it/s][A
 14%|█▍        | 32/221 [00:11<00:58,  3.25it/s][A
 15%|█▍        | 33/221 [00:11<00:52,  3.59it/s][A
 15%|█▌        | 34/221 [00:11<00:42,  4.41it/s][A
 16%|█▌        | 35/221 [00:11<00:40,  4.60it/s][A
 16%|█▋        | 36/221 [00:12<00:48,  3.78it/s][A
 17%|█▋        | 37/221 [00:12<01:00,  3.04it/s][A
 17%|█▋        | 38/221 [00:12<01:03,  2.90it/s][A
 18%|█▊        | 39/221 [00:13<00:55,  3.28it/s][A
 18%|█▊        | 40/221 [00:13<01:05,  2.78it/s][A
 19%|█▊        | 41/221 [00:13<00:51,  3.48it/s][A
 19%|█▉        | 42/221 [00:13<00:45,  3.94it/s][A
 20%|█▉        | 44/221 [00:14<00:30,  5.83it/s][A
 20%|██        | 45/221 [00:14<00:39,  4.49it/s][A
 21%|██        | 46/221 [00:14<00:37,  4.67it/s][A
 21%|██▏       | 47/221 [00:14<00:41,  4.23it/s][A
 22%|██▏       | 48/221 [00:15<00:34,  4.99it/s][A
 22%|██▏       | 49/221 [00:15<00:34,  5.03it/s][A
 23%|██▎       | 50/221 [00:15<00:32,  5.20it/s][A
 23%|██▎       | 51/221 [00:15<00:28,  5.96it/s][A
 24%|██▎       | 52/221 [00:15<00:26,  6.35it/s][A
 24%|██▍       | 53/221 [00:15<00:28,  5.94it/s][A
 24%|██▍       | 54/221 [00:16<00:36,  4.54it/s][A
 25%|██▍       | 55/221 [00:16<00:34,  4.83it/s][A
 25%|██▌       | 56/221 [00:16<00:34,  4.80it/s][A
 26%|██▌       | 57/221 [00:16<00:33,  4.83it/s][A
 26%|██▌       | 58/221 [00:17<00:41,  3.93it/s][A
 27%|██▋       | 59/221 [00:17<00:37,  4.38it/s][A
 27%|██▋       | 60/221 [00:17<00:57,  2.82it/s][A
 28%|██▊       | 61/221 [00:18<00:51,  3.12it/s][A
 28%|██▊       | 62/221 [00:18<00:46,  3.39it/s][A
 29%|██▊       | 63/221 [00:19<01:02,  2.53it/s][A
 29%|██▉       | 64/221 [00:19<00:54,  2.86it/s][A
 29%|██▉       | 65/221 [00:19<01:02,  2.48it/s][A
 30%|██▉       | 66/221 [00:19<00:50,  3.08it/s][A
 30%|███       | 67/221 [00:20<01:13,  2.10it/s][A
 31%|███       | 68/221 [00:20<00:57,  2.64it/s][A
 31%|███       | 69/221 [00:21<00:59,  2.54it/s][A
 32%|███▏      | 70/221 [00:21<00:54,  2.78it/s][A
 32%|███▏      | 71/221 [00:22<00:53,  2.81it/s][A
 33%|███▎      | 72/221 [00:22<00:54,  2.74it/s][A
 33%|███▎      | 73/221 [00:22<00:55,  2.69it/s][A
 33%|███▎      | 74/221 [00:23<00:48,  3.01it/s][A
 34%|███▍      | 75/221 [00:23<00:48,  3.03it/s][A
 34%|███▍      | 76/221 [00:23<00:38,  3.77it/s][A
 35%|███▍      | 77/221 [00:23<00:32,  4.49it/s][A
 35%|███▌      | 78/221 [00:23<00:33,  4.21it/s][A
 36%|███▌      | 79/221 [00:24<00:47,  2.99it/s][A
 36%|███▌      | 80/221 [00:24<00:45,  3.10it/s][A
 37%|███▋      | 81/221 [00:24<00:40,  3.44it/s][A
 37%|███▋      | 82/221 [00:25<00:48,  2.86it/s][A
 38%|███▊      | 83/221 [00:26<01:01,  2.24it/s][A
 38%|███▊      | 84/221 [00:26<00:58,  2.33it/s][A
 38%|███▊      | 85/221 [00:27<01:02,  2.18it/s][A
 39%|███▉      | 86/221 [00:27<00:57,  2.36it/s][A
 39%|███▉      | 87/221 [00:28<01:19,  1.69it/s][A
 40%|███▉      | 88/221 [00:28<01:09,  1.92it/s][A
 40%|████      | 89/221 [00:28<00:57,  2.29it/s][A
 41%|████      | 90/221 [00:29<00:48,  2.68it/s][A
 41%|████      | 91/221 [00:29<00:39,  3.25it/s][A
 42%|████▏     | 92/221 [00:29<00:36,  3.57it/s][A
 42%|████▏     | 93/221 [00:29<00:34,  3.66it/s][A
 43%|████▎     | 94/221 [00:29<00:30,  4.14it/s][A
 43%|████▎     | 95/221 [00:30<00:34,  3.61it/s][A
 43%|████▎     | 96/221 [00:30<00:29,  4.23it/s][A
 44%|████▍     | 97/221 [00:30<00:24,  5.05it/s][A
 44%|████▍     | 98/221 [00:30<00:26,  4.57it/s][A
 45%|████▍     | 99/221 [00:30<00:24,  5.02it/s][A
 45%|████▌     | 100/221 [00:31<00:21,  5.66it/s][A
 46%|████▌     | 101/221 [00:31<00:21,  5.64it/s][A
 46%|████▌     | 102/221 [00:31<00:19,  6.09it/s][A
 47%|████▋     | 103/221 [00:31<00:19,  5.93it/s][A
 47%|████▋     | 104/221 [00:31<00:17,  6.50it/s][A
 48%|████▊     | 105/221 [00:31<00:19,  6.06it/s][A
 48%|████▊     | 106/221 [00:32<00:22,  5.19it/s][A
 48%|████▊     | 107/221 [00:32<00:21,  5.30it/s][A
 49%|████▉     | 108/221 [00:32<00:24,  4.62it/s][A
 49%|████▉     | 109/221 [00:33<00:48,  2.30it/s][A
 50%|████▉     | 110/221 [00:34<00:51,  2.15it/s][A
 50%|█████     | 111/221 [00:34<00:51,  2.12it/s][A
 51%|█████     | 112/221 [00:34<00:42,  2.55it/s][A
 51%|█████     | 113/221 [00:35<00:37,  2.84it/s][A
 52%|█████▏    | 115/221 [00:35<00:25,  4.20it/s][A
 53%|█████▎    | 117/221 [00:35<00:24,  4.27it/s][A
 53%|█████▎    | 118/221 [00:36<00:29,  3.48it/s][A
 54%|█████▍    | 119/221 [00:36<00:31,  3.28it/s][A
 54%|█████▍    | 120/221 [00:36<00:32,  3.08it/s][A
 55%|█████▍    | 121/221 [00:37<00:26,  3.77it/s][A
 55%|█████▌    | 122/221 [00:37<00:22,  4.48it/s][A
 56%|█████▌    | 123/221 [00:37<00:22,  4.34it/s][A
 56%|█████▌    | 124/221 [00:37<00:26,  3.64it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.29it/s][A
 57%|█████▋    | 126/221 [00:38<00:29,  3.20it/s][A
 57%|█████▋    | 127/221 [00:38<00:24,  3.90it/s][A
 58%|█████▊    | 128/221 [00:38<00:24,  3.79it/s][A
 58%|█████▊    | 129/221 [00:39<00:19,  4.65it/s][A
 59%|█████▉    | 130/221 [00:39<00:23,  3.90it/s][A
 59%|█████▉    | 131/221 [00:39<00:19,  4.53it/s][A
 60%|█████▉    | 132/221 [00:39<00:16,  5.34it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.37it/s][A
 61%|██████    | 134/221 [00:40<00:22,  3.94it/s][A
 61%|██████    | 135/221 [00:40<00:23,  3.60it/s][A
 62%|██████▏   | 136/221 [00:40<00:25,  3.38it/s][A
 62%|██████▏   | 137/221 [00:41<00:23,  3.64it/s][A
 62%|██████▏   | 138/221 [00:41<00:23,  3.58it/s][A
 63%|██████▎   | 139/221 [00:41<00:21,  3.89it/s][A
 63%|██████▎   | 140/221 [00:41<00:18,  4.34it/s][A
 64%|██████▍   | 142/221 [00:42<00:17,  4.57it/s][A
 65%|██████▌   | 144/221 [00:42<00:15,  4.92it/s][A
 66%|██████▌   | 145/221 [00:42<00:15,  4.94it/s][A
 66%|██████▌   | 146/221 [00:43<00:16,  4.68it/s][A
 67%|██████▋   | 147/221 [00:43<00:14,  5.18it/s][A
 67%|██████▋   | 148/221 [00:43<00:16,  4.29it/s][A
 67%|██████▋   | 149/221 [00:43<00:14,  5.00it/s][A
 68%|██████▊   | 150/221 [00:44<00:18,  3.78it/s][A
 68%|██████▊   | 151/221 [00:44<00:18,  3.69it/s][A
 69%|██████▉   | 152/221 [00:45<00:32,  2.09it/s][A
 69%|██████▉   | 153/221 [00:45<00:31,  2.15it/s][A
 70%|██████▉   | 154/221 [00:46<00:30,  2.22it/s][A
 70%|███████   | 155/221 [00:46<00:27,  2.39it/s][A
 71%|███████   | 156/221 [00:46<00:24,  2.67it/s][A
 71%|███████   | 157/221 [00:47<00:22,  2.80it/s][A
 71%|███████▏  | 158/221 [00:47<00:20,  3.04it/s][A
 72%|███████▏  | 159/221 [00:47<00:19,  3.14it/s][A
 72%|███████▏  | 160/221 [00:47<00:18,  3.24it/s][A
 73%|███████▎  | 161/221 [00:48<00:15,  3.88it/s][A
 74%|███████▍  | 163/221 [00:48<00:12,  4.67it/s][A
 74%|███████▍  | 164/221 [00:48<00:14,  3.95it/s][A
 75%|███████▌  | 166/221 [00:49<00:16,  3.38it/s][A
 76%|███████▌  | 167/221 [00:49<00:13,  3.98it/s][A
 76%|███████▌  | 168/221 [00:49<00:12,  4.14it/s][A
 76%|███████▋  | 169/221 [00:50<00:11,  4.49it/s][A
 77%|███████▋  | 170/221 [00:50<00:14,  3.64it/s][A
 77%|███████▋  | 171/221 [00:50<00:14,  3.49it/s][A
 78%|███████▊  | 173/221 [00:50<00:09,  5.11it/s][A
 79%|███████▊  | 174/221 [00:51<00:12,  3.84it/s][A
 79%|███████▉  | 175/221 [00:51<00:12,  3.69it/s][A
 80%|███████▉  | 176/221 [00:51<00:12,  3.62it/s][A
 80%|████████  | 177/221 [00:52<00:10,  4.02it/s][A
 81%|████████  | 178/221 [00:52<00:09,  4.75it/s][A
 81%|████████  | 179/221 [00:52<00:10,  3.88it/s][A
 81%|████████▏ | 180/221 [00:52<00:09,  4.32it/s][A
 82%|████████▏ | 181/221 [00:52<00:08,  4.83it/s][A
 82%|████████▏ | 182/221 [00:53<00:07,  5.05it/s][A
 83%|████████▎ | 183/221 [00:53<00:06,  5.74it/s][A
 83%|████████▎ | 184/221 [00:53<00:10,  3.58it/s][A
 84%|████████▎ | 185/221 [00:54<00:09,  3.80it/s][A
 84%|████████▍ | 186/221 [00:54<00:09,  3.68it/s][A
 85%|████████▍ | 187/221 [00:54<00:07,  4.49it/s][A
 85%|████████▌ | 188/221 [00:55<00:11,  2.90it/s][A
 86%|████████▌ | 189/221 [00:55<00:12,  2.61it/s][A
 86%|████████▌ | 190/221 [00:56<00:13,  2.28it/s][A
 86%|████████▋ | 191/221 [00:56<00:11,  2.67it/s][A
 87%|████████▋ | 192/221 [00:56<00:09,  2.97it/s][A
 87%|████████▋ | 193/221 [00:56<00:07,  3.63it/s][A
 88%|████████▊ | 194/221 [00:57<00:11,  2.33it/s][A
 88%|████████▊ | 195/221 [00:58<00:12,  2.10it/s][A
 89%|████████▊ | 196/221 [00:58<00:12,  1.99it/s][A
 89%|████████▉ | 197/221 [00:58<00:10,  2.38it/s][A
 90%|████████▉ | 198/221 [00:58<00:07,  3.08it/s][A
 90%|█████████ | 200/221 [00:59<00:06,  3.27it/s][A
 91%|█████████ | 201/221 [00:59<00:05,  3.33it/s][A
 91%|█████████▏| 202/221 [00:59<00:04,  3.94it/s][A
 92%|█████████▏| 204/221 [01:00<00:04,  4.11it/s][A
 93%|█████████▎| 205/221 [01:00<00:03,  4.43it/s][A
 93%|█████████▎| 206/221 [01:00<00:04,  3.57it/s][A
 94%|█████████▎| 207/221 [01:01<00:03,  4.14it/s][A
 94%|█████████▍| 208/221 [01:01<00:03,  3.41it/s][A
 95%|█████████▍| 209/221 [01:02<00:04,  2.82it/s][A
 95%|█████████▌| 210/221 [01:02<00:03,  3.55it/s][A
 95%|█████████▌| 211/221 [01:02<00:02,  3.42it/s][A
 96%|█████████▌| 212/221 [01:02<00:02,  3.84it/s][A
 96%|█████████▋| 213/221 [01:03<00:03,  2.43it/s][A
 97%|█████████▋| 214/221 [01:03<00:02,  2.65it/s][A
 97%|█████████▋| 215/221 [01:04<00:02,  2.48it/s][A
 98%|█████████▊| 216/221 [01:04<00:01,  2.78it/s][A
 98%|█████████▊| 217/221 [01:04<00:01,  2.88it/s][A
 99%|█████████▊| 218/221 [01:04<00:00,  3.32it/s][A
 99%|█████████▉| 219/221 [01:05<00:00,  3.96it/s][A
100%|█████████▉| 220/221 [01:05<00:00,  4.16it/s][A
100%|██████████| 221/221 [01:05<00:00,  3.32it/s][A100%|██████████| 221/221 [01:05<00:00,  3.36it/s]
09/18/2024 17:17:12 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 349--===========

09/18/2024 17:17:12 - INFO - __main__ -   {'area_r1': 40.6, 'area_recall': '40.6/66.7/75.5', 'area_ravg': 60.9}
09/18/2024 17:17:12 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 349--===========

09/18/2024 17:17:12 - INFO - __main__ -   {'forward_r1': 37.7, 'forward_recall': '37.7/63.1/74.5', 'forward_ravg': 58.4}
09/18/2024 17:17:12 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 349--===========

09/18/2024 17:17:12 - INFO - __main__ -   {'area_video_r1': 40.3, 'area_video_recall': '40.3/67.3/77.8', 'area_video_ravg': 61.8}
09/18/2024 17:17:12 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 17:17:12 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 17:17:12 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 349--===========

09/18/2024 17:17:12 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.2/81.2', 'area_video_ravg': 68.1, 'area_video_back_r1': 49.4, 'area_video_back_recall': '49.4/69.5/77.5', 'area_video_back_ravg': 65.5}
09/18/2024 17:17:12 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 349=======

09/18/2024 17:17:12 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.2/81.2', 'area_video_ravg': 68.1, 'area_video_back_r1': 49.4, 'area_video_back_recall': '49.4/69.5/77.5', 'area_video_back_ravg': 65.5}
09/18/2024 17:17:12 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 349--===========

09/18/2024 17:17:12 - INFO - __main__ -   {'video_r1': 28.2, 'video_recall': '28.2/52.7/64.1', 'video_ravg': 48.3}
09/18/2024 17:17:12 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 17:17:12 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 17:17:12 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 349--===========

09/18/2024 17:17:12 - INFO - __main__ -   {'video_r1': 50.1, 'video_recall': '50.1/68.1/76.0', 'video_ravg': 64.7}
09/18/2024 17:17:12 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 17:17:12 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 17:17:37 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0005046189180575311, 'loss_ret%tv%ta--finetune_area/loss_area': 1.40821373462677, 'loss_ret%tv%ta--finetune_area/total_loss': 1.4087183475494385}
[h264 @ 0x5566055cac40] mmco: unref short failure
 12%|█▏        | 350/2910 [2:12:30<96:42:05, 135.99s/it] 12%|█▏        | 351/2910 [2:12:34<68:23:39, 96.22s/it]  12%|█▏        | 352/2910 [2:12:38<48:42:57, 68.56s/it][h264 @ 0x5565e6ec9900] mmco: unref short failure
 12%|█▏        | 353/2910 [2:12:42<34:57:11, 49.21s/it][h264 @ 0x55d8fe3713c0] mmco: unref short failure
[h264 @ 0x55d8fe3713c0] mmco: unref short failure
 12%|█▏        | 354/2910 [2:12:46<25:24:17, 35.78s/it][h264 @ 0x556600f9a700] mmco: unref short failure
[h264 @ 0x556600f9a700] mmco: unref short failure
[h264 @ 0x556600f9a700] mmco: unref short failure
[h264 @ 0x556600f9a700] mmco: unref short failure
 12%|█▏        | 355/2910 [2:12:51<18:51:47, 26.58s/it][h264 @ 0x5565eb930640] mmco: unref short failure
[h264 @ 0x5565eb930640] mmco: unref short failure
[h264 @ 0x55cb7a0f4280] mmco: unref short failure
[h264 @ 0x55cb7a0f4280] mmco: unref short failure
 12%|█▏        | 356/2910 [2:12:56<14:10:53, 19.99s/it] 12%|█▏        | 357/2910 [2:13:01<11:00:54, 15.53s/it][h264 @ 0x55cb7e5a9f40] mmco: unref short failure
 12%|█▏        | 358/2910 [2:13:07<9:00:35, 12.71s/it]  12%|█▏        | 359/2910 [2:13:13<7:32:44, 10.65s/it][h264 @ 0x5598e5b01580] mmco: unref short failure
[h264 @ 0x5598dc753480] mmco: unref short failure
 12%|█▏        | 360/2910 [2:13:19<6:31:40,  9.22s/it][h264 @ 0x5566054fee40] mmco: unref short failure
[h264 @ 0x5566054fee40] mmco: unref short failure
 12%|█▏        | 361/2910 [2:13:25<5:44:37,  8.11s/it] 12%|█▏        | 362/2910 [2:13:31<5:21:11,  7.56s/it] 12%|█▏        | 363/2910 [2:13:36<4:52:44,  6.90s/it][h264 @ 0x5598f0d7b540] mmco: unref short failure
[h264 @ 0x5598f0d7b540] mmco: unref short failure
 13%|█▎        | 364/2910 [2:13:41<4:31:24,  6.40s/it][h264 @ 0x5598e47be1c0] mmco: unref short failure
[h264 @ 0x5598e47be1c0] mmco: unref short failure
[h264 @ 0x5598e47be1c0] mmco: unref short failure
[h264 @ 0x5598e47be1c0] mmco: unref short failure
[h264 @ 0x5565f68d3200] mmco: unref short failure
[h264 @ 0x5565f68d3200] mmco: unref short failure
 13%|█▎        | 365/2910 [2:13:47<4:19:40,  6.12s/it][h264 @ 0x55d917753840] mmco: unref short failure
[h264 @ 0x55d917753840] mmco: unref short failure
09/18/2024 17:19:14 - INFO - __main__ -   current idx 3bpzUtvVmrY.16 from finetune_area returns wrong image/video, use 28433 instead.
[h264 @ 0x5598f9653480] mmco: unref short failure
[h264 @ 0x5598f9653480] mmco: unref short failure
[h264 @ 0x55cb8091c240] mmco: unref short failure
[h264 @ 0x55cb6c1dee00] mmco: unref short failure
[h264 @ 0x55cb6c1dee00] mmco: unref short failure
[h264 @ 0x5598e994a940] mmco: unref short failure
[h264 @ 0x5598e994a940] mmco: unref short failure
[h264 @ 0x556609ad9e80] mmco: unref short failure
[h264 @ 0x556609ad9e80] mmco: unref short failure
[h264 @ 0x556609ad9e80] mmco: unref short failure
[h264 @ 0x556609ad9e80] mmco: unref short failure
[h264 @ 0x55cb5fa60e80] mmco: unref short failure
[h264 @ 0x5598ed17dd40] mmco: unref short failure
[h264 @ 0x5598e5b01580] mmco: unref short failure
[h264 @ 0x5598e5b01580] mmco: unref short failure
[h264 @ 0x55d90fd20200] mmco: unref short failure
[h264 @ 0x55d90fd20200] mmco: unref short failure
 13%|█▎        | 366/2910 [2:14:35<13:16:13, 18.78s/it][h264 @ 0x5565f31367c0] mmco: unref short failure
[h264 @ 0x55cb66275680] mmco: unref short failure
[h264 @ 0x55cb66275680] mmco: unref short failure
[h264 @ 0x5565ecefdd80] mmco: unref short failure
[h264 @ 0x5565ecefdd80] mmco: unref short failure
09/18/2024 17:20:02 - INFO - __main__ -   current idx Qvo7ULfoowQ.43 from finetune_area returns wrong image/video, use 120356 instead.
[h264 @ 0x5598e3e58840] mmco: unref short failure
 13%|█▎        | 367/2910 [2:14:53<12:59:40, 18.40s/it][h264 @ 0x55cb791caac0] mmco: unref short failure
[h264 @ 0x55cb791caac0] mmco: unref short failure
 13%|█▎        | 368/2910 [2:14:59<10:27:07, 14.80s/it][h264 @ 0x55d917dccbc0] mmco: unref short failure
[h264 @ 0x55d917dccbc0] mmco: unref short failure
[h264 @ 0x5598faa03c80] mmco: unref short failure
[h264 @ 0x5565ff413f40] mmco: unref short failure
[h264 @ 0x5565ff413f40] mmco: unref short failure
 13%|█▎        | 369/2910 [2:15:08<9:08:53, 12.96s/it] [h264 @ 0x55cb73245ec0] mmco: unref short failure
[h264 @ 0x5598edad21c0] mmco: unref short failure
[h264 @ 0x55cb7b4f45c0] mmco: unref short failure
[h264 @ 0x55cb7b4f45c0] mmco: unref short failure
 13%|█▎        | 370/2910 [2:15:22<9:26:27, 13.38s/it][h264 @ 0x5565e803ff40] mmco: unref short failure
[h264 @ 0x5565e803ff40] mmco: unref short failure
 13%|█▎        | 371/2910 [2:15:28<7:49:25, 11.09s/it][av1 @ 0x5598fa39a600] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598fa39a600] Failed to get pixel format.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598fa39a600] Failed to get pixel format.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598fa39a600] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Missing Sequence Header.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
[av1 @ 0x5598eb18f800] Your platform doesn't suppport hardware accelerated AV1 decoding.
[av1 @ 0x5598eb18f800] Failed to get pixel format.
 13%|█▎        | 372/2910 [2:15:33<6:32:32,  9.28s/it][h264 @ 0x55cb5ec6ce00] mmco: unref short failure
[h264 @ 0x55cb5ec6ce00] mmco: unref short failure
[h264 @ 0x55cb5ec6ce00] mmco: unref short failure
 13%|█▎        | 373/2910 [2:15:40<5:58:42,  8.48s/it][h264 @ 0x55cb69e3d280] mmco: unref short failure
[h264 @ 0x5565fe97c380] mmco: unref short failure
[h264 @ 0x5598dc6d1ac0] mmco: unref short failure
[h264 @ 0x5598dc6d1ac0] mmco: unref short failure
[h264 @ 0x5565ebd08cc0] mmco: unref short failure
[h264 @ 0x5565ebd08cc0] mmco: unref short failure
[h264 @ 0x5565ebd08cc0] mmco: unref short failure
[h264 @ 0x5565ebd08cc0] mmco: unref short failure
09/18/2024 17:21:13 - INFO - __main__ -   current idx budftLR432g.60 from finetune_area returns wrong image/video, use 61327 instead.
[h264 @ 0x5565f6c64a80] mmco: unref short failure
[h264 @ 0x5598edad21c0] mmco: unref short failure
[h264 @ 0x5598edad21c0] mmco: unref short failure
[h264 @ 0x55cb654507c0] mmco: unref short failure
[h264 @ 0x55cb654507c0] mmco: unref short failure
[h264 @ 0x55d90d3d6e80] mmco: unref short failure
09/18/2024 17:21:24 - INFO - __main__ -   current idx WwLGb5RDcG0.21 from finetune_area returns wrong image/video, use 84277 instead.
09/18/2024 17:21:27 - INFO - __main__ -   current idx JZAjDUTk1oc.91 from finetune_area returns wrong image/video, use 146093 instead.
[h264 @ 0x5566057bc380] mmco: unref short failure
[h264 @ 0x5566057bc380] mmco: unref short failure
09/18/2024 17:21:32 - INFO - __main__ -   current idx _-um52MYvA8.26 from finetune_area returns wrong image/video, use 60048 instead.
[h264 @ 0x55cb5ec74f40] mmco: unref short failure
[h264 @ 0x55d907daf5c0] mmco: unref short failure
[h264 @ 0x55d907daf5c0] mmco: unref short failure
[h264 @ 0x55d907daf5c0] mmco: unref short failure
09/18/2024 17:21:50 - INFO - __main__ -   current idx ybadhoHjH4Q.28 from finetune_area returns wrong image/video, use 77530 instead.
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x55d9154dbe00] mmco: unref short failure
 13%|█▎        | 374/2910 [2:17:02<21:36:45, 30.68s/it][h264 @ 0x5565e6282580] mmco: unref short failure
[h264 @ 0x55cb67001e40] mmco: unref short failure
[h264 @ 0x55cb67001e40] mmco: unref short failure
[h264 @ 0x5598dc7d25c0] mmco: unref short failure
[h264 @ 0x5598dc7d25c0] mmco: unref short failure
[h264 @ 0x55cb7927fec0] mmco: unref short failure
[h264 @ 0x55cb7927fec0] mmco: unref short failure
[h264 @ 0x5598dc731500] mmco: unref short failure
[h264 @ 0x5598dc731500] mmco: unref short failure
 13%|█▎        | 375/2910 [2:17:21<19:12:14, 27.27s/it][h264 @ 0x5565fc802000] mmco: unref short failure
[h264 @ 0x5565fc802000] mmco: unref short failure
[h264 @ 0x5565ff3bb240] mmco: unref short failure
 13%|█▎        | 376/2910 [2:17:30<15:14:28, 21.65s/it][h264 @ 0x55cb7f7e9dc0] mmco: unref short failure
 13%|█▎        | 377/2910 [2:17:43<13:21:25, 18.98s/it][h264 @ 0x55cb74f4e3c0] mmco: unref short failure
[h264 @ 0x55cb6b9de540] mmco: unref short failure
[h264 @ 0x55cb6b9de540] mmco: unref short failure
[h264 @ 0x5598e5842a00] mmco: unref short failure
[h264 @ 0x5598e5842a00] mmco: unref short failure
 13%|█▎        | 378/2910 [2:17:55<11:53:20, 16.90s/it][h264 @ 0x55d8ff492bc0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6c7200] mmco: unref short failure
[h264 @ 0x5598dc6c7200] mmco: unref short failure
[h264 @ 0x5598dc6c7200] mmco: unref short failure
[h264 @ 0x5598dc6c7200] mmco: unref short failure
[h264 @ 0x5565f9e9dac0] mmco: unref short failure
 13%|█▎        | 379/2910 [2:18:06<10:41:29, 15.21s/it] 13%|█▎        | 380/2910 [2:18:12<8:43:20, 12.41s/it] [h264 @ 0x5598f83a2f80] mmco: unref short failure
 13%|█▎        | 381/2910 [2:18:17<7:13:55, 10.29s/it][h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x5598ea01f780] mmco: unref short failure
[h264 @ 0x5598ea01f780] mmco: unref short failure
[h264 @ 0x55d903539800] mmco: unref short failure
[h264 @ 0x55d903539800] mmco: unref short failure
[h264 @ 0x5598fbf6c880] mmco: unref short failure
[h264 @ 0x5598fbf6c880] mmco: unref short failure
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x55d91dff1800] mmco: unref short failure
[h264 @ 0x55d91dff1800] mmco: unref short failure
[h264 @ 0x55d901e5ecc0] mmco: unref short failure
[h264 @ 0x55d901e5ecc0] mmco: unref short failure
[h264 @ 0x5598dc73af00] mmco: unref short failure
[h264 @ 0x5598dc73af00] mmco: unref short failure
[h264 @ 0x55cb7cbf1700] mmco: unref short failure
[h264 @ 0x55cb7cbf1700] mmco: unref short failure
09/18/2024 17:24:04 - INFO - __main__ -   current idx VIh8Li4uUFk.0 from finetune_area returns wrong image/video, use 28920 instead.
[h264 @ 0x5598ec13bb80] mmco: unref short failure
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x556604ce1840] mmco: unref short failure
[h264 @ 0x556604ce1840] mmco: unref short failure
[h264 @ 0x5598dc764840] mmco: unref short failure
[h264 @ 0x5598dc764840] mmco: unref short failure
[h264 @ 0x55cb6c1dee00] mmco: unref short failure
[h264 @ 0x55cb7692e840] mmco: unref short failure
[h264 @ 0x55cb7692e840] mmco: unref short failure
[h264 @ 0x55cb7692e840] mmco: unref short failure
[h264 @ 0x55cb7692e840] mmco: unref short failure
[h264 @ 0x5565e7472e00] mmco: unref short failure
[h264 @ 0x5565e7472e00] mmco: unref short failure
[h264 @ 0x5566022ccdc0] mmco: unref short failure
[h264 @ 0x5566022ccdc0] mmco: unref short failure
[h264 @ 0x5566022ccdc0] mmco: unref short failure
[h264 @ 0x5566022ccdc0] mmco: unref short failure
[h264 @ 0x5566022ccdc0] mmco: unref short failure
[h264 @ 0x5566022ccdc0] mmco: unref short failure
[h264 @ 0x55cb7f1a0740] mmco: unref short failure
[h264 @ 0x55cb7f1a0740] mmco: unref short failure
[h264 @ 0x55cb7f1a0740] mmco: unref short failure
[h264 @ 0x55cb7f1a0740] mmco: unref short failure
 13%|█▎        | 382/2910 [2:19:37<21:53:33, 31.18s/it][h264 @ 0x5565ff3bb240] mmco: unref short failure
[h264 @ 0x55d912c20100] mmco: unref short failure
[h264 @ 0x5565f0f2cac0] mmco: unref short failure
 13%|█▎        | 383/2910 [2:19:56<19:15:36, 27.44s/it][h264 @ 0x55d90ce4b6c0] mmco: unref short failure
[h264 @ 0x55d90ce4b6c0] mmco: unref short failure
 13%|█▎        | 384/2910 [2:20:06<15:42:56, 22.40s/it][h264 @ 0x5598f19ed3c0] mmco: unref short failure
[h264 @ 0x5598f19ed3c0] mmco: unref short failure
[h264 @ 0x5598f19ed3c0] mmco: unref short failure
[h264 @ 0x5598f19ed3c0] mmco: unref short failure
[h264 @ 0x55cb7d83f280] mmco: unref short failure
[h264 @ 0x55cb7d83f280] mmco: unref short failure
 13%|█▎        | 385/2910 [2:20:19<13:39:59, 19.49s/it][h264 @ 0x5565f72fea00] mmco: unref short failure
[h264 @ 0x5565f72fea00] mmco: unref short failure
[h264 @ 0x5598f55721c0] mmco: unref short failure
[h264 @ 0x5598dc6cc400] mmco: unref short failure
[h264 @ 0x5598dc6cc400] mmco: unref short failure
09/18/2024 17:25:36 - INFO - __main__ -   current idx cUO419RDXpY.18 from finetune_area returns wrong image/video, use 142751 instead.
[h264 @ 0x55cb672cf600] mmco: unref short failure
[h264 @ 0x55cb672cf600] mmco: unref short failure
 13%|█▎        | 386/2910 [2:20:28<11:27:59, 16.35s/it] 13%|█▎        | 387/2910 [2:20:34<9:10:37, 13.09s/it]  13%|█▎        | 388/2910 [2:20:41<7:59:53, 11.42s/it] 13%|█▎        | 389/2910 [2:20:46<6:37:11,  9.45s/it]09/18/2024 17:25:56 - INFO - __main__ -   current idx bFHOzJ5hSFg.79 from finetune_area returns wrong image/video, use 93857 instead.
[h264 @ 0x5598dc6bfcc0] mmco: unref short failure
[h264 @ 0x55d8fe0a0440] mmco: unref short failure
[h264 @ 0x55d8fe0a0440] mmco: unref short failure
09/18/2024 17:26:03 - INFO - __main__ -   current idx aan7-sOxqOc.37 from finetune_area returns wrong image/video, use 133099 instead.
[h264 @ 0x55cb69586c00] mmco: unref short failure
[h264 @ 0x5566001e2300] mmco: unref short failure
[h264 @ 0x5566001e2300] mmco: unref short failure
[h264 @ 0x55cb79d253c0] mmco: unref short failure
[h264 @ 0x55cb79d253c0] mmco: unref short failure
[h264 @ 0x55d91b264f40] mmco: unref short failure
[h264 @ 0x55d91b264f40] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55cb7ab36f80] mmco: unref short failure
[h264 @ 0x55cb7ab36f80] mmco: unref short failure
09/18/2024 17:26:32 - INFO - __main__ -   current idx ulxAF450jro.5 from finetune_area returns wrong image/video, use 50708 instead.
[h264 @ 0x55d90ddf6840] mmco: unref short failure
[h264 @ 0x55d90ddf6840] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x55d90a250b80] mmco: unref short failure
[h264 @ 0x55d90a250b80] mmco: unref short failure
[h264 @ 0x55d91ac89740] mmco: unref short failure
 13%|█▎        | 390/2910 [2:22:04<21:02:26, 30.06s/it][h264 @ 0x5565f42d5f80] mmco: unref short failure
[h264 @ 0x5565f42d5f80] mmco: unref short failure
[h264 @ 0x55d90db31e80] mmco: unref short failure
[h264 @ 0x55d90db31e80] mmco: unref short failure
[h264 @ 0x55d90db31e80] mmco: unref short failure
[h264 @ 0x55d90db31e80] mmco: unref short failure
[h264 @ 0x5598f19f6480] mmco: unref short failure
 13%|█▎        | 391/2910 [2:22:19<17:52:53, 25.56s/it][h264 @ 0x5598dcc92140] mmco: unref short failure
[h264 @ 0x5598dcc92140] mmco: unref short failure
[h264 @ 0x5598fe7bfc00] mmco: unref short failure
[h264 @ 0x5598fe7bfc00] mmco: unref short failure
 13%|█▎        | 392/2910 [2:22:33<15:22:14, 21.98s/it][h264 @ 0x5598fbf68880] mmco: unref short failure
[h264 @ 0x55cb663cdf00] mmco: unref short failure
[h264 @ 0x55cb663cdf00] mmco: unref short failure
[h264 @ 0x5598fbf68880] mmco: unref short failure
[h264 @ 0x55cb663cdf00] mmco: unref short failure
[h264 @ 0x55cb663cdf00] mmco: unref short failure
[h264 @ 0x5598fbf68880] mmco: unref short failure
[h264 @ 0x5598e69f2800] mmco: unref short failure
 14%|█▎        | 393/2910 [2:22:46<13:28:39, 19.28s/it][h264 @ 0x55cb60ef4140] mmco: unref short failure
[h264 @ 0x55cb60ef4140] mmco: unref short failure
[h264 @ 0x55cb60ef4140] mmco: unref short failure
[h264 @ 0x55cb60ef4140] mmco: unref short failure
 14%|█▎        | 394/2910 [2:22:52<10:41:31, 15.30s/it] 14%|█▎        | 395/2910 [2:22:57<8:34:08, 12.27s/it] [h264 @ 0x5565f1b573c0] mmco: unref short failure
[h264 @ 0x5565e6561e40] mmco: unref short failure
[h264 @ 0x5565fcb3efc0] mmco: unref short failure
[h264 @ 0x5565fcb3efc0] mmco: unref short failure
[h264 @ 0x55660048c280] mmco: unref short failure
[h264 @ 0x55660048c280] mmco: unref short failure
[h264 @ 0x5565f7ba25c0] mmco: unref short failure
[h264 @ 0x5565f7ba25c0] mmco: unref short failure
 14%|█▎        | 396/2910 [2:23:11<8:54:39, 12.76s/it][h264 @ 0x5598e2566fc0] mmco: unref short failure
[h264 @ 0x5598e2566fc0] mmco: unref short failure
[h264 @ 0x5598e3c17900] mmco: unref short failure
[h264 @ 0x5565f65f0500] mmco: unref short failure
[h264 @ 0x5565f65f0500] mmco: unref short failure
[h264 @ 0x5565f65f0500] mmco: unref short failure
[h264 @ 0x5565f65f0500] mmco: unref short failure
[h264 @ 0x5565f65f0500] mmco: unref short failure
[h264 @ 0x5565f65f0500] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55d8fe80cd40] mmco: unref short failure
[h264 @ 0x55d8fe80cd40] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
[h264 @ 0x55cb67dea500] mmco: unref short failure
 14%|█▎        | 397/2910 [2:23:16<7:19:21, 10.49s/it][h264 @ 0x55d91b50d900] mmco: unref short failure
[h264 @ 0x5565e7be11c0] mmco: unref short failure
[h264 @ 0x55d9100571c0] mmco: unref short failure
[h264 @ 0x55d9100571c0] mmco: unref short failure
[h264 @ 0x5565ed5eab80] mmco: unref short failure
[h264 @ 0x5598f6f1aec0] mmco: unref short failure
[h264 @ 0x5598f6f1aec0] mmco: unref short failure
[h264 @ 0x55cb5f1bf140] mmco: unref short failure
[h264 @ 0x55cb5f1bf140] mmco: unref short failure
[h264 @ 0x55cb5f6d1240] mmco: unref short failure
[h264 @ 0x55cb5f6d1240] mmco: unref short failure
[h264 @ 0x55cb79f11840] mmco: unref short failure
[h264 @ 0x55cb79f11840] mmco: unref short failure
[h264 @ 0x55d91dff1800] mmco: unref short failure
[h264 @ 0x55d91dff1800] mmco: unref short failure
[h264 @ 0x5565fe621e80] mmco: unref short failure
[h264 @ 0x5566010f4b00] mmco: unref short failure
09/18/2024 17:29:19 - INFO - __main__ -   current idx eBDMGQmep9I.8 from finetune_area returns wrong image/video, use 47055 instead.
[h264 @ 0x55cb77ed7980] mmco: unref short failure
[h264 @ 0x55d90fd26e80] mmco: unref short failure
[h264 @ 0x55d90fd26e80] mmco: unref short failure
[h264 @ 0x55d90fd26e80] mmco: unref short failure
[h264 @ 0x55d90fd26e80] mmco: unref short failure
[h264 @ 0x55cb69e3d280] mmco: unref short failure
[h264 @ 0x55cb5e5d2d00] mmco: unref short failure
 14%|█▎        | 398/2910 [2:24:28<20:09:11, 28.88s/it][h264 @ 0x5565e7beba40] mmco: unref short failure
[h264 @ 0x55cb7cbf1700] mmco: unref short failure
 14%|█▎        | 399/2910 [2:24:41<16:54:14, 24.24s/it]09/18/2024 17:29:51 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 17:29:51 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb80899b80] mmco: unref short failure
[h264 @ 0x55cb80899b80] mmco: unref short failure
[h264 @ 0x5565e5f77f80] mmco: unref short failure
[h264 @ 0x55cb77c23d40] mmco: unref short failure
[h264 @ 0x55cb77c23d40] mmco: unref short failure
[h264 @ 0x55d8ff4c5f40] mmco: unref short failure
09/18/2024 17:30:11 - INFO - __main__ -   current idx YSb87fytH2o.16 from finetune_area returns wrong image/video, use 57613 instead.
[h264 @ 0x55cb66387e00] mmco: unref short failure
[h264 @ 0x55cb66387e00] mmco: unref short failure
[h264 @ 0x55cb66387e00] mmco: unref short failure
[h264 @ 0x55cb66387e00] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598f7efb880] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb5f1bf140] mmco: unref short failure
[h264 @ 0x55cb5f1bf140] mmco: unref short failure
[h264 @ 0x55d914b0f340] mmco: unref short failure
[h264 @ 0x55d914b0f340] mmco: unref short failure
[h264 @ 0x5566010f7a00] mmco: unref short failure
[h264 @ 0x5566010f7a00] mmco: unref short failure
[h264 @ 0x5598dc717100] mmco: unref short failure
[h264 @ 0x5598f51e0140] mmco: unref short failure
[h264 @ 0x55d90f798100] mmco: unref short failure
[h264 @ 0x5565ebe4f340] mmco: unref short failure
[h264 @ 0x55d91bfe06c0] mmco: unref short failure
[h264 @ 0x55d91bfe06c0] mmco: unref short failure
[h264 @ 0x55cb7279c600] mmco: unref short failure
[h264 @ 0x55cb7279c600] mmco: unref short failure
[h264 @ 0x55d91ac89740] mmco: unref short failure
[h264 @ 0x55cb6a5d0e40] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<03:46,  1.03s/it][A
  1%|          | 2/221 [00:01<02:50,  1.28it/s][A
  1%|▏         | 3/221 [00:01<02:05,  1.74it/s][A
  2%|▏         | 4/221 [00:02<01:35,  2.28it/s][A
  2%|▏         | 5/221 [00:02<01:20,  2.67it/s][A
  3%|▎         | 6/221 [00:02<01:10,  3.06it/s][A
  3%|▎         | 7/221 [00:02<01:02,  3.42it/s][A
  4%|▎         | 8/221 [00:03<01:10,  3.02it/s][A
  4%|▍         | 9/221 [00:03<01:17,  2.72it/s][A
  5%|▍         | 10/221 [00:04<01:34,  2.24it/s][A
  5%|▍         | 11/221 [00:04<01:23,  2.51it/s][A
  5%|▌         | 12/221 [00:05<01:55,  1.81it/s][A
  6%|▌         | 13/221 [00:06<01:46,  1.95it/s][A
  6%|▋         | 14/221 [00:06<02:10,  1.59it/s][A
  7%|▋         | 15/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 16/221 [00:07<01:48,  1.88it/s][A
  8%|▊         | 17/221 [00:08<01:56,  1.74it/s][A
  8%|▊         | 18/221 [00:08<01:41,  2.00it/s][A
  9%|▊         | 19/221 [00:08<01:22,  2.45it/s][A
  9%|▉         | 20/221 [00:09<01:15,  2.66it/s][A
 10%|▉         | 21/221 [00:09<01:05,  3.06it/s][A
 10%|▉         | 22/221 [00:09<00:59,  3.36it/s][A
 10%|█         | 23/221 [00:09<00:50,  3.90it/s][A
 11%|█         | 24/221 [00:09<00:43,  4.52it/s][A
 11%|█▏        | 25/221 [00:10<00:42,  4.61it/s][A
 12%|█▏        | 26/221 [00:10<01:02,  3.13it/s][A
 12%|█▏        | 27/221 [00:10<00:50,  3.83it/s][A
 13%|█▎        | 28/221 [00:11<01:10,  2.75it/s][A
 13%|█▎        | 29/221 [00:12<01:48,  1.77it/s][A
 14%|█▎        | 30/221 [00:12<01:40,  1.91it/s][A
 14%|█▍        | 31/221 [00:13<01:28,  2.14it/s][A
 14%|█▍        | 32/221 [00:13<01:14,  2.53it/s][A
 15%|█▍        | 33/221 [00:13<01:09,  2.72it/s][A
 15%|█▌        | 34/221 [00:14<01:02,  3.00it/s][A
 16%|█▌        | 35/221 [00:14<00:58,  3.17it/s][A
 16%|█▋        | 36/221 [00:14<01:02,  2.94it/s][A[h264 @ 0x55d91ac89740] mmco: unref short failure
[h264 @ 0x55d91ac89740] mmco: unref short failure

 17%|█▋        | 37/221 [00:15<01:32,  1.99it/s][A
 17%|█▋        | 38/221 [00:16<01:32,  1.98it/s][A
 18%|█▊        | 39/221 [00:16<01:34,  1.92it/s][A
 18%|█▊        | 40/221 [00:17<01:25,  2.13it/s][A
 19%|█▊        | 41/221 [00:17<01:06,  2.70it/s][A
 19%|█▉        | 42/221 [00:17<01:14,  2.39it/s][A
 19%|█▉        | 43/221 [00:17<01:03,  2.82it/s][A
 20%|█▉        | 44/221 [00:18<00:51,  3.45it/s][A
 20%|██        | 45/221 [00:18<01:19,  2.22it/s][A
 21%|██        | 46/221 [00:19<01:13,  2.37it/s][A
 21%|██▏       | 47/221 [00:19<01:14,  2.34it/s][A
 22%|██▏       | 48/221 [00:19<01:05,  2.63it/s][A
 22%|██▏       | 49/221 [00:20<01:21,  2.12it/s][A
 23%|██▎       | 50/221 [00:21<01:20,  2.13it/s][A
 24%|██▎       | 52/221 [00:21<00:53,  3.15it/s][A
 24%|██▍       | 53/221 [00:21<00:50,  3.31it/s][A
 24%|██▍       | 54/221 [00:23<01:37,  1.70it/s][A
 25%|██▍       | 55/221 [00:23<01:25,  1.95it/s][A
 25%|██▌       | 56/221 [00:23<01:12,  2.28it/s][A
 26%|██▌       | 57/221 [00:23<01:02,  2.62it/s][A
 26%|██▌       | 58/221 [00:24<00:56,  2.89it/s][A
 27%|██▋       | 59/221 [00:24<00:52,  3.07it/s][A
 27%|██▋       | 60/221 [00:25<01:22,  1.94it/s][A
 28%|██▊       | 61/221 [00:25<01:12,  2.22it/s][A
 28%|██▊       | 62/221 [00:25<01:05,  2.42it/s][A
 29%|██▊       | 63/221 [00:26<00:58,  2.72it/s][A
 29%|██▉       | 64/221 [00:26<00:48,  3.22it/s][A
 29%|██▉       | 65/221 [00:26<00:47,  3.26it/s][A
 30%|██▉       | 66/221 [00:27<00:49,  3.13it/s][A
 30%|███       | 67/221 [00:27<01:07,  2.29it/s][A
 31%|███       | 68/221 [00:27<00:56,  2.71it/s][A[h264 @ 0x5565f48ac080] mmco: unref short failure
[h264 @ 0x5565f48ac080] mmco: unref short failure

 31%|███       | 69/221 [00:28<01:07,  2.26it/s][A
 32%|███▏      | 70/221 [00:28<00:59,  2.53it/s][A
 32%|███▏      | 71/221 [00:29<01:08,  2.20it/s][A
 33%|███▎      | 72/221 [00:29<01:01,  2.42it/s][A
 33%|███▎      | 73/221 [00:30<00:59,  2.48it/s][A
 33%|███▎      | 74/221 [00:30<00:50,  2.91it/s][A
 34%|███▍      | 75/221 [00:30<00:55,  2.61it/s][A
 34%|███▍      | 76/221 [00:31<00:52,  2.79it/s][A
 35%|███▍      | 77/221 [00:31<00:52,  2.76it/s][A[h264 @ 0x5598dce5d580] mmco: unref short failure

 35%|███▌      | 78/221 [00:31<00:51,  2.76it/s][A
 36%|███▌      | 79/221 [00:32<01:09,  2.05it/s][A
 36%|███▌      | 80/221 [00:32<00:59,  2.36it/s][A
 37%|███▋      | 81/221 [00:33<00:52,  2.68it/s][A
 37%|███▋      | 82/221 [00:33<01:07,  2.05it/s][A
 38%|███▊      | 83/221 [00:34<01:13,  1.87it/s][A
 38%|███▊      | 84/221 [00:34<01:01,  2.21it/s][A
 38%|███▊      | 85/221 [00:34<00:48,  2.83it/s][A
 39%|███▉      | 86/221 [00:35<00:47,  2.85it/s][A
 39%|███▉      | 87/221 [00:35<00:54,  2.44it/s][A
 40%|███▉      | 88/221 [00:36<01:00,  2.21it/s][A
 40%|████      | 89/221 [00:36<00:56,  2.33it/s][A
 41%|████      | 90/221 [00:36<00:48,  2.71it/s][A
 41%|████      | 91/221 [00:37<00:38,  3.33it/s][A
 42%|████▏     | 92/221 [00:37<00:38,  3.35it/s][A
 42%|████▏     | 93/221 [00:37<00:40,  3.15it/s][A
 43%|████▎     | 94/221 [00:37<00:35,  3.54it/s][A
 43%|████▎     | 95/221 [00:38<00:38,  3.25it/s][A
 43%|████▎     | 96/221 [00:38<00:37,  3.35it/s][A
 44%|████▍     | 97/221 [00:38<00:32,  3.87it/s][A
 44%|████▍     | 98/221 [00:39<00:32,  3.73it/s][A
 45%|████▍     | 99/221 [00:39<00:30,  3.95it/s][A
 45%|████▌     | 100/221 [00:39<00:30,  4.00it/s][A
 46%|████▌     | 101/221 [00:39<00:25,  4.73it/s][A
 46%|████▌     | 102/221 [00:39<00:26,  4.47it/s][A
 47%|████▋     | 103/221 [00:40<00:27,  4.36it/s][A
 47%|████▋     | 104/221 [00:40<00:23,  4.97it/s][A
 48%|████▊     | 105/221 [00:40<00:25,  4.60it/s][A
 48%|████▊     | 106/221 [00:41<00:43,  2.65it/s][A
 48%|████▊     | 107/221 [00:41<00:36,  3.11it/s][A
 49%|████▉     | 108/221 [00:41<00:36,  3.12it/s][A
 49%|████▉     | 109/221 [00:42<00:39,  2.86it/s][A
 50%|████▉     | 110/221 [00:42<00:44,  2.47it/s][A
 50%|█████     | 111/221 [00:43<00:47,  2.31it/s][A
 51%|█████     | 112/221 [00:43<00:39,  2.79it/s][A
 51%|█████     | 113/221 [00:43<00:39,  2.71it/s][A
 52%|█████▏    | 114/221 [00:43<00:32,  3.30it/s][A
 52%|█████▏    | 115/221 [00:44<00:26,  4.05it/s][A
 52%|█████▏    | 116/221 [00:45<01:17,  1.35it/s][A[h264 @ 0x5565f26d2900] mmco: unref short failure
[h264 @ 0x5565f26d2900] mmco: unref short failure

 53%|█████▎    | 117/221 [00:46<01:03,  1.64it/s][A
 53%|█████▎    | 118/221 [00:46<00:53,  1.92it/s][A
 54%|█████▍    | 119/221 [00:47<00:50,  2.04it/s][A
 54%|█████▍    | 120/221 [00:47<00:47,  2.11it/s][A
 55%|█████▍    | 121/221 [00:47<00:38,  2.60it/s][A
 55%|█████▌    | 122/221 [00:48<00:37,  2.62it/s][A
 56%|█████▌    | 123/221 [00:48<00:38,  2.57it/s][A
 56%|█████▌    | 124/221 [00:48<00:36,  2.68it/s][A
 57%|█████▋    | 125/221 [00:49<00:38,  2.48it/s][A
 57%|█████▋    | 126/221 [00:49<00:35,  2.66it/s][A
 57%|█████▋    | 127/221 [00:50<00:42,  2.22it/s][A
 58%|█████▊    | 128/221 [00:50<00:41,  2.22it/s][A
 58%|█████▊    | 129/221 [00:50<00:34,  2.68it/s][A
 59%|█████▉    | 130/221 [00:51<00:36,  2.51it/s][A
 59%|█████▉    | 131/221 [00:51<00:32,  2.73it/s][A
 60%|█████▉    | 132/221 [00:51<00:29,  3.06it/s][A
 60%|██████    | 133/221 [00:52<00:34,  2.55it/s][A
 61%|██████    | 134/221 [00:52<00:29,  2.92it/s][A
 61%|██████    | 135/221 [00:52<00:28,  3.06it/s][A
 62%|██████▏   | 136/221 [00:53<00:31,  2.67it/s][A[h264 @ 0x55cb6a193bc0] mmco: unref short failure

 62%|██████▏   | 137/221 [00:53<00:27,  3.03it/s][A
 62%|██████▏   | 138/221 [00:53<00:29,  2.82it/s][A
 63%|██████▎   | 139/221 [00:54<00:31,  2.64it/s][A[h264 @ 0x55cb7bf75480] mmco: unref short failure
[h264 @ 0x55cb7bf75480] mmco: unref short failure

 63%|██████▎   | 140/221 [00:54<00:29,  2.76it/s][A
 64%|██████▍   | 141/221 [00:54<00:26,  3.07it/s][A
 64%|██████▍   | 142/221 [00:55<00:31,  2.53it/s][A
 65%|██████▍   | 143/221 [00:56<00:33,  2.35it/s][A
 65%|██████▌   | 144/221 [00:56<00:29,  2.60it/s][A
 66%|██████▌   | 145/221 [00:56<00:24,  3.15it/s][A
 66%|██████▌   | 146/221 [00:56<00:21,  3.50it/s][A[h264 @ 0x5565fcdb4540] mmco: unref short failure

 67%|██████▋   | 147/221 [00:56<00:20,  3.70it/s][A
 67%|██████▋   | 148/221 [00:57<00:22,  3.18it/s][A
 67%|██████▋   | 149/221 [00:57<00:19,  3.74it/s][A
 68%|██████▊   | 150/221 [00:57<00:20,  3.49it/s][A
 68%|██████▊   | 151/221 [00:58<00:27,  2.53it/s][A
 69%|██████▉   | 152/221 [00:59<00:49,  1.38it/s][A
 69%|██████▉   | 153/221 [01:00<00:42,  1.59it/s][A
 70%|██████▉   | 154/221 [01:00<00:34,  1.94it/s][A
 70%|███████   | 155/221 [01:00<00:28,  2.34it/s][A
 71%|███████   | 156/221 [01:01<00:26,  2.47it/s][A
 71%|███████   | 157/221 [01:01<00:29,  2.15it/s][A
 71%|███████▏  | 158/221 [01:02<00:26,  2.35it/s][A
 72%|███████▏  | 159/221 [01:02<00:23,  2.69it/s][A
 72%|███████▏  | 160/221 [01:02<00:22,  2.68it/s][A
 73%|███████▎  | 161/221 [01:02<00:18,  3.31it/s][A[h264 @ 0x5598df73a2c0] mmco: unref short failure

 73%|███████▎  | 162/221 [01:03<00:21,  2.73it/s][A
 74%|███████▍  | 163/221 [01:03<00:20,  2.84it/s][A
 74%|███████▍  | 164/221 [01:04<00:21,  2.60it/s][A
 75%|███████▍  | 165/221 [01:04<00:18,  2.98it/s][A
 75%|███████▌  | 166/221 [01:05<00:25,  2.14it/s][A
 76%|███████▌  | 167/221 [01:05<00:20,  2.68it/s][A
 76%|███████▌  | 168/221 [01:06<00:24,  2.18it/s][A
 76%|███████▋  | 169/221 [01:06<00:20,  2.59it/s][A
 77%|███████▋  | 170/221 [01:06<00:19,  2.59it/s][A
 77%|███████▋  | 171/221 [01:07<00:21,  2.34it/s][A
 78%|███████▊  | 172/221 [01:07<00:16,  2.90it/s][A
 78%|███████▊  | 173/221 [01:07<00:15,  3.19it/s][A
 79%|███████▊  | 174/221 [01:07<00:15,  3.09it/s][A
 79%|███████▉  | 175/221 [01:08<00:17,  2.56it/s][A
 80%|███████▉  | 176/221 [01:09<00:21,  2.13it/s][A
 80%|████████  | 177/221 [01:09<00:17,  2.55it/s][A
 81%|████████  | 178/221 [01:09<00:13,  3.14it/s][A
 81%|████████  | 179/221 [01:09<00:15,  2.70it/s][A
 81%|████████▏ | 180/221 [01:10<00:12,  3.37it/s][A
 82%|████████▏ | 181/221 [01:10<00:12,  3.21it/s][A
 82%|████████▏ | 182/221 [01:10<00:10,  3.86it/s][A
 83%|████████▎ | 183/221 [01:10<00:08,  4.42it/s][A
 83%|████████▎ | 184/221 [01:11<00:10,  3.63it/s][A
 84%|████████▎ | 185/221 [01:11<00:08,  4.21it/s][A
 84%|████████▍ | 186/221 [01:11<00:09,  3.72it/s][A
 85%|████████▍ | 187/221 [01:11<00:09,  3.77it/s][A
 85%|████████▌ | 188/221 [01:12<00:09,  3.59it/s][A
 86%|████████▌ | 189/221 [01:12<00:10,  3.16it/s][A[h264 @ 0x55d91922ba80] mmco: unref short failure

 86%|████████▌ | 190/221 [01:13<00:11,  2.65it/s][A
 86%|████████▋ | 191/221 [01:13<00:08,  3.34it/s][A
 87%|████████▋ | 192/221 [01:13<00:09,  3.11it/s][A
 87%|████████▋ | 193/221 [01:13<00:07,  3.64it/s][A
 88%|████████▊ | 194/221 [01:14<00:12,  2.24it/s][A
 88%|████████▊ | 195/221 [01:14<00:10,  2.57it/s][A
 89%|████████▊ | 196/221 [01:15<00:12,  2.07it/s][A
 89%|████████▉ | 197/221 [01:15<00:09,  2.55it/s][A
 90%|████████▉ | 198/221 [01:15<00:08,  2.85it/s][A
 90%|█████████ | 199/221 [01:16<00:06,  3.27it/s][A
 90%|█████████ | 200/221 [01:16<00:07,  2.97it/s][A
 91%|█████████ | 201/221 [01:16<00:06,  3.06it/s][A
 91%|█████████▏| 202/221 [01:17<00:05,  3.27it/s][A
 92%|█████████▏| 203/221 [01:17<00:05,  3.38it/s][A
 92%|█████████▏| 204/221 [01:17<00:05,  3.14it/s][A
 93%|█████████▎| 205/221 [01:17<00:04,  3.74it/s][A
 93%|█████████▎| 206/221 [01:18<00:05,  2.53it/s][A
 94%|█████████▎| 207/221 [01:18<00:04,  2.90it/s][A
 94%|█████████▍| 208/221 [01:19<00:04,  3.06it/s][A
 95%|█████████▍| 209/221 [01:19<00:04,  2.97it/s][A
 95%|█████████▌| 210/221 [01:19<00:03,  3.42it/s][A
 95%|█████████▌| 211/221 [01:20<00:03,  3.04it/s][A
 96%|█████████▌| 212/221 [01:20<00:02,  3.40it/s][A
 96%|█████████▋| 213/221 [01:20<00:02,  3.32it/s][A
 97%|█████████▋| 214/221 [01:20<00:02,  3.35it/s][A
 97%|█████████▋| 215/221 [01:21<00:01,  3.01it/s][A
 98%|█████████▊| 216/221 [01:21<00:01,  2.95it/s][A
 98%|█████████▊| 217/221 [01:22<00:01,  2.81it/s][A
 99%|█████████▊| 218/221 [01:22<00:01,  2.68it/s][A
 99%|█████████▉| 219/221 [01:22<00:00,  2.92it/s][A[h264 @ 0x5565fdc96e80] mmco: unref short failure
[h264 @ 0x5565fdc96e80] mmco: unref short failure

100%|█████████▉| 220/221 [01:25<00:01,  1.09s/it][A
100%|██████████| 221/221 [01:25<00:00,  1.20it/s][A100%|██████████| 221/221 [01:25<00:00,  2.57it/s]
[h264 @ 0x5565fd8ca8c0] mmco: unref short failure
[h264 @ 0x5565fd8ca8c0] mmco: unref short failure
[h264 @ 0x5565ffc94480] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:08,  3.21it/s][A
  1%|          | 2/221 [00:00<01:07,  3.24it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.26it/s][A
  2%|▏         | 4/221 [00:01<01:06,  3.27it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.23it/s][A
  3%|▎         | 6/221 [00:01<01:07,  3.21it/s][A
  3%|▎         | 7/221 [00:02<01:06,  3.22it/s][A
  4%|▎         | 8/221 [00:02<01:06,  3.22it/s][A
  4%|▍         | 9/221 [00:02<01:05,  3.24it/s][A
  5%|▍         | 10/221 [00:03<01:04,  3.26it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.23it/s][A
  5%|▌         | 12/221 [00:03<01:04,  3.25it/s][A
  6%|▌         | 13/221 [00:04<01:03,  3.25it/s][A
  6%|▋         | 14/221 [00:04<01:03,  3.24it/s][A
  7%|▋         | 15/221 [00:04<01:03,  3.26it/s][A
  7%|▋         | 16/221 [00:04<01:02,  3.26it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.26it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.27it/s][A
  9%|▊         | 19/221 [00:05<01:02,  3.25it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.26it/s][A
 10%|▉         | 21/221 [00:06<01:01,  3.24it/s][A
 10%|▉         | 22/221 [00:06<01:01,  3.26it/s][A
 10%|█         | 23/221 [00:07<01:01,  3.24it/s][A
 11%|█         | 24/221 [00:07<01:00,  3.26it/s][A
 11%|█▏        | 25/221 [00:07<01:00,  3.26it/s][A
 12%|█▏        | 26/221 [00:08<00:59,  3.26it/s][A
 12%|█▏        | 27/221 [00:08<01:00,  3.23it/s][A
 13%|█▎        | 28/221 [00:08<00:59,  3.25it/s][A
 13%|█▎        | 29/221 [00:08<00:59,  3.24it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.25it/s][A
 14%|█▍        | 31/221 [00:09<00:58,  3.25it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.26it/s][A
 15%|█▍        | 33/221 [00:10<00:57,  3.27it/s][A
 15%|█▌        | 34/221 [00:10<00:58,  3.22it/s][A
 16%|█▌        | 35/221 [00:10<00:58,  3.19it/s][A
 16%|█▋        | 36/221 [00:11<00:57,  3.22it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.24it/s][A
 17%|█▋        | 38/221 [00:11<00:56,  3.26it/s][A
 18%|█▊        | 39/221 [00:12<00:55,  3.27it/s][A
 18%|█▊        | 40/221 [00:12<00:55,  3.23it/s][A[h264 @ 0x5598dc6eb3c0] mmco: unref short failure
[h264 @ 0x5598dc6eb3c0] mmco: unref short failure

 19%|█▊        | 41/221 [00:12<00:55,  3.25it/s][A
 19%|█▉        | 42/221 [00:12<00:55,  3.25it/s][A
 19%|█▉        | 43/221 [00:13<00:55,  3.19it/s][A
 20%|█▉        | 44/221 [00:13<00:55,  3.19it/s][A
 20%|██        | 45/221 [00:13<00:54,  3.22it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.20it/s][A
 21%|██▏       | 47/221 [00:14<00:54,  3.21it/s][A
 22%|██▏       | 48/221 [00:14<00:53,  3.24it/s][A
 22%|██▏       | 49/221 [00:15<00:54,  3.14it/s][A
 23%|██▎       | 50/221 [00:15<00:53,  3.19it/s][A
 23%|██▎       | 51/221 [00:15<00:53,  3.20it/s][A
 24%|██▎       | 52/221 [00:16<00:52,  3.23it/s][A
 24%|██▍       | 53/221 [00:16<00:52,  3.23it/s][A
 24%|██▍       | 54/221 [00:16<00:52,  3.19it/s][A
 25%|██▍       | 55/221 [00:17<00:51,  3.22it/s][A
 25%|██▌       | 56/221 [00:17<00:50,  3.24it/s][A
 26%|██▌       | 57/221 [00:17<00:50,  3.26it/s][A
 26%|██▌       | 58/221 [00:17<00:50,  3.22it/s][A
 27%|██▋       | 59/221 [00:18<00:49,  3.25it/s][A
 27%|██▋       | 60/221 [00:18<00:49,  3.26it/s][A
 28%|██▊       | 61/221 [00:18<00:49,  3.26it/s][A
 28%|██▊       | 62/221 [00:19<00:48,  3.26it/s][A
 29%|██▊       | 63/221 [00:19<00:50,  3.10it/s][A
 29%|██▉       | 64/221 [00:19<00:50,  3.13it/s][A
 29%|██▉       | 65/221 [00:20<00:49,  3.16it/s][A
 30%|██▉       | 66/221 [00:20<00:48,  3.19it/s][A
 30%|███       | 67/221 [00:20<00:47,  3.22it/s][A
 31%|███       | 68/221 [00:21<00:47,  3.24it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.25it/s][A
 32%|███▏      | 70/221 [00:21<00:46,  3.25it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.27it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.27it/s][A
 33%|███▎      | 73/221 [00:22<00:45,  3.27it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.28it/s][A
 34%|███▍      | 75/221 [00:23<00:44,  3.28it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.29it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.29it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.29it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.30it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.30it/s][A
 37%|███▋      | 81/221 [00:24<00:42,  3.30it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.30it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.30it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.30it/s][A
 38%|███▊      | 85/221 [00:26<00:41,  3.30it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.30it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.30it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.30it/s][A
 40%|████      | 89/221 [00:27<00:39,  3.30it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.30it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.28it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:52,  4.15it/s][A
  1%|          | 2/221 [00:00<01:53,  1.93it/s][A
  2%|▏         | 4/221 [00:01<01:05,  3.29it/s][A
  2%|▏         | 5/221 [00:01<01:05,  3.30it/s][A
  3%|▎         | 6/221 [00:01<00:54,  3.94it/s][A
  3%|▎         | 7/221 [00:01<00:51,  4.15it/s][A
  4%|▎         | 8/221 [00:02<01:07,  3.17it/s][A
  4%|▍         | 9/221 [00:02<01:14,  2.85it/s][A
  5%|▍         | 10/221 [00:03<01:26,  2.45it/s][A
  5%|▍         | 11/221 [00:03<01:16,  2.73it/s][A
  5%|▌         | 12/221 [00:03<01:12,  2.90it/s][A
  6%|▌         | 13/221 [00:04<01:42,  2.03it/s][A
  6%|▋         | 14/221 [00:05<01:23,  2.47it/s][A
  7%|▋         | 16/221 [00:05<01:05,  3.15it/s][A
  8%|▊         | 17/221 [00:06<01:57,  1.74it/s][A
  8%|▊         | 18/221 [00:07<01:46,  1.91it/s][A
  9%|▊         | 19/221 [00:07<01:29,  2.25it/s][A
  9%|▉         | 20/221 [00:07<01:14,  2.70it/s][A
 10%|▉         | 21/221 [00:07<01:03,  3.15it/s][A
 10%|▉         | 22/221 [00:07<00:55,  3.62it/s][A
 10%|█         | 23/221 [00:08<00:47,  4.21it/s][A
 11%|█         | 24/221 [00:08<00:38,  5.08it/s][A
 11%|█▏        | 25/221 [00:08<00:46,  4.21it/s][A
 12%|█▏        | 26/221 [00:08<00:51,  3.80it/s][A
 13%|█▎        | 28/221 [00:09<01:12,  2.68it/s][A
 13%|█▎        | 29/221 [00:10<01:08,  2.78it/s][A
 14%|█▎        | 30/221 [00:10<01:08,  2.80it/s][A
 14%|█▍        | 31/221 [00:10<01:11,  2.64it/s][A
 14%|█▍        | 32/221 [00:11<00:59,  3.20it/s][A
 15%|█▍        | 33/221 [00:11<00:54,  3.48it/s][A
 15%|█▌        | 34/221 [00:11<00:44,  4.25it/s][A
 16%|█▌        | 35/221 [00:11<00:41,  4.53it/s][A
 16%|█▋        | 36/221 [00:12<00:49,  3.77it/s][A
 17%|█▋        | 37/221 [00:12<00:56,  3.23it/s][A
 17%|█▋        | 38/221 [00:12<01:00,  3.00it/s][A
 18%|█▊        | 39/221 [00:13<00:54,  3.35it/s][A
 18%|█▊        | 40/221 [00:13<01:04,  2.80it/s][A
 19%|█▊        | 41/221 [00:13<00:53,  3.37it/s][A
 19%|█▉        | 42/221 [00:13<00:45,  3.97it/s][A
 20%|█▉        | 44/221 [00:13<00:29,  6.04it/s][A
 20%|██        | 45/221 [00:14<00:38,  4.59it/s][A
 21%|██        | 46/221 [00:14<00:37,  4.66it/s][A
 21%|██▏       | 47/221 [00:14<00:41,  4.20it/s][A
 22%|██▏       | 48/221 [00:14<00:36,  4.76it/s][A
 22%|██▏       | 49/221 [00:15<00:35,  4.89it/s][A
 23%|██▎       | 50/221 [00:15<00:32,  5.26it/s][A
 23%|██▎       | 51/221 [00:15<00:28,  5.86it/s][A
 24%|██▎       | 52/221 [00:15<00:26,  6.38it/s][A
 24%|██▍       | 53/221 [00:15<00:31,  5.37it/s][A
 24%|██▍       | 54/221 [00:16<00:41,  4.02it/s][A
 25%|██▍       | 55/221 [00:16<00:36,  4.53it/s][A
 25%|██▌       | 56/221 [00:16<00:37,  4.44it/s][A
 26%|██▌       | 57/221 [00:16<00:35,  4.63it/s][A
 26%|██▌       | 58/221 [00:17<00:41,  3.91it/s][A
 27%|██▋       | 59/221 [00:17<00:35,  4.63it/s][A
 27%|██▋       | 60/221 [00:17<00:52,  3.04it/s][A
 28%|██▊       | 61/221 [00:18<00:47,  3.35it/s][A
 28%|██▊       | 62/221 [00:18<00:46,  3.44it/s][A
 29%|██▊       | 63/221 [00:18<00:57,  2.74it/s][A
 29%|██▉       | 64/221 [00:19<00:53,  2.91it/s][A
 29%|██▉       | 65/221 [00:19<01:02,  2.48it/s][A
 30%|██▉       | 66/221 [00:19<00:51,  2.99it/s][A
 30%|███       | 67/221 [00:20<01:09,  2.21it/s][A
 31%|███       | 68/221 [00:20<00:55,  2.76it/s][A
 31%|███       | 69/221 [00:21<00:54,  2.78it/s][A
 32%|███▏      | 70/221 [00:21<00:50,  3.02it/s][A
 32%|███▏      | 71/221 [00:21<00:46,  3.20it/s][A
 33%|███▎      | 72/221 [00:22<00:48,  3.07it/s][A
 33%|███▎      | 73/221 [00:22<00:51,  2.88it/s][A
 33%|███▎      | 74/221 [00:22<00:50,  2.94it/s][A
 34%|███▍      | 75/221 [00:23<00:49,  2.96it/s][A
 34%|███▍      | 76/221 [00:23<00:39,  3.69it/s][A
 35%|███▍      | 77/221 [00:23<00:35,  4.08it/s][A
 35%|███▌      | 78/221 [00:23<00:38,  3.72it/s][A
 36%|███▌      | 79/221 [00:24<00:51,  2.74it/s][A
 36%|███▌      | 80/221 [00:24<00:52,  2.70it/s][A
 37%|███▋      | 81/221 [00:24<00:46,  3.03it/s][A
 37%|███▋      | 82/221 [00:25<00:54,  2.54it/s][A
 38%|███▊      | 83/221 [00:26<01:10,  1.96it/s][A
 38%|███▊      | 84/221 [00:26<01:05,  2.08it/s][A
 38%|███▊      | 85/221 [00:26<00:57,  2.36it/s][A
 39%|███▉      | 86/221 [00:27<00:56,  2.41it/s][A
 39%|███▉      | 87/221 [00:28<01:19,  1.69it/s][A
 40%|███▉      | 88/221 [00:28<01:06,  2.01it/s][A
 40%|████      | 89/221 [00:28<00:55,  2.36it/s][A
 41%|████      | 90/221 [00:29<00:46,  2.84it/s][A
 41%|████      | 91/221 [00:29<00:37,  3.46it/s][A
 42%|████▏     | 92/221 [00:29<00:33,  3.90it/s][A
 42%|████▏     | 93/221 [00:29<00:29,  4.31it/s][A
 43%|████▎     | 94/221 [00:29<00:28,  4.49it/s][A
 43%|████▎     | 95/221 [00:30<00:34,  3.69it/s][A
 43%|████▎     | 96/221 [00:30<00:30,  4.13it/s][A
 44%|████▍     | 97/221 [00:30<00:25,  4.87it/s][A
 44%|████▍     | 98/221 [00:30<00:31,  3.87it/s][A
 45%|████▍     | 99/221 [00:30<00:26,  4.58it/s][A
 45%|████▌     | 100/221 [00:31<00:24,  4.84it/s][A
 46%|████▌     | 101/221 [00:31<00:22,  5.30it/s][A
 46%|████▌     | 102/221 [00:31<00:21,  5.61it/s][A
 47%|████▋     | 103/221 [00:31<00:18,  6.29it/s][A
 48%|████▊     | 105/221 [00:31<00:17,  6.69it/s][A
 48%|████▊     | 106/221 [00:32<00:20,  5.64it/s][A
 48%|████▊     | 107/221 [00:32<00:20,  5.46it/s][A
 49%|████▉     | 108/221 [00:32<00:23,  4.82it/s][A
 49%|████▉     | 109/221 [00:33<00:44,  2.50it/s][A
 50%|████▉     | 110/221 [00:33<00:47,  2.34it/s][A
 50%|█████     | 111/221 [00:34<00:45,  2.39it/s][A
 51%|█████     | 112/221 [00:34<00:39,  2.79it/s][A
 51%|█████     | 113/221 [00:34<00:35,  3.09it/s][A
 52%|█████▏    | 115/221 [00:35<00:23,  4.46it/s][A
 53%|█████▎    | 117/221 [00:35<00:23,  4.51it/s][A
 53%|█████▎    | 118/221 [00:35<00:27,  3.73it/s][A
 54%|█████▍    | 119/221 [00:36<00:26,  3.86it/s][A
 54%|█████▍    | 120/221 [00:36<00:29,  3.41it/s][A
 55%|█████▍    | 121/221 [00:36<00:26,  3.82it/s][A
 55%|█████▌    | 122/221 [00:36<00:22,  4.34it/s][A
 56%|█████▌    | 123/221 [00:37<00:23,  4.16it/s][A
 56%|█████▌    | 124/221 [00:37<00:25,  3.78it/s][A
 57%|█████▋    | 125/221 [00:37<00:31,  3.01it/s][A
 57%|█████▋    | 126/221 [00:38<00:31,  2.98it/s][A
 58%|█████▊    | 128/221 [00:38<00:24,  3.82it/s][A
 58%|█████▊    | 129/221 [00:38<00:20,  4.41it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.36it/s][A
 59%|█████▉    | 131/221 [00:39<00:23,  3.76it/s][A
 60%|█████▉    | 132/221 [00:39<00:20,  4.35it/s][A
 60%|██████    | 133/221 [00:40<00:30,  2.85it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.33it/s][A
 61%|██████    | 135/221 [00:40<00:28,  3.01it/s][A
 62%|██████▏   | 136/221 [00:41<00:29,  2.93it/s][A
 62%|██████▏   | 137/221 [00:41<00:27,  3.09it/s][A
 62%|██████▏   | 138/221 [00:41<00:26,  3.15it/s][A
 63%|██████▎   | 139/221 [00:41<00:23,  3.56it/s][A
 63%|██████▎   | 140/221 [00:42<00:21,  3.85it/s][A
 64%|██████▍   | 142/221 [00:42<00:18,  4.31it/s][A
 65%|██████▌   | 144/221 [00:42<00:16,  4.79it/s][A
 66%|██████▌   | 145/221 [00:43<00:15,  4.81it/s][A
 66%|██████▌   | 146/221 [00:43<00:16,  4.66it/s][A
 67%|██████▋   | 147/221 [00:43<00:14,  5.16it/s][A
 67%|██████▋   | 148/221 [00:43<00:18,  4.05it/s][A
 67%|██████▋   | 149/221 [00:43<00:15,  4.64it/s][A
 68%|██████▊   | 150/221 [00:44<00:20,  3.55it/s][A
 68%|██████▊   | 151/221 [00:44<00:18,  3.87it/s][A
 69%|██████▉   | 152/221 [00:45<00:31,  2.19it/s][A
 69%|██████▉   | 153/221 [00:46<00:31,  2.16it/s][A
 70%|██████▉   | 154/221 [00:46<00:30,  2.21it/s][A
 70%|███████   | 155/221 [00:46<00:26,  2.49it/s][A
 71%|███████   | 156/221 [00:46<00:22,  2.94it/s][A
 71%|███████   | 157/221 [00:47<00:21,  2.97it/s][A
 71%|███████▏  | 158/221 [00:47<00:20,  3.09it/s][A
 72%|███████▏  | 159/221 [00:47<00:19,  3.11it/s][A
 72%|███████▏  | 160/221 [00:48<00:19,  3.19it/s][A
 73%|███████▎  | 161/221 [00:48<00:16,  3.67it/s][A
 74%|███████▍  | 163/221 [00:48<00:12,  4.79it/s][A
 74%|███████▍  | 164/221 [00:48<00:13,  4.09it/s][A
 75%|███████▌  | 166/221 [00:49<00:14,  3.81it/s][A
 76%|███████▌  | 167/221 [00:49<00:12,  4.36it/s][A
 76%|███████▌  | 168/221 [00:49<00:12,  4.38it/s][A
 76%|███████▋  | 169/221 [00:50<00:11,  4.58it/s][A
 77%|███████▋  | 170/221 [00:50<00:15,  3.39it/s][A
 77%|███████▋  | 171/221 [00:50<00:15,  3.18it/s][A
 78%|███████▊  | 173/221 [00:51<00:10,  4.61it/s][A
 79%|███████▊  | 174/221 [00:51<00:13,  3.38it/s][A
 79%|███████▉  | 175/221 [00:52<00:13,  3.29it/s][A
 80%|███████▉  | 176/221 [00:52<00:13,  3.32it/s][A
 80%|████████  | 177/221 [00:52<00:11,  3.68it/s][A
 81%|████████  | 178/221 [00:52<00:10,  4.23it/s][A
 81%|████████  | 179/221 [00:53<00:11,  3.61it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.97it/s][A
 82%|████████▏ | 181/221 [00:53<00:08,  4.67it/s][A
 82%|████████▏ | 182/221 [00:53<00:07,  4.93it/s][A
 83%|████████▎ | 183/221 [00:53<00:07,  5.40it/s][A
 83%|████████▎ | 184/221 [00:54<00:10,  3.49it/s][A
 84%|████████▎ | 185/221 [00:54<00:09,  3.77it/s][A
 84%|████████▍ | 186/221 [00:54<00:10,  3.47it/s][A
 85%|████████▍ | 187/221 [00:54<00:07,  4.26it/s][A
 85%|████████▌ | 188/221 [00:55<00:10,  3.03it/s][A
 86%|████████▌ | 189/221 [00:55<00:12,  2.49it/s][A
 86%|████████▌ | 190/221 [00:56<00:14,  2.17it/s][A
 86%|████████▋ | 191/221 [00:56<00:11,  2.59it/s][A
 87%|████████▋ | 192/221 [00:57<00:10,  2.87it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.18it/s][A
 88%|████████▊ | 194/221 [00:57<00:11,  2.31it/s][A
 88%|████████▊ | 195/221 [00:58<00:12,  2.16it/s][A
 89%|████████▊ | 196/221 [00:59<00:12,  2.03it/s][A
 89%|████████▉ | 197/221 [00:59<00:10,  2.35it/s][A
 90%|█████████ | 199/221 [00:59<00:05,  3.72it/s][A
 90%|█████████ | 200/221 [01:00<00:07,  2.99it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.03it/s][A
 91%|█████████▏| 202/221 [01:00<00:05,  3.71it/s][A
 92%|█████████▏| 203/221 [01:00<00:04,  4.29it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.00it/s][A
 93%|█████████▎| 205/221 [01:01<00:04,  3.50it/s][A
 93%|█████████▎| 206/221 [01:01<00:04,  3.01it/s][A
 94%|█████████▎| 207/221 [01:01<00:03,  3.64it/s][A
 94%|█████████▍| 208/221 [01:02<00:04,  2.99it/s][A
 95%|█████████▍| 209/221 [01:02<00:04,  2.53it/s][A
 95%|█████████▌| 211/221 [01:03<00:03,  3.24it/s][A
 96%|█████████▌| 212/221 [01:03<00:02,  3.56it/s][A
 96%|█████████▋| 213/221 [01:04<00:03,  2.45it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  2.49it/s][A
 97%|█████████▋| 215/221 [01:05<00:02,  2.29it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  2.64it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  2.80it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.17it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.97it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  4.02it/s][A
100%|██████████| 221/221 [01:06<00:00,  3.17it/s][A100%|██████████| 221/221 [01:06<00:00,  3.31it/s]
09/18/2024 17:36:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 399--===========

09/18/2024 17:36:06 - INFO - __main__ -   {'area_r1': 39.8, 'area_recall': '39.8/66.2/75.5', 'area_ravg': 60.5}
09/18/2024 17:36:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 399--===========

09/18/2024 17:36:06 - INFO - __main__ -   {'forward_r1': 36.8, 'forward_recall': '36.8/64.3/75.0', 'forward_ravg': 58.7}
09/18/2024 17:36:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 399--===========

09/18/2024 17:36:06 - INFO - __main__ -   {'area_video_r1': 39.3, 'area_video_recall': '39.3/66.6/76.9', 'area_video_ravg': 60.9}
09/18/2024 17:36:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 17:36:06 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 17:36:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 399--===========

09/18/2024 17:36:06 - INFO - __main__ -   {'area_video_r1': 51.4, 'area_video_recall': '51.4/70.8/80.4', 'area_video_ravg': 67.5, 'area_video_back_r1': 49.1, 'area_video_back_recall': '49.1/68.7/77.5', 'area_video_back_ravg': 65.1}
09/18/2024 17:36:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 349=======

09/18/2024 17:36:06 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.2/81.2', 'area_video_ravg': 68.1, 'area_video_back_r1': 49.4, 'area_video_back_recall': '49.4/69.5/77.5', 'area_video_back_ravg': 65.5}
09/18/2024 17:36:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 399--===========

09/18/2024 17:36:06 - INFO - __main__ -   {'video_r1': 27.8, 'video_recall': '27.8/51.7/63.0', 'video_ravg': 47.5}
09/18/2024 17:36:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 17:36:06 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 17:36:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 399--===========

09/18/2024 17:36:06 - INFO - __main__ -   {'video_r1': 49.0, 'video_recall': '49.0/68.3/75.7', 'video_ravg': 64.3}
09/18/2024 17:36:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 17:36:06 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 17:36:27 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0013087502447888255, 'loss_ret%tv%ta--finetune_area/loss_area': 1.3452608585357666, 'loss_ret%tv%ta--finetune_area/total_loss': 1.3465696573257446}
not have audios ua_Kowav7hg.20
 14%|█▎        | 400/2910 [2:31:20<95:11:34, 136.53s/it] 14%|█▍        | 401/2910 [2:31:24<67:23:29, 96.70s/it] [h264 @ 0x55d920bcabc0] mmco: unref short failure
 14%|█▍        | 402/2910 [2:31:28<48:03:35, 68.99s/it] 14%|█▍        | 403/2910 [2:31:32<34:30:28, 49.55s/it] 14%|█▍        | 404/2910 [2:31:37<25:04:15, 36.02s/it] 14%|█▍        | 405/2910 [2:31:42<18:35:17, 26.71s/it] 14%|█▍        | 406/2910 [2:31:47<14:02:17, 20.18s/it] 14%|█▍        | 407/2910 [2:31:52<10:52:03, 15.63s/it][h264 @ 0x5565f1e32c40] mmco: unref short failure
[h264 @ 0x5598dc688d00] mmco: unref short failure
[h264 @ 0x5598dc688d00] mmco: unref short failure
 14%|█▍        | 408/2910 [2:31:57<8:39:10, 12.45s/it]  14%|█▍        | 409/2910 [2:32:02<7:07:32, 10.26s/it] 14%|█▍        | 410/2910 [2:32:07<6:05:39,  8.78s/it][h264 @ 0x5565ef9db980] mmco: unref short failure
[h264 @ 0x5565ef9db980] mmco: unref short failure
 14%|█▍        | 411/2910 [2:32:12<5:22:27,  7.74s/it][h264 @ 0x55d90fd15500] mmco: unref short failure
[h264 @ 0x55d90fd15500] mmco: unref short failure
 14%|█▍        | 412/2910 [2:32:18<4:51:33,  7.00s/it][h264 @ 0x55d908dba900] mmco: unref short failure
[h264 @ 0x55d908dba900] mmco: unref short failure
 14%|█▍        | 413/2910 [2:32:23<4:27:33,  6.43s/it]09/18/2024 17:37:32 - INFO - __main__ -   current idx ayNV3g3-K8g.35 from finetune_area returns wrong image/video, use 17312 instead.
[h264 @ 0x5565ea52bd00] mmco: unref short failure
[h264 @ 0x5565ea52bd00] mmco: unref short failure
 14%|█▍        | 414/2910 [2:32:28<4:11:04,  6.04s/it][h264 @ 0x5598dc688d00] mmco: unref short failure
 14%|█▍        | 415/2910 [2:32:33<3:59:03,  5.75s/it][h264 @ 0x5565fcae3540] mmco: unref short failure
[h264 @ 0x55cb7a597800] mmco: unref short failure
[h264 @ 0x55cb7a597800] mmco: unref short failure
[h264 @ 0x55cb7a597800] mmco: unref short failure
[h264 @ 0x55cb7a597800] mmco: unref short failure
[h264 @ 0x5598ecebd4c0] mmco: unref short failure
[h264 @ 0x55660048c280] mmco: unref short failure
[h264 @ 0x556601e76e80] mmco: unref short failure
[h264 @ 0x556601e76e80] mmco: unref short failure
[h264 @ 0x556601e76e80] mmco: unref short failure
[h264 @ 0x556601e76e80] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x55cb7b540e40] mmco: unref short failure
[h264 @ 0x5598e03745c0] mmco: unref short failure
[h264 @ 0x5565e6fdf280] mmco: unref short failure
[h264 @ 0x55d91dff1800] mmco: unref short failure
[h264 @ 0x55d91dff1800] mmco: unref short failure
[h264 @ 0x5598f942a880] mmco: unref short failure
[h264 @ 0x5598f942a880] mmco: unref short failure
[h264 @ 0x55cb65861f80] mmco: unref short failure
[h264 @ 0x55cb65861f80] mmco: unref short failure
[h264 @ 0x55cb65861f80] mmco: unref short failure
[h264 @ 0x55cb65861f80] mmco: unref short failure
[h264 @ 0x5598e03745c0] mmco: unref short failure
 14%|█▍        | 416/2910 [2:33:19<12:25:45, 17.94s/it][h264 @ 0x55d8fe9f5d40] mmco: unref short failure
[h264 @ 0x55d8fe9f5d40] mmco: unref short failure
[h264 @ 0x55cb7b540e40] mmco: unref short failure
[h264 @ 0x55cb7b540e40] mmco: unref short failure
 14%|█▍        | 417/2910 [2:33:44<13:44:38, 19.85s/it][h264 @ 0x55d91c2ed180] mmco: unref short failure
[h264 @ 0x55d8ff4c5f40] mmco: unref short failure
[h264 @ 0x55d8ff4c5f40] mmco: unref short failure
[h264 @ 0x55cb5ef08740] mmco: unref short failure
 14%|█▍        | 418/2910 [2:34:04<13:47:19, 19.92s/it][h264 @ 0x5565f493c140] mmco: unref short failure
[h264 @ 0x5565f493c140] mmco: unref short failure
 14%|█▍        | 419/2910 [2:34:09<10:40:26, 15.43s/it][h264 @ 0x5565eb8a54c0] mmco: unref short failure
[h264 @ 0x5565eb8a54c0] mmco: unref short failure
 14%|█▍        | 420/2910 [2:34:14<8:36:22, 12.44s/it]  14%|█▍        | 421/2910 [2:34:20<7:08:49, 10.34s/it] 15%|█▍        | 422/2910 [2:34:24<6:01:40,  8.72s/it][h264 @ 0x5598ecebd4c0] mmco: unref short failure
[h264 @ 0x5598ecebd4c0] mmco: unref short failure
[h264 @ 0x5598ecebd4c0] mmco: unref short failure
 15%|█▍        | 423/2910 [2:34:30<5:18:43,  7.69s/it][h264 @ 0x5565e5102140] mmco: unref short failure
[h264 @ 0x5565e5102140] mmco: unref short failure
[h264 @ 0x5598e087c0c0] mmco: unref short failure
[h264 @ 0x55cb7a91bec0] mmco: unref short failure
[h264 @ 0x55cb7a91bec0] mmco: unref short failure
[h264 @ 0x55cb65ccf900] mmco: unref short failure
09/18/2024 17:40:08 - INFO - __main__ -   current idx 5kvk51WxHH4.68 from finetune_area returns wrong image/video, use 114661 instead.
[h264 @ 0x5598dc713dc0] mmco: unref short failure
[h264 @ 0x5598dc713dc0] mmco: unref short failure
[h264 @ 0x55d9134f9780] mmco: unref short failure
[h264 @ 0x5598fb40b040] mmco: unref short failure
[h264 @ 0x5598fb40b040] mmco: unref short failure
[h264 @ 0x5598fb40b040] mmco: unref short failure
[h264 @ 0x5598fb40b040] mmco: unref short failure
[h264 @ 0x55d90ec06e80] mmco: unref short failure
[h264 @ 0x5565fa83d140] mmco: unref short failure
[h264 @ 0x5598f0626a00] mmco: unref short failure
[h264 @ 0x5598f0626a00] mmco: unref short failure
[h264 @ 0x5598f0626a00] mmco: unref short failure
[h264 @ 0x5598f0626a00] mmco: unref short failure
[h264 @ 0x55660c802940] mmco: unref short failure
[h264 @ 0x55660c802940] mmco: unref short failure
[h264 @ 0x5598f942fdc0] mmco: unref short failure
[h264 @ 0x5598f942fdc0] mmco: unref short failure
[h264 @ 0x55cb6e8f27c0] mmco: unref short failure
[h264 @ 0x55cb6e8f27c0] mmco: unref short failure
[h264 @ 0x55cb695b1d00] mmco: unref short failure
[h264 @ 0x55cb695b1d00] mmco: unref short failure
[h264 @ 0x55cb695b1d00] mmco: unref short failure
[h264 @ 0x55cb695b1d00] mmco: unref short failure
[h264 @ 0x5565fcdb4540] mmco: unref short failure
[h264 @ 0x5565fcdb4540] mmco: unref short failure
[h264 @ 0x5598fef77200] mmco: unref short failure
[h264 @ 0x5598fef77200] mmco: unref short failure
[h264 @ 0x5598fef77200] mmco: unref short failure
[h264 @ 0x5598fef77200] mmco: unref short failure
09/18/2024 17:40:52 - INFO - __main__ -   current idx JtVvdxUGRsg.70 from finetune_area returns wrong image/video, use 145939 instead.
[h264 @ 0x5598dc713dc0] mmco: unref short failure
 15%|█▍        | 424/2910 [2:35:48<19:57:58, 28.91s/it][h264 @ 0x55d91ce3d740] mmco: unref short failure
[h264 @ 0x5565ed36dd40] mmco: unref short failure
[h264 @ 0x5565ed36dd40] mmco: unref short failure
 15%|█▍        | 425/2910 [2:36:06<17:41:12, 25.62s/it][h264 @ 0x55cb630477c0] mmco: unref short failure
[h264 @ 0x55cb630477c0] mmco: unref short failure
[h264 @ 0x5565f820b080] mmco: unref short failure
[h264 @ 0x5565f820b080] mmco: unref short failure
[h264 @ 0x55d8fe976a80] mmco: unref short failure
[h264 @ 0x5598fc57ff00] mmco: unref short failure
[h264 @ 0x5565e5109b00] mmco: unref short failure
[h264 @ 0x5565e5109b00] mmco: unref short failure
[h264 @ 0x55d8fed6f440] mmco: unref short failure
 15%|█▍        | 426/2910 [2:36:39<19:15:45, 27.92s/it] 15%|█▍        | 427/2910 [2:36:46<14:50:42, 21.52s/it][h264 @ 0x55d8fe908dc0] mmco: unref short failure
[h264 @ 0x55d8fe908dc0] mmco: unref short failure
[h264 @ 0x55d8fe06cbc0] mmco: unref short failure
[h264 @ 0x55d8fe06cbc0] mmco: unref short failure
[h264 @ 0x55cb5ebaa200] mmco: unref short failure
[h264 @ 0x5565fb171b00] mmco: unref short failure
[h264 @ 0x5565fb171b00] mmco: unref short failure
[h264 @ 0x55cb660ff340] mmco: unref short failure
 15%|█▍        | 428/2910 [2:36:52<11:43:10, 17.00s/it][h264 @ 0x5565eb96c200] mmco: unref short failure
[h264 @ 0x5565eb96c200] mmco: unref short failure
 15%|█▍        | 429/2910 [2:36:58<9:23:30, 13.63s/it] [h264 @ 0x5598dc76ed00] mmco: unref short failure
[h264 @ 0x5598dc76ed00] mmco: unref short failure
 15%|█▍        | 430/2910 [2:37:03<7:32:03, 10.94s/it][h264 @ 0x55d8fe026040] mmco: unref short failure
[h264 @ 0x55d8fe026040] mmco: unref short failure
 15%|█▍        | 431/2910 [2:37:09<6:26:50,  9.36s/it][h264 @ 0x55cb6b401d80] mmco: unref short failure
[h264 @ 0x55cb6b401d80] mmco: unref short failure
[h264 @ 0x5598ecebd4c0] mmco: unref short failure
[h264 @ 0x5598dc838a80] mmco: unref short failure
[h264 @ 0x5598dc838a80] mmco: unref short failure
[h264 @ 0x5598dc789780] mmco: unref short failure
[h264 @ 0x55d919a1b7c0] mmco: unref short failure
[h264 @ 0x55d919a1b7c0] mmco: unref short failure
[h264 @ 0x5598ecebd4c0] mmco: unref short failure
[h264 @ 0x5598ecebd4c0] mmco: unref short failure
[h264 @ 0x55cb5ea396c0] mmco: unref short failure
[h264 @ 0x5598dc730100] mmco: unref short failure
[h264 @ 0x55cb5eef6780] mmco: unref short failure
[h264 @ 0x55cb5eef6780] mmco: unref short failure
[h264 @ 0x55cb5ec6fcc0] mmco: unref short failure
[h264 @ 0x55cb5ec6fcc0] mmco: unref short failure
[h264 @ 0x55d91e3549c0] mmco: unref short failure
[h264 @ 0x55d91e3549c0] mmco: unref short failure
[h264 @ 0x5598f4a7f900] mmco: unref short failure
[h264 @ 0x5598f4a7f900] mmco: unref short failure
[h264 @ 0x5565f430cc80] mmco: unref short failure
[h264 @ 0x5565f430cc80] mmco: unref short failure
[h264 @ 0x5598eb159480] mmco: unref short failure
[h264 @ 0x55cb5eaa6880] mmco: unref short failure
 15%|█▍        | 432/2910 [2:38:16<18:22:53, 26.70s/it][h264 @ 0x55cb5eef6780] mmco: unref short failure
[h264 @ 0x55cb5eef6780] mmco: unref short failure
[h264 @ 0x55cb5eaaee40] mmco: unref short failure
[h264 @ 0x55cb5eaaee40] mmco: unref short failure
 15%|█▍        | 433/2910 [2:38:35<16:53:56, 24.56s/it][h264 @ 0x55d8feb18680] mmco: unref short failure
[h264 @ 0x55cb5ec53240] mmco: unref short failure
[h264 @ 0x55cb5ec53240] mmco: unref short failure
[h264 @ 0x55d8feb18680] mmco: unref short failure
[h264 @ 0x5598ee7cec00] mmco: unref short failure
 15%|█▍        | 434/2910 [2:39:02<17:21:15, 25.23s/it]09/18/2024 17:44:13 - INFO - __main__ -   current idx hq2NNhzo-Lg.4 from finetune_area returns wrong image/video, use 114076 instead.
[h264 @ 0x5565f0f83c40] mmco: unref short failure
 15%|█▍        | 435/2910 [2:39:16<15:02:50, 21.89s/it][h264 @ 0x55d8fed60000] mmco: unref short failure
[h264 @ 0x55d8fed60000] mmco: unref short failure
[h264 @ 0x55cb5eb378c0] mmco: unref short failure
 15%|█▍        | 436/2910 [2:39:21<11:32:27, 16.79s/it] 15%|█▌        | 437/2910 [2:39:27<9:18:30, 13.55s/it] [h264 @ 0x5565e67b4c40] mmco: unref short failure
 15%|█▌        | 438/2910 [2:39:33<7:40:31, 11.18s/it] 15%|█▌        | 439/2910 [2:39:38<6:28:37,  9.44s/it][h264 @ 0x5598dc7af0c0] mmco: unref short failure
[h264 @ 0x5598dc7af0c0] mmco: unref short failure
[h264 @ 0x5598f5e55a40] mmco: unref short failure
[h264 @ 0x5598f5e55a40] mmco: unref short failure
[h264 @ 0x5598f5e55a40] mmco: unref short failure
09/18/2024 17:45:00 - INFO - __main__ -   current idx 4ZmeUj7cFu4.36 from finetune_area returns wrong image/video, use 90779 instead.
[h264 @ 0x55cb812d3140] mmco: unref short failure
[h264 @ 0x55cb812d3140] mmco: unref short failure
[h264 @ 0x55cb812d3140] mmco: unref short failure
[h264 @ 0x5565e5774900] mmco: unref short failure
[h264 @ 0x55cb5ea0af80] mmco: unref short failure
[h264 @ 0x55cb5ea0af80] mmco: unref short failure
[h264 @ 0x5598dbdf82c0] mmco: unref short failure
[h264 @ 0x5598dbdf82c0] mmco: unref short failure
[h264 @ 0x55cb5e973740] mmco: unref short failure
[h264 @ 0x55cb5e973740] mmco: unref short failure
[h264 @ 0x5598f0b69100] mmco: unref short failure
[h264 @ 0x5598f0b69100] mmco: unref short failure
[h264 @ 0x5598fc57ff00] mmco: unref short failure
[h264 @ 0x5598fc57ff00] mmco: unref short failure
[h264 @ 0x5598fc57ff00] mmco: unref short failure
 15%|█▌        | 440/2910 [2:40:41<17:32:15, 25.56s/it][h264 @ 0x55d913682ec0] mmco: unref short failure
[h264 @ 0x55d913682ec0] mmco: unref short failure
[h264 @ 0x55cb5eb3f400] mmco: unref short failure
[h264 @ 0x55cb5eb3f400] mmco: unref short failure
 15%|█▌        | 441/2910 [2:41:02<16:28:20, 24.02s/it][h264 @ 0x55d8fe9af7c0] mmco: unref short failure
[h264 @ 0x55d8fe9af7c0] mmco: unref short failure
09/18/2024 17:46:17 - INFO - __main__ -   current idx g6Wl0SZpFMk.13 from finetune_area returns wrong image/video, use 141986 instead.
[h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x55d8fe935340] mmco: unref short failure
[h264 @ 0x55d8fe935340] mmco: unref short failure
[h264 @ 0x55d90b2acd80] mmco: unref short failure
[h264 @ 0x55d90b2acd80] mmco: unref short failure
[h264 @ 0x5598dc713dc0] mmco: unref short failure
 15%|█▌        | 442/2910 [2:41:37<18:49:31, 27.46s/it] 15%|█▌        | 443/2910 [2:41:42<14:12:55, 20.74s/it] 15%|█▌        | 444/2910 [2:41:48<11:08:36, 16.27s/it] 15%|█▌        | 445/2910 [2:41:53<8:47:09, 12.83s/it]  15%|█▌        | 446/2910 [2:41:58<7:11:13, 10.50s/it] 15%|█▌        | 447/2910 [2:42:03<6:05:22,  8.90s/it][h264 @ 0x55d8fec88c40] mmco: unref short failure
09/18/2024 17:47:19 - INFO - __main__ -   current idx Pj4BTw_2jIM.11 from finetune_area returns wrong image/video, use 88573 instead.
[h264 @ 0x5598fc57ff00] mmco: unref short failure
[h264 @ 0x55d8fea1b840] mmco: unref short failure
[h264 @ 0x55d8fea1b840] mmco: unref short failure
[h264 @ 0x5565e9a32000] mmco: unref short failure
[h264 @ 0x5565e9a32000] mmco: unref short failure
[h264 @ 0x5598f57b12c0] mmco: unref short failure
[h264 @ 0x5598f57b12c0] mmco: unref short failure
[h264 @ 0x5598f57b12c0] mmco: unref short failure
[h264 @ 0x5598f57b12c0] mmco: unref short failure
[h264 @ 0x55cb5ea2d0c0] mmco: unref short failure
[h264 @ 0x55cb5ea2d0c0] mmco: unref short failure
[h264 @ 0x5565fcde9ac0] mmco: unref short failure
[h264 @ 0x55d8fe876a00] mmco: unref short failure
[h264 @ 0x55d8fe876a00] mmco: unref short failure
[h264 @ 0x5565fcdb4540] mmco: unref short failure
[h264 @ 0x5565fcdb4540] mmco: unref short failure
[h264 @ 0x5565e808d980] mmco: unref short failure
[h264 @ 0x5565e808d980] mmco: unref short failure
[h264 @ 0x5565e808d980] mmco: unref short failure
[h264 @ 0x5565e808d980] mmco: unref short failure
[h264 @ 0x55cb75ddba00] mmco: unref short failure
[h264 @ 0x55cb75ddba00] mmco: unref short failure
 15%|█▌        | 448/2910 [2:43:07<17:20:27, 25.36s/it][h264 @ 0x55d913b58f40] mmco: unref short failure
[h264 @ 0x55d913b58f40] mmco: unref short failure
[h264 @ 0x5598dc71d3c0] mmco: unref short failure
[h264 @ 0x55cb5e04df00] mmco: unref short failure
[h264 @ 0x55cb5e04df00] mmco: unref short failure
[h264 @ 0x55d8ff4c5f40] mmco: unref short failure
[h264 @ 0x5598f458d300] mmco: unref short failure
[h264 @ 0x5598f458d300] mmco: unref short failure
[h264 @ 0x5598eb159480] mmco: unref short failure
[h264 @ 0x55d919a1b7c0] mmco: unref short failure
[h264 @ 0x55d919a1b7c0] mmco: unref short failure
[h264 @ 0x55d919a1b7c0] mmco: unref short failure
[h264 @ 0x55d919a1b7c0] mmco: unref short failure
[h264 @ 0x55d8fed492c0] mmco: unref short failure
[h264 @ 0x55d8fed492c0] mmco: unref short failure
 15%|█▌        | 449/2910 [2:43:38<18:36:06, 27.21s/it]09/18/2024 17:48:48 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 17:48:48 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x55d913b58f40] mmco: unref short failure
[h264 @ 0x55d913b58f40] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598f49cd900] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5598e07a2a80] mmco: unref short failure
[h264 @ 0x5598e07a2a80] mmco: unref short failure
[h264 @ 0x55cb5eec2a80] mmco: unref short failure
[h264 @ 0x55cb5eec2a80] mmco: unref short failure
[h264 @ 0x55cb798f7b00] mmco: unref short failure
[h264 @ 0x55cb798f7b00] mmco: unref short failure
[h264 @ 0x55d914d52dc0] mmco: unref short failure
[h264 @ 0x55d914d52dc0] mmco: unref short failure
09/18/2024 17:50:33 - INFO - __main__ -   current idx WTKFIwRqMwk.18 from finetune_area returns wrong image/video, use 117529 instead.
[h264 @ 0x55cb5ea66a00] mmco: unref short failure
[h264 @ 0x55cb5ea66a00] mmco: unref short failure
[h264 @ 0x55cb5ea66a00] mmco: unref short failure
[h264 @ 0x55cb5ea66a00] mmco: unref short failure
[h264 @ 0x55cb5ea66a00] mmco: unref short failure
[h264 @ 0x5598fbaa2740] mmco: unref short failure
[h264 @ 0x5598dc788c40] mmco: unref short failure
[h264 @ 0x5598dc788c40] mmco: unref short failure
[h264 @ 0x5598dc788c40] mmco: unref short failure
[h264 @ 0x5598dc788c40] mmco: unref short failure
[h264 @ 0x55d90db31e80] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<03:43,  1.02s/it][A
  1%|          | 2/221 [00:01<03:17,  1.11it/s][A
  1%|▏         | 3/221 [00:02<02:11,  1.66it/s][A
  2%|▏         | 4/221 [00:02<01:33,  2.32it/s][A
  2%|▏         | 5/221 [00:02<01:16,  2.81it/s][A
  3%|▎         | 6/221 [00:02<01:01,  3.47it/s][A
  3%|▎         | 7/221 [00:02<00:56,  3.78it/s][A[h264 @ 0x55cb69586c00] mmco: unref short failure
[h264 @ 0x55cb69586c00] mmco: unref short failure

  4%|▎         | 8/221 [00:03<01:14,  2.84it/s][A
  4%|▍         | 9/221 [00:03<01:18,  2.71it/s][A
  5%|▍         | 10/221 [00:04<01:50,  1.92it/s][A
  5%|▍         | 11/221 [00:05<01:40,  2.09it/s][A
  5%|▌         | 12/221 [00:05<01:57,  1.79it/s][A
  6%|▌         | 13/221 [00:06<01:42,  2.02it/s][A
  6%|▋         | 14/221 [00:07<02:20,  1.48it/s][A
  7%|▋         | 15/221 [00:07<01:50,  1.87it/s][A
  7%|▋         | 16/221 [00:07<01:42,  2.00it/s][A[h264 @ 0x5598f2d06780] mmco: unref short failure
[h264 @ 0x5598f2d06780] mmco: unref short failure
[h264 @ 0x5598f2d06780] mmco: unref short failure
[h264 @ 0x5598f2d06780] mmco: unref short failure

  8%|▊         | 17/221 [00:08<01:51,  1.83it/s][A[h264 @ 0x5598dc789780] mmco: unref short failure

  8%|▊         | 18/221 [00:08<01:35,  2.13it/s][A
  9%|▊         | 19/221 [00:08<01:18,  2.59it/s][A
  9%|▉         | 20/221 [00:09<01:07,  2.99it/s][A
 10%|▉         | 21/221 [00:09<00:58,  3.43it/s][A
 10%|▉         | 22/221 [00:09<00:51,  3.86it/s][A
 10%|█         | 23/221 [00:09<00:43,  4.51it/s][A
 11%|█         | 24/221 [00:09<00:41,  4.70it/s][A
 11%|█▏        | 25/221 [00:10<00:44,  4.38it/s][A
 12%|█▏        | 26/221 [00:10<01:08,  2.85it/s][A
 12%|█▏        | 27/221 [00:10<00:54,  3.53it/s][A
 13%|█▎        | 28/221 [00:11<01:17,  2.49it/s][A
 13%|█▎        | 29/221 [00:12<01:44,  1.84it/s][A
 14%|█▎        | 30/221 [00:12<01:27,  2.18it/s][A09/18/2024 17:51:13 - INFO - __main__ -   current idx My9wyuAl8zM.76 from finetune_area returns wrong image/video, use 19048 instead.

 14%|█▍        | 31/221 [00:13<01:17,  2.45it/s][A
 14%|█▍        | 32/221 [00:13<01:05,  2.89it/s][A
 15%|█▍        | 33/221 [00:13<01:05,  2.89it/s][A
 15%|█▌        | 34/221 [00:13<01:01,  3.05it/s][A
 16%|█▌        | 35/221 [00:14<01:02,  2.98it/s][A
 16%|█▋        | 36/221 [00:14<01:02,  2.94it/s][A
 17%|█▋        | 37/221 [00:15<01:21,  2.26it/s][A
 17%|█▋        | 38/221 [00:15<01:25,  2.15it/s][h264 @ 0x5598f9413e00] mmco: unref short failure
[A
 18%|█▊        | 39/221 [00:16<01:19,  2.30it/s][A
 18%|█▊        | 40/221 [00:16<01:12,  2.49it/s][A
 19%|█▊        | 41/221 [00:16<00:58,  3.09it/s][A
 19%|█▉        | 42/221 [00:17<01:05,  2.73it/s][A
 19%|█▉        | 43/221 [00:17<01:02,  2.87it/s][A
 20%|█▉        | 44/221 [00:17<00:56,  3.13it/s][A
 20%|██        | 45/221 [00:19<01:55,  1.53it/s][A
 21%|██        | 46/221 [00:19<01:41,  1.73it/s][A
 21%|██▏       | 47/221 [00:20<01:41,  1.71it/s][A
 22%|██▏       | 48/221 [00:20<01:19,  2.16it/s][A
 22%|██▏       | 49/221 [00:21<01:35,  1.79it/s][A
 23%|██▎       | 50/221 [00:21<01:32,  1.85it/s][A
 23%|██▎       | 51/221 [00:21<01:11,  2.38it/s][A
 24%|██▎       | 52/221 [00:21<00:59,  2.82it/s][A
 24%|██▍       | 53/221 [00:22<00:48,  3.44it/s][A[h264 @ 0x55d8fefeb2c0] mmco: unref short failure
[h264 @ 0x55d8fefeb2c0] mmco: unref short failure

 24%|██▍       | 54/221 [00:23<01:56,  1.43it/s][A
 25%|██▍       | 55/221 [00:23<01:37,  1.70it/s][A
 25%|██▌       | 56/221 [00:24<01:27,  1.88it/s][A
 26%|██▌       | 57/221 [00:24<01:15,  2.16it/s][A
 26%|██▌       | 58/221 [00:24<01:04,  2.51it/s][A[h264 @ 0x55cb5ee61280] mmco: unref short failure

 27%|██▋       | 59/221 [00:25<00:58,  2.75it/s][A
 27%|██▋       | 60/221 [00:26<01:18,  2.04it/s][A
 28%|██▊       | 61/221 [00:26<01:08,  2.32it/s][A
 28%|██▊       | 62/221 [00:26<01:04,  2.47it/s][A
 29%|██▊       | 63/221 [00:26<00:58,  2.70it/s][A
 29%|██▉       | 64/221 [00:27<00:49,  3.20it/s][A
 29%|██▉       | 65/221 [00:27<00:47,  3.27it/s][A
 30%|██▉       | 66/221 [00:27<00:47,  3.24it/s][A[h264 @ 0x55d91151b3c0] mmco: unref short failure

 30%|███       | 67/221 [00:28<01:05,  2.34it/s][A[h264 @ 0x55d8fe9cbd80] mmco: unref short failure
[h264 @ 0x55d8fe9cbd80] mmco: unref short failure
[h264 @ 0x55d8fe9cbd80] mmco: unref short failure
[h264 @ 0x55d8fe9cbd80] mmco: unref short failure

 31%|███       | 68/221 [00:28<00:54,  2.78it/s][A
 31%|███       | 69/221 [00:29<01:14,  2.05it/s][A
 32%|███▏      | 70/221 [00:29<00:58,  2.59it/s][A
 32%|███▏      | 71/221 [00:30<01:14,  2.01it/s][A
 33%|███▎      | 72/221 [00:30<01:02,  2.37it/s][A
 33%|███▎      | 73/221 [00:30<01:01,  2.39it/s][A
 33%|███▎      | 74/221 [00:31<00:50,  2.91it/s][A
 34%|███▍      | 75/221 [00:31<00:54,  2.66it/s][A
 34%|███▍      | 76/221 [00:31<00:44,  3.25it/s][A
 35%|███▍      | 77/221 [00:31<00:41,  3.48it/s][A
 35%|███▌      | 78/221 [00:32<00:44,  3.19it/s][A
 36%|███▌      | 79/221 [00:32<00:55,  2.55it/s][A
 36%|███▌      | 80/221 [00:33<00:52,  2.71it/s][A
 37%|███▋      | 81/221 [00:33<00:48,  2.91it/s][A
 37%|███▋      | 82/221 [00:34<01:16,  1.83it/s][A
 38%|███▊      | 83/221 [00:35<01:12,  1.90it/s][A
 38%|███▊      | 84/221 [00:35<00:59,  2.29it/s][A
 38%|███▊      | 85/221 [00:35<00:47,  2.88it/s][A
 39%|███▉      | 86/221 [00:35<00:45,  2.97it/s][A
 39%|███▉      | 87/221 [00:36<00:55,  2.43it/s][A
 40%|███▉      | 88/221 [00:36<01:03,  2.09it/s][A
 40%|████      | 89/221 [00:37<01:06,  1.98it/s][A
 41%|████      | 90/221 [00:37<00:55,  2.34it/s][A
 41%|████      | 91/221 [00:37<00:43,  2.97it/s][A
 42%|████▏     | 92/221 [00:38<00:39,  3.25it/s][A[h264 @ 0x5598fbaa2740] mmco: unref short failure
[h264 @ 0x5598fbaa2740] mmco: unref short failure

 42%|████▏     | 93/221 [00:38<00:41,  3.07it/s][A
 43%|████▎     | 94/221 [00:38<00:38,  3.28it/s][A
 43%|████▎     | 95/221 [00:39<00:41,  3.02it/s][A
 43%|████▎     | 96/221 [00:39<00:37,  3.34it/s][A
 44%|████▍     | 97/221 [00:39<00:30,  4.01it/s][A
 44%|████▍     | 98/221 [00:39<00:33,  3.72it/s][A
 45%|████▍     | 99/221 [00:39<00:28,  4.28it/s][A
 45%|████▌     | 100/221 [00:40<00:25,  4.65it/s][A
 46%|████▌     | 101/221 [00:40<00:22,  5.37it/s][A
 46%|████▌     | 102/221 [00:40<00:23,  5.06it/s][A
 47%|████▋     | 103/221 [00:40<00:22,  5.34it/s][A
 47%|████▋     | 104/221 [00:40<00:20,  5.76it/s][A
 48%|████▊     | 105/221 [00:41<00:22,  5.15it/s][A
 48%|████▊     | 106/221 [00:41<00:40,  2.85it/s][A
 48%|████▊     | 107/221 [00:41<00:33,  3.40it/s][A
 49%|████▉     | 108/221 [00:42<00:28,  3.91it/s][A
 49%|████▉     | 109/221 [00:42<00:30,  3.62it/s][A[h264 @ 0x55d920869040] mmco: unref short failure
[h264 @ 0x55d920869040] mmco: unref short failure

 50%|████▉     | 110/221 [00:42<00:37,  2.95it/s][A
 50%|█████     | 111/221 [00:43<00:44,  2.49it/s][A
 51%|█████     | 112/221 [00:43<00:36,  2.96it/s][A
 51%|█████     | 113/221 [00:43<00:36,  2.92it/s][A
 52%|█████▏    | 114/221 [00:44<00:29,  3.66it/s][A
 52%|█████▏    | 115/221 [00:44<00:24,  4.37it/s][A
 52%|█████▏    | 116/221 [00:47<01:52,  1.07s/it][A[h264 @ 0x55d8fe8e9300] mmco: unref short failure

 53%|█████▎    | 117/221 [00:47<01:28,  1.17it/s][A
 53%|█████▎    | 118/221 [00:47<01:13,  1.41it/s][A
 54%|█████▍    | 119/221 [00:48<00:58,  1.74it/s][A
 54%|█████▍    | 120/221 [00:48<00:52,  1.93it/s][A
 55%|█████▌    | 122/221 [00:49<00:36,  2.68it/s][A
 56%|█████▌    | 123/221 [00:49<00:34,  2.88it/s][A
 56%|█████▌    | 124/221 [00:49<00:33,  2.93it/s][A
 57%|█████▋    | 125/221 [00:50<00:34,  2.76it/s][A
 57%|█████▋    | 126/221 [00:50<00:32,  2.92it/s][A
 57%|█████▋    | 127/221 [00:51<00:43,  2.18it/s][A
 58%|█████▊    | 128/221 [00:51<00:40,  2.29it/s][A
 58%|█████▊    | 129/221 [00:51<00:34,  2.65it/s][A
 59%|█████▉    | 130/221 [00:51<00:31,  2.89it/s][A
 59%|█████▉    | 131/221 [00:52<00:25,  3.49it/s][A
 60%|█████▉    | 132/221 [00:52<00:21,  4.16it/s][A
 60%|██████    | 133/221 [00:52<00:25,  3.44it/s][A
 61%|██████    | 134/221 [00:52<00:22,  3.91it/s][A
 61%|██████    | 135/221 [00:53<00:21,  3.94it/s][A
 62%|██████▏   | 136/221 [00:53<00:25,  3.37it/s][A
 62%|██████▏   | 137/221 [00:53<00:23,  3.63it/s][A
 62%|██████▏   | 138/221 [00:54<00:26,  3.15it/s][A
 63%|██████▎   | 139/221 [00:54<00:28,  2.89it/s][A
 63%|██████▎   | 140/221 [00:54<00:27,  2.98it/s][A
 64%|██████▍   | 141/221 [00:55<00:23,  3.35it/s][A
 64%|██████▍   | 142/221 [00:55<00:24,  3.17it/s][A
 65%|██████▍   | 143/221 [00:55<00:26,  2.98it/s][A
 65%|██████▌   | 144/221 [00:56<00:25,  3.05it/s][A
 66%|██████▌   | 145/221 [00:56<00:20,  3.78it/s][A
 66%|██████▌   | 146/221 [00:56<00:17,  4.33it/s][A
 67%|██████▋   | 147/221 [00:56<00:17,  4.24it/s][A
 67%|██████▋   | 148/221 [00:56<00:16,  4.38it/s][A
 67%|██████▋   | 149/221 [00:56<00:14,  4.94it/s][A
 68%|██████▊   | 150/221 [00:57<00:14,  4.85it/s][A[h264 @ 0x5565e6b84380] mmco: unref short failure

 68%|██████▊   | 151/221 [00:57<00:25,  2.73it/s][A09/18/2024 17:51:59 - INFO - __main__ -   current idx 0dGVRi4angY.7 from finetune_area returns wrong image/video, use 3133 instead.

 69%|██████▉   | 152/221 [00:59<00:53,  1.30it/s][A
 69%|██████▉   | 153/221 [00:59<00:44,  1.53it/s][A
 70%|██████▉   | 154/221 [01:00<00:37,  1.81it/s][A
 70%|███████   | 155/221 [01:00<00:28,  2.29it/s][A
 71%|███████   | 156/221 [01:00<00:25,  2.58it/s][A
 71%|███████   | 157/221 [01:01<00:30,  2.09it/s][A
 71%|███████▏  | 158/221 [01:01<00:25,  2.43it/s][A
 72%|███████▏  | 159/221 [01:01<00:22,  2.76it/s][A
 72%|███████▏  | 160/221 [01:02<00:22,  2.72it/s][A
 73%|███████▎  | 161/221 [01:02<00:17,  3.47it/s][A
 73%|███████▎  | 162/221 [01:03<00:24,  2.38it/s][A
 74%|███████▍  | 163/221 [01:03<00:21,  2.65it/s][A
 74%|███████▍  | 164/221 [01:03<00:24,  2.33it/s][A
 75%|███████▍  | 165/221 [01:04<00:19,  2.89it/s][A
 75%|███████▌  | 166/221 [01:04<00:23,  2.34it/s][A
 76%|███████▌  | 167/221 [01:04<00:18,  2.95it/s][A
 76%|███████▌  | 168/221 [01:05<00:21,  2.47it/s][A
 76%|███████▋  | 169/221 [01:05<00:18,  2.81it/s][A
 77%|███████▋  | 170/221 [01:06<00:19,  2.64it/s][A
 77%|███████▋  | 171/221 [01:06<00:19,  2.62it/s][A
 78%|███████▊  | 172/221 [01:06<00:15,  3.07it/s][A
 78%|███████▊  | 173/221 [01:06<00:14,  3.36it/s][A
 79%|███████▊  | 174/221 [01:07<00:13,  3.56it/s][A
 79%|███████▉  | 175/221 [01:07<00:16,  2.83it/s][A[h264 @ 0x55d8fe0282c0] mmco: unref short failure
[h264 @ 0x55d8fe0282c0] mmco: unref short failure

 80%|███████▉  | 176/221 [01:07<00:15,  2.97it/s][A
 80%|████████  | 177/221 [01:08<00:12,  3.51it/s][A
 81%|████████  | 178/221 [01:08<00:10,  4.17it/s][A
 81%|████████  | 179/221 [01:08<00:13,  3.12it/s][A
 81%|████████▏ | 180/221 [01:08<00:10,  3.74it/s][A
 82%|████████▏ | 181/221 [01:09<00:11,  3.52it/s][A
 82%|████████▏ | 182/221 [01:09<00:10,  3.77it/s][A
 83%|████████▎ | 183/221 [01:09<00:08,  4.29it/s][A
 83%|████████▎ | 184/221 [01:10<00:10,  3.62it/s][A
 84%|████████▎ | 185/221 [01:10<00:13,  2.75it/s][A
 84%|████████▍ | 186/221 [01:11<00:14,  2.46it/s][A
 85%|████████▍ | 187/221 [01:11<00:11,  2.96it/s][A
 85%|████████▌ | 188/221 [01:11<00:10,  3.04it/s][A
 86%|████████▌ | 189/221 [01:12<00:11,  2.79it/s][A
 86%|████████▌ | 190/221 [01:12<00:12,  2.52it/s][A
 86%|████████▋ | 191/221 [01:12<00:09,  3.15it/s][A
 87%|████████▋ | 192/221 [01:12<00:08,  3.25it/s][A
 88%|████████▊ | 194/221 [01:13<00:10,  2.61it/s][A
 88%|████████▊ | 195/221 [01:14<00:08,  3.05it/s][A
 89%|████████▊ | 196/221 [01:14<00:10,  2.46it/s][A
 89%|████████▉ | 197/221 [01:14<00:07,  3.04it/s][A
 90%|████████▉ | 198/221 [01:15<00:07,  3.04it/s][A
 90%|█████████ | 199/221 [01:15<00:06,  3.52it/s][A
 90%|█████████ | 200/221 [01:15<00:06,  3.47it/s][A
 91%|█████████ | 201/221 [01:15<00:05,  3.35it/s][A
 91%|█████████▏| 202/221 [01:16<00:05,  3.33it/s][A
 92%|█████████▏| 203/221 [01:16<00:05,  3.35it/s][A
 92%|█████████▏| 204/221 [01:16<00:05,  2.88it/s][A
 93%|█████████▎| 205/221 [01:17<00:04,  3.54it/s][A
 93%|█████████▎| 206/221 [01:17<00:06,  2.28it/s][A
 94%|█████████▎| 207/221 [01:18<00:05,  2.67it/s][A
 94%|█████████▍| 208/221 [01:18<00:04,  3.20it/s][A[h264 @ 0x5565f5d26fc0] mmco: unref short failure

 95%|█████████▍| 209/221 [01:18<00:04,  2.96it/s][A
 95%|█████████▌| 210/221 [01:18<00:03,  3.62it/s][A
 95%|█████████▌| 211/221 [01:19<00:03,  3.05it/s][A
 96%|█████████▌| 212/221 [01:19<00:02,  3.34it/s][A
 96%|█████████▋| 213/221 [01:19<00:02,  3.48it/s][A
 97%|█████████▋| 214/221 [01:20<00:02,  3.39it/s][A
 97%|█████████▋| 215/221 [01:20<00:01,  3.51it/s][A09/18/2024 17:52:21 - INFO - __main__ -   current idx YejxNzlOkeM.23 from finetune_area returns wrong image/video, use 104493 instead.

 98%|█████████▊| 216/221 [01:20<00:01,  3.38it/s][A[h264 @ 0x5598dcb15700] mmco: unref short failure

 98%|█████████▊| 217/221 [01:21<00:01,  2.93it/s][A
 99%|█████████▊| 218/221 [01:21<00:01,  2.82it/s][A
 99%|█████████▉| 219/221 [01:21<00:00,  3.14it/s][A
100%|█████████▉| 220/221 [01:24<00:01,  1.16s/it][A
100%|██████████| 221/221 [01:25<00:00,  1.13it/s][A100%|██████████| 221/221 [01:25<00:00,  2.60it/s]
[h264 @ 0x55cb74dfe140] mmco: unref short failure
[h264 @ 0x5565f131c3c0] mmco: unref short failure
[h264 @ 0x5565f131c3c0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.29it/s][A
  1%|          | 2/221 [00:00<01:06,  3.29it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.29it/s][A
  2%|▏         | 4/221 [00:01<01:06,  3.26it/s][A
  2%|▏         | 5/221 [00:01<01:05,  3.27it/s][A
  3%|▎         | 6/221 [00:01<01:05,  3.27it/s][A
  3%|▎         | 7/221 [00:02<01:05,  3.27it/s][A
  4%|▎         | 8/221 [00:02<01:06,  3.21it/s][A
  4%|▍         | 9/221 [00:02<01:05,  3.22it/s][A
  5%|▍         | 10/221 [00:03<01:05,  3.24it/s][A
  5%|▍         | 11/221 [00:03<01:05,  3.20it/s][A
  5%|▌         | 12/221 [00:03<01:05,  3.17it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.20it/s][A
  6%|▋         | 14/221 [00:04<01:04,  3.23it/s][A
  7%|▋         | 15/221 [00:04<01:05,  3.13it/s][A
  7%|▋         | 16/221 [00:05<01:06,  3.07it/s][A
  8%|▊         | 17/221 [00:05<01:05,  3.10it/s][A
  8%|▊         | 18/221 [00:05<01:05,  3.11it/s][A
  9%|▊         | 19/221 [00:05<01:06,  3.05it/s][A[h264 @ 0x55cb795a8780] mmco: unref short failure
[h264 @ 0x55cb795a8780] mmco: unref short failure

  9%|▉         | 20/221 [00:06<01:07,  2.96it/s][A
 10%|▉         | 21/221 [00:06<01:06,  3.02it/s][A
 10%|▉         | 22/221 [00:06<01:04,  3.06it/s][A
 10%|█         | 23/221 [00:07<01:03,  3.09it/s][A
 11%|█         | 24/221 [00:07<01:02,  3.15it/s][A
 11%|█▏        | 25/221 [00:07<01:01,  3.18it/s][A
 12%|█▏        | 26/221 [00:08<01:00,  3.21it/s][A
 12%|█▏        | 27/221 [00:08<01:00,  3.22it/s][A
 13%|█▎        | 28/221 [00:08<01:01,  3.14it/s][A
 13%|█▎        | 29/221 [00:09<01:00,  3.18it/s][A
 14%|█▎        | 30/221 [00:09<00:59,  3.21it/s][A
 14%|█▍        | 31/221 [00:09<00:58,  3.23it/s][A
 14%|█▍        | 32/221 [00:10<00:58,  3.24it/s][A
 15%|█▍        | 33/221 [00:10<00:57,  3.26it/s][A
 15%|█▌        | 34/221 [00:10<00:57,  3.25it/s][A
 16%|█▌        | 35/221 [00:10<00:57,  3.25it/s][A
 16%|█▋        | 36/221 [00:11<00:57,  3.20it/s][A
 17%|█▋        | 37/221 [00:11<00:57,  3.23it/s][A
 17%|█▋        | 38/221 [00:11<00:57,  3.21it/s][A
 18%|█▊        | 39/221 [00:12<00:56,  3.22it/s][A
 18%|█▊        | 40/221 [00:12<00:57,  3.14it/s][A
 19%|█▊        | 41/221 [00:12<00:56,  3.17it/s][A
 19%|█▉        | 42/221 [00:13<00:56,  3.17it/s][A
 19%|█▉        | 43/221 [00:13<00:55,  3.20it/s][A
 20%|█▉        | 44/221 [00:13<00:55,  3.18it/s][A
 20%|██        | 45/221 [00:14<00:55,  3.20it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.22it/s][A
 21%|██▏       | 47/221 [00:14<00:53,  3.23it/s][A
 22%|██▏       | 48/221 [00:15<00:53,  3.25it/s][A
 22%|██▏       | 49/221 [00:15<00:52,  3.26it/s][A[h264 @ 0x55d8fe876a00] mmco: unref short failure

 23%|██▎       | 50/221 [00:15<00:52,  3.26it/s][A
 23%|██▎       | 51/221 [00:15<00:52,  3.25it/s][A
 24%|██▎       | 52/221 [00:16<00:52,  3.23it/s][A
 24%|██▍       | 53/221 [00:16<00:51,  3.25it/s][A
 24%|██▍       | 54/221 [00:16<00:51,  3.22it/s][A
 25%|██▍       | 55/221 [00:17<00:51,  3.21it/s][A
 25%|██▌       | 56/221 [00:17<00:51,  3.23it/s][A
 26%|██▌       | 57/221 [00:17<00:50,  3.23it/s][A
 26%|██▌       | 58/221 [00:18<00:50,  3.25it/s][A
 27%|██▋       | 59/221 [00:18<00:49,  3.26it/s][A
 27%|██▋       | 60/221 [00:18<00:49,  3.27it/s][A
 28%|██▊       | 61/221 [00:19<00:48,  3.28it/s][A
 28%|██▊       | 62/221 [00:19<00:48,  3.28it/s][A
 29%|██▊       | 63/221 [00:19<00:48,  3.25it/s][A
 29%|██▉       | 64/221 [00:19<00:48,  3.22it/s][A
 29%|██▉       | 65/221 [00:20<00:48,  3.21it/s][A
 30%|██▉       | 66/221 [00:20<00:47,  3.23it/s][A
 30%|███       | 67/221 [00:20<00:47,  3.25it/s][A
 31%|███       | 68/221 [00:21<00:46,  3.26it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.27it/s][A
 32%|███▏      | 70/221 [00:21<00:46,  3.28it/s][A
 32%|███▏      | 71/221 [00:22<00:46,  3.24it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.24it/s][A
 33%|███▎      | 73/221 [00:22<00:45,  3.26it/s][A
 33%|███▎      | 74/221 [00:23<00:44,  3.27it/s][A
 34%|███▍      | 75/221 [00:23<00:44,  3.27it/s][A
 34%|███▍      | 76/221 [00:23<00:44,  3.24it/s][A
 35%|███▍      | 77/221 [00:23<00:44,  3.25it/s][A
 35%|███▌      | 78/221 [00:24<00:43,  3.26it/s][A
 36%|███▌      | 79/221 [00:24<00:43,  3.27it/s][A
 36%|███▌      | 80/221 [00:24<00:43,  3.27it/s][A
 37%|███▋      | 81/221 [00:25<00:42,  3.28it/s][A
 37%|███▋      | 82/221 [00:25<00:42,  3.29it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.29it/s][A
 38%|███▊      | 84/221 [00:26<00:41,  3.29it/s][A
 38%|███▊      | 85/221 [00:26<00:41,  3.29it/s][A
 39%|███▉      | 86/221 [00:26<00:41,  3.29it/s][A
 39%|███▉      | 87/221 [00:27<00:40,  3.29it/s][A
 40%|███▉      | 88/221 [00:27<00:40,  3.29it/s][A
 40%|████      | 89/221 [00:27<00:40,  3.29it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.30it/s][A
 41%|████      | 91/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:28<00:39,  3.30it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.30it/s][A
 43%|████▎     | 94/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 95/221 [00:29<00:38,  3.30it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.30it/s][A
 44%|████▍     | 97/221 [00:30<00:37,  3.30it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.30it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.30it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.30it/s][A
 46%|████▌     | 101/221 [00:31<00:36,  3.30it/s][A
 46%|████▌     | 102/221 [00:31<00:36,  3.30it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:32<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:33<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:34<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:36<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:36<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:37<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:37<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:39<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:40<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:40<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:41<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:42<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:42<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:43<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:44<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:45<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:46<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:46<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:47<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:48<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:49<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:50<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:50<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:51<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:52<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:52<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:53<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:53<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:54<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:55<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:56<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:56<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:57<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:58<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:58<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:59<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [01:00<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:01<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:01<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:02<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:06<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.27it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:34,  6.46it/s][A
  1%|          | 2/221 [00:00<01:56,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:12,  2.99it/s][A
  2%|▏         | 4/221 [00:01<01:05,  3.30it/s][A
  2%|▏         | 5/221 [00:01<01:09,  3.13it/s][A
  3%|▎         | 6/221 [00:01<00:56,  3.83it/s][A
  3%|▎         | 7/221 [00:02<00:51,  4.12it/s][A
  4%|▎         | 8/221 [00:02<01:10,  3.02it/s][A
  4%|▍         | 9/221 [00:02<01:14,  2.86it/s][A
  5%|▍         | 10/221 [00:03<01:22,  2.55it/s][A
  5%|▍         | 11/221 [00:03<01:14,  2.84it/s][A
  5%|▌         | 12/221 [00:03<01:06,  3.14it/s][A
  6%|▌         | 13/221 [00:04<01:38,  2.10it/s][A
  6%|▋         | 14/221 [00:04<01:17,  2.67it/s][A
  7%|▋         | 16/221 [00:05<01:00,  3.36it/s][A
  8%|▊         | 17/221 [00:06<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:06<01:39,  2.05it/s][A
  9%|▊         | 19/221 [00:07<01:24,  2.38it/s][A
  9%|▉         | 20/221 [00:07<01:10,  2.84it/s][A
 10%|▉         | 21/221 [00:07<01:01,  3.24it/s][A
 10%|▉         | 22/221 [00:07<00:53,  3.70it/s][A
 10%|█         | 23/221 [00:07<00:47,  4.15it/s][A
 11%|█         | 24/221 [00:07<00:40,  4.92it/s][A
 11%|█▏        | 25/221 [00:08<00:47,  4.13it/s][A
 12%|█▏        | 26/221 [00:08<00:54,  3.59it/s][A
 12%|█▏        | 27/221 [00:08<00:44,  4.39it/s][A
 13%|█▎        | 28/221 [00:09<01:25,  2.27it/s][A
 13%|█▎        | 29/221 [00:10<01:14,  2.56it/s][A
 14%|█▎        | 30/221 [00:10<01:11,  2.68it/s][A
 14%|█▍        | 31/221 [00:10<01:19,  2.39it/s][A
 14%|█▍        | 32/221 [00:10<01:01,  3.05it/s][A
 15%|█▍        | 33/221 [00:11<00:56,  3.33it/s][A
 15%|█▌        | 34/221 [00:11<00:45,  4.09it/s][A
 16%|█▌        | 35/221 [00:11<00:42,  4.38it/s][A
 16%|█▋        | 36/221 [00:11<00:46,  4.02it/s][A
 17%|█▋        | 37/221 [00:12<00:54,  3.37it/s][A
 17%|█▋        | 38/221 [00:12<01:01,  2.96it/s][A
 18%|█▊        | 39/221 [00:12<00:55,  3.29it/s][A
 18%|█▊        | 40/221 [00:13<01:02,  2.87it/s][A
 19%|█▊        | 41/221 [00:13<00:54,  3.31it/s][A
 19%|█▉        | 42/221 [00:13<00:45,  3.91it/s][A
 20%|█▉        | 44/221 [00:13<00:30,  5.84it/s][A
 20%|██        | 45/221 [00:14<00:40,  4.32it/s][A
 21%|██        | 46/221 [00:14<00:40,  4.33it/s][A
 21%|██▏       | 47/221 [00:14<00:46,  3.74it/s][A
 22%|██▏       | 48/221 [00:14<00:40,  4.30it/s][A
 22%|██▏       | 49/221 [00:15<00:38,  4.48it/s][A
 23%|██▎       | 50/221 [00:15<00:35,  4.82it/s][A
 24%|██▎       | 52/221 [00:15<00:27,  6.16it/s][A
 24%|██▍       | 53/221 [00:15<00:29,  5.78it/s][A
 24%|██▍       | 54/221 [00:16<00:41,  4.03it/s][A
 25%|██▍       | 55/221 [00:16<00:38,  4.35it/s][A
 25%|██▌       | 56/221 [00:16<00:38,  4.30it/s][A
 26%|██▌       | 57/221 [00:16<00:36,  4.47it/s][A
 26%|██▌       | 58/221 [00:17<00:43,  3.73it/s][A
 27%|██▋       | 59/221 [00:17<00:37,  4.37it/s][A
 27%|██▋       | 60/221 [00:17<00:50,  3.16it/s][A
 28%|██▊       | 61/221 [00:18<00:47,  3.35it/s][A
 28%|██▊       | 62/221 [00:18<00:47,  3.38it/s][A
 29%|██▊       | 63/221 [00:18<00:57,  2.75it/s][A
 29%|██▉       | 64/221 [00:19<00:56,  2.80it/s][A
 29%|██▉       | 65/221 [00:19<01:05,  2.38it/s][A
 30%|██▉       | 66/221 [00:20<00:54,  2.85it/s][A
 30%|███       | 67/221 [00:20<01:07,  2.29it/s][A
 31%|███       | 68/221 [00:20<00:55,  2.76it/s][A
 31%|███       | 69/221 [00:21<00:55,  2.73it/s][A
 32%|███▏      | 70/221 [00:21<00:50,  2.98it/s][A
 32%|███▏      | 71/221 [00:21<00:46,  3.19it/s][A
 33%|███▎      | 72/221 [00:22<00:44,  3.34it/s][A
 33%|███▎      | 73/221 [00:22<00:50,  2.93it/s][A
 33%|███▎      | 74/221 [00:22<00:47,  3.08it/s][A
 34%|███▍      | 75/221 [00:23<00:48,  3.03it/s][A
 34%|███▍      | 76/221 [00:23<00:38,  3.75it/s][A
 35%|███▍      | 77/221 [00:23<00:34,  4.22it/s][A
 35%|███▌      | 78/221 [00:23<00:37,  3.80it/s][A
 36%|███▌      | 79/221 [00:24<00:50,  2.79it/s][A
 36%|███▌      | 80/221 [00:24<01:00,  2.34it/s][A
 37%|███▋      | 81/221 [00:25<00:51,  2.71it/s][A
 37%|███▋      | 82/221 [00:25<00:59,  2.35it/s][A
 38%|███▊      | 83/221 [00:26<01:07,  2.05it/s][A
 38%|███▊      | 84/221 [00:26<01:03,  2.17it/s][A
 38%|███▊      | 85/221 [00:27<00:57,  2.37it/s][A
 39%|███▉      | 86/221 [00:27<00:56,  2.37it/s][A
 39%|███▉      | 87/221 [00:28<01:20,  1.66it/s][A
 40%|███▉      | 88/221 [00:28<01:05,  2.03it/s][A
 40%|████      | 89/221 [00:28<00:56,  2.35it/s][A
 41%|████      | 90/221 [00:29<00:46,  2.81it/s][A
 41%|████      | 91/221 [00:29<00:37,  3.47it/s][A
 42%|████▏     | 92/221 [00:29<00:32,  3.92it/s][A
 42%|████▏     | 93/221 [00:29<00:30,  4.17it/s][A
 43%|████▎     | 94/221 [00:29<00:28,  4.39it/s][A
 43%|████▎     | 95/221 [00:30<00:34,  3.60it/s][A
 43%|████▎     | 96/221 [00:30<00:31,  4.03it/s][A
 44%|████▍     | 97/221 [00:30<00:25,  4.78it/s][A
 44%|████▍     | 98/221 [00:30<00:32,  3.73it/s][A
 45%|████▍     | 99/221 [00:31<00:27,  4.37it/s][A
 45%|████▌     | 100/221 [00:31<00:25,  4.74it/s][A
 46%|████▌     | 101/221 [00:31<00:23,  5.04it/s][A
 46%|████▌     | 102/221 [00:31<00:22,  5.19it/s][A
 47%|████▋     | 103/221 [00:31<00:19,  6.06it/s][A
 47%|████▋     | 104/221 [00:31<00:17,  6.69it/s][A
 48%|████▊     | 105/221 [00:32<00:18,  6.35it/s][A
 48%|████▊     | 106/221 [00:32<00:21,  5.26it/s][A
 48%|████▊     | 107/221 [00:32<00:21,  5.30it/s][A
 49%|████▉     | 108/221 [00:32<00:23,  4.84it/s][A
 49%|████▉     | 109/221 [00:33<00:43,  2.59it/s][A
 50%|████▉     | 110/221 [00:34<00:45,  2.44it/s][A
 50%|█████     | 111/221 [00:34<00:43,  2.51it/s][A
 51%|█████     | 112/221 [00:34<00:36,  2.95it/s][A
 51%|█████     | 113/221 [00:34<00:33,  3.21it/s][A
 52%|█████▏    | 115/221 [00:35<00:24,  4.41it/s][A
 53%|█████▎    | 117/221 [00:35<00:23,  4.51it/s][A
 53%|█████▎    | 118/221 [00:35<00:28,  3.57it/s][A
 54%|█████▍    | 119/221 [00:36<00:27,  3.77it/s][A
 54%|█████▍    | 120/221 [00:36<00:32,  3.15it/s][A
 55%|█████▍    | 121/221 [00:36<00:28,  3.56it/s][A
 55%|█████▌    | 122/221 [00:37<00:24,  4.00it/s][A
 56%|█████▌    | 123/221 [00:37<00:25,  3.85it/s][A
 56%|█████▌    | 124/221 [00:37<00:28,  3.35it/s][A
 57%|█████▋    | 125/221 [00:38<00:34,  2.82it/s][A
 57%|█████▋    | 126/221 [00:38<00:33,  2.84it/s][A
 57%|█████▋    | 127/221 [00:38<00:26,  3.60it/s][A
 58%|█████▊    | 128/221 [00:38<00:25,  3.59it/s][A
 58%|█████▊    | 129/221 [00:39<00:22,  4.14it/s][A
 59%|█████▉    | 130/221 [00:39<00:26,  3.39it/s][A
 59%|█████▉    | 131/221 [00:39<00:22,  4.03it/s][A
 60%|█████▉    | 132/221 [00:39<00:20,  4.33it/s][A
 60%|██████    | 133/221 [00:40<00:28,  3.04it/s][A
 61%|██████    | 134/221 [00:40<00:23,  3.74it/s][A
 61%|██████    | 135/221 [00:40<00:25,  3.41it/s][A
 62%|██████▏   | 136/221 [00:41<00:27,  3.11it/s][A
 62%|██████▏   | 137/221 [00:41<00:26,  3.20it/s][A
 62%|██████▏   | 138/221 [00:41<00:26,  3.17it/s][A
 63%|██████▎   | 139/221 [00:42<00:22,  3.58it/s][A
 63%|██████▎   | 140/221 [00:42<00:22,  3.65it/s][A
 64%|██████▍   | 141/221 [00:42<00:17,  4.51it/s][A
 64%|██████▍   | 142/221 [00:42<00:19,  4.00it/s][A
 65%|██████▌   | 144/221 [00:43<00:16,  4.65it/s][A
 66%|██████▌   | 145/221 [00:43<00:15,  4.85it/s][A
 66%|██████▌   | 146/221 [00:43<00:16,  4.64it/s][A
 67%|██████▋   | 147/221 [00:43<00:14,  4.94it/s][A
 67%|██████▋   | 148/221 [00:43<00:16,  4.34it/s][A
 67%|██████▋   | 149/221 [00:44<00:14,  4.87it/s][A
 68%|██████▊   | 150/221 [00:44<00:21,  3.34it/s][A
 68%|██████▊   | 151/221 [00:44<00:19,  3.65it/s][A
 69%|██████▉   | 152/221 [00:45<00:29,  2.35it/s][A
 69%|██████▉   | 153/221 [00:46<00:30,  2.26it/s][A
 70%|██████▉   | 154/221 [00:46<00:28,  2.32it/s][A
 70%|███████   | 155/221 [00:46<00:24,  2.65it/s][A
 71%|███████   | 156/221 [00:47<00:21,  3.00it/s][A
 71%|███████   | 157/221 [00:47<00:21,  2.98it/s][A
 71%|███████▏  | 158/221 [00:47<00:19,  3.16it/s][A
 72%|███████▏  | 159/221 [00:47<00:19,  3.13it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.21it/s][A
 73%|███████▎  | 161/221 [00:48<00:16,  3.66it/s][A
 74%|███████▍  | 163/221 [00:48<00:13,  4.17it/s][A
 74%|███████▍  | 164/221 [00:49<00:15,  3.78it/s][A
 75%|███████▌  | 166/221 [00:49<00:15,  3.54it/s][A
 76%|███████▌  | 167/221 [00:49<00:13,  3.99it/s][A
 76%|███████▌  | 168/221 [00:50<00:13,  3.97it/s][A
 76%|███████▋  | 169/221 [00:50<00:12,  4.12it/s][A
 77%|███████▋  | 170/221 [00:50<00:16,  3.13it/s][A
 77%|███████▋  | 171/221 [00:51<00:16,  2.99it/s][A
 78%|███████▊  | 172/221 [00:51<00:13,  3.57it/s][A
 79%|███████▊  | 174/221 [00:51<00:12,  3.84it/s][A
 79%|███████▉  | 175/221 [00:52<00:12,  3.57it/s][A
 80%|███████▉  | 176/221 [00:52<00:13,  3.42it/s][A
 80%|████████  | 177/221 [00:52<00:11,  3.77it/s][A
 81%|████████  | 178/221 [00:52<00:09,  4.32it/s][A
 81%|████████  | 179/221 [00:53<00:11,  3.60it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.94it/s][A
 82%|████████▏ | 181/221 [00:53<00:08,  4.52it/s][A
 82%|████████▏ | 182/221 [00:53<00:08,  4.71it/s][A
 83%|████████▎ | 183/221 [00:53<00:07,  5.25it/s][A
 83%|████████▎ | 184/221 [00:54<00:10,  3.50it/s][A
 84%|████████▎ | 185/221 [00:54<00:10,  3.46it/s][A
 84%|████████▍ | 186/221 [00:55<00:11,  3.15it/s][A
 85%|████████▌ | 188/221 [00:55<00:10,  3.14it/s][A
 86%|████████▌ | 189/221 [00:56<00:12,  2.61it/s][A
 86%|████████▌ | 190/221 [00:57<00:13,  2.26it/s][A
 86%|████████▋ | 191/221 [00:57<00:11,  2.63it/s][A
 87%|████████▋ | 192/221 [00:57<00:09,  2.97it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.36it/s][A
 88%|████████▊ | 194/221 [00:58<00:11,  2.34it/s][A
 88%|████████▊ | 195/221 [00:58<00:11,  2.18it/s][A
 89%|████████▊ | 196/221 [00:59<00:12,  2.07it/s][A
 89%|████████▉ | 197/221 [00:59<00:10,  2.39it/s][A
 90%|████████▉ | 198/221 [00:59<00:07,  3.00it/s][A
 90%|█████████ | 199/221 [01:00<00:05,  3.69it/s][A
 90%|█████████ | 200/221 [01:00<00:07,  2.93it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.08it/s][A
 91%|█████████▏| 202/221 [01:00<00:04,  3.88it/s][A
 92%|█████████▏| 203/221 [01:01<00:04,  4.41it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.28it/s][A
 93%|█████████▎| 205/221 [01:01<00:04,  3.90it/s][A
 93%|█████████▎| 206/221 [01:02<00:05,  2.89it/s][A
 94%|█████████▎| 207/221 [01:02<00:03,  3.51it/s][A
 94%|█████████▍| 208/221 [01:02<00:04,  3.03it/s][A
 95%|█████████▍| 209/221 [01:03<00:04,  2.50it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.22it/s][A
 95%|█████████▌| 211/221 [01:03<00:03,  3.06it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.49it/s][A
 96%|█████████▋| 213/221 [01:04<00:03,  2.45it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  2.54it/s][A
 97%|█████████▋| 215/221 [01:05<00:02,  2.49it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  2.76it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  2.83it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.25it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  4.07it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  4.28it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.07it/s][A100%|██████████| 221/221 [01:07<00:00,  3.29it/s]
09/18/2024 17:54:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 449--===========

09/18/2024 17:54:47 - INFO - __main__ -   {'area_r1': 39.9, 'area_recall': '39.9/64.7/74.0', 'area_ravg': 59.5}
09/18/2024 17:54:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 449--===========

09/18/2024 17:54:47 - INFO - __main__ -   {'forward_r1': 36.2, 'forward_recall': '36.2/64.6/74.4', 'forward_ravg': 58.4}
09/18/2024 17:54:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 449--===========

09/18/2024 17:54:47 - INFO - __main__ -   {'area_video_r1': 39.5, 'area_video_recall': '39.5/66.6/77.4', 'area_video_ravg': 61.2}
09/18/2024 17:54:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 17:54:47 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 17:54:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 449--===========

09/18/2024 17:54:47 - INFO - __main__ -   {'area_video_r1': 51.4, 'area_video_recall': '51.4/72.3/80.4', 'area_video_ravg': 68.0, 'area_video_back_r1': 49.3, 'area_video_back_recall': '49.3/70.4/78.1', 'area_video_back_ravg': 65.9}
09/18/2024 17:54:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 349=======

09/18/2024 17:54:47 - INFO - __main__ -   {'area_video_r1': 52.0, 'area_video_recall': '52.0/71.2/81.2', 'area_video_ravg': 68.1, 'area_video_back_r1': 49.4, 'area_video_back_recall': '49.4/69.5/77.5', 'area_video_back_ravg': 65.5}
09/18/2024 17:54:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 449--===========

09/18/2024 17:54:47 - INFO - __main__ -   {'video_r1': 28.5, 'video_recall': '28.5/52.7/64.1', 'video_ravg': 48.5}
09/18/2024 17:54:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 17:54:47 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 17:54:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 449--===========

09/18/2024 17:54:47 - INFO - __main__ -   {'video_r1': 49.9, 'video_recall': '49.9/69.5/76.5', 'video_ravg': 65.3}
09/18/2024 17:54:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 17:54:47 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 17:55:08 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0006024720496498048, 'loss_ret%tv%ta--finetune_area/loss_area': 1.2986385822296143, 'loss_ret%tv%ta--finetune_area/total_loss': 1.299241065979004}
 15%|█▌        | 450/2910 [2:50:01<91:28:57, 133.88s/it] 15%|█▌        | 451/2910 [2:50:05<64:43:24, 94.76s/it]  16%|█▌        | 452/2910 [2:50:09<46:04:32, 67.48s/it][h264 @ 0x55d8fef56340] mmco: unref short failure
 16%|█▌        | 453/2910 [2:50:13<33:07:15, 48.53s/it] 16%|█▌        | 454/2910 [2:50:17<24:07:10, 35.35s/it] 16%|█▌        | 455/2910 [2:50:22<17:52:36, 26.21s/it][h264 @ 0x5598fa789c00] mmco: unref short failure
[h264 @ 0x5598fa789c00] mmco: unref short failure
[h264 @ 0x5598fa789c00] mmco: unref short failure
 16%|█▌        | 456/2910 [2:50:27<13:27:29, 19.74s/it][h264 @ 0x55cb5eb04280] mmco: unref short failure
 16%|█▌        | 457/2910 [2:50:32<10:30:48, 15.43s/it]09/18/2024 17:55:47 - INFO - __main__ -   current idx 37nPbMq5QY0.46 from finetune_area returns wrong image/video, use 120240 instead.
 16%|█▌        | 458/2910 [2:50:38<8:31:09, 12.51s/it] [h264 @ 0x55cb6b9de540] mmco: unref short failure
 16%|█▌        | 459/2910 [2:50:43<7:00:13, 10.29s/it][h264 @ 0x55d8fe8ea680] mmco: unref short failure
[h264 @ 0x55d8fe8ea680] mmco: unref short failure
 16%|█▌        | 460/2910 [2:50:48<5:58:13,  8.77s/it][h264 @ 0x55cb5ee61280] mmco: unref short failure
[h264 @ 0x55cb5ee61280] mmco: unref short failure
[h264 @ 0x5598dbd9c540] mmco: unref short failure
 16%|█▌        | 461/2910 [2:50:53<5:13:07,  7.67s/it][h264 @ 0x55cb6a98a5c0] mmco: unref short failure
[h264 @ 0x55cb6a98a5c0] mmco: unref short failure
[h264 @ 0x55cb6a98a5c0] mmco: unref short failure
[h264 @ 0x55cb6a98a5c0] mmco: unref short failure
 16%|█▌        | 462/2910 [2:50:59<4:49:32,  7.10s/it] 16%|█▌        | 463/2910 [2:51:04<4:21:56,  6.42s/it][h264 @ 0x5598f5d8b940] mmco: unref short failure
[h264 @ 0x5598f5d8b940] mmco: unref short failure
[h264 @ 0x55d8fed62c00] mmco: unref short failure
[h264 @ 0x55d8fed62c00] mmco: unref short failure
[h264 @ 0x5565f23d8ac0] mmco: unref short failure
 16%|█▌        | 464/2910 [2:51:09<4:05:55,  6.03s/it][h264 @ 0x55d9154fa9c0] mmco: unref short failure
[h264 @ 0x55d9154fa9c0] mmco: unref short failure
 16%|█▌        | 465/2910 [2:51:15<4:02:11,  5.94s/it][h264 @ 0x5598f49cd900] mmco: unref short failure
[h264 @ 0x5598f57b12c0] mmco: unref short failure
[h264 @ 0x5598f57b12c0] mmco: unref short failure
[h264 @ 0x5598ddcd7740] mmco: unref short failure
[h264 @ 0x5598ddcd7740] mmco: unref short failure
[h264 @ 0x5598ddcd7740] mmco: unref short failure
[h264 @ 0x5598ddcd7740] mmco: unref short failure
[h264 @ 0x55d8fe9e9d00] mmco: unref short failure
[h264 @ 0x55d8fe9e9d00] mmco: unref short failure
[h264 @ 0x55d8fe92d180] mmco: unref short failure
[h264 @ 0x55d8fe92d180] mmco: unref short failure
[h264 @ 0x55d8fe9e9d00] mmco: unref short failure
[h264 @ 0x55d8fe9e9d00] mmco: unref short failure
[h264 @ 0x5598f0b69100] mmco: unref short failure
[h264 @ 0x5598f0b69100] mmco: unref short failure
[h264 @ 0x5598de07c000] mmco: unref short failure
09/18/2024 17:56:55 - INFO - __main__ -   current idx rJOs530YXYo.53 from finetune_area returns wrong image/video, use 140395 instead.
 16%|█▌        | 466/2910 [2:52:07<13:31:02, 19.91s/it] 16%|█▌        | 467/2910 [2:52:17<11:21:18, 16.73s/it][h264 @ 0x55d8fea82f40] mmco: unref short failure
[h264 @ 0x55d8fea82f40] mmco: unref short failure
[h264 @ 0x55d8fea82f40] mmco: unref short failure
[h264 @ 0x55d8fea82f40] mmco: unref short failure
09/18/2024 17:57:42 - INFO - __main__ -   current idx EboJj8xv0Wo.48 from finetune_area returns wrong image/video, use 102431 instead.
[h264 @ 0x5598de07c000] mmco: unref short failure
 16%|█▌        | 468/2910 [2:52:46<13:48:45, 20.36s/it] 16%|█▌        | 469/2910 [2:52:51<10:40:49, 15.75s/it][h264 @ 0x55cb5e9f5780] mmco: unref short failure
[h264 @ 0x55cb5e9f5780] mmco: unref short failure
[h264 @ 0x55cb5e9f5780] mmco: unref short failure
[h264 @ 0x55cb5e9f5780] mmco: unref short failure
[h264 @ 0x5565e67b4c40] mmco: unref short failure
[h264 @ 0x5565e67b4c40] mmco: unref short failure
[h264 @ 0x5565fd0cad40] mmco: unref short failure
 16%|█▌        | 470/2910 [2:52:56<8:33:57, 12.64s/it] [h264 @ 0x55cb74dfe140] mmco: unref short failure
[h264 @ 0x55cb74dfe140] mmco: unref short failure
 16%|█▌        | 471/2910 [2:53:01<7:05:54, 10.48s/it][h264 @ 0x55d8ff409480] mmco: unref short failure
[h264 @ 0x5598dcb15700] mmco: unref short failure
[h264 @ 0x5598dcb15700] mmco: unref short failure
[h264 @ 0x5598dcb15700] mmco: unref short failure
[h264 @ 0x5598dcb15700] mmco: unref short failure
 16%|█▌        | 472/2910 [2:53:13<7:25:25, 10.96s/it][h264 @ 0x55cb5e91b580] mmco: unref short failure
 16%|█▋        | 473/2910 [2:53:19<6:19:31,  9.34s/it][h264 @ 0x55cb7bf70580] mmco: unref short failure
[h264 @ 0x5598dcb15700] mmco: unref short failure
[h264 @ 0x5598dcb15700] mmco: unref short failure
[h264 @ 0x5565e66f92c0] mmco: unref short failure
[h264 @ 0x5598fb308040] mmco: unref short failure
[h264 @ 0x55d90de65080] mmco: unref short failure
[h264 @ 0x55d90de65080] mmco: unref short failure
[h264 @ 0x55d90e80a6c0] mmco: unref short failure
[h264 @ 0x55d90e80a6c0] mmco: unref short failure
[h264 @ 0x5598dc6fc7c0] mmco: unref short failure
[h264 @ 0x5598dc688d00] mmco: unref short failure
[h264 @ 0x5598dc688d00] mmco: unref short failure
[h264 @ 0x5565e6b84380] mmco: unref short failure
[h264 @ 0x5565e6b84380] mmco: unref short failure
[h264 @ 0x55d8feaa2040] mmco: unref short failure
[h264 @ 0x55cb69586c00] mmco: unref short failure
[h264 @ 0x55d91f121480] mmco: unref short failure
[h264 @ 0x55d91f121480] mmco: unref short failure
[h264 @ 0x55d91792dd80] mmco: unref short failure
[h264 @ 0x55d91792dd80] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure
 16%|█▋        | 474/2910 [2:54:37<20:12:55, 29.88s/it][h264 @ 0x5598dcc86d80] mmco: unref short failure
[h264 @ 0x5598dcc86d80] mmco: unref short failure
[h264 @ 0x5598dcc86d80] mmco: unref short failure
[h264 @ 0x5598dcc86d80] mmco: unref short failure
[h264 @ 0x55d8fe0282c0] mmco: unref short failure
[h264 @ 0x55d9089eda40] mmco: unref short failure
[h264 @ 0x55d9022ca440] mmco: unref short failure
 16%|█▋        | 475/2910 [2:54:51<16:56:58, 25.06s/it][h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5ef0ef80] mmco: unref short failure
[h264 @ 0x55cb5ef0ef80] mmco: unref short failure
09/18/2024 18:00:25 - INFO - __main__ -   current idx JyHu6qXDg1E.525 from finetune_area returns wrong image/video, use 123006 instead.
 16%|█▋        | 476/2910 [2:55:17<17:07:46, 25.34s/it]09/18/2024 18:00:27 - INFO - __main__ -   current idx bgTXI7WKWoY.26 from finetune_area returns wrong image/video, use 83183 instead.
 16%|█▋        | 477/2910 [2:55:22<13:06:17, 19.39s/it][h264 @ 0x55cb5e9d2440] mmco: unref short failure
[h264 @ 0x55cb5e9d2440] mmco: unref short failure
09/18/2024 18:00:36 - INFO - __main__ -   current idx 1wKPYAWLNkA.112 from finetune_area returns wrong image/video, use 50961 instead.
 16%|█▋        | 478/2910 [2:55:28<10:21:57, 15.34s/it][h264 @ 0x55cb5e920a80] mmco: unref short failure
[h264 @ 0x55cb5e920a80] mmco: unref short failure
[h264 @ 0x55d8fe8e9300] mmco: unref short failure
[h264 @ 0x55d8fe8e9300] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5eaa2780] mmco: unref short failure
[h264 @ 0x55cb5eaa2780] mmco: unref short failure
 16%|█▋        | 479/2910 [2:55:34<8:24:21, 12.45s/it] [h264 @ 0x5598dcb15700] mmco: unref short failure
 16%|█▋        | 480/2910 [2:55:40<7:08:20, 10.58s/it][h264 @ 0x55d9081b5340] mmco: unref short failure
 17%|█▋        | 481/2910 [2:55:45<6:06:13,  9.05s/it][h264 @ 0x5598dcc86d80] mmco: unref short failure
[h264 @ 0x55cb5eab8200] mmco: unref short failure
[h264 @ 0x55cb5eab8200] mmco: unref short failure
[h264 @ 0x5565ecac09c0] mmco: unref short failure
[h264 @ 0x5565ecac09c0] mmco: unref short failure
[h264 @ 0x5598dd7d3740] mmco: unref short failure
[h264 @ 0x5598dd7d3740] mmco: unref short failure
[h264 @ 0x5598dd7d3740] mmco: unref short failure
[h264 @ 0x5598dd7d3740] mmco: unref short failure
[h264 @ 0x5565e8090440] mmco: unref short failure
[h264 @ 0x5598e51b2c40] mmco: unref short failure
[h264 @ 0x55d91db64f00] mmco: unref short failure
[h264 @ 0x5598e4ade840] mmco: unref short failure
[h264 @ 0x5598e4ade840] mmco: unref short failure
09/18/2024 18:01:47 - INFO - __main__ -   current idx WdVJ8VSAKso.55 from finetune_area returns wrong image/video, use 115551 instead.
[h264 @ 0x55d8fe9ce2c0] mmco: unref short failure
[h264 @ 0x55d8fe9ce2c0] mmco: unref short failure
[h264 @ 0x55cb5eef5740] mmco: unref short failure
[h264 @ 0x55d904b36d00] mmco: unref short failure
[h264 @ 0x55d904b36d00] mmco: unref short failure
[h264 @ 0x55d8fe9e9d00] mmco: unref short failure
[h264 @ 0x55d91918d680] mmco: unref short failure
[h264 @ 0x55d8fec8a640] mmco: unref short failure
[h264 @ 0x55d8fec8a640] mmco: unref short failure
[h264 @ 0x5598f5d8b940] mmco: unref short failure
[h264 @ 0x5598f5d8b940] mmco: unref short failure
 17%|█▋        | 482/2910 [2:57:07<20:48:23, 30.85s/it]09/18/2024 18:02:27 - INFO - __main__ -   current idx 0Hu1HI7Jvzs.159 from finetune_area returns wrong image/video, use 104011 instead.
[h264 @ 0x5565f5d26fc0] mmco: unref short failure
[h264 @ 0x55cb68593900] mmco: unref short failure
[h264 @ 0x55cb68593900] mmco: unref short failure
[h264 @ 0x5598dc779c40] mmco: unref short failure
 17%|█▋        | 483/2910 [2:57:28<18:45:04, 27.81s/it][h264 @ 0x5598dc6fb340] mmco: unref short failure
 17%|█▋        | 484/2910 [2:57:42<15:56:44, 23.66s/it][h264 @ 0x5565f1d5aa80] mmco: unref short failure
[h264 @ 0x5565f1d5aa80] mmco: unref short failure
09/18/2024 18:02:55 - INFO - __main__ -   current idx 9Dbcu8BMaBw.7 from finetune_area returns wrong image/video, use 76933 instead.
 17%|█▋        | 485/2910 [2:57:47<12:15:38, 18.20s/it]09/18/2024 18:02:57 - INFO - __main__ -   current idx GuiTdPE-2G8.5 from finetune_area returns wrong image/video, use 36650 instead.
[h264 @ 0x5565e8c20700] mmco: unref short failure
[h264 @ 0x5565e8c20700] mmco: unref short failure
 17%|█▋        | 486/2910 [2:57:55<10:03:16, 14.93s/it][h264 @ 0x5598eb159480] mmco: unref short failure
[h264 @ 0x5565ed881680] mmco: unref short failure
[h264 @ 0x5565ed881680] mmco: unref short failure
 17%|█▋        | 487/2910 [2:58:00<8:03:44, 11.98s/it] [h264 @ 0x55660bbefe40] mmco: unref short failure
[h264 @ 0x55660bbefe40] mmco: unref short failure
[h264 @ 0x55660bbefe40] mmco: unref short failure
 17%|█▋        | 488/2910 [2:58:07<7:03:38, 10.50s/it] 17%|█▋        | 489/2910 [2:58:13<6:11:55,  9.22s/it][h264 @ 0x55cb5ea32b00] mmco: unref short failure
[h264 @ 0x55cb5ea32b00] mmco: unref short failure
[h264 @ 0x55d8ff3880c0] mmco: unref short failure
[h264 @ 0x55d8ff3880c0] mmco: unref short failure
[h264 @ 0x5566054f2400] mmco: unref short failure
[h264 @ 0x5566054f2400] mmco: unref short failure
[h264 @ 0x55d91c2256c0] mmco: unref short failure
[h264 @ 0x55d91c2256c0] mmco: unref short failure
[h264 @ 0x55d8fec8a640] mmco: unref short failure
[h264 @ 0x55d8fec8a640] mmco: unref short failure
[h264 @ 0x5598e4ac6700] mmco: unref short failure
[h264 @ 0x5598fa4df800] mmco: unref short failure
[h264 @ 0x5598fa4df800] mmco: unref short failure
[h264 @ 0x5598fa4df800] mmco: unref short failure
[h264 @ 0x5598fa4df800] mmco: unref short failure
[h264 @ 0x5598fa4df800] mmco: unref short failure
[h264 @ 0x5598fa4df800] mmco: unref short failure
[h264 @ 0x55cb5eb27080] mmco: unref short failure
[h264 @ 0x55cb5eb27080] mmco: unref short failure
[h264 @ 0x5565ffedb1c0] mmco: unref short failure
[h264 @ 0x55cb5eb27080] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55d8ff77bd40] mmco: unref short failure
[h264 @ 0x5598e160bb80] mmco: unref short failure
[h264 @ 0x5598f0b69100] mmco: unref short failure
[h264 @ 0x5598f0b69100] mmco: unref short failure
[h264 @ 0x55cb660ff340] mmco: unref short failure
[h264 @ 0x55cb660ff340] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x55cb5e4d80c0] mmco: unref short failure
[h264 @ 0x5598e51b2c40] mmco: unref short failure
[h264 @ 0x5598e51b2c40] mmco: unref short failure
[h264 @ 0x55cb5ea0f180] mmco: unref short failure
 17%|█▋        | 490/2910 [2:59:38<21:25:40, 31.88s/it][h264 @ 0x55d9046f4280] mmco: unref short failure
[h264 @ 0x55d9046f4280] mmco: unref short failure
 17%|█▋        | 491/2910 [3:00:03<20:07:08, 29.94s/it]09/18/2024 18:05:14 - INFO - __main__ -   current idx DWhC_3SVyTc.13 from finetune_area returns wrong image/video, use 22882 instead.
[h264 @ 0x5565f61907c0] mmco: unref short failure
[h264 @ 0x5565f61907c0] mmco: unref short failure
 17%|█▋        | 492/2910 [3:00:18<17:03:28, 25.40s/it][h264 @ 0x55d8feaa2040] mmco: unref short failure
 17%|█▋        | 493/2910 [3:00:23<12:57:50, 19.31s/it][h264 @ 0x55d908e56a40] mmco: unref short failure
[h264 @ 0x55d908e56a40] mmco: unref short failure
[h264 @ 0x5598e0876e40] mmco: unref short failure
[h264 @ 0x5598e0876e40] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
09/18/2024 18:05:37 - INFO - __main__ -   current idx sisprxhsc6I.5 from finetune_area returns wrong image/video, use 53399 instead.
 17%|█▋        | 494/2910 [3:00:29<10:19:35, 15.39s/it][h264 @ 0x55d9089eda40] mmco: unref short failure
[h264 @ 0x55d9089eda40] mmco: unref short failure
 17%|█▋        | 495/2910 [3:00:35<8:18:09, 12.38s/it] [h264 @ 0x55d911580a00] mmco: unref short failure
[h264 @ 0x55d911580a00] mmco: unref short failure
 17%|█▋        | 496/2910 [3:00:40<6:59:15, 10.42s/it][h264 @ 0x55d8fea74600] mmco: unref short failure
[h264 @ 0x55cb6123f840] mmco: unref short failure
 17%|█▋        | 497/2910 [3:00:45<5:53:10,  8.78s/it][h264 @ 0x55d8ffb32c80] mmco: unref short failure
[h264 @ 0x5598e6b8d480] mmco: unref short failure
[h264 @ 0x5598e6b8d480] mmco: unref short failure
[h264 @ 0x5598e6b8d480] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure
[h264 @ 0x5565eb150e00] mmco: unref short failure
[h264 @ 0x5565eb150e00] mmco: unref short failure
[h264 @ 0x55d8fe9af7c0] mmco: unref short failure
[h264 @ 0x5598df6d0b40] mmco: unref short failure
[h264 @ 0x5598df6d0b40] mmco: unref short failure
[h264 @ 0x556604c77740] mmco: unref short failure
[h264 @ 0x556604c77740] mmco: unref short failure
[h264 @ 0x556604c77740] mmco: unref short failure
[h264 @ 0x556604c77740] mmco: unref short failure
[h264 @ 0x55cb5ea29440] mmco: unref short failure
[h264 @ 0x55cb652e3600] mmco: unref short failure
[h264 @ 0x5598dc7d8940] mmco: unref short failure
[h264 @ 0x556601bcc640] mmco: unref short failure
[h264 @ 0x556601bcc640] mmco: unref short failure
[h264 @ 0x556601bcc640] mmco: unref short failure
[h264 @ 0x556601bcc640] mmco: unref short failure
[h264 @ 0x55d91918d680] mmco: unref short failure
[h264 @ 0x55d91918d680] mmco: unref short failure
[h264 @ 0x55d91918d680] mmco: unref short failure
[h264 @ 0x55d91918d680] mmco: unref short failure
[h264 @ 0x5566070f3000] mmco: unref short failure
[h264 @ 0x55d916dba700] mmco: unref short failure
[h264 @ 0x5598dc7e1180] mmco: unref short failure
[h264 @ 0x55d908e56a40] mmco: unref short failure
[h264 @ 0x55d908e56a40] mmco: unref short failure
[h264 @ 0x5565fdec0200] mmco: unref short failure
[h264 @ 0x5565fdec0200] mmco: unref short failure
 17%|█▋        | 498/2910 [3:02:10<21:10:38, 31.61s/it][h264 @ 0x55660794ed00] mmco: unref short failure
[h264 @ 0x55660794ed00] mmco: unref short failure
[h264 @ 0x5598e050c840] mmco: unref short failure
[h264 @ 0x5598e050c840] mmco: unref short failure
[h264 @ 0x556609c43a80] mmco: unref short failure
[h264 @ 0x556609c43a80] mmco: unref short failure
[h264 @ 0x5598e983f480] mmco: unref short failure
[h264 @ 0x5598e983f480] mmco: unref short failure
 17%|█▋        | 499/2910 [3:02:29<18:34:26, 27.73s/it]09/18/2024 18:07:39 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 18:07:39 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x5565eae292c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55d8fe9aa100] mmco: unref short failure
[h264 @ 0x55d8fe9aa100] mmco: unref short failure
[h264 @ 0x55d8fe9aa100] mmco: unref short failure
[h264 @ 0x55d8fe9aa100] mmco: unref short failure
[h264 @ 0x55cb68593900] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 18:09:23 - INFO - __main__ -   current idx -A-7FmorgRU.15 from finetune_area returns wrong image/video, use 18171 instead.
[h264 @ 0x5565ffedb1c0] mmco: unref short failure
[h264 @ 0x5565ffedb1c0] mmco: unref short failure
[h264 @ 0x5565fffc35c0] mmco: unref short failure
[h264 @ 0x5565fffc35c0] mmco: unref short failure
[h264 @ 0x5598e92431c0] mmco: unref short failure
[h264 @ 0x5598e92431c0] mmco: unref short failure
[h264 @ 0x5598e92431c0] mmco: unref short failure
[h264 @ 0x5598e92431c0] mmco: unref short failure
[h264 @ 0x5598e2064ac0] mmco: unref short failure
[h264 @ 0x5598e2064ac0] mmco: unref short failure
[h264 @ 0x55d8fecf7bc0] mmco: unref short failure
[h264 @ 0x5598dc7d8940] mmco: unref short failure
[h264 @ 0x55cb78762000] mmco: unref short failure
[h264 @ 0x5565fe9d7b00] mmco: unref short failure
[h264 @ 0x5565fe9d7b00] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<02:44,  1.34it/s][A[h264 @ 0x5565fb07ef40] mmco: unref short failure
[h264 @ 0x5565fb07ef40] mmco: unref short failure
[h264 @ 0x5565e679fec0] mmco: unref short failure

  1%|          | 2/221 [00:01<02:24,  1.52it/s][A[h264 @ 0x5565e679fec0] mmco: unref short failure

  1%|▏         | 3/221 [00:01<01:45,  2.07it/s][A
  2%|▏         | 4/221 [00:01<01:17,  2.81it/s][A
  2%|▏         | 5/221 [00:02<01:07,  3.18it/s][A
  3%|▎         | 6/221 [00:02<00:55,  3.88it/s][A
  3%|▎         | 7/221 [00:02<00:59,  3.61it/s][A
  4%|▎         | 8/221 [00:02<01:12,  2.94it/s][A
  4%|▍         | 9/221 [00:03<01:15,  2.81it/s][A
  5%|▍         | 10/221 [00:03<01:31,  2.31it/s][A
  5%|▍         | 11/221 [00:04<01:17,  2.71it/s][A
  5%|▌         | 12/221 [00:04<01:39,  2.10it/s][A
  6%|▌         | 13/221 [00:05<01:31,  2.27it/s][A
  6%|▋         | 14/221 [00:06<02:04,  1.66it/s][A
  7%|▋         | 15/221 [00:06<01:44,  1.97it/s][A[h264 @ 0x55cb69c9cc40] mmco: unref short failure

  7%|▋         | 16/221 [00:07<01:42,  2.00it/s][A
  8%|▊         | 17/221 [00:07<01:56,  1.75it/s][A
  8%|▊         | 18/221 [00:08<01:43,  1.96it/s][A
  9%|▊         | 19/221 [00:08<01:24,  2.40it/s][A
  9%|▉         | 20/221 [00:08<01:10,  2.85it/s][A
 10%|▉         | 21/221 [00:08<01:02,  3.18it/s][A
 10%|▉         | 22/221 [00:08<00:56,  3.51it/s][A
 10%|█         | 23/221 [00:09<00:46,  4.26it/s][A
 11%|█         | 24/221 [00:09<00:40,  4.88it/s][A
 11%|█▏        | 25/221 [00:09<00:42,  4.60it/s][A[h264 @ 0x55d91db64f00] mmco: unref short failure

 12%|█▏        | 26/221 [00:09<00:58,  3.33it/s][A
 12%|█▏        | 27/221 [00:10<00:47,  4.08it/s][A[h264 @ 0x5565eceda440] mmco: unref short failure
[h264 @ 0x5565eceda440] mmco: unref short failure
[h264 @ 0x5565eceda440] mmco: unref short failure
[h264 @ 0x5565eceda440] mmco: unref short failure

 13%|█▎        | 28/221 [00:10<01:05,  2.95it/s][A[h264 @ 0x55d9180bda80] mmco: unref short failure
[h264 @ 0x55d9180bda80] mmco: unref short failure
[h264 @ 0x55d916dba700] mmco: unref short failure
[h264 @ 0x55d916dba700] mmco: unref short failure

 13%|█▎        | 29/221 [00:11<01:41,  1.90it/s][A
 14%|█▎        | 30/221 [00:11<01:28,  2.16it/s][A
 14%|█▍        | 31/221 [00:12<01:16,  2.48it/s][A
 15%|█▍        | 33/221 [00:12<00:58,  3.22it/s][A
 15%|█▌        | 34/221 [00:12<01:01,  3.02it/s][A
 16%|█▌        | 35/221 [00:13<00:54,  3.43it/s][A
 16%|█▋        | 36/221 [00:13<00:57,  3.22it/s][A[h264 @ 0x55d920fa4a40] mmco: unref short failure
[h264 @ 0x55d920fa4a40] mmco: unref short failure

 17%|█▋        | 37/221 [00:14<01:17,  2.38it/s][A
 17%|█▋        | 38/221 [00:14<01:18,  2.34it/s][A
 18%|█▊        | 39/221 [00:15<01:14,  2.44it/s][A
 18%|█▊        | 40/221 [00:15<01:18,  2.29it/s][A
 19%|█▊        | 41/221 [00:15<01:08,  2.62it/s][A
 19%|█▉        | 42/221 [00:16<01:14,  2.42it/s][A
 19%|█▉        | 43/221 [00:16<00:58,  3.07it/s][A
 20%|█▉        | 44/221 [00:16<00:46,  3.84it/s][A
 20%|██        | 45/221 [00:17<01:32,  1.90it/s][A
 21%|██        | 46/221 [00:18<01:31,  1.92it/s][A
 21%|██▏       | 47/221 [00:18<01:36,  1.79it/s][A
 22%|██▏       | 48/221 [00:18<01:14,  2.31it/s][A
 22%|██▏       | 49/221 [00:20<01:49,  1.56it/s][A
 23%|██▎       | 50/221 [00:20<01:58,  1.45it/s][A
 23%|██▎       | 51/221 [00:20<01:29,  1.91it/s][A
 24%|██▎       | 52/221 [00:21<01:09,  2.42it/s][A
 24%|██▍       | 53/221 [00:21<00:55,  3.04it/s][A
 24%|██▍       | 54/221 [00:23<02:21,  1.18it/s][A
 25%|██▍       | 55/221 [00:23<01:50,  1.50it/s][A
 25%|██▌       | 56/221 [00:23<01:30,  1.82it/s][A
 26%|██▌       | 57/221 [00:24<01:14,  2.21it/s][A
 26%|██▌       | 58/221 [00:24<01:02,  2.62it/s][A
 27%|██▋       | 59/221 [00:24<00:59,  2.73it/s][A
 27%|██▋       | 60/221 [00:25<01:30,  1.77it/s][A
 28%|██▊       | 61/221 [00:25<01:14,  2.16it/s][A
 28%|██▊       | 62/221 [00:26<01:04,  2.45it/s][A[h264 @ 0x5565fe81f500] mmco: unref short failure
[h264 @ 0x5565fe81f500] mmco: unref short failure

 29%|██▊       | 63/221 [00:26<00:58,  2.71it/s][A
 29%|██▉       | 64/221 [00:26<00:50,  3.10it/s][A
 29%|██▉       | 65/221 [00:26<00:50,  3.12it/s][A
 30%|██▉       | 66/221 [00:27<00:50,  3.05it/s][A
 30%|███       | 67/221 [00:28<01:13,  2.09it/s][A
 31%|███       | 68/221 [00:28<01:00,  2.53it/s][A
 31%|███       | 69/221 [00:29<01:15,  2.00it/s][A
 32%|███▏      | 70/221 [00:29<01:02,  2.42it/s][A
 32%|███▏      | 71/221 [00:29<01:08,  2.18it/s][A
 33%|███▎      | 72/221 [00:30<01:00,  2.45it/s][A
 33%|███▎      | 73/221 [00:30<01:01,  2.42it/s][A
 33%|███▎      | 74/221 [00:30<00:49,  3.00it/s][A
 34%|███▍      | 75/221 [00:31<00:51,  2.86it/s][A[h264 @ 0x5598dcb15700] mmco: unref short failure

 34%|███▍      | 76/221 [00:31<00:44,  3.22it/s][A
 35%|███▍      | 77/221 [00:31<00:45,  3.16it/s][A[h264 @ 0x5565f8d271c0] mmco: unref short failure
[h264 @ 0x5565f8d271c0] mmco: unref short failure

 35%|███▌      | 78/221 [00:31<00:42,  3.35it/s][A
 36%|███▌      | 79/221 [00:32<00:56,  2.50it/s][A
 36%|███▌      | 80/221 [00:32<00:47,  2.96it/s][A
 37%|███▋      | 81/221 [00:32<00:42,  3.26it/s][A
 37%|███▋      | 82/221 [00:34<01:12,  1.91it/s][A
 38%|███▊      | 83/221 [00:34<01:12,  1.90it/s][A
 38%|███▊      | 84/221 [00:34<01:03,  2.14it/s][A
 38%|███▊      | 85/221 [00:35<00:57,  2.36it/s][A
 39%|███▉      | 86/221 [00:35<00:52,  2.55it/s][A
 39%|███▉      | 87/221 [00:36<00:58,  2.28it/s][A
 40%|███▉      | 88/221 [00:36<01:01,  2.15it/s][A
 40%|████      | 89/221 [00:37<01:04,  2.05it/s][A
 41%|████      | 90/221 [00:37<00:55,  2.37it/s][A
 41%|████      | 91/221 [00:37<00:44,  2.95it/s][A[h264 @ 0x5598e7114f80] mmco: unref short failure

 42%|████▏     | 92/221 [00:37<00:38,  3.32it/s][A
 42%|████▏     | 93/221 [00:38<00:41,  3.10it/s][A
 43%|████▎     | 94/221 [00:38<00:37,  3.41it/s][A
 43%|████▎     | 95/221 [00:38<00:37,  3.37it/s][A
 43%|████▎     | 96/221 [00:38<00:36,  3.44it/s][A
 44%|████▍     | 97/221 [00:39<00:30,  4.07it/s][A
 44%|████▍     | 98/221 [00:39<00:32,  3.84it/s][A
 45%|████▍     | 99/221 [00:39<00:30,  4.00it/s][A
 45%|████▌     | 100/221 [00:39<00:28,  4.31it/s][A
 46%|████▌     | 101/221 [00:40<00:28,  4.20it/s][A
 46%|████▌     | 102/221 [00:40<00:30,  3.96it/s][A
 47%|████▋     | 103/221 [00:40<00:25,  4.63it/s][A
 47%|████▋     | 104/221 [00:40<00:22,  5.11it/s][A
 48%|████▊     | 105/221 [00:40<00:24,  4.64it/s][A
 48%|████▊     | 106/221 [00:41<00:43,  2.63it/s][A
 48%|████▊     | 107/221 [00:41<00:35,  3.17it/s][A
 49%|████▉     | 108/221 [00:42<00:34,  3.24it/s][A
 49%|████▉     | 109/221 [00:42<00:36,  3.03it/s][A
 50%|████▉     | 110/221 [00:42<00:34,  3.21it/s][A
 50%|█████     | 111/221 [00:43<00:39,  2.76it/s][A
 51%|█████     | 112/221 [00:43<00:33,  3.30it/s][A
 51%|█████     | 113/221 [00:43<00:34,  3.10it/s][A
 52%|█████▏    | 115/221 [00:44<00:24,  4.29it/s][A[h264 @ 0x5565e6962880] mmco: unref short failure
[h264 @ 0x5565e6962880] mmco: unref short failure
[h264 @ 0x5565e6962880] mmco: unref short failure
[h264 @ 0x5565e6962880] mmco: unref short failure

 52%|█████▏    | 116/221 [00:47<01:41,  1.04it/s][A
 53%|█████▎    | 117/221 [00:47<01:21,  1.27it/s][A
 53%|█████▎    | 118/221 [00:47<01:06,  1.54it/s][A
 54%|█████▍    | 119/221 [00:48<00:54,  1.86it/s][A
 54%|█████▍    | 120/221 [00:48<00:48,  2.10it/s][A
 55%|█████▍    | 121/221 [00:48<00:36,  2.70it/s][A
 55%|█████▌    | 122/221 [00:48<00:31,  3.13it/s][A[h264 @ 0x55d90bcea540] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure

 56%|█████▌    | 123/221 [00:48<00:27,  3.62it/s][A
 56%|█████▌    | 124/221 [00:49<00:25,  3.84it/s][A[h264 @ 0x55cb7d2e53c0] mmco: unref short failure
[h264 @ 0x55cb7d2e53c0] mmco: unref short failure
[h264 @ 0x55cb7d2e53c0] mmco: unref short failure

 57%|█████▋    | 125/221 [00:49<00:28,  3.36it/s][A[h264 @ 0x55d91918d680] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure

 57%|█████▋    | 126/221 [00:49<00:27,  3.46it/s][A
 57%|█████▋    | 127/221 [00:50<00:34,  2.70it/s][A[h264 @ 0x55d90bcea540] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure

 58%|█████▊    | 128/221 [00:50<00:34,  2.69it/s][A
 58%|█████▊    | 129/221 [00:50<00:28,  3.25it/s][A[h264 @ 0x55d90bcea540] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure

 59%|█████▉    | 130/221 [00:51<00:29,  3.09it/s][A
 59%|█████▉    | 131/221 [00:51<00:25,  3.49it/s][A
 60%|█████▉    | 132/221 [00:51<00:23,  3.73it/s][A[h264 @ 0x55d90bcea540] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure

 60%|██████    | 133/221 [00:52<00:29,  3.01it/s][A
 61%|██████    | 134/221 [00:52<00:24,  3.51it/s][A
 61%|██████    | 135/221 [00:52<00:25,  3.35it/s][A[h264 @ 0x55d90bcea540] mmco: unref short failure
[h264 @ 0x55d90bcea540] mmco: unref short failure

 62%|██████▏   | 136/221 [00:52<00:28,  2.99it/s][A
 62%|██████▏   | 137/221 [00:53<00:25,  3.24it/s][A
 62%|██████▏   | 138/221 [00:53<00:27,  2.97it/s][A
 63%|██████▎   | 139/221 [00:54<00:29,  2.78it/s][A
 63%|██████▎   | 140/221 [00:54<00:28,  2.83it/s][A
 64%|██████▍   | 141/221 [00:54<00:26,  3.00it/s][A
 64%|██████▍   | 142/221 [00:54<00:25,  3.06it/s][A
 65%|██████▍   | 143/221 [00:55<00:26,  2.92it/s][A
 65%|██████▌   | 144/221 [00:55<00:24,  3.11it/s][A
 66%|██████▌   | 145/221 [00:55<00:19,  3.84it/s][A
 66%|██████▌   | 146/221 [00:55<00:17,  4.19it/s][A
 67%|██████▋   | 147/221 [00:56<00:18,  4.11it/s][A[h264 @ 0x5565ff4310c0] mmco: unref short failure
[h264 @ 0x5565ff4310c0] mmco: unref short failure

 67%|██████▋   | 148/221 [00:56<00:19,  3.68it/s][A
 67%|██████▋   | 149/221 [00:56<00:16,  4.38it/s][A
 68%|██████▊   | 150/221 [00:56<00:15,  4.51it/s][A
 68%|██████▊   | 151/221 [00:57<00:22,  3.08it/s][A
 69%|██████▉   | 152/221 [00:58<00:47,  1.46it/s][A
 69%|██████▉   | 153/221 [00:59<00:40,  1.66it/s][A
 70%|██████▉   | 154/221 [00:59<00:34,  1.97it/s][A
 70%|███████   | 155/221 [00:59<00:25,  2.54it/s][A
 71%|███████   | 156/221 [00:59<00:21,  3.02it/s][A
 71%|███████   | 157/221 [01:00<00:32,  1.96it/s][A
 71%|███████▏  | 158/221 [01:01<00:27,  2.32it/s][A
 72%|███████▏  | 159/221 [01:01<00:23,  2.68it/s][A
 72%|███████▏  | 160/221 [01:01<00:22,  2.65it/s][A
 73%|███████▎  | 161/221 [01:01<00:19,  3.07it/s][A
 73%|███████▎  | 162/221 [01:02<00:23,  2.46it/s][A
 74%|███████▍  | 163/221 [01:02<00:20,  2.81it/s][A
 74%|███████▍  | 164/221 [01:03<00:24,  2.33it/s][A
 75%|███████▍  | 165/221 [01:03<00:18,  2.96it/s][A[h264 @ 0x55d915d2b880] mmco: unref short failure

 75%|███████▌  | 166/221 [01:03<00:19,  2.82it/s][A
 76%|███████▌  | 167/221 [01:04<00:15,  3.41it/s][A
 76%|███████▌  | 168/221 [01:05<00:25,  2.05it/s][A
 76%|███████▋  | 169/221 [01:05<00:21,  2.46it/s][A
 77%|███████▋  | 170/221 [01:05<00:20,  2.51it/s][A
 77%|███████▋  | 171/221 [01:05<00:18,  2.67it/s][A
 78%|███████▊  | 172/221 [01:06<00:16,  3.03it/s][A
 78%|███████▊  | 173/221 [01:06<00:14,  3.30it/s][A
 79%|███████▊  | 174/221 [01:06<00:13,  3.45it/s][A
 79%|███████▉  | 175/221 [01:07<00:17,  2.56it/s][A
 80%|███████▉  | 176/221 [01:07<00:16,  2.77it/s][A
 80%|████████  | 177/221 [01:07<00:13,  3.21it/s][A
 81%|████████  | 178/221 [01:07<00:10,  3.96it/s][A
 81%|████████  | 179/221 [01:08<00:12,  3.30it/s][A
 81%|████████▏ | 180/221 [01:08<00:09,  4.11it/s][A
 82%|████████▏ | 181/221 [01:08<00:09,  4.15it/s][A
 82%|████████▏ | 182/221 [01:08<00:08,  4.48it/s][A
 83%|████████▎ | 183/221 [01:09<00:09,  4.01it/s][A
 83%|████████▎ | 184/221 [01:09<00:09,  3.86it/s][A
 84%|████████▎ | 185/221 [01:09<00:08,  4.33it/s][A
 84%|████████▍ | 186/221 [01:10<00:10,  3.38it/s][A
 85%|████████▍ | 187/221 [01:10<00:09,  3.74it/s][A
 85%|████████▌ | 188/221 [01:10<00:08,  3.96it/s][A
 86%|████████▌ | 189/221 [01:10<00:09,  3.24it/s][A
 86%|████████▌ | 190/221 [01:11<00:10,  3.03it/s][A
 86%|████████▋ | 191/221 [01:11<00:08,  3.35it/s][A
 87%|████████▋ | 192/221 [01:11<00:08,  3.44it/s][A
 88%|████████▊ | 194/221 [01:12<00:09,  2.78it/s][A
 88%|████████▊ | 195/221 [01:12<00:08,  3.19it/s][A
 89%|████████▊ | 196/221 [01:13<00:10,  2.46it/s][A
 89%|████████▉ | 197/221 [01:13<00:07,  3.09it/s][A[h264 @ 0x5598dd3682c0] mmco: unref short failure

 90%|████████▉ | 198/221 [01:13<00:07,  3.16it/s][A
 90%|█████████ | 199/221 [01:14<00:05,  3.71it/s][A
 90%|█████████ | 200/221 [01:14<00:06,  3.28it/s][A
 91%|█████████ | 201/221 [01:14<00:05,  3.49it/s][A
 91%|█████████▏| 202/221 [01:14<00:04,  3.91it/s][A
 92%|█████████▏| 203/221 [01:14<00:04,  4.44it/s][A
 92%|█████████▏| 204/221 [01:15<00:04,  3.81it/s][A
 93%|█████████▎| 205/221 [01:15<00:03,  4.61it/s][A
 93%|█████████▎| 206/221 [01:16<00:04,  3.13it/s][A
 94%|█████████▎| 207/221 [01:16<00:04,  3.39it/s][A
 94%|█████████▍| 208/221 [01:16<00:03,  4.10it/s][A
 95%|█████████▍| 209/221 [01:16<00:03,  3.97it/s][A
 95%|█████████▌| 211/221 [01:17<00:02,  4.04it/s][A
 96%|█████████▌| 212/221 [01:17<00:01,  4.53it/s][A
 96%|█████████▋| 213/221 [01:17<00:01,  4.13it/s][A
 97%|█████████▋| 214/221 [01:17<00:01,  3.80it/s][A
 97%|█████████▋| 215/221 [01:18<00:01,  3.85it/s][A
 98%|█████████▊| 216/221 [01:18<00:01,  3.59it/s][A
 98%|█████████▊| 217/221 [01:18<00:01,  3.12it/s][A
 99%|█████████▊| 218/221 [01:19<00:01,  2.96it/s][A
 99%|█████████▉| 219/221 [01:19<00:00,  3.28it/s][A[h264 @ 0x55cb7e5932c0] mmco: unref short failure

100%|█████████▉| 220/221 [01:23<00:01,  1.27s/it][A
100%|██████████| 221/221 [01:23<00:00,  1.05it/s][A100%|██████████| 221/221 [01:23<00:00,  2.65it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.30it/s][A
  1%|          | 2/221 [00:00<01:06,  3.30it/s][A
  1%|▏         | 3/221 [00:00<01:07,  3.23it/s][A
  2%|▏         | 4/221 [00:01<01:06,  3.26it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.27it/s][A
  3%|▎         | 6/221 [00:01<01:05,  3.28it/s][A
  3%|▎         | 7/221 [00:02<01:05,  3.28it/s][A
  4%|▎         | 8/221 [00:02<01:05,  3.26it/s][A
  4%|▍         | 9/221 [00:02<01:04,  3.27it/s][A
  5%|▍         | 10/221 [00:03<01:05,  3.23it/s][A
  5%|▍         | 11/221 [00:03<01:04,  3.25it/s][A
  5%|▌         | 12/221 [00:03<01:04,  3.26it/s][A
  6%|▌         | 13/221 [00:03<01:04,  3.24it/s][A
  6%|▋         | 14/221 [00:04<01:03,  3.25it/s][A
  7%|▋         | 15/221 [00:04<01:03,  3.27it/s][A
  7%|▋         | 16/221 [00:04<01:02,  3.27it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.28it/s][A
  8%|▊         | 18/221 [00:05<01:01,  3.28it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.27it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.25it/s][A
 10%|▉         | 21/221 [00:06<01:01,  3.26it/s][A
 10%|▉         | 22/221 [00:06<01:01,  3.22it/s][A
 10%|█         | 23/221 [00:07<01:01,  3.24it/s][A
 11%|█         | 24/221 [00:07<01:00,  3.26it/s][A
 11%|█▏        | 25/221 [00:07<00:59,  3.27it/s][A
 12%|█▏        | 26/221 [00:07<00:59,  3.28it/s][A
 12%|█▏        | 27/221 [00:08<00:59,  3.28it/s][A
 13%|█▎        | 28/221 [00:08<00:59,  3.24it/s][A
 13%|█▎        | 29/221 [00:08<00:58,  3.25it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.27it/s][A
 14%|█▍        | 31/221 [00:09<00:58,  3.27it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.28it/s][A
 15%|█▍        | 33/221 [00:10<00:58,  3.24it/s][A
 15%|█▌        | 34/221 [00:10<00:57,  3.25it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.27it/s][A
 16%|█▋        | 36/221 [00:11<00:56,  3.27it/s][A
 17%|█▋        | 37/221 [00:11<00:56,  3.28it/s][A
 17%|█▋        | 38/221 [00:11<00:55,  3.27it/s][A
 18%|█▊        | 39/221 [00:11<00:55,  3.27it/s][A
 18%|█▊        | 40/221 [00:12<00:55,  3.27it/s][A
 19%|█▊        | 41/221 [00:12<00:55,  3.27it/s][A
 19%|█▉        | 42/221 [00:12<00:55,  3.25it/s][A
 19%|█▉        | 43/221 [00:13<00:54,  3.26it/s][A
 20%|█▉        | 44/221 [00:13<00:54,  3.27it/s][A
 20%|██        | 45/221 [00:13<00:54,  3.21it/s][A
 21%|██        | 46/221 [00:14<00:54,  3.21it/s][A
 21%|██▏       | 47/221 [00:14<00:53,  3.24it/s][A
 22%|██▏       | 48/221 [00:14<00:53,  3.21it/s][A
 22%|██▏       | 49/221 [00:15<00:53,  3.23it/s][A
 23%|██▎       | 50/221 [00:15<00:52,  3.25it/s][A
 23%|██▎       | 51/221 [00:15<00:52,  3.27it/s][A
 24%|██▎       | 52/221 [00:15<00:51,  3.27it/s][A
 24%|██▍       | 53/221 [00:16<00:51,  3.28it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.28it/s][A
 25%|██▍       | 55/221 [00:16<00:50,  3.29it/s][A
 25%|██▌       | 56/221 [00:17<00:50,  3.29it/s][A
 26%|██▌       | 57/221 [00:17<00:49,  3.29it/s][A
 26%|██▌       | 58/221 [00:17<00:49,  3.29it/s][A
 27%|██▋       | 59/221 [00:18<00:49,  3.29it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.29it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.29it/s][A
 28%|██▊       | 62/221 [00:18<00:48,  3.29it/s][A
 29%|██▊       | 63/221 [00:19<00:47,  3.29it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.30it/s][A
 29%|██▉       | 65/221 [00:19<00:47,  3.30it/s][A[h264 @ 0x55d906422f00] mmco: unref short failure
[h264 @ 0x55d906422f00] mmco: unref short failure

 30%|██▉       | 66/221 [00:20<00:47,  3.30it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.30it/s][A
 31%|███       | 68/221 [00:20<00:46,  3.30it/s][A
 31%|███       | 69/221 [00:21<00:46,  3.30it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.30it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.30it/s][A
 33%|███▎      | 72/221 [00:22<00:45,  3.31it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.31it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 76/221 [00:23<00:43,  3.31it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.31it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.31it/s][A
 36%|███▌      | 79/221 [00:24<00:42,  3.31it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 81/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 82/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 85/221 [00:25<00:41,  3.31it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.31it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.31it/s][A
 40%|███▉      | 88/221 [00:26<00:40,  3.31it/s][A
 40%|████      | 89/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 91/221 [00:27<00:39,  3.31it/s][A
 42%|████▏     | 92/221 [00:28<00:38,  3.31it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:29<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:33<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:35<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:35<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:36<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:38<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:39<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:41<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:43<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:46<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:48<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:49<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:51<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:54<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.30it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:37,  5.93it/s][A
  1%|          | 2/221 [00:00<01:49,  1.99it/s][A
  2%|▏         | 4/221 [00:01<01:05,  3.33it/s][A
  2%|▏         | 5/221 [00:01<01:09,  3.10it/s][A
  3%|▎         | 6/221 [00:01<00:57,  3.74it/s][A
  3%|▎         | 7/221 [00:02<00:54,  3.90it/s][A
  4%|▎         | 8/221 [00:02<01:08,  3.11it/s][A
  4%|▍         | 9/221 [00:02<01:15,  2.80it/s][A
  5%|▍         | 10/221 [00:03<01:17,  2.72it/s][A
  5%|▍         | 11/221 [00:03<01:10,  2.96it/s][A
  5%|▌         | 12/221 [00:03<01:09,  3.02it/s][A
  6%|▌         | 13/221 [00:04<01:41,  2.06it/s][A
  6%|▋         | 14/221 [00:04<01:23,  2.49it/s][A
  7%|▋         | 16/221 [00:05<01:03,  3.21it/s][A
  8%|▊         | 17/221 [00:06<01:44,  1.94it/s][A
  8%|▊         | 18/221 [00:06<01:38,  2.06it/s][A
  9%|▊         | 19/221 [00:07<01:26,  2.35it/s][A
  9%|▉         | 20/221 [00:07<01:11,  2.81it/s][A
 10%|▉         | 21/221 [00:07<01:01,  3.23it/s][A
 10%|▉         | 22/221 [00:07<00:55,  3.58it/s][A
 10%|█         | 23/221 [00:07<00:46,  4.22it/s][A
 11%|█         | 24/221 [00:07<00:40,  4.92it/s][A
 11%|█▏        | 25/221 [00:08<00:59,  3.31it/s][A
 12%|█▏        | 26/221 [00:08<01:05,  2.99it/s][A
 13%|█▎        | 28/221 [00:09<01:19,  2.42it/s][A
 13%|█▎        | 29/221 [00:10<01:11,  2.68it/s][A
 14%|█▎        | 30/221 [00:10<01:13,  2.61it/s][A
 14%|█▍        | 31/221 [00:11<01:16,  2.49it/s][A
 14%|█▍        | 32/221 [00:11<01:01,  3.09it/s][A
 15%|█▍        | 33/221 [00:11<00:56,  3.35it/s][A
 16%|█▌        | 35/221 [00:11<00:39,  4.69it/s][A
 16%|█▋        | 36/221 [00:11<00:44,  4.20it/s][A
 17%|█▋        | 37/221 [00:12<00:55,  3.34it/s][A
 17%|█▋        | 38/221 [00:12<01:00,  3.05it/s][A
 18%|█▊        | 39/221 [00:13<00:53,  3.40it/s][A
 18%|█▊        | 40/221 [00:13<01:03,  2.87it/s][A
 19%|█▊        | 41/221 [00:13<00:53,  3.34it/s][A
 19%|█▉        | 42/221 [00:13<00:45,  3.92it/s][A
 20%|█▉        | 44/221 [00:13<00:29,  5.93it/s][A
 20%|██        | 45/221 [00:14<00:37,  4.64it/s][A
 21%|██        | 46/221 [00:14<00:41,  4.20it/s][A
 21%|██▏       | 47/221 [00:15<00:47,  3.68it/s][A
 22%|██▏       | 48/221 [00:15<00:40,  4.24it/s][A
 22%|██▏       | 49/221 [00:15<00:38,  4.43it/s][A
 23%|██▎       | 50/221 [00:15<00:34,  4.92it/s][A
 23%|██▎       | 51/221 [00:15<00:32,  5.18it/s][A
 24%|██▎       | 52/221 [00:15<00:29,  5.80it/s][A
 24%|██▍       | 53/221 [00:16<00:30,  5.43it/s][A
 24%|██▍       | 54/221 [00:16<00:47,  3.50it/s][A
 25%|██▍       | 55/221 [00:16<00:42,  3.89it/s][A
 25%|██▌       | 56/221 [00:16<00:40,  4.11it/s][A
 26%|██▌       | 57/221 [00:17<00:37,  4.34it/s][A
 26%|██▌       | 58/221 [00:17<00:42,  3.87it/s][A
 27%|██▋       | 59/221 [00:17<00:35,  4.62it/s][A
 27%|██▋       | 60/221 [00:18<00:52,  3.07it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.33it/s][A
 28%|██▊       | 62/221 [00:18<00:45,  3.51it/s][A
 29%|██▊       | 63/221 [00:19<00:54,  2.91it/s][A
 29%|██▉       | 64/221 [00:19<00:58,  2.70it/s][A
 29%|██▉       | 65/221 [00:20<01:05,  2.39it/s][A
 30%|██▉       | 66/221 [00:20<00:56,  2.72it/s][A
 30%|███       | 67/221 [00:20<01:06,  2.32it/s][A
 31%|███       | 68/221 [00:21<00:53,  2.83it/s][A
 31%|███       | 69/221 [00:21<00:51,  2.93it/s][A
 32%|███▏      | 70/221 [00:21<00:47,  3.15it/s][A
 32%|███▏      | 71/221 [00:21<00:46,  3.25it/s][A
 33%|███▎      | 72/221 [00:22<00:46,  3.19it/s][A
 33%|███▎      | 73/221 [00:22<00:51,  2.87it/s][A
 33%|███▎      | 74/221 [00:23<00:48,  3.03it/s][A
 34%|███▍      | 75/221 [00:23<00:46,  3.11it/s][A
 34%|███▍      | 76/221 [00:23<00:38,  3.81it/s][A
 35%|███▍      | 77/221 [00:23<00:34,  4.21it/s][A
 35%|███▌      | 78/221 [00:23<00:40,  3.53it/s][A
 36%|███▌      | 79/221 [00:24<00:58,  2.45it/s][A
 36%|███▌      | 80/221 [00:25<00:57,  2.44it/s][A
 37%|███▋      | 81/221 [00:25<00:49,  2.81it/s][A
 37%|███▋      | 82/221 [00:25<00:55,  2.51it/s][A
 38%|███▊      | 83/221 [00:26<01:01,  2.24it/s][A
 38%|███▊      | 84/221 [00:26<01:03,  2.15it/s][A
 38%|███▊      | 85/221 [00:27<00:54,  2.52it/s][A
 39%|███▉      | 86/221 [00:27<00:56,  2.38it/s][A
 39%|███▉      | 87/221 [00:28<01:09,  1.93it/s][A
 40%|███▉      | 88/221 [00:28<00:57,  2.32it/s][A
 40%|████      | 89/221 [00:28<00:52,  2.52it/s][A
 41%|████      | 90/221 [00:29<00:43,  2.98it/s][A
 41%|████      | 91/221 [00:29<00:35,  3.68it/s][A
 42%|████▏     | 92/221 [00:29<00:31,  4.16it/s][A
 42%|████▏     | 93/221 [00:29<00:28,  4.57it/s][A
 43%|████▎     | 94/221 [00:29<00:26,  4.73it/s][A
 43%|████▎     | 95/221 [00:30<00:32,  3.83it/s][A
 43%|████▎     | 96/221 [00:30<00:30,  4.11it/s][A
 44%|████▍     | 97/221 [00:30<00:26,  4.70it/s][A
 44%|████▍     | 98/221 [00:30<00:34,  3.52it/s][A
 45%|████▍     | 99/221 [00:31<00:28,  4.29it/s][A
 45%|████▌     | 100/221 [00:31<00:25,  4.78it/s][A
 46%|████▌     | 101/221 [00:31<00:23,  5.02it/s][A
 46%|████▌     | 102/221 [00:31<00:23,  5.06it/s][A
 47%|████▋     | 104/221 [00:31<00:18,  6.49it/s][A
 48%|████▊     | 105/221 [00:31<00:18,  6.27it/s][A
 48%|████▊     | 106/221 [00:32<00:23,  4.99it/s][A
 48%|████▊     | 107/221 [00:32<00:22,  4.96it/s][A
 49%|████▉     | 108/221 [00:32<00:27,  4.10it/s][A
 49%|████▉     | 109/221 [00:33<00:40,  2.76it/s][A
 50%|████▉     | 110/221 [00:33<00:39,  2.84it/s][A
 50%|█████     | 111/221 [00:34<00:40,  2.69it/s][A
 51%|█████     | 112/221 [00:34<00:35,  3.04it/s][A
 51%|█████     | 113/221 [00:34<00:34,  3.15it/s][A
 52%|█████▏    | 115/221 [00:35<00:24,  4.29it/s][A
 53%|█████▎    | 117/221 [00:35<00:22,  4.71it/s][A
 53%|█████▎    | 118/221 [00:35<00:29,  3.54it/s][A
 54%|█████▍    | 119/221 [00:36<00:26,  3.82it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.27it/s][A
 55%|█████▍    | 121/221 [00:36<00:26,  3.72it/s][A
 55%|█████▌    | 122/221 [00:36<00:23,  4.15it/s][A
 56%|█████▌    | 123/221 [00:37<00:24,  3.94it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.32it/s][A
 57%|█████▋    | 125/221 [00:38<00:38,  2.48it/s][A
 57%|█████▋    | 126/221 [00:38<00:36,  2.58it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.30it/s][A
 58%|█████▊    | 128/221 [00:38<00:28,  3.23it/s][A
 58%|█████▊    | 129/221 [00:39<00:25,  3.64it/s][A
 59%|█████▉    | 130/221 [00:39<00:31,  2.92it/s][A
 59%|█████▉    | 131/221 [00:39<00:26,  3.44it/s][A
 60%|█████▉    | 132/221 [00:40<00:22,  3.90it/s][A
 60%|██████    | 133/221 [00:40<00:31,  2.82it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.32it/s][A
 61%|██████    | 135/221 [00:41<00:29,  2.94it/s][A
 62%|██████▏   | 136/221 [00:41<00:30,  2.81it/s][A
 62%|██████▏   | 137/221 [00:41<00:28,  2.99it/s][A
 62%|██████▏   | 138/221 [00:42<00:27,  2.97it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.36it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.28it/s][A
 64%|██████▍   | 141/221 [00:42<00:19,  4.06it/s][A
 64%|██████▍   | 142/221 [00:43<00:20,  3.89it/s][A
 65%|██████▌   | 144/221 [00:43<00:15,  4.94it/s][A
 66%|██████▌   | 145/221 [00:43<00:15,  5.05it/s][A
 66%|██████▌   | 146/221 [00:43<00:15,  4.87it/s][A
 67%|██████▋   | 147/221 [00:43<00:13,  5.51it/s][A
 67%|██████▋   | 148/221 [00:44<00:17,  4.29it/s][A
 67%|██████▋   | 149/221 [00:44<00:15,  4.61it/s][A
 68%|██████▊   | 150/221 [00:45<00:22,  3.15it/s][A
 68%|██████▊   | 151/221 [00:45<00:19,  3.62it/s][A
 69%|██████▉   | 152/221 [00:46<00:29,  2.34it/s][A
 69%|██████▉   | 153/221 [00:46<00:30,  2.24it/s][A
 70%|██████▉   | 154/221 [00:46<00:30,  2.22it/s][A
 70%|███████   | 155/221 [00:47<00:26,  2.53it/s][A
 71%|███████   | 156/221 [00:47<00:21,  2.96it/s][A
 71%|███████   | 157/221 [00:47<00:21,  2.92it/s][A
 71%|███████▏  | 158/221 [00:48<00:21,  2.97it/s][A
 72%|███████▏  | 159/221 [00:48<00:20,  3.08it/s][A
 72%|███████▏  | 160/221 [00:48<00:19,  3.06it/s][A
 73%|███████▎  | 161/221 [00:48<00:15,  3.78it/s][A
 74%|███████▍  | 163/221 [00:49<00:12,  4.64it/s][A
 74%|███████▍  | 164/221 [00:49<00:14,  4.05it/s][A
 75%|███████▌  | 166/221 [00:50<00:13,  3.94it/s][A
 76%|███████▌  | 167/221 [00:50<00:12,  4.27it/s][A
 76%|███████▌  | 168/221 [00:50<00:13,  4.07it/s][A
 76%|███████▋  | 169/221 [00:50<00:13,  3.76it/s][A
 77%|███████▋  | 170/221 [00:51<00:17,  2.88it/s][A
 77%|███████▋  | 171/221 [00:51<00:17,  2.84it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.27it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.17it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.17it/s][A
 80%|███████▉  | 176/221 [00:53<00:14,  3.00it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.51it/s][A
 81%|████████  | 179/221 [00:54<00:13,  3.14it/s][A
 81%|████████▏ | 180/221 [00:54<00:11,  3.57it/s][A
 82%|████████▏ | 181/221 [00:54<00:09,  4.32it/s][A
 82%|████████▏ | 182/221 [00:54<00:09,  4.06it/s][A
 83%|████████▎ | 183/221 [00:54<00:08,  4.39it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.13it/s][A
 84%|████████▎ | 185/221 [00:55<00:10,  3.54it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.36it/s][A
 85%|████████▍ | 187/221 [00:56<00:08,  4.19it/s][A
 85%|████████▌ | 188/221 [00:56<00:10,  3.22it/s][A
 86%|████████▌ | 189/221 [00:57<00:12,  2.61it/s][A
 86%|████████▌ | 190/221 [00:57<00:12,  2.44it/s][A
 86%|████████▋ | 191/221 [00:57<00:10,  2.81it/s][A
 87%|████████▋ | 192/221 [00:58<00:09,  3.18it/s][A
 87%|████████▋ | 193/221 [00:58<00:07,  3.57it/s][A
 88%|████████▊ | 194/221 [00:58<00:11,  2.45it/s][A
 88%|████████▊ | 195/221 [00:59<00:11,  2.25it/s][A
 89%|████████▊ | 196/221 [01:00<00:12,  2.04it/s][A
 89%|████████▉ | 197/221 [01:00<00:10,  2.33it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.53it/s][A
 90%|█████████ | 200/221 [01:01<00:08,  2.62it/s][A
 91%|█████████ | 201/221 [01:01<00:07,  2.79it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.41it/s][A
 92%|█████████▏| 203/221 [01:01<00:04,  3.96it/s][A
 92%|█████████▏| 204/221 [01:02<00:06,  2.63it/s][A
 93%|█████████▎| 205/221 [01:02<00:05,  3.11it/s][A
 93%|█████████▎| 206/221 [01:03<00:05,  2.74it/s][A
 94%|█████████▎| 207/221 [01:03<00:04,  3.30it/s][A
 94%|█████████▍| 208/221 [01:03<00:04,  2.88it/s][A
 95%|█████████▍| 209/221 [01:04<00:04,  2.55it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.25it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.03it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.46it/s][A
 96%|█████████▋| 213/221 [01:05<00:03,  2.38it/s][A
 97%|█████████▋| 214/221 [01:06<00:03,  2.26it/s][A
 97%|█████████▋| 215/221 [01:06<00:02,  2.20it/s][A
 98%|█████████▊| 216/221 [01:06<00:02,  2.48it/s][A
 98%|█████████▊| 217/221 [01:07<00:01,  2.63it/s][A
 99%|█████████▊| 218/221 [01:07<00:00,  3.08it/s][A
 99%|█████████▉| 219/221 [01:07<00:00,  3.78it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.89it/s][A
100%|██████████| 221/221 [01:08<00:00,  3.17it/s][A100%|██████████| 221/221 [01:08<00:00,  3.24it/s]
09/18/2024 18:13:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 499--===========

09/18/2024 18:13:47 - INFO - __main__ -   {'area_r1': 39.9, 'area_recall': '39.9/66.4/75.0', 'area_ravg': 60.4}
09/18/2024 18:13:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 499--===========

09/18/2024 18:13:47 - INFO - __main__ -   {'forward_r1': 36.0, 'forward_recall': '36.0/65.2/75.3', 'forward_ravg': 58.8}
09/18/2024 18:13:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 499--===========

09/18/2024 18:13:47 - INFO - __main__ -   {'area_video_r1': 39.0, 'area_video_recall': '39.0/66.7/76.6', 'area_video_ravg': 60.8}
09/18/2024 18:13:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 18:13:47 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 18:13:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 499--===========

09/18/2024 18:13:47 - INFO - __main__ -   {'area_video_r1': 52.6, 'area_video_recall': '52.6/73.1/81.2', 'area_video_ravg': 69.0, 'area_video_back_r1': 49.5, 'area_video_back_recall': '49.5/71.5/78.2', 'area_video_back_ravg': 66.4}
09/18/2024 18:13:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 499=======

09/18/2024 18:13:47 - INFO - __main__ -   {'area_video_r1': 52.6, 'area_video_recall': '52.6/73.1/81.2', 'area_video_ravg': 69.0, 'area_video_back_r1': 49.5, 'area_video_back_recall': '49.5/71.5/78.2', 'area_video_back_ravg': 66.4}
09/18/2024 18:13:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 499--===========

09/18/2024 18:13:47 - INFO - __main__ -   {'video_r1': 28.7, 'video_recall': '28.7/52.5/63.2', 'video_ravg': 48.2}
09/18/2024 18:13:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 18:13:47 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 18:13:47 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 499--===========

09/18/2024 18:13:47 - INFO - __main__ -   {'video_r1': 49.7, 'video_recall': '49.7/69.5/76.4', 'video_ravg': 65.2}
09/18/2024 18:13:47 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 18:13:47 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 18:14:13 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0004650571499951184, 'loss_ret%tv%ta--finetune_area/loss_area': 1.2701078653335571, 'loss_ret%tv%ta--finetune_area/total_loss': 1.2705729007720947}
[h264 @ 0x55cb71e16e00] mmco: unref short failure
[h264 @ 0x55cb71e16e00] mmco: unref short failure
[h264 @ 0x55cb71e16e00] mmco: unref short failure
[h264 @ 0x55cb71e16e00] mmco: unref short failure
[h264 @ 0x55d914d52dc0] mmco: unref short failure
 17%|█▋        | 500/2910 [3:09:06<92:41:12, 138.45s/it] 17%|█▋        | 501/2910 [3:09:09<65:33:54, 97.98s/it] [h264 @ 0x55d8fe9aa100] mmco: unref short failure
[h264 @ 0x55d8fe9aa100] mmco: unref short failure
 17%|█▋        | 502/2910 [3:09:13<46:40:58, 69.79s/it] 17%|█▋        | 503/2910 [3:09:18<33:31:00, 50.13s/it] 17%|█▋        | 504/2910 [3:09:22<24:22:27, 36.47s/it]09/18/2024 18:14:32 - INFO - __main__ -   current idx OTXYBNGzWpQ.110 from finetune_area returns wrong image/video, use 97549 instead.
[h264 @ 0x5565fffc35c0] mmco: unref short failure
 17%|█▋        | 505/2910 [3:09:27<17:58:39, 26.91s/it] 17%|█▋        | 506/2910 [3:09:32<13:33:38, 20.31s/it][h264 @ 0x5598e0a42300] mmco: unref short failure
 17%|█▋        | 507/2910 [3:09:38<10:42:52, 16.05s/it] 17%|█▋        | 508/2910 [3:09:43<8:34:32, 12.85s/it]  17%|█▋        | 509/2910 [3:09:50<7:16:23, 10.91s/it] 18%|█▊        | 510/2910 [3:09:55<6:13:19,  9.33s/it][h264 @ 0x55d91c2256c0] mmco: unref short failure
[h264 @ 0x55d91c2256c0] mmco: unref short failure
 18%|█▊        | 511/2910 [3:10:01<5:29:35,  8.24s/it][h264 @ 0x55d90c17f2c0] mmco: unref short failure
[h264 @ 0x55d90c17f2c0] mmco: unref short failure
 18%|█▊        | 512/2910 [3:10:07<5:03:41,  7.60s/it][h264 @ 0x55cb64a3c540] mmco: unref short failure
[h264 @ 0x55cb64a3c540] mmco: unref short failure
[h264 @ 0x55cb64a3c540] mmco: unref short failure
 18%|█▊        | 513/2910 [3:10:12<4:37:02,  6.93s/it] 18%|█▊        | 514/2910 [3:10:18<4:21:25,  6.55s/it][h264 @ 0x5565eac378c0] mmco: unref short failure
[h264 @ 0x5565eac378c0] mmco: unref short failure
 18%|█▊        | 515/2910 [3:10:24<4:17:08,  6.44s/it][h264 @ 0x55d90b285540] mmco: unref short failure
[h264 @ 0x55d8fea23180] mmco: unref short failure
[h264 @ 0x55d8fea23180] mmco: unref short failure
[h264 @ 0x55d8fea23180] mmco: unref short failure
[h264 @ 0x55d8fea23180] mmco: unref short failure
[h264 @ 0x55cb7c1a28c0] mmco: unref short failure
[h264 @ 0x55cb7c1a28c0] mmco: unref short failure
[h264 @ 0x5598f2cf04c0] mmco: unref short failure
[h264 @ 0x5598f2cf04c0] mmco: unref short failure
[h264 @ 0x55d8febb1bc0] mmco: unref short failure
[h264 @ 0x55d8febb1bc0] mmco: unref short failure
[h264 @ 0x55d8fea18280] mmco: unref short failure
[h264 @ 0x5565f61907c0] mmco: unref short failure
[h264 @ 0x55d8feaa2040] mmco: unref short failure
[h264 @ 0x55d8feaa2040] mmco: unref short failure
[h264 @ 0x55d8feaa2040] mmco: unref short failure
[h264 @ 0x55d8feaa2040] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55d9180bda80] mmco: unref short failure
[h264 @ 0x55d90b45e200] mmco: unref short failure
[h264 @ 0x55d90b45e200] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x5565ed881680] mmco: unref short failure
 18%|█▊        | 516/2910 [3:11:27<15:32:04, 23.36s/it][h264 @ 0x55d920fa5b80] mmco: unref short failure
[h264 @ 0x55d920fa5b80] mmco: unref short failure
[h264 @ 0x55d9138f1c40] mmco: unref short failure
 18%|█▊        | 517/2910 [3:11:35<12:25:42, 18.70s/it][h264 @ 0x5565f15d32c0] mmco: unref short failure
 18%|█▊        | 518/2910 [3:11:40<9:44:54, 14.67s/it] [h264 @ 0x5565e5fc4d00] mmco: unref short failure
[h264 @ 0x5565e5fc4d00] mmco: unref short failure
[h264 @ 0x5565e5fc4d00] mmco: unref short failure
[h264 @ 0x5598dc657800] mmco: unref short failure
[h264 @ 0x5598dc657800] mmco: unref short failure
[h264 @ 0x556605860840] mmco: unref short failure
[h264 @ 0x556605860840] mmco: unref short failure
[h264 @ 0x55d920fa5b80] mmco: unref short failure
[h264 @ 0x556601d31d80] mmco: unref short failure
 18%|█▊        | 519/2910 [3:11:47<8:11:54, 12.34s/it][h264 @ 0x55cb725bc2c0] mmco: unref short failure
[h264 @ 0x55cb725bc2c0] mmco: unref short failure
 18%|█▊        | 520/2910 [3:11:53<6:53:10, 10.37s/it][h264 @ 0x5598e92431c0] mmco: unref short failure
[h264 @ 0x5598e92431c0] mmco: unref short failure
[h264 @ 0x5565ff4310c0] mmco: unref short failure
[h264 @ 0x5565ff4310c0] mmco: unref short failure
 18%|█▊        | 521/2910 [3:12:08<7:45:39, 11.69s/it][h264 @ 0x5565ed881680] mmco: unref short failure
 18%|█▊        | 522/2910 [3:12:13<6:28:40,  9.77s/it][h264 @ 0x5566043dfa40] mmco: unref short failure
 18%|█▊        | 523/2910 [3:12:28<7:31:39, 11.35s/it][h264 @ 0x5565f4bb8440] mmco: unref short failure
[h264 @ 0x5565f4bb8440] mmco: unref short failure
[h264 @ 0x5598dc91ea80] mmco: unref short failure
[h264 @ 0x55cb5f2e87c0] mmco: unref short failure
[h264 @ 0x5598e2064ac0] mmco: unref short failure
[h264 @ 0x55cb6077a6c0] mmco: unref short failure
[h264 @ 0x55cb6077a6c0] mmco: unref short failure
09/18/2024 18:18:07 - INFO - __main__ -   current idx ZJ3l8xjgJQw.15 from finetune_area returns wrong image/video, use 19056 instead.
[h264 @ 0x5598e98f3300] mmco: unref short failure
[h264 @ 0x5598e98f3300] mmco: unref short failure
[h264 @ 0x55d900928b80] mmco: unref short failure
[h264 @ 0x5598dc911500] mmco: unref short failure
[h264 @ 0x5598f4a23ec0] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x5565e7a75480] mmco: unref short failure
[h264 @ 0x5565e85b6800] mmco: unref short failure
[h264 @ 0x5565e85b6800] mmco: unref short failure
[h264 @ 0x55d90977ec40] mmco: unref short failure
[h264 @ 0x55d90977ec40] mmco: unref short failure
[h264 @ 0x55d90977ec40] mmco: unref short failure
[h264 @ 0x55d90977ec40] mmco: unref short failure
[h264 @ 0x55cb6077a6c0] mmco: unref short failure
 18%|█▊        | 524/2910 [3:13:54<22:26:59, 33.87s/it] 18%|█▊        | 525/2910 [3:14:06<18:01:45, 27.21s/it][h264 @ 0x55d8ff256840] mmco: unref short failure
[h264 @ 0x55d8ff256840] mmco: unref short failure
09/18/2024 18:19:16 - INFO - __main__ -   current idx cXoOWKkZ_K0.13 from finetune_area returns wrong image/video, use 51075 instead.
[h264 @ 0x55d9101839c0] mmco: unref short failure
[h264 @ 0x5566043dfa40] mmco: unref short failure
 18%|█▊        | 526/2910 [3:14:12<13:43:22, 20.72s/it][h264 @ 0x5565f5b74b00] mmco: unref short failure
[h264 @ 0x5565f5b74b00] mmco: unref short failure
[h264 @ 0x5565f5b74b00] mmco: unref short failure
[h264 @ 0x5565f5b74b00] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
 18%|█▊        | 527/2910 [3:14:23<11:54:43, 18.00s/it] 18%|█▊        | 528/2910 [3:14:29<9:28:33, 14.32s/it] [h264 @ 0x55cb6077a6c0] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
 18%|█▊        | 529/2910 [3:14:34<7:38:15, 11.55s/it] 18%|█▊        | 530/2910 [3:14:40<6:25:48,  9.73s/it][h264 @ 0x55d8fea76f40] mmco: unref short failure
[h264 @ 0x55d8fea76f40] mmco: unref short failure
[h264 @ 0x55cb6123f840] mmco: unref short failure
[h264 @ 0x55cb6123f840] mmco: unref short failure
[h264 @ 0x5565f3ddcf00] mmco: unref short failure
[h264 @ 0x5565f3ddcf00] mmco: unref short failure
[h264 @ 0x55cb5ec16580] mmco: unref short failure
[h264 @ 0x55cb5ec16580] mmco: unref short failure
[h264 @ 0x55cb5ec16580] mmco: unref short failure
[h264 @ 0x55cb5ec16580] mmco: unref short failure
 18%|█▊        | 531/2910 [3:15:05<9:31:03, 14.40s/it][h264 @ 0x5598dea6ec00] mmco: unref short failure
[h264 @ 0x5566043dfa40] mmco: unref short failure
[h264 @ 0x5566043dfa40] mmco: unref short failure
[h264 @ 0x5598e3ceb0c0] mmco: unref short failure
[h264 @ 0x5598e3ceb0c0] mmco: unref short failure
[h264 @ 0x55d9129a3f40] mmco: unref short failure
[h264 @ 0x55cb8091c300] mmco: unref short failure
[h264 @ 0x55cb8091c300] mmco: unref short failure
[h264 @ 0x55cb7118ff40] mmco: unref short failure
[h264 @ 0x55cb7118ff40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x55cb6f48ec40] mmco: unref short failure
[h264 @ 0x5565f1538c40] mmco: unref short failure
[h264 @ 0x5565f1538c40] mmco: unref short failure
[h264 @ 0x5598dea6ec00] mmco: unref short failure
09/18/2024 18:21:21 - INFO - __main__ -   current idx EMPLLx_tfq8.250 from finetune_area returns wrong image/video, use 27288 instead.
[h264 @ 0x5565ffedb1c0] mmco: unref short failure
09/18/2024 18:21:31 - INFO - __main__ -   current idx hvuxWKYHJlQ.82 from finetune_area returns wrong image/video, use 113831 instead.
 18%|█▊        | 532/2910 [3:16:28<23:07:58, 35.02s/it]09/18/2024 18:21:39 - INFO - __main__ -   current idx 4Svr6BsWgeU.14 from finetune_area returns wrong image/video, use 37350 instead.
 18%|█▊        | 533/2910 [3:16:33<17:15:14, 26.13s/it]09/18/2024 18:21:45 - INFO - __main__ -   current idx DfKw_sMqEaA.16 from finetune_area returns wrong image/video, use 33748 instead.
[h264 @ 0x5565ec93e780] mmco: unref short failure
[h264 @ 0x5565ec93e780] mmco: unref short failure
 18%|█▊        | 534/2910 [3:16:39<13:06:24, 19.86s/it][h264 @ 0x55cb6123f840] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5598dc88f280] mmco: unref short failure
[h264 @ 0x5598dc88f280] mmco: unref short failure
[h264 @ 0x5598dc88f280] mmco: unref short failure
[h264 @ 0x5598dc88f280] mmco: unref short failure
 18%|█▊        | 535/2910 [3:16:49<11:16:41, 17.10s/it][h264 @ 0x55cb700fd940] mmco: unref short failure
[h264 @ 0x5566005b6200] mmco: unref short failure
 18%|█▊        | 536/2910 [3:16:54<8:54:32, 13.51s/it]  18%|█▊        | 537/2910 [3:17:00<7:25:52, 11.27s/it][h264 @ 0x55d908e94a80] mmco: unref short failure
[h264 @ 0x55cb67bc4200] mmco: unref short failure
[h264 @ 0x5565ecac09c0] mmco: unref short failure
[h264 @ 0x5565ecac09c0] mmco: unref short failure
 18%|█▊        | 538/2910 [3:17:08<6:36:24, 10.03s/it][h264 @ 0x5565ecac09c0] mmco: unref short failure
[h264 @ 0x5565ecac09c0] mmco: unref short failure
[h264 @ 0x5565ef079540] mmco: unref short failure
[h264 @ 0x5565ef079540] mmco: unref short failure
[h264 @ 0x5598f95eeac0] mmco: unref short failure
[h264 @ 0x5598f95eeac0] mmco: unref short failure
[h264 @ 0x55cb6c6e4100] mmco: unref short failure
[h264 @ 0x55d905627f00] mmco: unref short failure
[h264 @ 0x55d9180bda80] mmco: unref short failure
[h264 @ 0x55d9180bda80] mmco: unref short failure
[h264 @ 0x55d9180bda80] mmco: unref short failure
[h264 @ 0x5598e98f3300] mmco: unref short failure
[h264 @ 0x5598e98f3300] mmco: unref short failure
[h264 @ 0x5598e98f3300] mmco: unref short failure
[h264 @ 0x5565f5d26fc0] mmco: unref short failure
 19%|█▊        | 539/2910 [3:17:31<9:09:38, 13.91s/it][h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x55cb605d1a80] mmco: unref short failure
[h264 @ 0x5565fdec0200] mmco: unref short failure
[h264 @ 0x5565fdec0200] mmco: unref short failure
[h264 @ 0x5565e5fc4d00] mmco: unref short failure
[h264 @ 0x5565e5fc4d00] mmco: unref short failure
[h264 @ 0x55cb7d88b700] mmco: unref short failure
[h264 @ 0x5598e042f700] mmco: unref short failure
[h264 @ 0x55cb7e1baf40] mmco: unref short failure
[h264 @ 0x55cb618ca980] mmco: unref short failure
[h264 @ 0x55cb618ca980] mmco: unref short failure
[h264 @ 0x55cb618ca980] mmco: unref short failure
[h264 @ 0x5598e3ceb0c0] mmco: unref short failure
[h264 @ 0x55cb5ec35d40] mmco: unref short failure
[h264 @ 0x55cb5ec35d40] mmco: unref short failure
[h264 @ 0x5598dc84cdc0] mmco: unref short failure
 19%|█▊        | 540/2910 [3:18:54<22:55:17, 34.82s/it][h264 @ 0x5565eade0d80] mmco: unref short failure
[h264 @ 0x5565eade0d80] mmco: unref short failure
09/18/2024 18:24:16 - INFO - __main__ -   current idx 4hOPcrHDBvU.10 from finetune_area returns wrong image/video, use 131957 instead.
[h264 @ 0x55d90de65080] mmco: unref short failure
[h264 @ 0x55d90de65080] mmco: unref short failure
[h264 @ 0x5565e7133800] mmco: unref short failure
[h264 @ 0x5565e7133800] mmco: unref short failure
[h264 @ 0x5565eade0d80] mmco: unref short failure
[h264 @ 0x5565eade0d80] mmco: unref short failure
[h264 @ 0x55d900e79300] mmco: unref short failure
 19%|█▊        | 541/2910 [3:19:30<23:05:31, 35.09s/it] 19%|█▊        | 542/2910 [3:19:35<17:06:06, 26.00s/it] 19%|█▊        | 543/2910 [3:19:40<12:56:48, 19.69s/it] 19%|█▊        | 544/2910 [3:19:44<10:00:06, 15.22s/it][h264 @ 0x55cb5ec35d40] mmco: unref short failure
[h264 @ 0x55cb5ec35d40] mmco: unref short failure
[h264 @ 0x5565e7133800] mmco: unref short failure
 19%|█▊        | 545/2910 [3:19:49<7:55:50, 12.07s/it] [h264 @ 0x5598e3f85a40] mmco: unref short failure
[h264 @ 0x5598e2064ac0] mmco: unref short failure
[h264 @ 0x5598e2064ac0] mmco: unref short failure
 19%|█▉        | 546/2910 [3:19:54<6:34:20, 10.01s/it]09/18/2024 18:25:07 - INFO - __main__ -   current idx E852WPZZnME.32 from finetune_area returns wrong image/video, use 83757 instead.
 19%|█▉        | 547/2910 [3:19:59<5:32:14,  8.44s/it][h264 @ 0x55d8fe967240] mmco: unref short failure
[h264 @ 0x55d8fe967240] mmco: unref short failure
[h264 @ 0x55d8fe967240] mmco: unref short failure
[h264 @ 0x55d8fe967240] mmco: unref short failure
[h264 @ 0x55d918501640] mmco: unref short failure
[h264 @ 0x5598fb3ed480] mmco: unref short failure
[h264 @ 0x5598fb3ed480] mmco: unref short failure
09/18/2024 18:25:27 - INFO - __main__ -   current idx _Z6qrnWrgYM.80 from finetune_area returns wrong image/video, use 87635 instead.
[h264 @ 0x5565f7c97200] mmco: unref short failure
[h264 @ 0x5598ea3a37c0] mmco: unref short failure
[h264 @ 0x5598ea3a37c0] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x55d9198ac100] mmco: unref short failure
[h264 @ 0x55d9198ac100] mmco: unref short failure
[h264 @ 0x55d906aa5080] mmco: unref short failure
[h264 @ 0x55d906aa5080] mmco: unref short failure
[h264 @ 0x5565fcf17240] mmco: unref short failure
[h264 @ 0x5598ee7a05c0] mmco: unref short failure
[h264 @ 0x5598ee7a05c0] mmco: unref short failure
[h264 @ 0x5598ee7a05c0] mmco: unref short failure
[h264 @ 0x5598ee7a05c0] mmco: unref short failure
[h264 @ 0x5598fbaa2740] mmco: unref short failure
[h264 @ 0x5598fbaa2740] mmco: unref short failure
[h264 @ 0x5598ed335380] mmco: unref short failure
[h264 @ 0x5598ed335380] mmco: unref short failure
[h264 @ 0x55d919557440] mmco: unref short failure
[h264 @ 0x55cb78762000] mmco: unref short failure
 19%|█▉        | 548/2910 [3:21:22<20:11:30, 30.78s/it][h264 @ 0x5565e4834600] mmco: unref short failure
[h264 @ 0x5565e4834600] mmco: unref short failure
[h264 @ 0x5565ec50e200] mmco: unref short failure
[h264 @ 0x5565f0fe3b40] mmco: unref short failure
[h264 @ 0x5565f0fe3b40] mmco: unref short failure
[h264 @ 0x5565f0fe3b40] mmco: unref short failure
[h264 @ 0x5565f0fe3b40] mmco: unref short failure
[h264 @ 0x5565f941cc00] mmco: unref short failure
[h264 @ 0x5565f941cc00] mmco: unref short failure
[h264 @ 0x55d919557440] mmco: unref short failure
[h264 @ 0x55d919557440] mmco: unref short failure
[h264 @ 0x55d919557440] mmco: unref short failure
[h264 @ 0x55d919557440] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5565e5f09380] mmco: unref short failure
[h264 @ 0x5565f3ddcf00] mmco: unref short failure
[h264 @ 0x5565f3ddcf00] mmco: unref short failure
[h264 @ 0x5598dc7eff00] mmco: unref short failure
[h264 @ 0x5598dc7eff00] mmco: unref short failure
[h264 @ 0x55d902e3f0c0] mmco: unref short failure
[h264 @ 0x55d908e94a80] mmco: unref short failure
[h264 @ 0x55d908e94a80] mmco: unref short failure
 19%|█▉        | 549/2910 [3:21:54<20:27:05, 31.18s/it]09/18/2024 18:27:04 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 18:27:04 - INFO - __main__ -   start running ret%tvas validation...
[h264 @ 0x5565ef890500] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55d8ffac24c0] mmco: unref short failure
[h264 @ 0x55d8ffac24c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55cb602a8f40] mmco: unref short failure
[h264 @ 0x55cb602a8f40] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x5565ec5e58c0] mmco: unref short failure
[h264 @ 0x55cb8091c300] mmco: unref short failure
[h264 @ 0x5598e84e1080] mmco: unref short failure
[h264 @ 0x5565f7c97200] mmco: unref short failure
[h264 @ 0x5565f7c97200] mmco: unref short failure
[h264 @ 0x5565f4bb8440] mmco: unref short failure
[h264 @ 0x5565f4bb8440] mmco: unref short failure
[h264 @ 0x5565e65a3a00] mmco: unref short failure
[h264 @ 0x55d914371980] mmco: unref short failure
[h264 @ 0x55cb6505a2c0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<02:54,  1.26it/s][A
  1%|          | 2/221 [00:01<02:16,  1.61it/s][A
  1%|▏         | 3/221 [00:01<01:42,  2.13it/s][A[h264 @ 0x55d8fead9640] mmco: unref short failure
[h264 @ 0x55d8fead9640] mmco: unref short failure

  2%|▏         | 4/221 [00:01<01:17,  2.81it/s][A
  2%|▏         | 5/221 [00:02<01:18,  2.76it/s][A
  3%|▎         | 6/221 [00:02<01:14,  2.90it/s][A
  3%|▎         | 7/221 [00:02<01:09,  3.08it/s][A
  4%|▎         | 8/221 [00:03<01:14,  2.88it/s][A[h264 @ 0x5598e8bac240] mmco: unref short failure
[h264 @ 0x5598e8bac240] mmco: unref short failure

  4%|▍         | 9/221 [00:03<01:13,  2.87it/s][A
  5%|▍         | 10/221 [00:04<01:30,  2.33it/s][A
  5%|▍         | 11/221 [00:04<01:18,  2.68it/s][A
  5%|▌         | 12/221 [00:04<01:32,  2.25it/s][A[h264 @ 0x55cb78d8ec80] mmco: unref short failure
[h264 @ 0x55cb78d8ec80] mmco: unref short failure

  6%|▌         | 13/221 [00:05<01:26,  2.39it/s][A
  6%|▋         | 14/221 [00:06<02:02,  1.69it/s][A
  7%|▋         | 15/221 [00:06<01:34,  2.17it/s][A
  7%|▋         | 16/221 [00:06<01:29,  2.30it/s][A
  8%|▊         | 17/221 [00:07<01:56,  1.76it/s][A
  8%|▊         | 18/221 [00:07<01:38,  2.06it/s][A
  9%|▊         | 19/221 [00:08<01:16,  2.65it/s][A
  9%|▉         | 20/221 [00:08<01:04,  3.13it/s][A
 10%|▉         | 21/221 [00:08<00:57,  3.46it/s][A
 10%|▉         | 22/221 [00:08<00:54,  3.66it/s][A
 10%|█         | 23/221 [00:08<00:43,  4.51it/s][A
 11%|█         | 24/221 [00:09<00:45,  4.31it/s][A
 11%|█▏        | 25/221 [00:09<00:43,  4.46it/s][A
 12%|█▏        | 26/221 [00:09<00:59,  3.28it/s][A
 12%|█▏        | 27/221 [00:09<00:49,  3.89it/s][A
 13%|█▎        | 28/221 [00:10<01:02,  3.07it/s][A
 13%|█▎        | 29/221 [00:11<01:34,  2.04it/s][A
 14%|█▎        | 30/221 [00:11<01:23,  2.29it/s][A
 14%|█▍        | 31/221 [00:11<01:14,  2.54it/s][A
 15%|█▍        | 33/221 [00:12<00:51,  3.67it/s][A
 15%|█▌        | 34/221 [00:12<00:50,  3.70it/s][A
 16%|█▌        | 35/221 [00:12<00:43,  4.28it/s][A
 16%|█▋        | 36/221 [00:13<00:53,  3.49it/s][A
 17%|█▋        | 37/221 [00:13<01:08,  2.67it/s][A
 17%|█▋        | 38/221 [00:13<01:08,  2.68it/s][A
 18%|█▊        | 39/221 [00:14<01:05,  2.76it/s][A
 18%|█▊        | 40/221 [00:14<01:12,  2.51it/s][A
 19%|█▊        | 41/221 [00:14<00:57,  3.11it/s][A
 19%|█▉        | 42/221 [00:15<01:03,  2.83it/s][A
 19%|█▉        | 43/221 [00:15<00:50,  3.53it/s][A
 20%|██        | 45/221 [00:16<01:24,  2.09it/s][A
 21%|██        | 46/221 [00:17<01:23,  2.10it/s][A
 21%|██▏       | 47/221 [00:17<01:26,  2.02it/s][A
 22%|██▏       | 48/221 [00:18<01:08,  2.51it/s][A
 22%|██▏       | 49/221 [00:19<01:46,  1.62it/s][A
 23%|██▎       | 50/221 [00:19<01:38,  1.73it/s][A
 24%|██▎       | 52/221 [00:19<01:03,  2.67it/s][A
 24%|██▍       | 53/221 [00:20<00:52,  3.19it/s][A[h264 @ 0x5565f0c9e5c0] mmco: unref short failure
[h264 @ 0x5598dc9ca900] mmco: unref short failure

 24%|██▍       | 54/221 [00:22<02:11,  1.27it/s][A[h264 @ 0x55d8fea76f40] mmco: unref short failure
[h264 @ 0x55d8fea76f40] mmco: unref short failure

 25%|██▍       | 55/221 [00:22<01:51,  1.49it/s][A
 25%|██▌       | 56/221 [00:22<01:31,  1.81it/s][A
 26%|██▌       | 57/221 [00:23<01:14,  2.20it/s][A
 26%|██▌       | 58/221 [00:23<01:02,  2.60it/s][A[h264 @ 0x55cb722b8140] mmco: unref short failure

 27%|██▋       | 59/221 [00:23<00:54,  2.98it/s][A[h264 @ 0x55cb79086bc0] mmco: unref short failure

 27%|██▋       | 60/221 [00:24<01:09,  2.31it/s][A
 28%|██▊       | 61/221 [00:24<00:58,  2.72it/s][A
 28%|██▊       | 62/221 [00:24<00:53,  2.96it/s][A
 29%|██▊       | 63/221 [00:24<00:54,  2.91it/s][A
 29%|██▉       | 64/221 [00:25<00:45,  3.48it/s][A
 29%|██▉       | 65/221 [00:25<00:41,  3.80it/s][A
 30%|██▉       | 66/221 [00:25<00:46,  3.32it/s][A
 30%|███       | 67/221 [00:26<00:59,  2.58it/s][A
 31%|███       | 68/221 [00:26<00:48,  3.13it/s][A
 31%|███       | 69/221 [00:27<01:14,  2.03it/s][A
 32%|███▏      | 70/221 [00:27<01:02,  2.42it/s][A
 32%|███▏      | 71/221 [00:28<01:19,  1.88it/s][A
 33%|███▎      | 72/221 [00:28<01:11,  2.09it/s][A
 33%|███▎      | 73/221 [00:29<01:02,  2.35it/s][A
 33%|███▎      | 74/221 [00:29<00:50,  2.93it/s][A
 34%|███▍      | 75/221 [00:29<00:53,  2.73it/s][A
 34%|███▍      | 76/221 [00:29<00:44,  3.28it/s][A
 35%|███▍      | 77/221 [00:30<00:43,  3.29it/s][A
 35%|███▌      | 78/221 [00:30<00:40,  3.55it/s][A
 36%|███▌      | 79/221 [00:30<00:50,  2.82it/s][A
 36%|███▌      | 80/221 [00:31<00:42,  3.34it/s][A
 37%|███▋      | 81/221 [00:31<00:38,  3.61it/s][A
 37%|███▋      | 82/221 [00:32<01:03,  2.20it/s][A
 38%|███▊      | 83/221 [00:32<01:07,  2.04it/s][A
 38%|███▊      | 84/221 [00:32<00:55,  2.48it/s][A
 38%|███▊      | 85/221 [00:33<00:42,  3.20it/s][A
 39%|███▉      | 86/221 [00:33<00:42,  3.21it/s][A
 39%|███▉      | 87/221 [00:33<00:51,  2.58it/s][A
 40%|███▉      | 88/221 [00:34<00:53,  2.51it/s][A
 40%|████      | 89/221 [00:34<00:54,  2.40it/s][A
 41%|████      | 90/221 [00:34<00:46,  2.84it/s][A
 41%|████      | 91/221 [00:35<00:36,  3.60it/s][A
 42%|████▏     | 92/221 [00:35<00:32,  4.01it/s][A
 42%|████▏     | 93/221 [00:35<00:36,  3.49it/s][A
 43%|████▎     | 94/221 [00:35<00:36,  3.48it/s][A
 43%|████▎     | 95/221 [00:36<00:38,  3.25it/s][A
 43%|████▎     | 96/221 [00:36<00:35,  3.52it/s][A
 44%|████▍     | 97/221 [00:36<00:29,  4.16it/s][A
 44%|████▍     | 98/221 [00:36<00:28,  4.28it/s][A
 45%|████▍     | 99/221 [00:36<00:24,  5.08it/s][A
 45%|████▌     | 100/221 [00:37<00:25,  4.76it/s][A
 46%|████▌     | 101/221 [00:37<00:22,  5.27it/s][A
 46%|████▌     | 102/221 [00:37<00:27,  4.28it/s][A
 47%|████▋     | 103/221 [00:37<00:23,  4.99it/s][A
 47%|████▋     | 104/221 [00:38<00:24,  4.74it/s][A
 48%|████▊     | 105/221 [00:38<00:29,  3.96it/s][A
 48%|████▊     | 106/221 [00:39<00:42,  2.68it/s][A
 48%|████▊     | 107/221 [00:39<00:34,  3.33it/s][A
 49%|████▉     | 108/221 [00:39<00:29,  3.84it/s][A
 49%|████▉     | 109/221 [00:39<00:27,  4.05it/s][A
 50%|████▉     | 110/221 [00:39<00:27,  4.07it/s][A
 50%|█████     | 111/221 [00:40<00:32,  3.38it/s][A
 51%|█████     | 112/221 [00:41<00:49,  2.20it/s][A
 51%|█████     | 113/221 [00:41<00:43,  2.47it/s][A
 52%|█████▏    | 115/221 [00:41<00:26,  4.03it/s][A
 52%|█████▏    | 116/221 [00:45<02:09,  1.24s/it][A
 53%|█████▎    | 117/221 [00:45<01:43,  1.01it/s][A
 53%|█████▎    | 118/221 [00:46<01:21,  1.27it/s][A
 54%|█████▍    | 119/221 [00:46<01:03,  1.61it/s][A
 54%|█████▍    | 120/221 [00:46<00:53,  1.90it/s][A
 55%|█████▌    | 122/221 [00:47<00:35,  2.81it/s][A
 56%|█████▌    | 123/221 [00:47<00:29,  3.35it/s][A
 56%|█████▌    | 124/221 [00:47<00:26,  3.73it/s][A
 57%|█████▋    | 125/221 [00:47<00:27,  3.54it/s][A
 57%|█████▋    | 126/221 [00:47<00:24,  3.84it/s][A
 57%|█████▋    | 127/221 [00:48<00:33,  2.80it/s][A
 58%|█████▊    | 128/221 [00:48<00:33,  2.81it/s][A
 58%|█████▊    | 129/221 [00:48<00:28,  3.23it/s][A
 59%|█████▉    | 130/221 [00:49<00:26,  3.42it/s][A
 59%|█████▉    | 131/221 [00:49<00:21,  4.15it/s][A
 60%|█████▉    | 132/221 [00:49<00:17,  5.00it/s][A
 60%|██████    | 133/221 [00:49<00:24,  3.61it/s][A
 61%|██████    | 134/221 [00:50<00:22,  3.95it/s][A[h264 @ 0x5565fcdc4d40] mmco: unref short failure
[h264 @ 0x5565fcdc4d40] mmco: unref short failure

 61%|██████    | 135/221 [00:50<00:26,  3.30it/s][A
 62%|██████▏   | 136/221 [00:50<00:27,  3.11it/s][A
 62%|██████▏   | 137/221 [00:51<00:23,  3.53it/s][A
 62%|██████▏   | 138/221 [00:51<00:25,  3.28it/s][A
 63%|██████▎   | 139/221 [00:51<00:26,  3.13it/s][A
 63%|██████▎   | 140/221 [00:52<00:25,  3.15it/s][A
 64%|██████▍   | 141/221 [00:52<00:22,  3.57it/s][A
 64%|██████▍   | 142/221 [00:52<00:23,  3.41it/s][A
 65%|██████▍   | 143/221 [00:53<00:26,  2.99it/s][A
 65%|██████▌   | 144/221 [00:53<00:22,  3.36it/s][A
 66%|██████▌   | 146/221 [00:53<00:14,  5.26it/s][A
 67%|██████▋   | 147/221 [00:53<00:14,  5.24it/s][A
 67%|██████▋   | 148/221 [00:53<00:15,  4.67it/s][A
 67%|██████▋   | 149/221 [00:53<00:13,  5.45it/s][A
 68%|██████▊   | 150/221 [00:54<00:13,  5.39it/s][A
 68%|██████▊   | 151/221 [00:54<00:23,  2.96it/s][A
 69%|██████▉   | 152/221 [00:56<00:41,  1.67it/s][A
 69%|██████▉   | 153/221 [00:56<00:34,  1.97it/s][A
 70%|██████▉   | 154/221 [00:56<00:29,  2.26it/s][A
 70%|███████   | 155/221 [00:56<00:23,  2.86it/s][A
 71%|███████   | 156/221 [00:57<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:58<00:33,  1.93it/s][A
 71%|███████▏  | 158/221 [00:58<00:26,  2.38it/s][A
 72%|███████▏  | 159/221 [00:58<00:21,  2.89it/s][A
 72%|███████▏  | 160/221 [00:58<00:20,  3.00it/s][A
 73%|███████▎  | 162/221 [01:00<00:28,  2.07it/s][A
 74%|███████▍  | 163/221 [01:00<00:24,  2.36it/s][A
 74%|███████▍  | 164/221 [01:01<00:29,  1.91it/s][A
 75%|███████▍  | 165/221 [01:01<00:23,  2.40it/s][A
 75%|███████▌  | 166/221 [01:01<00:21,  2.53it/s][A
 76%|███████▌  | 167/221 [01:01<00:17,  3.11it/s][A
 76%|███████▌  | 168/221 [01:02<00:27,  1.91it/s][A
 76%|███████▋  | 169/221 [01:02<00:22,  2.34it/s][A
 77%|███████▋  | 170/221 [01:03<00:21,  2.35it/s][A
 77%|███████▋  | 171/221 [01:03<00:20,  2.50it/s][A
 78%|███████▊  | 172/221 [01:03<00:16,  2.91it/s][A
 78%|███████▊  | 173/221 [01:04<00:13,  3.52it/s][A
 79%|███████▊  | 174/221 [01:04<00:13,  3.46it/s][A
 79%|███████▉  | 175/221 [01:04<00:17,  2.64it/s][A
 80%|███████▉  | 176/221 [01:05<00:16,  2.74it/s][A
 80%|████████  | 177/221 [01:05<00:13,  3.33it/s][A
 81%|████████  | 178/221 [01:05<00:11,  3.66it/s][A
 81%|████████  | 179/221 [01:05<00:12,  3.26it/s][A
 82%|████████▏ | 181/221 [01:06<00:09,  4.27it/s][A
 82%|████████▏ | 182/221 [01:06<00:08,  4.53it/s][A
 83%|████████▎ | 183/221 [01:06<00:08,  4.74it/s][A
 83%|████████▎ | 184/221 [01:07<00:09,  3.99it/s][A
 84%|████████▎ | 185/221 [01:07<00:08,  4.28it/s][A
 84%|████████▍ | 186/221 [01:07<00:09,  3.56it/s][A
 85%|████████▍ | 187/221 [01:07<00:08,  3.91it/s][A
 85%|████████▌ | 188/221 [01:08<00:08,  3.94it/s][A[h264 @ 0x55d8fea12940] mmco: unref short failure

 86%|████████▌ | 189/221 [01:08<00:11,  2.81it/s][A
 86%|████████▌ | 190/221 [01:09<00:11,  2.78it/s][A
 87%|████████▋ | 192/221 [01:09<00:07,  3.67it/s][A
 88%|████████▊ | 194/221 [01:10<00:09,  2.96it/s][A
 88%|████████▊ | 195/221 [01:10<00:07,  3.37it/s][A
 89%|████████▊ | 196/221 [01:11<00:09,  2.61it/s][A
 89%|████████▉ | 197/221 [01:11<00:07,  3.18it/s][A
 90%|████████▉ | 198/221 [01:11<00:07,  3.23it/s][A
 90%|█████████ | 199/221 [01:11<00:06,  3.66it/s][A
 90%|█████████ | 200/221 [01:11<00:05,  3.55it/s][A
 91%|█████████ | 201/221 [01:12<00:05,  3.63it/s][A
 91%|█████████▏| 202/221 [01:12<00:04,  4.22it/s][A
 92%|█████████▏| 203/221 [01:12<00:03,  4.73it/s][A
 92%|█████████▏| 204/221 [01:12<00:04,  3.91it/s][A
 93%|█████████▎| 206/221 [01:13<00:04,  3.69it/s][A
 94%|█████████▎| 207/221 [01:13<00:03,  3.87it/s][A
 94%|█████████▍| 208/221 [01:13<00:02,  4.44it/s][A
 95%|█████████▍| 209/221 [01:14<00:02,  4.35it/s][A
 95%|█████████▌| 210/221 [01:14<00:02,  5.00it/s][A
 95%|█████████▌| 211/221 [01:14<00:02,  3.95it/s][A
 96%|█████████▌| 212/221 [01:14<00:02,  4.48it/s][A
 96%|█████████▋| 213/221 [01:14<00:01,  4.29it/s][A
 97%|█████████▋| 214/221 [01:15<00:01,  4.21it/s][A
 97%|█████████▋| 215/221 [01:15<00:01,  4.38it/s][A
 98%|█████████▊| 216/221 [01:15<00:01,  4.07it/s][A
 98%|█████████▊| 217/221 [01:16<00:01,  3.39it/s][A
 99%|█████████▊| 218/221 [01:16<00:00,  3.19it/s][A
 99%|█████████▉| 219/221 [01:16<00:00,  3.43it/s][A
100%|█████████▉| 220/221 [01:20<00:01,  1.39s/it][A
100%|██████████| 221/221 [01:20<00:00,  1.02s/it][A100%|██████████| 221/221 [01:20<00:00,  2.74it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.30it/s][A
  1%|          | 2/221 [00:00<01:06,  3.30it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.30it/s][A
  2%|▏         | 4/221 [00:01<01:05,  3.30it/s][A
  2%|▏         | 5/221 [00:01<01:05,  3.30it/s][A
  3%|▎         | 6/221 [00:01<01:05,  3.30it/s][A
  3%|▎         | 7/221 [00:02<01:04,  3.30it/s][A
  4%|▎         | 8/221 [00:02<01:04,  3.30it/s][A
  4%|▍         | 9/221 [00:02<01:04,  3.30it/s][A
  5%|▍         | 10/221 [00:03<01:03,  3.30it/s][A
  5%|▍         | 11/221 [00:03<01:03,  3.30it/s][A
  5%|▌         | 12/221 [00:03<01:03,  3.30it/s][A
  6%|▌         | 13/221 [00:03<01:03,  3.30it/s][A
  6%|▋         | 14/221 [00:04<01:02,  3.30it/s][A
  7%|▋         | 15/221 [00:04<01:02,  3.30it/s][A[h264 @ 0x55d90a7318c0] mmco: unref short failure

  7%|▋         | 16/221 [00:04<01:02,  3.30it/s][A
  8%|▊         | 17/221 [00:05<01:01,  3.30it/s][A
  8%|▊         | 18/221 [00:05<01:01,  3.30it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.30it/s][A
  9%|▉         | 20/221 [00:06<01:00,  3.31it/s][A
 10%|▉         | 21/221 [00:06<01:00,  3.31it/s][A
 10%|▉         | 22/221 [00:06<01:00,  3.31it/s][A
 10%|█         | 23/221 [00:06<00:59,  3.31it/s][A
 11%|█         | 24/221 [00:07<00:59,  3.31it/s][A
 11%|█▏        | 25/221 [00:07<00:59,  3.31it/s][A
 12%|█▏        | 26/221 [00:07<00:58,  3.31it/s][A
 12%|█▏        | 27/221 [00:08<00:58,  3.31it/s][A
 13%|█▎        | 28/221 [00:08<00:58,  3.31it/s][A
 13%|█▎        | 29/221 [00:08<00:58,  3.31it/s][A
 14%|█▎        | 30/221 [00:09<00:57,  3.31it/s][A
 14%|█▍        | 31/221 [00:09<00:57,  3.31it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.31it/s][A
 15%|█▍        | 33/221 [00:09<00:56,  3.31it/s][A
 15%|█▌        | 34/221 [00:10<00:56,  3.31it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.31it/s][A
 16%|█▋        | 36/221 [00:10<00:55,  3.31it/s][A
 17%|█▋        | 37/221 [00:11<00:55,  3.31it/s][A
 17%|█▋        | 38/221 [00:11<00:55,  3.31it/s][A
 18%|█▊        | 39/221 [00:11<00:54,  3.31it/s][A
 18%|█▊        | 40/221 [00:12<00:54,  3.31it/s][A
 19%|█▊        | 41/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 42/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 43/221 [00:13<00:53,  3.31it/s][A
 20%|█▉        | 44/221 [00:13<00:53,  3.31it/s][A
 20%|██        | 45/221 [00:13<00:53,  3.31it/s][A
 21%|██        | 46/221 [00:13<00:52,  3.31it/s][A
 21%|██▏       | 47/221 [00:14<00:52,  3.31it/s][A
 22%|██▏       | 48/221 [00:14<00:52,  3.31it/s][A
 22%|██▏       | 49/221 [00:14<00:51,  3.31it/s][A
 23%|██▎       | 50/221 [00:15<00:51,  3.31it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.31it/s][A
 24%|██▎       | 52/221 [00:15<00:51,  3.31it/s][A
 24%|██▍       | 53/221 [00:16<00:50,  3.31it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.31it/s][A
 25%|██▍       | 55/221 [00:16<00:50,  3.31it/s][A
 25%|██▌       | 56/221 [00:16<00:49,  3.31it/s][A
 26%|██▌       | 57/221 [00:17<00:49,  3.31it/s][A
 26%|██▌       | 58/221 [00:17<00:49,  3.31it/s][A
 27%|██▋       | 59/221 [00:17<00:48,  3.31it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 62/221 [00:18<00:48,  3.31it/s][A
 29%|██▊       | 63/221 [00:19<00:47,  3.31it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.31it/s][A
 29%|██▉       | 65/221 [00:19<00:47,  3.31it/s][A
 30%|██▉       | 66/221 [00:19<00:46,  3.31it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.31it/s][A
 31%|███       | 68/221 [00:20<00:46,  3.31it/s][A
 31%|███       | 69/221 [00:20<00:45,  3.31it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.31it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 72/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.31it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 76/221 [00:22<00:43,  3.31it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.31it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.31it/s][A
 36%|███▌      | 79/221 [00:23<00:42,  3.31it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 81/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 82/221 [00:24<00:41,  3.31it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 85/221 [00:25<00:41,  3.31it/s][A
 39%|███▉      | 86/221 [00:25<00:40,  3.31it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.31it/s][A
 40%|███▉      | 88/221 [00:26<00:40,  3.31it/s][A
 40%|████      | 89/221 [00:26<00:39,  3.31it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 91/221 [00:27<00:39,  3.31it/s][A
 42%|████▏     | 92/221 [00:27<00:38,  3.31it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:29<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:29<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:30<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:32<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:33<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:33<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:35<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:35<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:36<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:38<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:39<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:39<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:40<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:41<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:42<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:43<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:43<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:45<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:46<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:46<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:47<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:48<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:49<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:49<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:50<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:52<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:55<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:58<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [00:59<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:01<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:02<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:03<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:04<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:05<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:06<00:00,  3.31it/s][A100%|██████████| 221/221 [01:06<00:00,  3.31it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:38,  5.67it/s][A
  1%|          | 2/221 [00:00<01:48,  2.02it/s][A
  1%|▏         | 3/221 [00:01<01:11,  3.04it/s][A
  2%|▏         | 4/221 [00:01<01:04,  3.36it/s][A
  2%|▏         | 5/221 [00:01<01:04,  3.34it/s][A
  3%|▎         | 6/221 [00:01<00:53,  4.04it/s][A
  3%|▎         | 7/221 [00:01<00:50,  4.24it/s][A
  4%|▎         | 8/221 [00:02<01:03,  3.37it/s][A
  4%|▍         | 9/221 [00:02<01:17,  2.72it/s][A
  5%|▍         | 10/221 [00:03<01:19,  2.66it/s][A
  5%|▍         | 11/221 [00:03<01:06,  3.14it/s][A
  5%|▌         | 12/221 [00:03<01:09,  3.02it/s][A
  6%|▌         | 13/221 [00:04<01:30,  2.31it/s][A
  6%|▋         | 14/221 [00:04<01:11,  2.91it/s][A
  7%|▋         | 16/221 [00:05<00:57,  3.55it/s][A
  8%|▊         | 17/221 [00:06<01:40,  2.03it/s][A
  8%|▊         | 18/221 [00:06<01:32,  2.19it/s][A
  9%|▊         | 19/221 [00:06<01:19,  2.54it/s][A
  9%|▉         | 20/221 [00:06<01:05,  3.09it/s][A
 10%|▉         | 21/221 [00:07<00:58,  3.41it/s][A
 10%|▉         | 22/221 [00:07<00:52,  3.78it/s][A
 10%|█         | 23/221 [00:07<00:44,  4.43it/s][A
 11%|█         | 24/221 [00:07<00:38,  5.15it/s][A
 11%|█▏        | 25/221 [00:08<00:53,  3.65it/s][A
 12%|█▏        | 26/221 [00:08<00:56,  3.45it/s][A
 12%|█▏        | 27/221 [00:08<00:46,  4.21it/s][A
 13%|█▎        | 28/221 [00:09<01:21,  2.36it/s][A
 13%|█▎        | 29/221 [00:09<01:12,  2.66it/s][A
 14%|█▎        | 30/221 [00:09<01:12,  2.64it/s][A
 14%|█▍        | 31/221 [00:10<01:15,  2.51it/s][A
 14%|█▍        | 32/221 [00:10<00:59,  3.20it/s][A
 15%|█▍        | 33/221 [00:10<00:51,  3.62it/s][A
 16%|█▌        | 35/221 [00:10<00:36,  5.16it/s][A
 16%|█▋        | 36/221 [00:11<00:43,  4.26it/s][A
 17%|█▋        | 37/221 [00:11<00:54,  3.39it/s][A
 17%|█▋        | 38/221 [00:12<00:56,  3.22it/s][A
 18%|█▊        | 39/221 [00:12<00:51,  3.54it/s][A
 18%|█▊        | 40/221 [00:12<01:01,  2.96it/s][A
 19%|█▊        | 41/221 [00:13<00:54,  3.28it/s][A
 19%|█▉        | 42/221 [00:13<00:47,  3.77it/s][A
 20%|█▉        | 44/221 [00:13<00:29,  5.91it/s][A
 20%|██        | 45/221 [00:13<00:38,  4.56it/s][A
 21%|██        | 46/221 [00:14<00:44,  3.96it/s][A
 21%|██▏       | 47/221 [00:14<00:49,  3.55it/s][A
 22%|██▏       | 48/221 [00:14<00:42,  4.09it/s][A
 22%|██▏       | 49/221 [00:14<00:38,  4.49it/s][A
 23%|██▎       | 50/221 [00:14<00:34,  4.93it/s][A
 23%|██▎       | 51/221 [00:15<00:34,  4.95it/s][A
 24%|██▍       | 53/221 [00:15<00:32,  5.18it/s][A
 24%|██▍       | 54/221 [00:15<00:46,  3.56it/s][A
 25%|██▍       | 55/221 [00:16<00:41,  3.97it/s][A
 25%|██▌       | 56/221 [00:16<00:39,  4.20it/s][A
 26%|██▌       | 57/221 [00:16<00:37,  4.34it/s][A
 26%|██▌       | 58/221 [00:16<00:39,  4.18it/s][A
 27%|██▋       | 59/221 [00:16<00:32,  4.96it/s][A
 27%|██▋       | 60/221 [00:17<00:53,  3.04it/s][A
 28%|██▊       | 61/221 [00:17<00:49,  3.23it/s][A
 28%|██▊       | 62/221 [00:18<00:48,  3.27it/s][A
 29%|██▊       | 63/221 [00:18<00:57,  2.73it/s][A
 29%|██▉       | 64/221 [00:19<00:59,  2.64it/s][A
 29%|██▉       | 65/221 [00:19<01:06,  2.35it/s][A
 30%|██▉       | 66/221 [00:19<00:56,  2.72it/s][A
 30%|███       | 67/221 [00:20<01:02,  2.47it/s][A
 31%|███       | 68/221 [00:20<00:52,  2.91it/s][A
 31%|███       | 69/221 [00:20<00:51,  2.92it/s][A
 32%|███▏      | 70/221 [00:21<00:48,  3.13it/s][A
 32%|███▏      | 71/221 [00:21<00:46,  3.22it/s][A
 33%|███▎      | 72/221 [00:21<00:47,  3.15it/s][A
 33%|███▎      | 73/221 [00:22<00:52,  2.81it/s][A
 33%|███▎      | 74/221 [00:22<00:51,  2.87it/s][A
 34%|███▍      | 75/221 [00:22<00:48,  2.98it/s][A
 34%|███▍      | 76/221 [00:22<00:40,  3.60it/s][A
 35%|███▍      | 77/221 [00:23<00:37,  3.80it/s][A
 35%|███▌      | 78/221 [00:23<00:41,  3.45it/s][A
 36%|███▌      | 79/221 [00:24<00:57,  2.46it/s][A
 36%|███▌      | 80/221 [00:24<00:56,  2.48it/s][A
 37%|███▋      | 81/221 [00:24<00:50,  2.78it/s][A
 37%|███▋      | 82/221 [00:25<00:54,  2.53it/s][A
 38%|███▊      | 83/221 [00:25<01:02,  2.20it/s][A
 38%|███▊      | 84/221 [00:26<01:01,  2.23it/s][A
 38%|███▊      | 85/221 [00:26<00:53,  2.55it/s][A
 39%|███▉      | 86/221 [00:27<00:55,  2.44it/s][A
 39%|███▉      | 87/221 [00:28<01:15,  1.77it/s][A
 40%|███▉      | 88/221 [00:28<01:01,  2.16it/s][A
 40%|████      | 89/221 [00:28<00:53,  2.44it/s][A
 41%|████      | 90/221 [00:28<00:45,  2.87it/s][A
 41%|████      | 91/221 [00:28<00:36,  3.53it/s][A
 42%|████▏     | 92/221 [00:29<00:32,  3.96it/s][A
 42%|████▏     | 93/221 [00:29<00:28,  4.56it/s][A
 43%|████▎     | 94/221 [00:29<00:27,  4.57it/s][A
 43%|████▎     | 95/221 [00:29<00:33,  3.71it/s][A
 43%|████▎     | 96/221 [00:29<00:31,  4.01it/s][A
 44%|████▍     | 97/221 [00:30<00:26,  4.73it/s][A
 44%|████▍     | 98/221 [00:30<00:37,  3.27it/s][A
 45%|████▍     | 99/221 [00:30<00:31,  3.89it/s][A
 45%|████▌     | 100/221 [00:30<00:29,  4.16it/s][A
 46%|████▌     | 101/221 [00:31<00:26,  4.50it/s][A
 46%|████▌     | 102/221 [00:31<00:25,  4.68it/s][A
 47%|████▋     | 104/221 [00:31<00:18,  6.38it/s][A
 48%|████▊     | 105/221 [00:31<00:18,  6.16it/s][A
 48%|████▊     | 106/221 [00:32<00:24,  4.65it/s][A
 48%|████▊     | 107/221 [00:32<00:24,  4.59it/s][A
 49%|████▉     | 108/221 [00:32<00:29,  3.88it/s][A
 49%|████▉     | 109/221 [00:33<00:40,  2.75it/s][A
 50%|████▉     | 110/221 [00:33<00:39,  2.83it/s][A
 50%|█████     | 111/221 [00:33<00:38,  2.83it/s][A
 51%|█████     | 112/221 [00:34<00:35,  3.10it/s][A
 51%|█████     | 113/221 [00:34<00:31,  3.39it/s][A
 52%|█████▏    | 115/221 [00:34<00:22,  4.64it/s][A
 53%|█████▎    | 117/221 [00:35<00:20,  4.97it/s][A
 53%|█████▎    | 118/221 [00:35<00:28,  3.61it/s][A
 54%|█████▍    | 119/221 [00:35<00:25,  3.96it/s][A
 54%|█████▍    | 120/221 [00:36<00:29,  3.40it/s][A
 55%|█████▍    | 121/221 [00:36<00:26,  3.74it/s][A
 55%|█████▌    | 122/221 [00:36<00:25,  3.82it/s][A
 56%|█████▌    | 123/221 [00:36<00:25,  3.80it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.29it/s][A
 57%|█████▋    | 125/221 [00:37<00:39,  2.44it/s][A
 57%|█████▋    | 126/221 [00:38<00:35,  2.71it/s][A
 57%|█████▋    | 127/221 [00:38<00:27,  3.38it/s][A
 58%|█████▊    | 128/221 [00:38<00:27,  3.39it/s][A
 58%|█████▊    | 129/221 [00:38<00:24,  3.70it/s][A
 59%|█████▉    | 130/221 [00:39<00:29,  3.12it/s][A
 59%|█████▉    | 131/221 [00:39<00:24,  3.69it/s][A
 60%|█████▉    | 132/221 [00:39<00:21,  4.16it/s][A
 60%|██████    | 133/221 [00:40<00:31,  2.76it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.23it/s][A
 61%|██████    | 135/221 [00:40<00:31,  2.73it/s][A
 62%|██████▏   | 136/221 [00:41<00:31,  2.72it/s][A
 62%|██████▏   | 137/221 [00:41<00:28,  2.97it/s][A
 62%|██████▏   | 138/221 [00:41<00:27,  3.00it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.35it/s][A
 63%|██████▎   | 140/221 [00:42<00:25,  3.22it/s][A
 64%|██████▍   | 141/221 [00:42<00:21,  3.79it/s][A
 64%|██████▍   | 142/221 [00:42<00:21,  3.76it/s][A
 65%|██████▌   | 144/221 [00:43<00:16,  4.68it/s][A
 66%|██████▌   | 145/221 [00:43<00:14,  5.10it/s][A
 66%|██████▌   | 146/221 [00:43<00:14,  5.21it/s][A
 67%|██████▋   | 147/221 [00:43<00:12,  5.86it/s][A
 67%|██████▋   | 148/221 [00:43<00:16,  4.45it/s][A
 67%|██████▋   | 149/221 [00:44<00:15,  4.74it/s][A
 68%|██████▊   | 150/221 [00:44<00:23,  3.05it/s][A
 68%|██████▊   | 151/221 [00:44<00:19,  3.66it/s][A
 69%|██████▉   | 152/221 [00:45<00:27,  2.54it/s][A
 69%|██████▉   | 153/221 [00:46<00:26,  2.52it/s][A
 70%|██████▉   | 154/221 [00:46<00:28,  2.38it/s][A
 70%|███████   | 155/221 [00:46<00:23,  2.79it/s][A
 71%|███████   | 156/221 [00:46<00:20,  3.12it/s][A
 71%|███████   | 157/221 [00:47<00:20,  3.06it/s][A
 71%|███████▏  | 158/221 [00:47<00:21,  2.92it/s][A
 72%|███████▏  | 159/221 [00:48<00:22,  2.81it/s][A
 72%|███████▏  | 160/221 [00:48<00:21,  2.88it/s][A
 73%|███████▎  | 161/221 [00:48<00:17,  3.48it/s][A
 74%|███████▍  | 163/221 [00:48<00:14,  3.97it/s][A
 74%|███████▍  | 164/221 [00:49<00:14,  3.84it/s][A
 75%|███████▌  | 166/221 [00:49<00:14,  3.89it/s][A
 76%|███████▌  | 167/221 [00:49<00:13,  4.15it/s][A
 76%|███████▌  | 168/221 [00:50<00:13,  4.02it/s][A
 76%|███████▋  | 169/221 [00:50<00:13,  3.89it/s][A
 77%|███████▋  | 170/221 [00:50<00:16,  3.06it/s][A
 77%|███████▋  | 171/221 [00:51<00:17,  2.85it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.32it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.18it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.23it/s][A
 80%|███████▉  | 176/221 [00:52<00:15,  2.97it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.20it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.51it/s][A
 81%|████████  | 179/221 [00:53<00:13,  3.20it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.81it/s][A
 82%|████████▏ | 182/221 [00:54<00:08,  4.49it/s][A
 83%|████████▎ | 183/221 [00:54<00:08,  4.73it/s][A
 83%|████████▎ | 184/221 [00:55<00:13,  2.81it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.16it/s][A
 84%|████████▍ | 186/221 [00:55<00:11,  3.10it/s][A
 85%|████████▍ | 187/221 [00:55<00:08,  3.85it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:56<00:11,  2.72it/s][A
 86%|████████▌ | 190/221 [00:57<00:13,  2.25it/s][A
 86%|████████▋ | 191/221 [00:57<00:11,  2.63it/s][A
 87%|████████▋ | 192/221 [00:57<00:09,  2.90it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.14it/s][A
 88%|████████▊ | 194/221 [00:58<00:12,  2.20it/s][A
 88%|████████▊ | 195/221 [00:59<00:13,  1.98it/s][A
 89%|████████▊ | 196/221 [01:00<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:00<00:11,  2.03it/s][A
 90%|█████████ | 199/221 [01:00<00:07,  3.12it/s][A
 90%|█████████ | 200/221 [01:01<00:08,  2.35it/s][A
 91%|█████████ | 201/221 [01:01<00:08,  2.47it/s][A
 92%|█████████▏| 203/221 [01:02<00:05,  3.54it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  2.91it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.40it/s][A
 93%|█████████▎| 206/221 [01:03<00:04,  3.08it/s][A
 94%|█████████▎| 207/221 [01:03<00:03,  3.60it/s][A
 94%|█████████▍| 208/221 [01:03<00:04,  3.01it/s][A
 95%|█████████▍| 209/221 [01:04<00:04,  2.45it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.11it/s][A
 96%|█████████▌| 212/221 [01:05<00:02,  3.47it/s][A
 96%|█████████▋| 213/221 [01:05<00:03,  2.47it/s][A
 97%|█████████▋| 214/221 [01:06<00:02,  2.44it/s][A
 97%|█████████▋| 215/221 [01:06<00:02,  2.26it/s][A
 98%|█████████▊| 216/221 [01:07<00:01,  2.58it/s][A
 98%|█████████▊| 217/221 [01:07<00:01,  2.68it/s][A
 99%|█████████▊| 218/221 [01:07<00:01,  2.95it/s][A
 99%|█████████▉| 219/221 [01:07<00:00,  3.64it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.75it/s][A
100%|██████████| 221/221 [01:08<00:00,  3.38it/s][A100%|██████████| 221/221 [01:08<00:00,  3.23it/s]
09/18/2024 18:33:09 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 549--===========

09/18/2024 18:33:09 - INFO - __main__ -   {'area_r1': 40.3, 'area_recall': '40.3/67.2/75.6', 'area_ravg': 61.0}
09/18/2024 18:33:09 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 549--===========

09/18/2024 18:33:09 - INFO - __main__ -   {'forward_r1': 36.7, 'forward_recall': '36.7/65.7/76.5', 'forward_ravg': 59.6}
09/18/2024 18:33:09 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 549--===========

09/18/2024 18:33:09 - INFO - __main__ -   {'area_video_r1': 39.0, 'area_video_recall': '39.0/67.5/77.8', 'area_video_ravg': 61.5}
09/18/2024 18:33:09 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 18:33:09 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 18:33:09 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 549--===========

09/18/2024 18:33:09 - INFO - __main__ -   {'area_video_r1': 52.4, 'area_video_recall': '52.4/72.5/81.8', 'area_video_ravg': 68.9, 'area_video_back_r1': 48.8, 'area_video_back_recall': '48.8/71.2/77.9', 'area_video_back_ravg': 66.0}
09/18/2024 18:33:09 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 499=======

09/18/2024 18:33:09 - INFO - __main__ -   {'area_video_r1': 52.6, 'area_video_recall': '52.6/73.1/81.2', 'area_video_ravg': 69.0, 'area_video_back_r1': 49.5, 'area_video_back_recall': '49.5/71.5/78.2', 'area_video_back_ravg': 66.4}
09/18/2024 18:33:09 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 549--===========

09/18/2024 18:33:09 - INFO - __main__ -   {'video_r1': 27.7, 'video_recall': '27.7/53.3/64.1', 'video_ravg': 48.4}
09/18/2024 18:33:09 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 18:33:09 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 18:33:09 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 549--===========

09/18/2024 18:33:09 - INFO - __main__ -   {'video_r1': 50.2, 'video_recall': '50.2/70.0/77.4', 'video_ravg': 65.9}
09/18/2024 18:33:09 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 18:33:09 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 18:33:28 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.00040731183253228664, 'loss_ret%tv%ta--finetune_area/loss_area': 1.0779976844787598, 'loss_ret%tv%ta--finetune_area/total_loss': 1.0784050226211548}
[h264 @ 0x556603494d80] mmco: unref short failure
[h264 @ 0x556603494d80] mmco: unref short failure
[h264 @ 0x556603494d80] mmco: unref short failure
[h264 @ 0x556603494d80] mmco: unref short failure
[h264 @ 0x556603494d80] mmco: unref short failure
[h264 @ 0x556603494d80] mmco: unref short failure
 19%|█▉        | 550/2910 [3:28:21<90:20:13, 137.80s/it] 19%|█▉        | 551/2910 [3:28:24<63:55:33, 97.56s/it] [h264 @ 0x55d90b2fd340] mmco: unref short failure
[h264 @ 0x55d90b2fd340] mmco: unref short failure
 19%|█▉        | 552/2910 [3:28:28<45:29:23, 69.45s/it][h264 @ 0x556603494d80] mmco: unref short failure
 19%|█▉        | 553/2910 [3:28:32<32:38:53, 49.87s/it][h264 @ 0x5565e4834600] mmco: unref short failure
 19%|█▉        | 554/2910 [3:28:37<23:42:21, 36.22s/it][h264 @ 0x5565e5fc4d00] mmco: unref short failure
[h264 @ 0x5565e5fc4d00] mmco: unref short failure
 19%|█▉        | 555/2910 [3:28:41<17:27:42, 26.69s/it][h264 @ 0x55cb614f3200] mmco: unref short failure
[h264 @ 0x55cb614f3200] mmco: unref short failure
[h264 @ 0x55d916e3bd80] mmco: unref short failure
[h264 @ 0x55d916e3bd80] mmco: unref short failure
[h264 @ 0x55d916e3bd80] mmco: unref short failure
not have audios CqzowavAOpc.38
 19%|█▉        | 556/2910 [3:28:47<13:18:16, 20.35s/it] 19%|█▉        | 557/2910 [3:28:52<10:24:04, 15.91s/it] 19%|█▉        | 558/2910 [3:28:57<8:16:17, 12.66s/it] [h264 @ 0x556607b6fc80] mmco: unref short failure
[h264 @ 0x556607b6fc80] mmco: unref short failure
 19%|█▉        | 559/2910 [3:29:03<6:52:26, 10.53s/it][h264 @ 0x55cb654507c0] mmco: unref short failure
09/18/2024 18:34:16 - INFO - __main__ -   current idx _wZn-Jd_Dhg.25 from finetune_area returns wrong image/video, use 148226 instead.
[h264 @ 0x5598dbddcec0] mmco: unref short failure
[h264 @ 0x5565ef5c8380] mmco: unref short failure
 19%|█▉        | 560/2910 [3:29:09<5:52:58,  9.01s/it] 19%|█▉        | 561/2910 [3:29:14<5:14:13,  8.03s/it] 19%|█▉        | 562/2910 [3:29:20<4:47:56,  7.36s/it][h264 @ 0x556604959e40] mmco: unref short failure
[h264 @ 0x556604959e40] mmco: unref short failure
 19%|█▉        | 563/2910 [3:29:25<4:21:52,  6.69s/it] 19%|█▉        | 564/2910 [3:29:31<4:09:30,  6.38s/it] 19%|█▉        | 565/2910 [3:29:36<3:59:01,  6.12s/it][h264 @ 0x55cb6b7397c0] mmco: unref short failure
[h264 @ 0x55cb6b7397c0] mmco: unref short failure
[h264 @ 0x55cb6859cac0] mmco: unref short failure
[h264 @ 0x55cb6e83aa40] mmco: unref short failure
[h264 @ 0x55cb6e83aa40] mmco: unref short failure
[h264 @ 0x55cb6e83aa40] mmco: unref short failure
[h264 @ 0x55cb6e83aa40] mmco: unref short failure
[h264 @ 0x55cb6e83aa40] mmco: unref short failure
[h264 @ 0x5598e0794d40] mmco: unref short failure
[h264 @ 0x5598e0794d40] mmco: unref short failure
[h264 @ 0x5598e9613300] mmco: unref short failure
[h264 @ 0x5598e9613300] mmco: unref short failure
[h264 @ 0x5565e6962840] mmco: unref short failure
[h264 @ 0x55cb6859cac0] mmco: unref short failure
[h264 @ 0x5598ed335380] mmco: unref short failure
[h264 @ 0x55d9013574c0] mmco: unref short failure
[h264 @ 0x55d9013574c0] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55cb5e00d600] mmco: unref short failure
[h264 @ 0x55cb60c2fcc0] mmco: unref short failure
 19%|█▉        | 566/2910 [3:30:24<12:03:35, 18.52s/it][h264 @ 0x55d9013574c0] mmco: unref short failure
[h264 @ 0x55d9013574c0] mmco: unref short failure
[h264 @ 0x55d9013574c0] mmco: unref short failure
[h264 @ 0x55d9013574c0] mmco: unref short failure
 19%|█▉        | 567/2910 [3:30:35<10:34:47, 16.26s/it][h264 @ 0x5598dd872b40] mmco: unref short failure
[h264 @ 0x55d8fec80300] mmco: unref short failure
[h264 @ 0x55d8fec80300] mmco: unref short failure
[h264 @ 0x55d8fec80300] mmco: unref short failure
[h264 @ 0x55d8fec80300] mmco: unref short failure
[h264 @ 0x55d904a2b300] mmco: unref short failure
[h264 @ 0x55d904a2b300] mmco: unref short failure
 20%|█▉        | 568/2910 [3:30:47<9:47:52, 15.06s/it] [h264 @ 0x55cb7b414240] mmco: unref short failure
[h264 @ 0x55cb7b414240] mmco: unref short failure
[h264 @ 0x55d9046f4280] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x55cb6bdd4300] mmco: unref short failure
[h264 @ 0x5565f5e4f240] mmco: unref short failure
[h264 @ 0x5565f5e4f240] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x5598dc983780] mmco: unref short failure
[h264 @ 0x55cb80ff3300] mmco: unref short failure
[h264 @ 0x55cb80ff3300] mmco: unref short failure
[h264 @ 0x5565ed881680] mmco: unref short failure
[h264 @ 0x5565ed881680] mmco: unref short failure
[h264 @ 0x55d902e3f0c0] mmco: unref short failure
 20%|█▉        | 569/2910 [3:31:11<11:32:49, 17.76s/it] 20%|█▉        | 570/2910 [3:31:16<9:04:41, 13.97s/it] [h264 @ 0x55d91634b9c0] mmco: unref short failure
[h264 @ 0x55d91634b9c0] mmco: unref short failure
[h264 @ 0x55cb6bdd4300] mmco: unref short failure
[h264 @ 0x55cb6bdd4300] mmco: unref short failure
[h264 @ 0x55cb6bdd4300] mmco: unref short failure
[h264 @ 0x55cb6bdd4300] mmco: unref short failure
 20%|█▉        | 571/2910 [3:31:24<7:51:39, 12.10s/it] 20%|█▉        | 572/2910 [3:31:29<6:31:35, 10.05s/it][h264 @ 0x55cb5eac6000] mmco: unref short failure
[h264 @ 0x55cb5eac6000] mmco: unref short failure
[h264 @ 0x5565fce93a80] mmco: unref short failure
[h264 @ 0x5565fce93a80] mmco: unref short failure
 20%|█▉        | 573/2910 [3:31:35<5:44:01,  8.83s/it][h264 @ 0x55cb5ea54780] mmco: unref short failure
[h264 @ 0x55cb5eaaee40] mmco: unref short failure
[h264 @ 0x55cb5eaaee40] mmco: unref short failure
[h264 @ 0x55cb722b8140] mmco: unref short failure
[h264 @ 0x55d908e94a80] mmco: unref short failure
[h264 @ 0x55d908e94a80] mmco: unref short failure
[h264 @ 0x5565e6962840] mmco: unref short failure
[h264 @ 0x5565e6962840] mmco: unref short failure
[h264 @ 0x55d90f963980] mmco: unref short failure
[h264 @ 0x55d90f963980] mmco: unref short failure
[h264 @ 0x55d90f963980] mmco: unref short failure
[h264 @ 0x55d90f963980] mmco: unref short failure
[h264 @ 0x55d90f963980] mmco: unref short failure
[h264 @ 0x55d90f963980] mmco: unref short failure
[h264 @ 0x55d90977ec40] mmco: unref short failure
[h264 @ 0x55d90977ec40] mmco: unref short failure
[h264 @ 0x5565fe970040] mmco: unref short failure
[h264 @ 0x5565e598d740] mmco: unref short failure
[h264 @ 0x5565e598d740] mmco: unref short failure
[h264 @ 0x5565f5d26fc0] mmco: unref short failure
[h264 @ 0x5565f5d26fc0] mmco: unref short failure
[h264 @ 0x55d917930840] mmco: unref short failure
[h264 @ 0x5598e0794d40] mmco: unref short failure
[h264 @ 0x5598e0794d40] mmco: unref short failure
 20%|█▉        | 574/2910 [3:32:58<20:07:54, 31.03s/it] 20%|█▉        | 575/2910 [3:33:03<14:57:47, 23.07s/it][h264 @ 0x5598ef52a840] mmco: unref short failure
[h264 @ 0x55cb74bda400] mmco: unref short failure
[h264 @ 0x5598dc955580] mmco: unref short failure
 20%|█▉        | 576/2910 [3:33:09<11:41:01, 18.02s/it][h264 @ 0x55cb78fbe0c0] mmco: unref short failure
[h264 @ 0x55cb78fbe0c0] mmco: unref short failure
[h264 @ 0x55cb74bda400] mmco: unref short failure
[h264 @ 0x55cb74bda400] mmco: unref short failure
 20%|█▉        | 577/2910 [3:33:19<10:07:40, 15.63s/it] 20%|█▉        | 578/2910 [3:33:22<7:45:11, 11.97s/it]  20%|█▉        | 579/2910 [3:33:26<6:04:03,  9.37s/it] 20%|█▉        | 580/2910 [3:33:29<4:53:46,  7.57s/it] 20%|█▉        | 581/2910 [3:33:32<4:04:34,  6.30s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55d9b0b4c900] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55665d030a80] mmco: unref short failure
[h264 @ 0x55665d243d40] mmco: unref short failure
[h264 @ 0x55665d243d40] mmco: unref short failure
[h264 @ 0x55d9b2dac180] mmco: unref short failure
[h264 @ 0x55d9b2dac180] mmco: unref short failure
[h264 @ 0x55665c804e80] mmco: unref short failure
[h264 @ 0x55d9b3035340] mmco: unref short failure
[h264 @ 0x55d9b3035340] mmco: unref short failure
[h264 @ 0x55d9b3035340] mmco: unref short failure
[h264 @ 0x55665cdd3640] mmco: unref short failure
[h264 @ 0x55665cdd3640] mmco: unref short failure
[h264 @ 0x55665cdd3640] mmco: unref short failure
[h264 @ 0x55665cdd3640] mmco: unref short failure
[h264 @ 0x55665d0e28c0] mmco: unref short failure
[h264 @ 0x55665d0e28c0] mmco: unref short failure
[h264 @ 0x55cc15e00980] mmco: unref short failure
[h264 @ 0x55665cfe16c0] mmco: unref short failure
[h264 @ 0x55665cfe16c0] mmco: unref short failure
[h264 @ 0x5599932666c0] mmco: unref short failure
[h264 @ 0x5599932666c0] mmco: unref short failure
[h264 @ 0x5599932666c0] mmco: unref short failure
[h264 @ 0x5599932666c0] mmco: unref short failure
[h264 @ 0x55665e4013c0] mmco: unref short failure
[h264 @ 0x559992e57d00] mmco: unref short failure
[h264 @ 0x559992e57d00] mmco: unref short failure
[h264 @ 0x55999292f000] mmco: unref short failure
[h264 @ 0x55999292f000] mmco: unref short failure
[h264 @ 0x55665e40f240] mmco: unref short failure
[h264 @ 0x55665e40f240] mmco: unref short failure
[h264 @ 0x55665d0adf00] mmco: unref short failure
[h264 @ 0x55665d0adf00] mmco: unref short failure
[h264 @ 0x55665d0adf00] mmco: unref short failure
[h264 @ 0x55665d0adf00] mmco: unref short failure
[h264 @ 0x556695eb0000] mmco: unref short failure
[h264 @ 0x556695eb0000] mmco: unref short failure
[h264 @ 0x55d9b3f34200] mmco: unref short failure
[h264 @ 0x55999155b7c0] mmco: unref short failure
[h264 @ 0x55999a2aee80] mmco: unref short failure
[h264 @ 0x55999a2aee80] mmco: unref short failure
[h264 @ 0x556696e6c340] mmco: unref short failure
[h264 @ 0x556696e6c340] mmco: unref short failure
[h264 @ 0x55d9b6686d80] mmco: unref short failure
[h264 @ 0x55d9b6686d80] mmco: unref short failure
[h264 @ 0x55d9b6686d80] mmco: unref short failure
[h264 @ 0x55d9b6686d80] mmco: unref short failure
[h264 @ 0x559996d313c0] mmco: unref short failure
[h264 @ 0x559996d313c0] mmco: unref short failure
09/18/2024 18:41:03 - INFO - __main__ -   current idx A2e7HyCQqvE.54 from finetune_area returns wrong image/video, use 23649 instead.
[h264 @ 0x55cc1324f800] mmco: unref short failure
[h264 @ 0x55cc1324f800] mmco: unref short failure
[h264 @ 0x55665e78edc0] mmco: unref short failure
[h264 @ 0x55cc15995a40] mmco: unref short failure
[h264 @ 0x55cc15995a40] mmco: unref short failure
[h264 @ 0x55cc15995a40] mmco: unref short failure
[h264 @ 0x55665e4c9f40] mmco: unref short failure
[h264 @ 0x55665e4c9f40] mmco: unref short failure
[h264 @ 0x5599919562c0] mmco: unref short failure
[h264 @ 0x5599919562c0] mmco: unref short failure
[h264 @ 0x55d9b711ee00] mmco: unref short failure
[h264 @ 0x55d9b711ee00] mmco: unref short failure
 20%|██        | 582/2910 [3:36:29<37:11:49, 57.52s/it][h264 @ 0x5599924022c0] mmco: unref short failure
 20%|██        | 583/2910 [3:36:35<27:02:51, 41.84s/it] 20%|██        | 584/2910 [3:36:41<20:08:35, 31.18s/it][h264 @ 0x55cc1e7ad500] mmco: unref short failure
[h264 @ 0x55cc1e7ad500] mmco: unref short failure
 20%|██        | 585/2910 [3:36:46<15:09:02, 23.46s/it][h264 @ 0x55d9b923d900] mmco: unref short failure
[h264 @ 0x55d9b923d900] mmco: unref short failure
 20%|██        | 586/2910 [3:36:52<11:42:07, 18.13s/it]09/18/2024 18:42:03 - INFO - __main__ -   current idx PHgL4zWa_eU.158 from finetune_area returns wrong image/video, use 134332 instead.
 20%|██        | 587/2910 [3:36:58<9:18:09, 14.42s/it]  20%|██        | 588/2910 [3:37:03<7:28:04, 11.58s/it][h264 @ 0x55d9b8c6f180] mmco: unref short failure
[h264 @ 0x55d9b8c6f180] mmco: unref short failure
[h264 @ 0x55d9b5133300] mmco: unref short failure
[h264 @ 0x55d9b5133300] mmco: unref short failure
[h264 @ 0x559991eb8940] mmco: unref short failure
[h264 @ 0x55cc1c7a7400] mmco: unref short failure
[h264 @ 0x55cc1c7a7400] mmco: unref short failure
 20%|██        | 589/2910 [3:37:08<6:11:48,  9.61s/it][h264 @ 0x559991d73f00] mmco: unref short failure
[h264 @ 0x559991d73f00] mmco: unref short failure
[h264 @ 0x559991d73f00] mmco: unref short failure
[h264 @ 0x559991d73f00] mmco: unref short failure
[h264 @ 0x5599940cefc0] mmco: unref short failure
[h264 @ 0x5599940cefc0] mmco: unref short failure
[h264 @ 0x55cc1739f640] mmco: unref short failure
[h264 @ 0x556698f3c540] mmco: unref short failure
[h264 @ 0x559992bbe7c0] mmco: unref short failure
[h264 @ 0x559992bbe7c0] mmco: unref short failure
[h264 @ 0x55999cf64800] mmco: unref short failure
[h264 @ 0x55999cf64800] mmco: unref short failure
[h264 @ 0x55999cf64800] mmco: unref short failure
[h264 @ 0x55999cf64800] mmco: unref short failure
09/18/2024 18:42:41 - INFO - __main__ -   current idx KQezF6-NY_o.24 from finetune_area returns wrong image/video, use 3469 instead.
[h264 @ 0x55669e4fd080] mmco: unref short failure
[h264 @ 0x55669e4fd080] mmco: unref short failure
[h264 @ 0x55d9bb184100] mmco: unref short failure
[h264 @ 0x55d9b89ef6c0] mmco: unref short failure
[h264 @ 0x559991309880] mmco: unref short failure
[h264 @ 0x556696e071c0] mmco: unref short failure
[h264 @ 0x556696e071c0] mmco: unref short failure
[h264 @ 0x556696e071c0] mmco: unref short failure
[h264 @ 0x5599939fc700] mmco: unref short failure
[h264 @ 0x55cc15e8b4c0] mmco: unref short failure
 20%|██        | 590/2910 [3:38:54<24:47:19, 38.47s/it][h264 @ 0x55cc1b81edc0] mmco: unref short failure
 20%|██        | 591/2910 [3:38:59<18:25:07, 28.59s/it] 20%|██        | 592/2910 [3:39:04<13:52:30, 21.55s/it][h264 @ 0x55665e17ba80] mmco: unref short failure
[h264 @ 0x559994e32480] mmco: unref short failure
[h264 @ 0x559994e32480] mmco: unref short failure
 20%|██        | 593/2910 [3:39:10<10:51:55, 16.88s/it] 20%|██        | 594/2910 [3:39:15<8:35:59, 13.37s/it] [h264 @ 0x5566a093edc0] mmco: unref short failure
 20%|██        | 595/2910 [3:39:22<7:19:06, 11.38s/it][h264 @ 0x559995c60ec0] mmco: unref short failure
[h264 @ 0x559995c60ec0] mmco: unref short failure
[h264 @ 0x55d9bc821540] mmco: unref short failure
[h264 @ 0x55d9bc821540] mmco: unref short failure
[h264 @ 0x55d9bc821540] mmco: unref short failure
[h264 @ 0x55d9bc821540] mmco: unref short failure
 20%|██        | 596/2910 [3:39:27<6:05:39,  9.48s/it][h264 @ 0x559993a9f0c0] mmco: unref short failure
[h264 @ 0x55999b7a1b00] mmco: unref short failure
[h264 @ 0x55999b7a1b00] mmco: unref short failure
[h264 @ 0x55d9b9217300] mmco: unref short failure
[h264 @ 0x55d9b9217300] mmco: unref short failure
[h264 @ 0x55cc182f14c0] mmco: unref short failure
[h264 @ 0x55cc182f14c0] mmco: unref short failure
[h264 @ 0x55cc182f14c0] mmco: unref short failure
[h264 @ 0x55cc182f14c0] mmco: unref short failure
 21%|██        | 597/2910 [3:39:32<5:17:50,  8.24s/it][h264 @ 0x55999cb59540] mmco: unref short failure
[h264 @ 0x55999cb59540] mmco: unref short failure
[h264 @ 0x559999f98bc0] mmco: unref short failure
[h264 @ 0x559999f98bc0] mmco: unref short failure
[h264 @ 0x55cc182dcd40] mmco: unref short failure
[h264 @ 0x55cc14e71240] mmco: unref short failure
[h264 @ 0x55cc14e71240] mmco: unref short failure
[h264 @ 0x55cc12d92200] mmco: unref short failure
[h264 @ 0x55cc12d92200] mmco: unref short failure
[h264 @ 0x559999f8f780] mmco: unref short failure
[h264 @ 0x559999f9a1c0] mmco: unref short failure
[h264 @ 0x55cc1439fac0] mmco: unref short failure
[h264 @ 0x55d9b6248200] mmco: unref short failure
not have audios GAwav3sZcGw.4
09/18/2024 18:45:35 - INFO - __main__ -   current idx vMl-g-GJ1Ac.24 from finetune_area returns wrong image/video, use 25892 instead.
[h264 @ 0x55999c276780] mmco: unref short failure
[h264 @ 0x55cc190d2a40] mmco: unref short failure
[h264 @ 0x55cc190d2a40] mmco: unref short failure
[h264 @ 0x55d9b3c51780] mmco: unref short failure
[h264 @ 0x55d9b3c51780] mmco: unref short failure
[h264 @ 0x556699ad3bc0] mmco: unref short failure
[h264 @ 0x55d9bfbbcbc0] mmco: unref short failure
[h264 @ 0x5566a1a75900] mmco: unref short failure
[h264 @ 0x55669e4e06c0] mmco: unref short failure
[h264 @ 0x55669e4e06c0] mmco: unref short failure
[h264 @ 0x55669e4e06c0] mmco: unref short failure
[h264 @ 0x55669e4e06c0] mmco: unref short failure
[h264 @ 0x55669616a2c0] mmco: unref short failure
09/18/2024 18:46:00 - INFO - __main__ -   current idx -c6ksbh044A.74 from finetune_area returns wrong image/video, use 34459 instead.
[h264 @ 0x55cc13638bc0] mmco: unref short failure
[h264 @ 0x55cc13638bc0] mmco: unref short failure
[h264 @ 0x55cc13638bc0] mmco: unref short failure
[h264 @ 0x55cc13638bc0] mmco: unref short failure
[h264 @ 0x55d9b4b266c0] mmco: unref short failure
[h264 @ 0x55d9b4b266c0] mmco: unref short failure
 21%|██        | 598/2910 [3:41:18<23:58:09, 37.32s/it][h264 @ 0x55d9be6202c0] mmco: unref short failure
[h264 @ 0x55d9c18ab6c0] mmco: unref short failure
[h264 @ 0x55d9c18ab6c0] mmco: unref short failure
 21%|██        | 599/2910 [3:41:23<17:53:17, 27.87s/it]09/18/2024 18:46:33 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 18:46:33 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55999bb1cd80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55665d6fca80] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55665e5a14c0] mmco: unref short failure
[h264 @ 0x55665e5a14c0] mmco: unref short failure
[h264 @ 0x55cc20c47f00] mmco: unref short failure
[h264 @ 0x55d9bc8364c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x55d9c0f8cc40] mmco: unref short failure
[h264 @ 0x55d9c0f8cc40] mmco: unref short failure
[h264 @ 0x55d9b73948c0] mmco: unref short failure
[h264 @ 0x55cc2533ba40] mmco: unref short failure
[h264 @ 0x55cc2533ba40] mmco: unref short failure
[h264 @ 0x55999871d800] mmco: unref short failure
[h264 @ 0x559999f99500] mmco: unref short failure
[h264 @ 0x559999f99500] mmco: unref short failure
[h264 @ 0x55cc13a78680] mmco: unref short failure
[h264 @ 0x55cc13a78680] mmco: unref short failure
[h264 @ 0x55cc13a78680] mmco: unref short failure
[h264 @ 0x55cc13a78680] mmco: unref short failure
[h264 @ 0x55d9b5339b00] mmco: unref short failure
[h264 @ 0x55d9b5339b00] mmco: unref short failure
[h264 @ 0x55d9bffd6e40] mmco: unref short failure
[h264 @ 0x55cc16154540] mmco: unref short failure
[h264 @ 0x55cc16154540] mmco: unref short failure
[h264 @ 0x55cc19b8e500] mmco: unref short failure
[h264 @ 0x55999c2a3380] mmco: unref short failure
[h264 @ 0x55d9be1abf80] mmco: unref short failure
09/18/2024 18:48:31 - INFO - __main__ -   current idx zpVEmyBr_Hg.14 from finetune_area returns wrong image/video, use 126865 instead.
[h264 @ 0x55d9b532c0c0] mmco: unref short failure
[h264 @ 0x5566a1d5cc00] mmco: unref short failure
[h264 @ 0x5566a1d5cc00] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<03:08,  1.17it/s][A
  1%|          | 2/221 [00:01<02:43,  1.34it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:01<01:22,  2.63it/s][A
  2%|▏         | 5/221 [00:02<01:11,  3.04it/s][A
  3%|▎         | 6/221 [00:02<00:56,  3.79it/s][A
  3%|▎         | 7/221 [00:02<00:56,  3.80it/s][A
  4%|▎         | 8/221 [00:03<01:09,  3.06it/s][A
  4%|▍         | 9/221 [00:03<01:08,  3.11it/s][A
  5%|▍         | 10/221 [00:03<01:14,  2.84it/s][A
  5%|▍         | 11/221 [00:03<01:02,  3.33it/s][A
  5%|▌         | 12/221 [00:04<01:13,  2.86it/s][A
  6%|▌         | 13/221 [00:04<01:17,  2.68it/s][A
  6%|▋         | 14/221 [00:05<02:01,  1.70it/s][A
  7%|▋         | 15/221 [00:06<01:45,  1.95it/s][A
  7%|▋         | 16/221 [00:06<01:44,  1.96it/s][A
  8%|▊         | 17/221 [00:07<01:49,  1.86it/s][A
  8%|▊         | 18/221 [00:07<01:33,  2.17it/s][A
  9%|▊         | 19/221 [00:07<01:16,  2.63it/s][A
  9%|▉         | 20/221 [00:08<01:08,  2.93it/s][A
 10%|▉         | 21/221 [00:08<01:01,  3.25it/s][A
 10%|▉         | 22/221 [00:08<01:00,  3.28it/s][A
 10%|█         | 23/221 [00:08<00:50,  3.93it/s][A
 11%|█         | 24/221 [00:08<00:43,  4.50it/s][A
 11%|█▏        | 25/221 [00:09<00:40,  4.79it/s][A
 12%|█▏        | 26/221 [00:09<00:53,  3.66it/s][A
 13%|█▎        | 28/221 [00:10<00:58,  3.29it/s][A
 13%|█▎        | 29/221 [00:11<01:23,  2.31it/s][A
 14%|█▎        | 30/221 [00:11<01:21,  2.35it/s][A
 14%|█▍        | 31/221 [00:11<01:16,  2.49it/s][A
 14%|█▍        | 32/221 [00:11<01:02,  3.02it/s][A
 15%|█▍        | 33/221 [00:12<00:57,  3.28it/s][A
 15%|█▌        | 34/221 [00:12<00:57,  3.23it/s][A
 16%|█▌        | 35/221 [00:12<00:49,  3.78it/s][A
 16%|█▋        | 36/221 [00:13<00:54,  3.42it/s][A
 17%|█▋        | 37/221 [00:13<01:18,  2.33it/s][A
 17%|█▋        | 38/221 [00:14<01:13,  2.50it/s][A
 18%|█▊        | 39/221 [00:14<01:17,  2.34it/s][A
 18%|█▊        | 40/221 [00:14<01:14,  2.43it/s][A
 19%|█▊        | 41/221 [00:15<01:08,  2.64it/s][A
 19%|█▉        | 42/221 [00:15<01:13,  2.44it/s][A
 19%|█▉        | 43/221 [00:15<01:02,  2.85it/s][A[h264 @ 0x5566a021b880] mmco: unref short failure
[h264 @ 0x5566a021b880] mmco: unref short failure

 20%|██        | 45/221 [00:17<01:44,  1.68it/s][A
 21%|██        | 46/221 [00:18<01:41,  1.73it/s][A
 21%|██▏       | 47/221 [00:18<01:40,  1.73it/s][A
 22%|██▏       | 49/221 [00:19<01:34,  1.81it/s][A
 23%|██▎       | 50/221 [00:20<01:33,  1.83it/s][A
 23%|██▎       | 51/221 [00:20<01:15,  2.25it/s][A
 24%|██▎       | 52/221 [00:20<01:03,  2.67it/s][A
 24%|██▍       | 53/221 [00:20<00:55,  3.04it/s][A[h264 @ 0x5566a02d7fc0] mmco: unref short failure

 24%|██▍       | 54/221 [00:22<02:14,  1.24it/s][A
 25%|██▍       | 55/221 [00:23<01:53,  1.47it/s][A
 25%|██▌       | 56/221 [00:23<01:32,  1.79it/s][A
 26%|██▌       | 57/221 [00:23<01:15,  2.17it/s][A
 26%|██▌       | 58/221 [00:24<01:04,  2.55it/s][A
 27%|██▋       | 59/221 [00:24<00:55,  2.92it/s][A
 27%|██▋       | 60/221 [00:24<01:11,  2.25it/s][A
 28%|██▊       | 61/221 [00:25<01:00,  2.63it/s][A
 28%|██▊       | 62/221 [00:25<00:58,  2.72it/s][A
 29%|██▊       | 63/221 [00:25<00:54,  2.91it/s][A
 29%|██▉       | 64/221 [00:25<00:44,  3.49it/s][A
 29%|██▉       | 65/221 [00:26<00:42,  3.67it/s][A
 30%|██▉       | 66/221 [00:26<00:49,  3.15it/s][A
 30%|███       | 67/221 [00:27<01:01,  2.51it/s][A
 31%|███       | 68/221 [00:27<00:53,  2.87it/s][A
 31%|███       | 69/221 [00:28<01:11,  2.13it/s][A
 32%|███▏      | 70/221 [00:28<00:55,  2.72it/s][A
 32%|███▏      | 71/221 [00:29<01:17,  1.95it/s][A
 33%|███▎      | 72/221 [00:29<01:05,  2.28it/s][A
 33%|███▎      | 73/221 [00:29<01:01,  2.42it/s][A
 33%|███▎      | 74/221 [00:29<00:49,  2.96it/s][A
 34%|███▍      | 75/221 [00:30<00:54,  2.67it/s][A
 34%|███▍      | 76/221 [00:30<00:45,  3.16it/s][A
 35%|███▍      | 77/221 [00:30<00:42,  3.40it/s][A
 35%|███▌      | 78/221 [00:31<00:46,  3.11it/s][A
 36%|███▌      | 79/221 [00:31<00:52,  2.69it/s][A
 36%|███▌      | 80/221 [00:31<00:49,  2.86it/s][A
 37%|███▋      | 81/221 [00:32<00:48,  2.86it/s][A
 37%|███▋      | 82/221 [00:33<01:08,  2.02it/s][A
 38%|███▊      | 83/221 [00:33<01:05,  2.11it/s][A
 38%|███▊      | 84/221 [00:33<00:55,  2.46it/s][A
 39%|███▉      | 86/221 [00:34<00:42,  3.20it/s][A
 39%|███▉      | 87/221 [00:34<00:49,  2.69it/s][A
 40%|███▉      | 88/221 [00:35<00:55,  2.39it/s][A
 40%|████      | 89/221 [00:35<01:01,  2.16it/s][A
 41%|████      | 90/221 [00:36<00:51,  2.54it/s][A
 41%|████      | 91/221 [00:36<00:41,  3.14it/s][A
 42%|████▏     | 92/221 [00:36<00:35,  3.63it/s][A
 42%|████▏     | 93/221 [00:36<00:41,  3.09it/s][A
 43%|████▎     | 94/221 [00:37<00:43,  2.95it/s][A
 43%|████▎     | 95/221 [00:37<00:42,  2.93it/s][A
 43%|████▎     | 96/221 [00:37<00:40,  3.12it/s][A
 44%|████▍     | 97/221 [00:38<00:32,  3.78it/s][A
 44%|████▍     | 98/221 [00:38<00:32,  3.76it/s][A
 45%|████▍     | 99/221 [00:38<00:26,  4.57it/s][A
 45%|████▌     | 100/221 [00:38<00:25,  4.66it/s][A
 46%|████▌     | 101/221 [00:38<00:22,  5.32it/s][A
 46%|████▌     | 102/221 [00:39<00:27,  4.38it/s][A
 47%|████▋     | 103/221 [00:39<00:23,  5.02it/s][A
 48%|████▊     | 105/221 [00:39<00:21,  5.29it/s][A
 48%|████▊     | 106/221 [00:40<00:36,  3.19it/s][A
 48%|████▊     | 107/221 [00:40<00:30,  3.71it/s][A
 49%|████▉     | 108/221 [00:40<00:28,  3.91it/s][A
 49%|████▉     | 109/221 [00:40<00:28,  3.93it/s][A
 50%|████▉     | 110/221 [00:41<00:27,  4.04it/s][A
 50%|█████     | 111/221 [00:41<00:34,  3.20it/s][A
 51%|█████     | 112/221 [00:41<00:29,  3.73it/s][A
 51%|█████     | 113/221 [00:41<00:29,  3.63it/s][A
 52%|█████▏    | 115/221 [00:42<00:20,  5.21it/s][A[h264 @ 0x55cc14de13c0] mmco: unref short failure
[h264 @ 0x55999301c700] mmco: unref short failure
[h264 @ 0x55999301c700] mmco: unref short failure
[h264 @ 0x55999301c700] mmco: unref short failure
[h264 @ 0x55999301c700] mmco: unref short failure
[h264 @ 0x55cc18237480] mmco: unref short failure
[h264 @ 0x55cc18237480] mmco: unref short failure
[h264 @ 0x55cc18237480] mmco: unref short failure
[h264 @ 0x55cc18237480] mmco: unref short failure

 52%|█████▏    | 116/221 [00:46<02:05,  1.19s/it][A
 53%|█████▎    | 117/221 [00:46<01:40,  1.04it/s][A
 53%|█████▎    | 118/221 [00:47<01:21,  1.26it/s][A
 54%|█████▍    | 119/221 [00:47<01:04,  1.57it/s][A
 54%|█████▍    | 120/221 [00:47<00:53,  1.88it/s][A
 55%|█████▌    | 122/221 [00:47<00:34,  2.87it/s][A
 56%|█████▌    | 123/221 [00:47<00:29,  3.36it/s][A
 56%|█████▌    | 124/221 [00:48<00:26,  3.68it/s][A
 57%|█████▋    | 125/221 [00:48<00:27,  3.55it/s][A
 57%|█████▋    | 126/221 [00:48<00:25,  3.79it/s][A
 57%|█████▋    | 127/221 [00:49<00:42,  2.22it/s][A
 58%|█████▊    | 128/221 [00:49<00:38,  2.39it/s][A
 58%|█████▊    | 129/221 [00:50<00:32,  2.81it/s][A
 59%|█████▉    | 130/221 [00:50<00:29,  3.08it/s][A
 59%|█████▉    | 131/221 [00:50<00:23,  3.83it/s][A
 60%|█████▉    | 132/221 [00:50<00:20,  4.34it/s][A
 60%|██████    | 133/221 [00:51<00:27,  3.23it/s][A
 61%|██████    | 134/221 [00:51<00:23,  3.70it/s][A
 61%|██████    | 135/221 [00:51<00:25,  3.35it/s][A
 62%|██████▏   | 136/221 [00:52<00:26,  3.18it/s][A
 62%|██████▏   | 137/221 [00:52<00:22,  3.72it/s][A
 62%|██████▏   | 138/221 [00:52<00:23,  3.60it/s][A
 63%|██████▎   | 139/221 [00:52<00:26,  3.15it/s][A
 63%|██████▎   | 140/221 [00:53<00:25,  3.24it/s][A
 64%|██████▍   | 141/221 [00:53<00:21,  3.69it/s][A[h264 @ 0x556698434500] mmco: unref short failure
[h264 @ 0x556698434500] mmco: unref short failure

 64%|██████▍   | 142/221 [00:53<00:22,  3.58it/s][A
 65%|██████▍   | 143/221 [00:54<00:25,  3.07it/s][A
 65%|██████▌   | 144/221 [00:54<00:22,  3.43it/s][A
 66%|██████▌   | 146/221 [00:54<00:13,  5.37it/s][A
 67%|██████▋   | 147/221 [00:54<00:13,  5.34it/s][A
 67%|██████▋   | 148/221 [00:54<00:14,  5.05it/s][A
 68%|██████▊   | 150/221 [00:55<00:12,  5.86it/s][A
 68%|██████▊   | 151/221 [00:55<00:20,  3.38it/s][A
 69%|██████▉   | 152/221 [00:57<00:34,  1.97it/s][A
 69%|██████▉   | 153/221 [00:57<00:30,  2.22it/s][A
 70%|██████▉   | 154/221 [00:57<00:27,  2.47it/s][A
 70%|███████   | 155/221 [00:57<00:21,  3.02it/s][A
 71%|███████   | 156/221 [00:57<00:18,  3.43it/s][A
 71%|███████   | 157/221 [00:58<00:28,  2.22it/s][A
 71%|███████▏  | 158/221 [00:58<00:24,  2.59it/s][A
 72%|███████▏  | 159/221 [00:59<00:19,  3.18it/s][A
 72%|███████▏  | 160/221 [00:59<00:18,  3.37it/s][A
 73%|███████▎  | 162/221 [01:00<00:22,  2.57it/s][A
 74%|███████▍  | 163/221 [01:00<00:20,  2.80it/s][A
 74%|███████▍  | 164/221 [01:01<00:23,  2.43it/s][A
 75%|███████▍  | 165/221 [01:01<00:18,  3.03it/s][A
 75%|███████▌  | 166/221 [01:01<00:18,  2.90it/s][A
 76%|███████▌  | 167/221 [01:01<00:15,  3.55it/s][A
 76%|███████▌  | 168/221 [01:02<00:21,  2.48it/s][A
 76%|███████▋  | 169/221 [01:02<00:17,  2.91it/s][A[h264 @ 0x556698437840] mmco: unref short failure

 77%|███████▋  | 170/221 [01:03<00:17,  2.88it/s][A[h264 @ 0x55d9ba6bb2c0] mmco: unref short failure
[h264 @ 0x55d9ba6bb2c0] mmco: unref short failure

 77%|███████▋  | 171/221 [01:03<00:17,  2.85it/s][A
 78%|███████▊  | 172/221 [01:03<00:15,  3.24it/s][A[h264 @ 0x55999b6c2840] mmco: unref short failure
[h264 @ 0x55999b6c2840] mmco: unref short failure

 78%|███████▊  | 173/221 [01:03<00:12,  3.91it/s][A
 79%|███████▊  | 174/221 [01:04<00:12,  3.88it/s][A
 79%|███████▉  | 175/221 [01:04<00:16,  2.80it/s][A
 80%|███████▉  | 176/221 [01:04<00:15,  2.94it/s][A
 80%|████████  | 177/221 [01:05<00:11,  3.71it/s][A
 81%|████████  | 178/221 [01:05<00:12,  3.53it/s][A
 81%|████████  | 179/221 [01:05<00:13,  3.22it/s][A
 82%|████████▏ | 181/221 [01:06<00:09,  4.30it/s][A
 82%|████████▏ | 182/221 [01:06<00:08,  4.75it/s][A
 83%|████████▎ | 183/221 [01:06<00:07,  4.90it/s][A
 83%|████████▎ | 184/221 [01:06<00:09,  4.02it/s][A
 84%|████████▎ | 185/221 [01:06<00:08,  4.34it/s][A
 84%|████████▍ | 186/221 [01:07<00:10,  3.49it/s][A
 85%|████████▍ | 187/221 [01:07<00:08,  3.98it/s][A
 85%|████████▌ | 188/221 [01:07<00:08,  3.99it/s][A
 86%|████████▌ | 189/221 [01:08<00:10,  3.06it/s][A
 86%|████████▌ | 190/221 [01:08<00:10,  2.85it/s][A
 86%|████████▋ | 191/221 [01:08<00:08,  3.60it/s][A
 87%|████████▋ | 192/221 [01:08<00:07,  3.87it/s][A
 88%|████████▊ | 194/221 [01:09<00:09,  2.79it/s][A
 88%|████████▊ | 195/221 [01:10<00:07,  3.28it/s][A
 89%|████████▊ | 196/221 [01:10<00:10,  2.28it/s][A
 89%|████████▉ | 197/221 [01:11<00:08,  2.81it/s][A
 90%|████████▉ | 198/221 [01:11<00:07,  3.00it/s][A
 90%|█████████ | 199/221 [01:11<00:06,  3.58it/s][A
 90%|█████████ | 200/221 [01:11<00:05,  3.58it/s][A
 91%|█████████ | 201/221 [01:11<00:05,  3.72it/s][A09/18/2024 18:50:03 - INFO - __main__ -   current idx MVC6nKCrOgI.12 from finetune_area returns wrong image/video, use 25694 instead.

 91%|█████████▏| 202/221 [01:12<00:04,  4.02it/s][A
 92%|█████████▏| 203/221 [01:12<00:04,  4.39it/s][A
 92%|█████████▏| 204/221 [01:12<00:04,  3.69it/s][A[h264 @ 0x55cc1cca95c0] mmco: unref short failure
[h264 @ 0x55cc1cca95c0] mmco: unref short failure

 93%|█████████▎| 205/221 [01:12<00:03,  4.42it/s][A
 93%|█████████▎| 206/221 [01:13<00:04,  3.36it/s][A
 94%|█████████▎| 207/221 [01:13<00:03,  3.71it/s][A
 94%|█████████▍| 208/221 [01:13<00:03,  4.27it/s][A
 95%|█████████▍| 209/221 [01:13<00:02,  4.45it/s][A
 95%|█████████▌| 211/221 [01:14<00:02,  4.50it/s][A
 96%|█████████▌| 212/221 [01:14<00:01,  4.95it/s][A
 96%|█████████▋| 213/221 [01:14<00:01,  4.48it/s][A
 97%|█████████▋| 214/221 [01:14<00:01,  4.13it/s][A
 97%|█████████▋| 215/221 [01:15<00:01,  4.21it/s][A
 98%|█████████▊| 216/221 [01:15<00:01,  4.08it/s][A
 98%|█████████▊| 217/221 [01:15<00:01,  3.67it/s][A
 99%|█████████▊| 218/221 [01:16<00:00,  3.50it/s][A
 99%|█████████▉| 219/221 [01:16<00:00,  3.73it/s][A
100%|█████████▉| 220/221 [01:20<00:01,  1.41s/it][A
100%|██████████| 221/221 [01:20<00:00,  1.06s/it][A100%|██████████| 221/221 [01:20<00:00,  2.74it/s]
[h264 @ 0x55665d1bfdc0] mmco: unref short failure
[h264 @ 0x55665d1bfdc0] mmco: unref short failure
[h264 @ 0x55665d1bfdc0] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.30it/s][A
  1%|          | 2/221 [00:00<01:06,  3.28it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.29it/s][A
  2%|▏         | 4/221 [00:01<01:06,  3.29it/s][A
  2%|▏         | 5/221 [00:01<01:05,  3.29it/s][A
  3%|▎         | 6/221 [00:01<01:06,  3.24it/s][A
  3%|▎         | 7/221 [00:02<01:06,  3.23it/s][A
  4%|▎         | 8/221 [00:02<01:15,  2.82it/s][A
  4%|▍         | 9/221 [00:02<01:11,  2.95it/s][A
  5%|▍         | 10/221 [00:03<01:09,  3.05it/s][A
  5%|▍         | 11/221 [00:03<01:07,  3.12it/s][A
  5%|▌         | 12/221 [00:03<01:05,  3.17it/s][A
  6%|▌         | 13/221 [00:04<01:04,  3.21it/s][A
  6%|▋         | 14/221 [00:04<01:04,  3.23it/s][A
  7%|▋         | 15/221 [00:04<01:03,  3.25it/s][A
  7%|▋         | 16/221 [00:05<01:02,  3.26it/s][A
  8%|▊         | 17/221 [00:05<01:02,  3.27it/s][A
  8%|▊         | 18/221 [00:05<01:02,  3.26it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.27it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.28it/s][A
 10%|▉         | 21/221 [00:06<01:00,  3.28it/s][A
 10%|▉         | 22/221 [00:06<01:00,  3.29it/s][A
 10%|█         | 23/221 [00:07<01:00,  3.29it/s][A
 11%|█         | 24/221 [00:07<00:59,  3.29it/s][A
 11%|█▏        | 25/221 [00:07<00:59,  3.29it/s][A
 12%|█▏        | 26/221 [00:08<00:59,  3.30it/s][A
 12%|█▏        | 27/221 [00:08<00:58,  3.30it/s][A
 13%|█▎        | 28/221 [00:08<00:58,  3.30it/s][A
 13%|█▎        | 29/221 [00:08<00:58,  3.30it/s][A
 14%|█▎        | 30/221 [00:09<00:57,  3.30it/s][A
 14%|█▍        | 31/221 [00:09<00:57,  3.30it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.31it/s][A
 15%|█▍        | 33/221 [00:10<00:56,  3.31it/s][A
 15%|█▌        | 34/221 [00:10<00:56,  3.31it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.31it/s][A
 16%|█▋        | 36/221 [00:11<00:55,  3.31it/s][A
 17%|█▋        | 37/221 [00:11<00:55,  3.31it/s][A
 17%|█▋        | 38/221 [00:11<00:55,  3.31it/s][A
 18%|█▊        | 39/221 [00:12<00:54,  3.31it/s][A
 18%|█▊        | 40/221 [00:12<00:54,  3.31it/s][A
 19%|█▊        | 41/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 42/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 43/221 [00:13<00:53,  3.31it/s][A
 20%|█▉        | 44/221 [00:13<00:53,  3.31it/s][A
 20%|██        | 45/221 [00:13<00:53,  3.31it/s][A
 21%|██        | 46/221 [00:14<00:52,  3.31it/s][A
 21%|██▏       | 47/221 [00:14<00:52,  3.31it/s][A
 22%|██▏       | 48/221 [00:14<00:52,  3.31it/s][A
 22%|██▏       | 49/221 [00:15<00:51,  3.31it/s][A
 23%|██▎       | 50/221 [00:15<00:51,  3.31it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.31it/s][A
 24%|██▎       | 52/221 [00:15<00:51,  3.31it/s][A
 24%|██▍       | 53/221 [00:16<00:50,  3.31it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.31it/s][A
 25%|██▍       | 55/221 [00:16<00:50,  3.31it/s][A
 25%|██▌       | 56/221 [00:17<00:49,  3.31it/s][A
 26%|██▌       | 57/221 [00:17<00:49,  3.31it/s][A
 26%|██▌       | 58/221 [00:17<00:49,  3.31it/s][A
 27%|██▋       | 59/221 [00:18<00:48,  3.31it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 62/221 [00:18<00:48,  3.31it/s][A
 29%|██▊       | 63/221 [00:19<00:47,  3.31it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.31it/s][A
 29%|██▉       | 65/221 [00:19<00:47,  3.31it/s][A
 30%|██▉       | 66/221 [00:20<00:46,  3.31it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.31it/s][A
 31%|███       | 68/221 [00:20<00:46,  3.31it/s][A
 31%|███       | 69/221 [00:21<00:45,  3.31it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.31it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 72/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.31it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 76/221 [00:23<00:43,  3.31it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.31it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.31it/s][A
 36%|███▌      | 79/221 [00:24<00:42,  3.31it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 81/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 82/221 [00:24<00:41,  3.31it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 85/221 [00:25<00:41,  3.31it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.31it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.31it/s][A
 40%|███▉      | 88/221 [00:26<00:40,  3.31it/s][A
 40%|████      | 89/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 91/221 [00:27<00:39,  3.31it/s][A
 42%|████▏     | 92/221 [00:28<00:38,  3.31it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:29<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:33<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:34<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:35<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:36<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:39<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:41<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:43<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:44<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:46<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:47<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:49<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:50<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:50<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:03<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  3.30it/s][A
 99%|█████████▊| 218/221 [01:06<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:06<00:00,  3.31it/s][A100%|██████████| 221/221 [01:06<00:00,  3.30it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:32,  6.76it/s][A
  1%|          | 2/221 [00:00<01:56,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:18,  2.79it/s][A
  2%|▏         | 4/221 [00:01<01:04,  3.35it/s][A
  2%|▏         | 5/221 [00:01<01:00,  3.57it/s][A
  3%|▎         | 6/221 [00:01<00:51,  4.21it/s][A
  3%|▎         | 7/221 [00:01<00:54,  3.95it/s][A
  4%|▎         | 8/221 [00:02<01:09,  3.07it/s][A
  4%|▍         | 9/221 [00:03<01:24,  2.50it/s][A
  5%|▍         | 10/221 [00:03<01:20,  2.64it/s][A
  5%|▍         | 11/221 [00:03<01:03,  3.32it/s][A
  5%|▌         | 12/221 [00:03<01:03,  3.32it/s][A
  6%|▌         | 13/221 [00:04<01:32,  2.25it/s][A
  6%|▋         | 14/221 [00:04<01:11,  2.89it/s][A
  7%|▋         | 16/221 [00:05<00:58,  3.49it/s][A
  8%|▊         | 17/221 [00:06<01:36,  2.11it/s][A
  8%|▊         | 18/221 [00:06<01:28,  2.28it/s][A
  9%|▊         | 19/221 [00:06<01:14,  2.70it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.27it/s][A
 10%|▉         | 21/221 [00:07<00:54,  3.66it/s][A
 10%|▉         | 22/221 [00:07<00:50,  3.95it/s][A
 10%|█         | 23/221 [00:07<00:43,  4.59it/s][A
 11%|█         | 24/221 [00:07<00:37,  5.30it/s][A
 11%|█▏        | 25/221 [00:07<00:50,  3.85it/s][A
 12%|█▏        | 26/221 [00:08<00:52,  3.74it/s][A
 12%|█▏        | 27/221 [00:08<00:42,  4.51it/s][A
 13%|█▎        | 28/221 [00:09<01:21,  2.38it/s][A
 13%|█▎        | 29/221 [00:09<01:10,  2.72it/s][A
 14%|█▎        | 30/221 [00:09<01:09,  2.75it/s][A
 14%|█▍        | 31/221 [00:10<01:15,  2.51it/s][A
 15%|█▍        | 33/221 [00:10<00:53,  3.54it/s][A
 15%|█▌        | 34/221 [00:10<00:44,  4.22it/s][A
 16%|█▌        | 35/221 [00:10<00:38,  4.88it/s][A
 16%|█▋        | 36/221 [00:11<00:45,  4.08it/s][A
 17%|█▋        | 37/221 [00:11<00:53,  3.42it/s][A
 17%|█▋        | 38/221 [00:11<00:57,  3.20it/s][A
 18%|█▊        | 39/221 [00:12<00:52,  3.48it/s][A
 18%|█▊        | 40/221 [00:12<00:58,  3.11it/s][A
 19%|█▊        | 41/221 [00:12<00:53,  3.34it/s][A
 19%|█▉        | 42/221 [00:12<00:45,  3.90it/s][A
 20%|█▉        | 44/221 [00:13<00:28,  6.13it/s][A
 20%|██        | 45/221 [00:13<00:37,  4.72it/s][A
 21%|██        | 46/221 [00:13<00:43,  4.04it/s][A
 21%|██▏       | 47/221 [00:14<00:49,  3.55it/s][A
 22%|██▏       | 48/221 [00:14<00:42,  4.12it/s][A
 22%|██▏       | 49/221 [00:14<00:40,  4.25it/s][A
 23%|██▎       | 50/221 [00:14<00:35,  4.83it/s][A
 23%|██▎       | 51/221 [00:14<00:36,  4.67it/s][A
 24%|██▎       | 52/221 [00:14<00:30,  5.54it/s][A
 24%|██▍       | 53/221 [00:15<00:33,  5.00it/s][A
 24%|██▍       | 54/221 [00:15<00:51,  3.25it/s][A
 25%|██▍       | 55/221 [00:15<00:45,  3.66it/s][A
 25%|██▌       | 56/221 [00:16<00:42,  3.93it/s][A
 26%|██▌       | 57/221 [00:16<00:38,  4.31it/s][A
 26%|██▌       | 58/221 [00:16<00:39,  4.10it/s][A
 27%|██▋       | 60/221 [00:17<00:45,  3.52it/s][A
 28%|██▊       | 61/221 [00:17<00:43,  3.66it/s][A
 28%|██▊       | 62/221 [00:17<00:44,  3.60it/s][A
 29%|██▊       | 63/221 [00:18<00:48,  3.25it/s][A
 29%|██▉       | 64/221 [00:18<00:52,  3.00it/s][A
 29%|██▉       | 65/221 [00:19<00:59,  2.62it/s][A
 30%|██▉       | 66/221 [00:19<00:52,  2.95it/s][A
 30%|███       | 67/221 [00:19<01:02,  2.47it/s][A
 31%|███       | 68/221 [00:20<00:50,  3.05it/s][A
 31%|███       | 69/221 [00:20<00:46,  3.29it/s][A
 32%|███▏      | 70/221 [00:20<00:44,  3.40it/s][A
 32%|███▏      | 71/221 [00:20<00:45,  3.29it/s][A
 33%|███▎      | 72/221 [00:21<00:46,  3.20it/s][A
 33%|███▎      | 73/221 [00:21<00:53,  2.77it/s][A
 33%|███▎      | 74/221 [00:21<00:50,  2.94it/s][A
 34%|███▍      | 75/221 [00:22<00:46,  3.14it/s][A
 34%|███▍      | 76/221 [00:22<00:37,  3.89it/s][A
 35%|███▍      | 77/221 [00:22<00:35,  4.04it/s][A
 35%|███▌      | 78/221 [00:23<00:45,  3.15it/s][A
 36%|███▌      | 79/221 [00:23<00:58,  2.42it/s][A
 36%|███▌      | 80/221 [00:24<00:58,  2.42it/s][A
 37%|███▋      | 81/221 [00:24<00:51,  2.73it/s][A
 37%|███▋      | 82/221 [00:24<00:58,  2.38it/s][A
 38%|███▊      | 83/221 [00:25<01:07,  2.04it/s][A
 38%|███▊      | 84/221 [00:26<01:08,  2.00it/s][A
 38%|███▊      | 85/221 [00:26<00:58,  2.33it/s][A
 39%|███▉      | 86/221 [00:26<01:00,  2.25it/s][A
 39%|███▉      | 87/221 [00:27<01:11,  1.87it/s][A
 40%|███▉      | 88/221 [00:27<00:57,  2.32it/s][A
 40%|████      | 89/221 [00:28<00:50,  2.61it/s][A
 41%|████      | 90/221 [00:28<00:42,  3.05it/s][A
 41%|████      | 91/221 [00:28<00:34,  3.79it/s][A
 42%|████▏     | 92/221 [00:28<00:31,  4.06it/s][A
 43%|████▎     | 94/221 [00:28<00:25,  4.93it/s][A
 43%|████▎     | 95/221 [00:29<00:32,  3.90it/s][A
 43%|████▎     | 96/221 [00:29<00:30,  4.16it/s][A
 44%|████▍     | 97/221 [00:29<00:26,  4.74it/s][A
 44%|████▍     | 98/221 [00:30<00:35,  3.42it/s][A
 45%|████▍     | 99/221 [00:30<00:30,  4.05it/s][A
 45%|████▌     | 100/221 [00:30<00:29,  4.11it/s][A
 46%|████▌     | 101/221 [00:30<00:29,  4.08it/s][A
 46%|████▌     | 102/221 [00:31<00:30,  3.92it/s][A
 47%|████▋     | 104/221 [00:31<00:20,  5.68it/s][A
 48%|████▊     | 105/221 [00:31<00:20,  5.61it/s][A
 48%|████▊     | 106/221 [00:31<00:28,  4.01it/s][A
 48%|████▊     | 107/221 [00:32<00:27,  4.07it/s][A
 49%|████▉     | 108/221 [00:32<00:31,  3.64it/s][A
 49%|████▉     | 109/221 [00:32<00:40,  2.76it/s][A
 50%|████▉     | 110/221 [00:33<00:38,  2.90it/s][A
 50%|█████     | 111/221 [00:33<00:38,  2.88it/s][A
 51%|█████     | 112/221 [00:33<00:35,  3.10it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.33it/s][A
 52%|█████▏    | 115/221 [00:34<00:23,  4.59it/s][A
 53%|█████▎    | 117/221 [00:34<00:21,  4.73it/s][A
 53%|█████▎    | 118/221 [00:35<00:29,  3.51it/s][A
 54%|█████▍    | 119/221 [00:35<00:26,  3.83it/s][A
 54%|█████▍    | 120/221 [00:35<00:29,  3.41it/s][A
 55%|█████▍    | 121/221 [00:36<00:28,  3.53it/s][A
 55%|█████▌    | 122/221 [00:36<00:27,  3.60it/s][A
 56%|█████▌    | 123/221 [00:36<00:26,  3.74it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.33it/s][A
 57%|█████▋    | 125/221 [00:37<00:36,  2.62it/s][A
 57%|█████▋    | 126/221 [00:37<00:34,  2.76it/s][A
 57%|█████▋    | 127/221 [00:38<00:26,  3.51it/s][A
 58%|█████▊    | 128/221 [00:38<00:26,  3.47it/s][A
 58%|█████▊    | 129/221 [00:38<00:23,  3.86it/s][A
 59%|█████▉    | 130/221 [00:38<00:27,  3.29it/s][A
 59%|█████▉    | 131/221 [00:39<00:23,  3.89it/s][A
 60%|█████▉    | 132/221 [00:39<00:20,  4.34it/s][A
 60%|██████    | 133/221 [00:39<00:31,  2.79it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.25it/s][A
 61%|██████    | 135/221 [00:40<00:31,  2.74it/s][A
 62%|██████▏   | 136/221 [00:40<00:31,  2.72it/s][A
 62%|██████▏   | 137/221 [00:41<00:27,  3.01it/s][A
 62%|██████▏   | 138/221 [00:41<00:27,  3.03it/s][A
 63%|██████▎   | 139/221 [00:41<00:25,  3.21it/s][A
 63%|██████▎   | 140/221 [00:42<00:26,  3.09it/s][A
 64%|██████▍   | 141/221 [00:42<00:23,  3.40it/s][A
 64%|██████▍   | 142/221 [00:42<00:22,  3.50it/s][A
 65%|██████▌   | 144/221 [00:42<00:16,  4.56it/s][A
 66%|██████▌   | 145/221 [00:43<00:15,  4.99it/s][A
 66%|██████▌   | 146/221 [00:43<00:14,  5.16it/s][A
 67%|██████▋   | 147/221 [00:43<00:12,  5.86it/s][A
 67%|██████▋   | 148/221 [00:43<00:15,  4.57it/s][A
 67%|██████▋   | 149/221 [00:43<00:14,  4.84it/s][A
 68%|██████▊   | 150/221 [00:44<00:22,  3.19it/s][A
 68%|██████▊   | 151/221 [00:44<00:17,  3.89it/s][A
 69%|██████▉   | 152/221 [00:45<00:25,  2.74it/s][A
 69%|██████▉   | 153/221 [00:45<00:25,  2.63it/s][A
 70%|██████▉   | 154/221 [00:46<00:28,  2.35it/s][A
 70%|███████   | 155/221 [00:46<00:24,  2.72it/s][A
 71%|███████   | 156/221 [00:46<00:21,  3.03it/s][A
 71%|███████   | 157/221 [00:46<00:21,  2.98it/s][A
 71%|███████▏  | 158/221 [00:47<00:22,  2.81it/s][A
 72%|███████▏  | 159/221 [00:47<00:20,  3.10it/s][A
 72%|███████▏  | 160/221 [00:47<00:19,  3.05it/s][A
 73%|███████▎  | 161/221 [00:48<00:16,  3.67it/s][A
 73%|███████▎  | 162/221 [00:48<00:13,  4.48it/s][A
 74%|███████▍  | 163/221 [00:48<00:14,  4.05it/s][A
 74%|███████▍  | 164/221 [00:48<00:14,  4.06it/s][A
 75%|███████▌  | 166/221 [00:49<00:13,  3.97it/s][A
 76%|███████▌  | 167/221 [00:49<00:13,  4.08it/s][A
 76%|███████▌  | 168/221 [00:49<00:14,  3.64it/s][A
 76%|███████▋  | 169/221 [00:50<00:14,  3.60it/s][A
 77%|███████▋  | 170/221 [00:50<00:17,  2.84it/s][A
 77%|███████▋  | 171/221 [00:51<00:18,  2.75it/s][A
 78%|███████▊  | 172/221 [00:51<00:14,  3.33it/s][A
 79%|███████▊  | 174/221 [00:51<00:14,  3.23it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.19it/s][A
 80%|███████▉  | 176/221 [00:52<00:15,  2.94it/s][A
 80%|████████  | 177/221 [00:52<00:14,  3.13it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.46it/s][A
 81%|████████  | 179/221 [00:53<00:13,  3.19it/s][A
 81%|████████▏ | 180/221 [00:53<00:10,  3.87it/s][A
 82%|████████▏ | 182/221 [00:53<00:08,  4.67it/s][A
 83%|████████▎ | 183/221 [00:54<00:07,  4.86it/s][A
 83%|████████▎ | 184/221 [00:54<00:13,  2.76it/s][A
 84%|████████▎ | 185/221 [00:55<00:11,  3.00it/s][A
 84%|████████▍ | 186/221 [00:55<00:11,  3.04it/s][A
 85%|████████▍ | 187/221 [00:55<00:09,  3.72it/s][A
 85%|████████▌ | 188/221 [00:56<00:10,  3.25it/s][A
 86%|████████▌ | 189/221 [00:56<00:11,  2.83it/s][A
 86%|████████▌ | 190/221 [00:57<00:13,  2.27it/s][A
 86%|████████▋ | 191/221 [00:57<00:11,  2.63it/s][A
 87%|████████▋ | 192/221 [00:57<00:09,  3.11it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.44it/s][A
 88%|████████▊ | 194/221 [00:58<00:10,  2.59it/s][A
 88%|████████▊ | 195/221 [00:59<00:12,  2.16it/s][A
 89%|████████▊ | 196/221 [00:59<00:12,  2.03it/s][A
 89%|████████▉ | 197/221 [00:59<00:11,  2.14it/s][A
 90%|████████▉ | 198/221 [01:00<00:08,  2.75it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.36it/s][A
 90%|█████████ | 200/221 [01:01<00:09,  2.25it/s][A
 91%|█████████ | 201/221 [01:01<00:08,  2.49it/s][A
 91%|█████████▏| 202/221 [01:01<00:06,  3.14it/s][A
 92%|█████████▏| 203/221 [01:01<00:04,  3.72it/s][A
 92%|█████████▏| 204/221 [01:02<00:06,  2.83it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.42it/s][A
 93%|█████████▎| 206/221 [01:02<00:05,  2.83it/s][A
 94%|█████████▎| 207/221 [01:02<00:04,  3.43it/s][A
 94%|█████████▍| 208/221 [01:03<00:04,  2.81it/s][A
 95%|█████████▍| 209/221 [01:03<00:04,  2.47it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.16it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.08it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.60it/s][A
 96%|█████████▋| 213/221 [01:05<00:02,  2.70it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  2.63it/s][A
 97%|█████████▋| 215/221 [01:06<00:02,  2.38it/s][A
 98%|█████████▊| 216/221 [01:06<00:01,  2.77it/s][A
 98%|█████████▊| 217/221 [01:06<00:01,  2.85it/s][A
 99%|█████████▊| 218/221 [01:06<00:01,  2.93it/s][A
 99%|█████████▉| 219/221 [01:07<00:00,  3.43it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.69it/s][A
100%|██████████| 221/221 [01:07<00:00,  3.31it/s][A100%|██████████| 221/221 [01:07<00:00,  3.26it/s]
09/18/2024 18:52:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 599--===========

09/18/2024 18:52:34 - INFO - __main__ -   {'area_r1': 40.2, 'area_recall': '40.2/66.5/76.1', 'area_ravg': 60.9}
09/18/2024 18:52:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 599--===========

09/18/2024 18:52:34 - INFO - __main__ -   {'forward_r1': 36.2, 'forward_recall': '36.2/65.4/75.6', 'forward_ravg': 59.0}
09/18/2024 18:52:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 599--===========

09/18/2024 18:52:34 - INFO - __main__ -   {'area_video_r1': 38.5, 'area_video_recall': '38.5/66.9/77.8', 'area_video_ravg': 61.0}
09/18/2024 18:52:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 18:52:34 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 18:52:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 599--===========

09/18/2024 18:52:34 - INFO - __main__ -   {'area_video_r1': 52.5, 'area_video_recall': '52.5/73.0/81.4', 'area_video_ravg': 69.0, 'area_video_back_r1': 49.4, 'area_video_back_recall': '49.4/71.4/78.4', 'area_video_back_ravg': 66.4}
09/18/2024 18:52:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 499=======

09/18/2024 18:52:34 - INFO - __main__ -   {'area_video_r1': 52.6, 'area_video_recall': '52.6/73.1/81.2', 'area_video_ravg': 69.0, 'area_video_back_r1': 49.5, 'area_video_back_recall': '49.5/71.5/78.2', 'area_video_back_ravg': 66.4}
09/18/2024 18:52:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 599--===========

09/18/2024 18:52:34 - INFO - __main__ -   {'video_r1': 27.6, 'video_recall': '27.6/52.7/63.2', 'video_ravg': 47.9}
09/18/2024 18:52:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 18:52:34 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 18:52:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 599--===========

09/18/2024 18:52:34 - INFO - __main__ -   {'video_r1': 49.9, 'video_recall': '49.9/69.2/77.0', 'video_ravg': 65.4}
09/18/2024 18:52:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 18:52:34 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 18:52:55 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.00030376866925507784, 'loss_ret%tv%ta--finetune_area/loss_area': 1.1537144184112549, 'loss_ret%tv%ta--finetune_area/total_loss': 1.1540181636810303}
 21%|██        | 600/2910 [3:47:48<86:30:18, 134.81s/it] 21%|██        | 601/2910 [3:47:51<61:13:10, 95.45s/it] [h264 @ 0x55cc1a6b2f80] mmco: unref short failure
 21%|██        | 602/2910 [3:47:56<43:39:57, 68.11s/it] 21%|██        | 603/2910 [3:48:00<31:24:13, 49.00s/it] 21%|██        | 604/2910 [3:48:05<22:58:32, 35.87s/it][h264 @ 0x55665e6e0600] mmco: unref short failure
 21%|██        | 605/2910 [3:48:10<16:57:14, 26.48s/it][h264 @ 0x5599930d8b00] mmco: unref short failure
[h264 @ 0x5599930d8b00] mmco: unref short failure
[h264 @ 0x5599930d8b00] mmco: unref short failure
[h264 @ 0x5599930d8b00] mmco: unref short failure
 21%|██        | 606/2910 [3:48:15<12:53:56, 20.15s/it] 21%|██        | 607/2910 [3:48:21<10:02:42, 15.70s/it][h264 @ 0x556698682580] mmco: unref short failure
 21%|██        | 608/2910 [3:48:26<8:00:29, 12.52s/it] [h264 @ 0x55999ee9b5c0] mmco: unref short failure
 21%|██        | 609/2910 [3:48:32<6:44:49, 10.56s/it] 21%|██        | 610/2910 [3:48:37<5:48:52,  9.10s/it][h264 @ 0x559998d74d00] mmco: unref short failure
[h264 @ 0x55cc220a7a40] mmco: unref short failure
[h264 @ 0x55cc220a7a40] mmco: unref short failure
 21%|██        | 611/2910 [3:48:42<5:01:29,  7.87s/it][h264 @ 0x556697dcd580] mmco: unref short failure
[h264 @ 0x556697dcd580] mmco: unref short failure
 21%|██        | 612/2910 [3:48:50<4:52:11,  7.63s/it][h264 @ 0x55cc14402200] mmco: unref short failure
[h264 @ 0x55cc14402200] mmco: unref short failure
 21%|██        | 613/2910 [3:48:56<4:39:47,  7.31s/it] 21%|██        | 614/2910 [3:49:02<4:21:24,  6.83s/it][h264 @ 0x5599a1d26d40] mmco: unref short failure
[h264 @ 0x5599a1d26d40] mmco: unref short failure
[h264 @ 0x5599a1d26d40] mmco: unref short failure
 21%|██        | 615/2910 [3:49:07<4:04:23,  6.39s/it][h264 @ 0x55d9bc7a0ac0] mmco: unref short failure
[h264 @ 0x5599a14cad40] mmco: unref short failure
[h264 @ 0x5599a14cad40] mmco: unref short failure
[h264 @ 0x55cc1f849500] mmco: unref short failure
[h264 @ 0x5599a38a4380] mmco: unref short failure
[h264 @ 0x5599992a90c0] mmco: unref short failure
[h264 @ 0x5599992a90c0] mmco: unref short failure
[h264 @ 0x5599992a90c0] mmco: unref short failure
[h264 @ 0x5599992a90c0] mmco: unref short failure
[h264 @ 0x55d9b4eaf280] mmco: unref short failure
[h264 @ 0x55d9b4eaf280] mmco: unref short failure
09/18/2024 18:54:38 - INFO - __main__ -   current idx S4W-P5aCWJs.99 from finetune_area returns wrong image/video, use 18278 instead.
[h264 @ 0x55d9b5325dc0] mmco: unref short failure
[h264 @ 0x5566a4999480] mmco: unref short failure
[h264 @ 0x5566a4999480] mmco: unref short failure
[h264 @ 0x5599a3310440] mmco: unref short failure
[h264 @ 0x559995d1c280] mmco: unref short failure
[h264 @ 0x5599953076c0] mmco: unref short failure
[h264 @ 0x559995479000] mmco: unref short failure
[h264 @ 0x559995479000] mmco: unref short failure
[h264 @ 0x559995479000] mmco: unref short failure
[h264 @ 0x559995479000] mmco: unref short failure
[h264 @ 0x55d9b4e9a180] mmco: unref short failure
[h264 @ 0x55d9b4e9a180] mmco: unref short failure
[h264 @ 0x559991790740] mmco: unref short failure
 21%|██        | 616/2910 [3:49:54<11:52:41, 18.64s/it][h264 @ 0x55665c94abc0] mmco: unref short failure
[h264 @ 0x55665c94abc0] mmco: unref short failure
 21%|██        | 617/2910 [3:50:06<10:31:27, 16.52s/it][h264 @ 0x55d9bf2bce80] mmco: unref short failure
[h264 @ 0x55d9bf2bce80] mmco: unref short failure
[h264 @ 0x55cc182e3880] mmco: unref short failure
[h264 @ 0x559999f99740] mmco: unref short failure
[h264 @ 0x559999f99740] mmco: unref short failure
[h264 @ 0x5599a0f27140] mmco: unref short failure
[h264 @ 0x5599a0f27140] mmco: unref short failure
[h264 @ 0x5599a0f27140] mmco: unref short failure
[h264 @ 0x5599a0f27140] mmco: unref short failure
 21%|██        | 618/2910 [3:50:21<10:14:37, 16.09s/it] 21%|██▏       | 619/2910 [3:50:26<8:07:52, 12.78s/it] [h264 @ 0x55d9bf324e80] mmco: unref short failure
[h264 @ 0x55d9bf324e80] mmco: unref short failure
[h264 @ 0x55d9bf324e80] mmco: unref short failure
[h264 @ 0x55d9bf324e80] mmco: unref short failure
[h264 @ 0x5566987c0380] mmco: unref short failure
[h264 @ 0x55665cb37940] mmco: unref short failure
[h264 @ 0x55665cb37940] mmco: unref short failure
 21%|██▏       | 620/2910 [3:50:36<7:31:39, 11.83s/it] 21%|██▏       | 621/2910 [3:50:44<6:49:49, 10.74s/it]09/18/2024 18:55:57 - INFO - __main__ -   current idx 3Dl8wLo1W6E.35 from finetune_area returns wrong image/video, use 143646 instead.
 21%|██▏       | 622/2910 [3:50:55<6:52:02, 10.81s/it][h264 @ 0x55d9b6ec7b00] mmco: unref short failure
 21%|██▏       | 623/2910 [3:51:00<5:46:16,  9.08s/it][h264 @ 0x55669ef99640] mmco: unref short failure
[h264 @ 0x55669ef99640] mmco: unref short failure
[h264 @ 0x55d9c362a480] mmco: unref short failure
[h264 @ 0x55d9c362a480] mmco: unref short failure
[h264 @ 0x55cc142cd880] mmco: unref short failure
[h264 @ 0x55cc142cd880] mmco: unref short failure
[h264 @ 0x55cc142cd880] mmco: unref short failure
[h264 @ 0x55669f77cd40] mmco: unref short failure
[h264 @ 0x55d9b302b780] mmco: unref short failure
[h264 @ 0x55d9be620540] mmco: unref short failure
[h264 @ 0x55d9be620540] mmco: unref short failure
09/18/2024 18:56:38 - INFO - __main__ -   current idx EAcu0rdv1mY.94 from finetune_area returns wrong image/video, use 77101 instead.
[h264 @ 0x55cc1ceab6c0] mmco: unref short failure
[h264 @ 0x55d9bc8b8d80] mmco: unref short failure
[h264 @ 0x55cc1855a000] mmco: unref short failure
[h264 @ 0x55cc1855a000] mmco: unref short failure
[h264 @ 0x55669b8c3700] mmco: unref short failure
[h264 @ 0x55cc1539d240] mmco: unref short failure
[h264 @ 0x55cc1539d240] mmco: unref short failure
[h264 @ 0x55999f2bda40] mmco: unref short failure
[h264 @ 0x55cc239511c0] mmco: unref short failure
[h264 @ 0x55cc239511c0] mmco: unref short failure
[h264 @ 0x5566a2b5af80] mmco: unref short failure
[h264 @ 0x5566a2b5af80] mmco: unref short failure
[h264 @ 0x55669c0f2980] mmco: unref short failure
[h264 @ 0x55d9c2f93b80] mmco: unref short failure
[h264 @ 0x556695344240] mmco: unref short failure
[h264 @ 0x556695344240] mmco: unref short failure
 21%|██▏       | 624/2910 [3:52:15<18:25:59, 29.03s/it][h264 @ 0x559999c81fc0] mmco: unref short failure
[h264 @ 0x55d9c91d4780] mmco: unref short failure
[h264 @ 0x55d9c4642c80] mmco: unref short failure
[h264 @ 0x55d9cb87d680] mmco: unref short failure
 21%|██▏       | 625/2910 [3:52:36<16:50:40, 26.54s/it]09/18/2024 18:57:58 - INFO - __main__ -   current idx QzpF1yDPHf0.29 from finetune_area returns wrong image/video, use 96137 instead.
 22%|██▏       | 626/2910 [3:52:50<14:27:36, 22.79s/it][h264 @ 0x55999644a540] mmco: unref short failure
[h264 @ 0x55999644a540] mmco: unref short failure
[h264 @ 0x55d9b9b759c0] mmco: unref short failure
[h264 @ 0x55d9b9b759c0] mmco: unref short failure
 22%|██▏       | 627/2910 [3:52:56<11:07:37, 17.55s/it] 22%|██▏       | 628/2910 [3:53:01<8:44:52, 13.80s/it] [h264 @ 0x55669663f800] mmco: unref short failure
[h264 @ 0x55669663f800] mmco: unref short failure
[h264 @ 0x55669f9aeec0] mmco: unref short failure
[h264 @ 0x55669f9aeec0] mmco: unref short failure
[h264 @ 0x55999f8584c0] mmco: unref short failure
[h264 @ 0x55d9b9889e80] mmco: unref short failure
 22%|██▏       | 629/2910 [3:53:12<8:15:18, 13.03s/it][h264 @ 0x55999b79cd00] mmco: unref short failure
 22%|██▏       | 630/2910 [3:53:22<7:42:03, 12.16s/it][h264 @ 0x5599912a2c80] mmco: unref short failure
[h264 @ 0x556699134000] mmco: unref short failure
[h264 @ 0x55665d89b700] mmco: unref short failure
09/18/2024 18:58:36 - INFO - __main__ -   current idx g_6rVdVgpd0.43 from finetune_area returns wrong image/video, use 48645 instead.
 22%|██▏       | 631/2910 [3:53:27<6:21:27, 10.04s/it][h264 @ 0x5566986f1a00] mmco: unref short failure
[h264 @ 0x5566986f1a00] mmco: unref short failure
[h264 @ 0x55d9bbe65c80] mmco: unref short failure
[h264 @ 0x55d9bbe65c80] mmco: unref short failure
[h264 @ 0x55cc1a1f8940] mmco: unref short failure
[h264 @ 0x5566963a2f80] mmco: unref short failure
[h264 @ 0x5566963a2f80] mmco: unref short failure
[h264 @ 0x5566963a2f80] mmco: unref short failure
[h264 @ 0x5566963a2f80] mmco: unref short failure
[h264 @ 0x55d9b3555c40] mmco: unref short failure
[h264 @ 0x55d9b3555c40] mmco: unref short failure
09/18/2024 18:59:35 - INFO - __main__ -   current idx O_qInVZBZaw.18 from finetune_area returns wrong image/video, use 66880 instead.
[h264 @ 0x55d9b79ca900] mmco: unref short failure
[h264 @ 0x55d9c9ef89c0] mmco: unref short failure
[h264 @ 0x55cc1a304c80] mmco: unref short failure
09/18/2024 18:59:47 - INFO - __main__ -   current idx eB3AXJxM634.13 from finetune_area returns wrong image/video, use 33993 instead.
[h264 @ 0x5599a2440700] mmco: unref short failure
[h264 @ 0x5599a30c17c0] mmco: unref short failure
[h264 @ 0x556698ab6a40] mmco: unref short failure
 22%|██▏       | 632/2910 [3:54:50<20:10:59, 31.90s/it][h264 @ 0x55cc17b03f40] mmco: unref short failure
[h264 @ 0x55cc17b03f40] mmco: unref short failure
[h264 @ 0x55d9c5fac540] mmco: unref short failure
[h264 @ 0x55d9c7271080] mmco: unref short failure
[h264 @ 0x55d9c7271080] mmco: unref short failure
 22%|██▏       | 633/2910 [3:55:12<18:12:30, 28.79s/it][h264 @ 0x55d9c2a78040] mmco: unref short failure
[h264 @ 0x55d9c2a78040] mmco: unref short failure
[h264 @ 0x55d9c2a78040] mmco: unref short failure
 22%|██▏       | 634/2910 [3:55:17<13:43:45, 21.72s/it][h264 @ 0x55cc16328880] mmco: unref short failure
[h264 @ 0x55cc16328880] mmco: unref short failure
 22%|██▏       | 635/2910 [3:55:22<10:37:40, 16.82s/it][h264 @ 0x55cc297dacc0] mmco: unref short failure
 22%|██▏       | 636/2910 [3:55:27<8:24:47, 13.32s/it] [h264 @ 0x55d9b7f00880] mmco: unref short failure
 22%|██▏       | 637/2910 [3:55:36<7:33:20, 11.97s/it][h264 @ 0x55d9c6995ec0] mmco: unref short failure
[h264 @ 0x55d9c6995ec0] mmco: unref short failure
[h264 @ 0x55d9c57377c0] mmco: unref short failure
[h264 @ 0x55d9c57377c0] mmco: unref short failure
 22%|██▏       | 638/2910 [3:55:50<8:00:35, 12.69s/it][h264 @ 0x556695c20f40] mmco: unref short failure
 22%|██▏       | 639/2910 [3:55:56<6:37:57, 10.51s/it][h264 @ 0x5599a766bcc0] mmco: unref short failure
[h264 @ 0x5599a766bcc0] mmco: unref short failure
[h264 @ 0x5599913b6dc0] mmco: unref short failure
[h264 @ 0x5599913b6dc0] mmco: unref short failure
[h264 @ 0x55cc17788280] mmco: unref short failure
[h264 @ 0x55cc17788280] mmco: unref short failure
[h264 @ 0x559997acb6c0] mmco: unref short failure
[h264 @ 0x559997acb6c0] mmco: unref short failure
[h264 @ 0x55d9ba61b000] mmco: unref short failure
[h264 @ 0x55d9ba61b000] mmco: unref short failure
[h264 @ 0x55d9ba61b000] mmco: unref short failure
[h264 @ 0x55999ca88940] mmco: unref short failure
[h264 @ 0x55999ca88940] mmco: unref short failure
[h264 @ 0x55d9b380e1c0] mmco: unref short failure
[h264 @ 0x55d9b380e1c0] mmco: unref short failure
[h264 @ 0x55d9b437bd80] mmco: unref short failure
[h264 @ 0x55d9b437bd80] mmco: unref short failure
[h264 @ 0x55d9b2f20880] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x5566a9c34380] mmco: unref short failure
[h264 @ 0x55d9be274780] mmco: unref short failure
[h264 @ 0x55d9be274780] mmco: unref short failure
[h264 @ 0x55d9be274780] mmco: unref short failure
[h264 @ 0x55d9bcd9b140] mmco: unref short failure
[h264 @ 0x55cc28f056c0] mmco: unref short failure
[h264 @ 0x55cc2266bbc0] mmco: unref short failure
[h264 @ 0x55cc2266bbc0] mmco: unref short failure
[h264 @ 0x55d9b5917600] mmco: unref short failure
09/18/2024 19:02:09 - INFO - __main__ -   current idx TjXtZRjeWIo.2 from finetune_area returns wrong image/video, use 118793 instead.
 22%|██▏       | 640/2910 [3:57:08<18:15:32, 28.96s/it]09/18/2024 19:02:40 - INFO - __main__ -   current idx 7i_0A4oZMd4.75 from finetune_area returns wrong image/video, use 44474 instead.
[h264 @ 0x5566a56d77c0] mmco: unref short failure
[h264 @ 0x5566a56d77c0] mmco: unref short failure
[h264 @ 0x5566a56d77c0] mmco: unref short failure
[h264 @ 0x55d9c1890480] mmco: unref short failure
[h264 @ 0x55999ffee240] mmco: unref short failure
 22%|██▏       | 641/2910 [3:57:40<18:50:47, 29.90s/it][h264 @ 0x55d9c1899440] mmco: unref short failure
[h264 @ 0x55d9c1899440] mmco: unref short failure
 22%|██▏       | 642/2910 [3:57:46<14:15:47, 22.64s/it][h264 @ 0x5599925e8a40] mmco: unref short failure
[h264 @ 0x5599925e8a40] mmco: unref short failure
09/18/2024 19:02:57 - INFO - __main__ -   current idx 1o1hMc8PbOU.62 from finetune_area returns wrong image/video, use 91762 instead.
 22%|██▏       | 643/2910 [3:57:51<10:56:24, 17.37s/it] 22%|██▏       | 644/2910 [3:57:56<8:43:46, 13.87s/it] [h264 @ 0x55d9c6ff7c40] mmco: unref short failure
[h264 @ 0x55d9c6ff7c40] mmco: unref short failure
[h264 @ 0x55d9c6ff7c40] mmco: unref short failure
[h264 @ 0x55d9c6ff7c40] mmco: unref short failure
[h264 @ 0x55669717ad00] mmco: unref short failure
 22%|██▏       | 645/2910 [3:58:03<7:18:30, 11.62s/it][h264 @ 0x55d9c948d1c0] mmco: unref short failure
[h264 @ 0x55d9c948d1c0] mmco: unref short failure
[h264 @ 0x55d9c948d1c0] mmco: unref short failure
[h264 @ 0x55d9c948d1c0] mmco: unref short failure
[h264 @ 0x5566a8d27a00] mmco: unref short failure
 22%|██▏       | 646/2910 [3:58:19<8:08:04, 12.93s/it][h264 @ 0x55d9b85dc680] mmco: unref short failure
[h264 @ 0x55d9b85dc680] mmco: unref short failure
 22%|██▏       | 647/2910 [3:58:24<6:40:21, 10.61s/it][h264 @ 0x55d9b6a5cf40] mmco: unref short failure
[h264 @ 0x55669a958f00] mmco: unref short failure
[h264 @ 0x55669a958f00] mmco: unref short failure
[h264 @ 0x5566a81d4a40] mmco: unref short failure
[h264 @ 0x5566a81d4a40] mmco: unref short failure
[h264 @ 0x5566a81d4a40] mmco: unref short failure
[h264 @ 0x5566a81d4a40] mmco: unref short failure
[h264 @ 0x55669891cb00] mmco: unref short failure
[h264 @ 0x55cc1790e700] mmco: unref short failure
[h264 @ 0x55cc1790e700] mmco: unref short failure
[h264 @ 0x55cc1790e700] mmco: unref short failure
[h264 @ 0x55cc1790e700] mmco: unref short failure
[h264 @ 0x55999b766200] mmco: unref short failure
[h264 @ 0x55669b1e7b00] mmco: unref short failure
[h264 @ 0x55669b1e7b00] mmco: unref short failure
[h264 @ 0x559995331f00] mmco: unref short failure
[h264 @ 0x559995331f00] mmco: unref short failure
[h264 @ 0x559995331f00] mmco: unref short failure
[h264 @ 0x559995331f00] mmco: unref short failure
[h264 @ 0x5599a90ba980] mmco: unref short failure
[h264 @ 0x5599a90ba980] mmco: unref short failure
[h264 @ 0x55d9cb7f11c0] mmco: unref short failure
[h264 @ 0x55d9cb7f11c0] mmco: unref short failure
[h264 @ 0x55d9cb7f11c0] mmco: unref short failure
[h264 @ 0x55d9cb7f11c0] mmco: unref short failure
[h264 @ 0x55d9bbece040] mmco: unref short failure
[h264 @ 0x559992e093c0] mmco: unref short failure
 22%|██▏       | 648/2910 [3:59:41<19:13:01, 30.58s/it][h264 @ 0x55d9cb705b80] mmco: unref short failure
[h264 @ 0x5566a37cd700] mmco: unref short failure
[h264 @ 0x5566a37cd700] mmco: unref short failure
[h264 @ 0x55d9cd04a980] mmco: unref short failure
[h264 @ 0x55d9cd04a980] mmco: unref short failure
 22%|██▏       | 649/2910 [4:00:15<19:53:29, 31.67s/it]09/18/2024 19:05:25 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/18/2024 19:05:25 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x556698e91d40] mmco: unref short failure
[h264 @ 0x5599ad00bc40] mmco: unref short failure
[h264 @ 0x55d9bbeee340] mmco: unref short failure
[h264 @ 0x55d9bbeee340] mmco: unref short failure
09/18/2024 19:05:52 - INFO - __main__ -   current idx 2xMe1diCHmI.53 from finetune_area returns wrong image/video, use 19642 instead.
[h264 @ 0x55665ced2400] mmco: unref short failure
[h264 @ 0x5566a4aa0440] mmco: unref short failure
[h264 @ 0x5566a4aa0440] mmco: unref short failure
[h264 @ 0x5599a1de2440] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x5599987fb6c0] mmco: unref short failure
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/18/2024 19:06:06 - INFO - __main__ -   current idx SQ8PD29d_RE.6 from finetune_area returns wrong image/video, use 136394 instead.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
[h264 @ 0x559992a21600] mmco: unref short failure
[h264 @ 0x559992a21600] mmco: unref short failure
[h264 @ 0x5566a4aa7c40] mmco: unref short failure
[h264 @ 0x55cc1eafd680] mmco: unref short failure
[h264 @ 0x55cc1eafd680] mmco: unref short failure
[h264 @ 0x55cc1eafd680] mmco: unref short failure
[h264 @ 0x55cc1eafd680] mmco: unref short failure
[h264 @ 0x55d9c49792c0] mmco: unref short failure
[h264 @ 0x559997c71e00] mmco: unref short failure
[h264 @ 0x559997c71e00] mmco: unref short failure
[h264 @ 0x55cc196ff100] mmco: unref short failure
[h264 @ 0x5599a2450080] mmco: unref short failure
[h264 @ 0x55d9baaf7340] mmco: unref short failure
[h264 @ 0x55d9baaf7340] mmco: unref short failure
[h264 @ 0x5566a929a840] mmco: unref short failure
[h264 @ 0x55d9c5a67980] mmco: unref short failure
[h264 @ 0x55cc244a9a80] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<02:21,  1.56it/s][A
  1%|          | 2/221 [00:01<01:57,  1.87it/s][A
  1%|▏         | 3/221 [00:01<01:23,  2.61it/s][A
  2%|▏         | 4/221 [00:01<01:00,  3.57it/s][A[h264 @ 0x5566a18abe40] mmco: unref short failure
[h264 @ 0x5566a18abe40] mmco: unref short failure

  2%|▏         | 5/221 [00:01<00:50,  4.30it/s][A[h264 @ 0x5566a18abe40] mmco: unref short failure
[h264 @ 0x5566a18abe40] mmco: unref short failure

  3%|▎         | 6/221 [00:01<00:46,  4.64it/s][A
  3%|▎         | 7/221 [00:01<00:47,  4.55it/s][A
  4%|▎         | 8/221 [00:02<01:01,  3.47it/s][A
  4%|▍         | 9/221 [00:02<01:04,  3.31it/s][A
  5%|▍         | 10/221 [00:03<01:07,  3.11it/s][A
  5%|▍         | 11/221 [00:03<01:00,  3.45it/s][A
  5%|▌         | 12/221 [00:03<01:12,  2.90it/s][A
  6%|▌         | 13/221 [00:04<01:17,  2.70it/s][A
  6%|▋         | 14/221 [00:05<01:52,  1.83it/s][A
  7%|▋         | 15/221 [00:05<01:30,  2.27it/s][A
  7%|▋         | 16/221 [00:05<01:29,  2.30it/s][A
  8%|▊         | 17/221 [00:06<01:43,  1.98it/s][A
  8%|▊         | 18/221 [00:06<01:31,  2.22it/s][A
  9%|▊         | 19/221 [00:06<01:12,  2.79it/s][A
  9%|▉         | 20/221 [00:07<00:58,  3.41it/s][A[h264 @ 0x55cc1c56b900] mmco: unref short failure

 10%|▉         | 21/221 [00:07<00:55,  3.61it/s][A
 10%|▉         | 22/221 [00:07<00:59,  3.37it/s][A
 11%|█         | 24/221 [00:07<00:41,  4.73it/s][A
 11%|█▏        | 25/221 [00:08<00:41,  4.78it/s][A
 12%|█▏        | 26/221 [00:08<00:55,  3.48it/s][A
 12%|█▏        | 27/221 [00:08<00:47,  4.08it/s][A
 13%|█▎        | 28/221 [00:09<00:57,  3.38it/s][A[h264 @ 0x5566af677480] mmco: unref short failure

 13%|█▎        | 29/221 [00:09<01:25,  2.25it/s][A
 14%|█▎        | 30/221 [00:10<01:15,  2.53it/s][A
 14%|█▍        | 31/221 [00:10<01:15,  2.52it/s][A
 14%|█▍        | 32/221 [00:10<01:07,  2.79it/s][A
 15%|█▍        | 33/221 [00:11<01:04,  2.91it/s][A
 15%|█▌        | 34/221 [00:11<01:00,  3.07it/s][A
 16%|█▌        | 35/221 [00:11<00:54,  3.43it/s][A
 16%|█▋        | 36/221 [00:12<00:58,  3.16it/s][A
 17%|█▋        | 37/221 [00:12<01:24,  2.18it/s][A
 17%|█▋        | 38/221 [00:13<01:19,  2.31it/s][A[h264 @ 0x55cc299288c0] mmco: unref short failure
[h264 @ 0x55cc299288c0] mmco: unref short failure

 18%|█▊        | 39/221 [00:13<01:12,  2.52it/s][A[h264 @ 0x55d9ca271600] mmco: unref short failure
[h264 @ 0x55d9ca271600] mmco: unref short failure

 18%|█▊        | 40/221 [00:13<01:08,  2.65it/s][A
 19%|█▊        | 41/221 [00:14<00:55,  3.26it/s][A
 19%|█▉        | 42/221 [00:14<01:01,  2.90it/s][A
 19%|█▉        | 43/221 [00:14<00:49,  3.61it/s][A
 20%|█▉        | 44/221 [00:14<00:41,  4.28it/s][A
 20%|██        | 45/221 [00:15<01:31,  1.93it/s][A
 21%|██        | 46/221 [00:16<01:31,  1.92it/s][A
 21%|██▏       | 47/221 [00:17<01:37,  1.79it/s][A
 22%|██▏       | 48/221 [00:17<01:18,  2.20it/s][A[h264 @ 0x5566a47fa3c0] mmco: unref short failure
[h264 @ 0x5566a47fa3c0] mmco: unref short failure

 22%|██▏       | 49/221 [00:18<01:36,  1.78it/s][A
 23%|██▎       | 50/221 [00:18<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:18<01:12,  2.34it/s][A
 24%|██▎       | 52/221 [00:18<00:58,  2.89it/s][A
 24%|██▍       | 53/221 [00:19<00:50,  3.34it/s][A
 24%|██▍       | 54/221 [00:21<02:21,  1.18it/s][A
 25%|██▍       | 55/221 [00:21<01:56,  1.43it/s][A
 25%|██▌       | 56/221 [00:21<01:31,  1.80it/s][A
 26%|██▌       | 57/221 [00:22<01:13,  2.24it/s][A
 26%|██▌       | 58/221 [00:22<01:02,  2.62it/s][A
 27%|██▋       | 59/221 [00:22<00:55,  2.92it/s][A
 27%|██▋       | 60/221 [00:23<01:13,  2.19it/s][A
 28%|██▊       | 61/221 [00:23<01:02,  2.55it/s][A
 28%|██▊       | 62/221 [00:23<00:55,  2.85it/s][A
 29%|██▊       | 63/221 [00:23<00:48,  3.27it/s][A
 29%|██▉       | 64/221 [00:24<00:41,  3.75it/s][A[h264 @ 0x55cc1ed56380] mmco: unref short failure

 29%|██▉       | 65/221 [00:24<00:37,  4.21it/s][A
 30%|██▉       | 66/221 [00:24<00:51,  3.03it/s][A
 30%|███       | 67/221 [00:25<01:02,  2.48it/s][A
 31%|███       | 68/221 [00:25<00:52,  2.89it/s][A
 31%|███       | 69/221 [00:26<01:11,  2.14it/s][A
 32%|███▏      | 70/221 [00:26<00:56,  2.67it/s][A
 32%|███▏      | 71/221 [00:27<01:14,  2.00it/s][A
 33%|███▎      | 72/221 [00:27<01:03,  2.35it/s][A
 33%|███▎      | 73/221 [00:27<01:01,  2.40it/s][A
 33%|███▎      | 74/221 [00:28<00:50,  2.93it/s][A
 34%|███▍      | 75/221 [00:28<00:51,  2.85it/s][A
 34%|███▍      | 76/221 [00:28<00:42,  3.40it/s][A
 35%|███▍      | 77/221 [00:28<00:40,  3.53it/s][A
 35%|███▌      | 78/221 [00:29<00:36,  3.87it/s][A
 36%|███▌      | 79/221 [00:29<00:46,  3.06it/s][A[h264 @ 0x55cc1a1f8bc0] mmco: unref short failure
[h264 @ 0x55cc1a1f8bc0] mmco: unref short failure

 36%|███▌      | 80/221 [00:29<00:43,  3.26it/s][A
 37%|███▋      | 81/221 [00:30<00:41,  3.40it/s][A
 37%|███▋      | 82/221 [00:31<01:09,  2.01it/s][A
 38%|███▊      | 83/221 [00:31<01:05,  2.11it/s][A
 38%|███▊      | 84/221 [00:31<00:55,  2.45it/s][A
 38%|███▊      | 85/221 [00:31<00:45,  2.98it/s][A
 39%|███▉      | 86/221 [00:32<00:44,  3.07it/s][A
 39%|███▉      | 87/221 [00:32<00:57,  2.34it/s][A
 40%|███▉      | 88/221 [00:33<00:57,  2.33it/s][A
 40%|████      | 89/221 [00:33<01:05,  2.00it/s][A
 41%|████      | 90/221 [00:34<00:57,  2.27it/s][A
 41%|████      | 91/221 [00:34<00:44,  2.94it/s][A
 42%|████▏     | 92/221 [00:34<00:43,  2.98it/s][A
 42%|████▏     | 93/221 [00:35<00:49,  2.60it/s][A
 43%|████▎     | 94/221 [00:35<00:47,  2.68it/s][A
 43%|████▎     | 95/221 [00:35<00:42,  2.94it/s][A
 43%|████▎     | 96/221 [00:36<00:39,  3.14it/s][A
 44%|████▍     | 97/221 [00:36<00:32,  3.83it/s][A
 44%|████▍     | 98/221 [00:36<00:33,  3.68it/s][A
 45%|████▌     | 100/221 [00:36<00:24,  4.94it/s][A
 46%|████▌     | 102/221 [00:37<00:25,  4.64it/s][A
 47%|████▋     | 103/221 [00:37<00:23,  5.13it/s][A
 47%|████▋     | 104/221 [00:37<00:20,  5.77it/s][A
 48%|████▊     | 105/221 [00:37<00:26,  4.38it/s][A
 48%|████▊     | 106/221 [00:38<00:44,  2.59it/s][A
 48%|████▊     | 107/221 [00:38<00:38,  2.98it/s][A
 49%|████▉     | 108/221 [00:39<00:32,  3.46it/s][A
 49%|████▉     | 109/221 [00:39<00:36,  3.10it/s][A
 50%|████▉     | 110/221 [00:39<00:34,  3.25it/s][A
 50%|█████     | 111/221 [00:40<00:38,  2.88it/s][A
 51%|█████     | 112/221 [00:40<00:35,  3.11it/s][A
 51%|█████     | 113/221 [00:40<00:35,  3.01it/s][A
 52%|█████▏    | 115/221 [00:40<00:23,  4.49it/s][A
 52%|█████▏    | 116/221 [00:45<02:13,  1.27s/it][A
 53%|█████▎    | 117/221 [00:45<01:46,  1.02s/it][A
 53%|█████▎    | 118/221 [00:46<01:24,  1.21it/s][A09/18/2024 19:08:40 - INFO - __main__ -   current idx flS6D6P73vs.20 from finetune_area returns wrong image/video, use 40933 instead.

 54%|█████▍    | 119/221 [00:46<01:05,  1.55it/s][A
 54%|█████▍    | 120/221 [00:46<00:55,  1.81it/s][A
 55%|█████▌    | 122/221 [00:46<00:36,  2.72it/s][A
 56%|█████▌    | 123/221 [00:46<00:30,  3.26it/s][A
 56%|█████▌    | 124/221 [00:47<00:26,  3.66it/s][A
 57%|█████▋    | 125/221 [00:47<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:47<00:27,  3.41it/s][A
 57%|█████▋    | 127/221 [00:48<00:35,  2.63it/s][A
 58%|█████▊    | 128/221 [00:48<00:36,  2.52it/s][A
 58%|█████▊    | 129/221 [00:49<00:34,  2.69it/s][A
 59%|█████▉    | 130/221 [00:49<00:29,  3.08it/s][A
 60%|█████▉    | 132/221 [00:49<00:20,  4.42it/s][A
 60%|██████    | 133/221 [00:50<00:28,  3.11it/s][A
 61%|██████    | 134/221 [00:50<00:25,  3.43it/s][A
 61%|██████    | 135/221 [00:50<00:27,  3.15it/s][A
 62%|██████▏   | 136/221 [00:51<00:29,  2.88it/s][A
 62%|██████▏   | 137/221 [00:51<00:26,  3.19it/s][A
 62%|██████▏   | 138/221 [00:51<00:27,  3.03it/s][A
 63%|██████▎   | 139/221 [00:52<00:28,  2.85it/s][A
 63%|██████▎   | 140/221 [00:52<00:27,  2.97it/s][A
 64%|██████▍   | 141/221 [00:52<00:23,  3.37it/s][A
 64%|██████▍   | 142/221 [00:52<00:23,  3.40it/s][A
[h264 @ 0x559997c44680] mmco: unref short failure
 65%|██████▍   | 143/221 [00:53<00:28,  2.77it/s][A
 65%|██████▌   | 144/221 [00:53<00:26,  2.95it/s][A
 66%|██████▌   | 146/221 [00:53<00:16,  4.49it/s][A
 67%|██████▋   | 147/221 [00:54<00:15,  4.69it/s][A
 67%|██████▋   | 148/221 [00:54<00:15,  4.78it/s][A
 67%|██████▋   | 149/221 [00:54<00:13,  5.46it/s][A
 68%|██████▊   | 150/221 [00:54<00:12,  5.54it/s][A
 68%|██████▊   | 151/221 [00:55<00:23,  3.02it/s][A
 69%|██████▉   | 152/221 [00:56<00:39,  1.76it/s][A
 69%|██████▉   | 153/221 [00:56<00:32,  2.06it/s][A
 70%|██████▉   | 154/221 [00:57<00:31,  2.15it/s][A
 70%|███████   | 155/221 [00:57<00:23,  2.77it/s][A
 71%|███████   | 156/221 [00:57<00:20,  3.16it/s][A
 71%|███████   | 157/221 [00:59<00:51,  1.25it/s][A
 71%|███████▏  | 158/221 [00:59<00:40,  1.54it/s][A
 72%|███████▏  | 159/221 [00:59<00:31,  1.99it/s][A
 72%|███████▏  | 160/221 [01:00<00:26,  2.32it/s][A
 73%|███████▎  | 161/221 [01:00<00:19,  3.01it/s][A[h264 @ 0x5599a3c69500] mmco: unref short failure

 73%|███████▎  | 162/221 [01:01<00:31,  1.88it/s][A
 74%|███████▍  | 163/221 [01:01<00:26,  2.21it/s][A
 74%|███████▍  | 164/221 [01:02<00:30,  1.86it/s][A
 75%|███████▍  | 165/221 [01:02<00:23,  2.38it/s][A
 75%|███████▌  | 166/221 [01:02<00:21,  2.52it/s][A
 76%|███████▌  | 167/221 [01:02<00:17,  3.12it/s][A
 76%|███████▌  | 168/221 [01:03<00:27,  1.93it/s][A
 76%|███████▋  | 169/221 [01:04<00:21,  2.38it/s][A
 77%|███████▋  | 170/221 [01:04<00:21,  2.42it/s][A
 77%|███████▋  | 171/221 [01:04<00:18,  2.72it/s][A[h264 @ 0x55999ae43300] mmco: unref short failure
[h264 @ 0x55999ae43300] mmco: unref short failure

 78%|███████▊  | 172/221 [01:04<00:15,  3.15it/s][A
 78%|███████▊  | 173/221 [01:05<00:12,  3.83it/s][A
 79%|███████▊  | 174/221 [01:05<00:11,  4.16it/s][A
 79%|███████▉  | 175/221 [01:05<00:17,  2.68it/s][A
 80%|███████▉  | 176/221 [01:06<00:15,  2.99it/s][A
 81%|████████  | 178/221 [01:06<00:10,  3.97it/s][A
 81%|████████  | 179/221 [01:06<00:11,  3.52it/s][A
 82%|████████▏ | 181/221 [01:07<00:08,  4.56it/s][A
 82%|████████▏ | 182/221 [01:07<00:07,  5.00it/s][A
 83%|████████▎ | 183/221 [01:07<00:07,  5.12it/s][A
 83%|████████▎ | 184/221 [01:07<00:08,  4.51it/s][A
 84%|████████▎ | 185/221 [01:07<00:07,  5.01it/s][A
 84%|████████▍ | 186/221 [01:08<00:08,  4.01it/s][A
 85%|████████▍ | 187/221 [01:08<00:07,  4.59it/s][A
 85%|████████▌ | 188/221 [01:08<00:07,  4.30it/s][A
 86%|████████▌ | 189/221 [01:09<00:10,  2.96it/s][A
 86%|████████▌ | 190/221 [01:09<00:11,  2.78it/s][A
 87%|████████▋ | 192/221 [01:09<00:07,  3.84it/s][A[h264 @ 0x5599ab915d80] mmco: unref short failure

 88%|████████▊ | 194/221 [01:10<00:09,  2.72it/s][A
 88%|████████▊ | 195/221 [01:11<00:08,  3.23it/s][A
 89%|████████▊ | 196/221 [01:11<00:09,  2.59it/s][A
 89%|████████▉ | 197/221 [01:11<00:07,  3.11it/s][A
 90%|████████▉ | 198/221 [01:12<00:07,  3.26it/s][A
 90%|█████████ | 199/221 [01:12<00:05,  3.71it/s][A
 90%|█████████ | 200/221 [01:12<00:05,  3.75it/s][A
 91%|█████████ | 201/221 [01:12<00:05,  3.99it/s][A
 91%|█████████▏| 202/221 [01:12<00:04,  4.16it/s][A
 92%|█████████▏| 203/221 [01:13<00:03,  4.76it/s][A
 92%|█████████▏| 204/221 [01:13<00:03,  4.32it/s][A
 93%|█████████▎| 205/221 [01:13<00:03,  5.18it/s][A
 93%|█████████▎| 206/221 [01:13<00:04,  3.51it/s][A
 94%|█████████▎| 207/221 [01:14<00:03,  3.81it/s][A
 94%|█████████▍| 208/221 [01:14<00:02,  4.41it/s][A
 95%|█████████▍| 209/221 [01:14<00:02,  4.46it/s][A
 95%|█████████▌| 211/221 [01:15<00:02,  4.43it/s][A
 96%|█████████▌| 212/221 [01:15<00:01,  5.01it/s][A
 96%|█████████▋| 213/221 [01:15<00:01,  4.52it/s][A
 97%|█████████▋| 214/221 [01:15<00:01,  4.44it/s][A
 97%|█████████▋| 215/221 [01:15<00:01,  4.37it/s][A
 98%|█████████▊| 216/221 [01:16<00:01,  4.16it/s][A[h264 @ 0x55d9cd898f00] mmco: unref short failure
[h264 @ 0x55d9cd898f00] mmco: unref short failure

 98%|█████████▊| 217/221 [01:16<00:01,  3.41it/s][A
 99%|█████████▊| 218/221 [01:16<00:00,  3.15it/s][A
 99%|█████████▉| 219/221 [01:17<00:00,  3.49it/s][A[h264 @ 0x55d9c6d56500] mmco: unref short failure
[h264 @ 0x55d9c6d56500] mmco: unref short failure

100%|█████████▉| 220/221 [01:21<00:01,  1.47s/it][A
100%|██████████| 221/221 [01:21<00:00,  1.09s/it][A100%|██████████| 221/221 [01:21<00:00,  2.71it/s]
[h264 @ 0x5599a3c83300] mmco: unref short failure
[h264 @ 0x5599a3c83300] mmco: unref short failure

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:06,  3.30it/s][A
  1%|          | 2/221 [00:00<01:06,  3.28it/s][A
  1%|▏         | 3/221 [00:00<01:06,  3.27it/s][A
  2%|▏         | 4/221 [00:01<01:06,  3.26it/s][A
  2%|▏         | 5/221 [00:01<01:06,  3.26it/s][A
  3%|▎         | 6/221 [00:01<01:06,  3.23it/s][A
  3%|▎         | 7/221 [00:02<01:05,  3.25it/s][A
  4%|▎         | 8/221 [00:02<01:05,  3.26it/s][A
  4%|▍         | 9/221 [00:02<01:04,  3.27it/s][A
  5%|▍         | 10/221 [00:03<01:04,  3.28it/s][A
  5%|▍         | 11/221 [00:03<01:03,  3.28it/s][A
  5%|▌         | 12/221 [00:03<01:03,  3.29it/s][A
  6%|▌         | 13/221 [00:03<01:03,  3.29it/s][A
  6%|▋         | 14/221 [00:04<01:02,  3.29it/s][A[h264 @ 0x55d9c541b700] mmco: unref short failure
[h264 @ 0x55d9c541b700] mmco: unref short failure

  7%|▋         | 15/221 [00:04<01:02,  3.29it/s][A
  7%|▋         | 16/221 [00:04<01:02,  3.29it/s][A
  8%|▊         | 17/221 [00:05<01:01,  3.29it/s][A
  8%|▊         | 18/221 [00:05<01:01,  3.29it/s][A
  9%|▊         | 19/221 [00:05<01:01,  3.29it/s][A
  9%|▉         | 20/221 [00:06<01:01,  3.29it/s][A
 10%|▉         | 21/221 [00:06<01:00,  3.29it/s][A
 10%|▉         | 22/221 [00:06<01:00,  3.29it/s][A
 10%|█         | 23/221 [00:07<01:00,  3.29it/s][A
 11%|█         | 24/221 [00:07<00:59,  3.30it/s][A
 11%|█▏        | 25/221 [00:07<01:03,  3.10it/s][A
 12%|█▏        | 26/221 [00:07<01:01,  3.16it/s][A
 12%|█▏        | 27/221 [00:08<01:00,  3.20it/s][A
 13%|█▎        | 28/221 [00:08<00:59,  3.23it/s][A
 13%|█▎        | 29/221 [00:08<00:59,  3.25it/s][A
 14%|█▎        | 30/221 [00:09<00:58,  3.27it/s][A
 14%|█▍        | 31/221 [00:09<00:57,  3.28it/s][A
 14%|█▍        | 32/221 [00:09<00:57,  3.29it/s][A
 15%|█▍        | 33/221 [00:10<00:57,  3.29it/s][A
 15%|█▌        | 34/221 [00:10<00:56,  3.30it/s][A
 16%|█▌        | 35/221 [00:10<00:56,  3.30it/s][A
 16%|█▋        | 36/221 [00:11<00:56,  3.30it/s][A
 17%|█▋        | 37/221 [00:11<00:55,  3.30it/s][A
 17%|█▋        | 38/221 [00:11<00:55,  3.31it/s][A
 18%|█▊        | 39/221 [00:11<00:55,  3.31it/s][A
 18%|█▊        | 40/221 [00:12<00:54,  3.31it/s][A
 19%|█▊        | 41/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 42/221 [00:12<00:54,  3.31it/s][A
 19%|█▉        | 43/221 [00:13<00:53,  3.31it/s][A
 20%|█▉        | 44/221 [00:13<00:53,  3.31it/s][A
 20%|██        | 45/221 [00:13<00:53,  3.31it/s][A
 21%|██        | 46/221 [00:14<00:52,  3.31it/s][A
 21%|██▏       | 47/221 [00:14<00:52,  3.31it/s][A
 22%|██▏       | 48/221 [00:14<00:52,  3.31it/s][A
 22%|██▏       | 49/221 [00:14<00:51,  3.31it/s][A
 23%|██▎       | 50/221 [00:15<00:51,  3.31it/s][A
 23%|██▎       | 51/221 [00:15<00:51,  3.31it/s][A
 24%|██▎       | 52/221 [00:15<00:51,  3.31it/s][A
 24%|██▍       | 53/221 [00:16<00:50,  3.31it/s][A
 24%|██▍       | 54/221 [00:16<00:50,  3.31it/s][A
 25%|██▍       | 55/221 [00:16<00:50,  3.31it/s][A
 25%|██▌       | 56/221 [00:17<00:49,  3.31it/s][A
 26%|██▌       | 57/221 [00:17<00:49,  3.31it/s][A
 26%|██▌       | 58/221 [00:17<00:49,  3.31it/s][A
 27%|██▋       | 59/221 [00:17<00:48,  3.31it/s][A
 27%|██▋       | 60/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 61/221 [00:18<00:48,  3.31it/s][A
 28%|██▊       | 62/221 [00:18<00:48,  3.31it/s][A
 29%|██▊       | 63/221 [00:19<00:47,  3.31it/s][A
 29%|██▉       | 64/221 [00:19<00:47,  3.31it/s][A
 29%|██▉       | 65/221 [00:19<00:47,  3.31it/s][A
 30%|██▉       | 66/221 [00:20<00:46,  3.31it/s][A
 30%|███       | 67/221 [00:20<00:46,  3.31it/s][A
 31%|███       | 68/221 [00:20<00:46,  3.31it/s][A
 31%|███       | 69/221 [00:20<00:45,  3.31it/s][A
 32%|███▏      | 70/221 [00:21<00:45,  3.31it/s][A
 32%|███▏      | 71/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 72/221 [00:21<00:45,  3.31it/s][A
 33%|███▎      | 73/221 [00:22<00:44,  3.31it/s][A
 33%|███▎      | 74/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 75/221 [00:22<00:44,  3.31it/s][A
 34%|███▍      | 76/221 [00:23<00:43,  3.31it/s][A
 35%|███▍      | 77/221 [00:23<00:43,  3.31it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.31it/s][A
 36%|███▌      | 79/221 [00:23<00:42,  3.31it/s][A
 36%|███▌      | 80/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 81/221 [00:24<00:42,  3.31it/s][A
 37%|███▋      | 82/221 [00:24<00:41,  3.31it/s][A
 38%|███▊      | 83/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 84/221 [00:25<00:41,  3.31it/s][A
 38%|███▊      | 85/221 [00:25<00:41,  3.31it/s][A
 39%|███▉      | 86/221 [00:26<00:40,  3.31it/s][A
 39%|███▉      | 87/221 [00:26<00:40,  3.31it/s][A
 40%|███▉      | 88/221 [00:26<00:40,  3.31it/s][A
 40%|████      | 89/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 90/221 [00:27<00:39,  3.31it/s][A
 41%|████      | 91/221 [00:27<00:39,  3.31it/s][A
 42%|████▏     | 92/221 [00:27<00:38,  3.31it/s][A
 42%|████▏     | 93/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 94/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 95/221 [00:28<00:38,  3.31it/s][A
 43%|████▎     | 96/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 97/221 [00:29<00:37,  3.31it/s][A
 44%|████▍     | 98/221 [00:29<00:37,  3.31it/s][A
 45%|████▍     | 99/221 [00:30<00:36,  3.31it/s][A
 45%|████▌     | 100/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 101/221 [00:30<00:36,  3.31it/s][A
 46%|████▌     | 102/221 [00:30<00:35,  3.31it/s][A
 47%|████▋     | 103/221 [00:31<00:35,  3.31it/s][A
 47%|████▋     | 104/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 105/221 [00:31<00:35,  3.31it/s][A
 48%|████▊     | 106/221 [00:32<00:34,  3.31it/s][A
 48%|████▊     | 107/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 108/221 [00:32<00:34,  3.31it/s][A
 49%|████▉     | 109/221 [00:33<00:33,  3.31it/s][A
 50%|████▉     | 110/221 [00:33<00:33,  3.31it/s][A
 50%|█████     | 111/221 [00:33<00:33,  3.31it/s][A
 51%|█████     | 112/221 [00:33<00:32,  3.31it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 114/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 115/221 [00:34<00:32,  3.31it/s][A
 52%|█████▏    | 116/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 117/221 [00:35<00:31,  3.31it/s][A
 53%|█████▎    | 118/221 [00:35<00:31,  3.31it/s][A
 54%|█████▍    | 119/221 [00:36<00:30,  3.31it/s][A
 54%|█████▍    | 120/221 [00:36<00:30,  3.31it/s][A
 55%|█████▍    | 121/221 [00:36<00:30,  3.31it/s][A
 55%|█████▌    | 122/221 [00:36<00:29,  3.31it/s][A
 56%|█████▌    | 123/221 [00:37<00:29,  3.31it/s][A
 56%|█████▌    | 124/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 125/221 [00:37<00:29,  3.31it/s][A
 57%|█████▋    | 126/221 [00:38<00:28,  3.31it/s][A
 57%|█████▋    | 127/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 128/221 [00:38<00:28,  3.31it/s][A
 58%|█████▊    | 129/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 130/221 [00:39<00:27,  3.31it/s][A
 59%|█████▉    | 131/221 [00:39<00:27,  3.31it/s][A
 60%|█████▉    | 132/221 [00:40<00:26,  3.31it/s][A
 60%|██████    | 133/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 134/221 [00:40<00:26,  3.31it/s][A
 61%|██████    | 135/221 [00:40<00:25,  3.31it/s][A
 62%|██████▏   | 136/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 137/221 [00:41<00:25,  3.31it/s][A
 62%|██████▏   | 138/221 [00:41<00:25,  3.31it/s][A
 63%|██████▎   | 139/221 [00:42<00:24,  3.31it/s][A
 63%|██████▎   | 140/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 141/221 [00:42<00:24,  3.31it/s][A
 64%|██████▍   | 142/221 [00:43<00:23,  3.31it/s][A
 65%|██████▍   | 143/221 [00:43<00:23,  3.31it/s][A
 65%|██████▌   | 144/221 [00:43<00:23,  3.31it/s][A
 66%|██████▌   | 145/221 [00:43<00:22,  3.31it/s][A
 66%|██████▌   | 146/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 147/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 148/221 [00:44<00:22,  3.31it/s][A
 67%|██████▋   | 149/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 150/221 [00:45<00:21,  3.31it/s][A
 68%|██████▊   | 151/221 [00:45<00:21,  3.31it/s][A
 69%|██████▉   | 152/221 [00:46<00:20,  3.31it/s][A
 69%|██████▉   | 153/221 [00:46<00:20,  3.31it/s][A
 70%|██████▉   | 154/221 [00:46<00:20,  3.31it/s][A
 70%|███████   | 155/221 [00:46<00:19,  3.31it/s][A
 71%|███████   | 156/221 [00:47<00:19,  3.31it/s][A
 71%|███████   | 157/221 [00:47<00:19,  3.31it/s][A
 71%|███████▏  | 158/221 [00:47<00:19,  3.31it/s][A
 72%|███████▏  | 159/221 [00:48<00:18,  3.31it/s][A
 72%|███████▏  | 160/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 161/221 [00:48<00:18,  3.31it/s][A
 73%|███████▎  | 162/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 163/221 [00:49<00:17,  3.31it/s][A
 74%|███████▍  | 164/221 [00:49<00:17,  3.31it/s][A
 75%|███████▍  | 165/221 [00:49<00:16,  3.31it/s][A
 75%|███████▌  | 166/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 167/221 [00:50<00:16,  3.31it/s][A
 76%|███████▌  | 168/221 [00:50<00:16,  3.31it/s][A
 76%|███████▋  | 169/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 170/221 [00:51<00:15,  3.31it/s][A
 77%|███████▋  | 171/221 [00:51<00:15,  3.31it/s][A
 78%|███████▊  | 172/221 [00:52<00:14,  3.31it/s][A
 78%|███████▊  | 173/221 [00:52<00:14,  3.31it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.31it/s][A
 79%|███████▉  | 175/221 [00:53<00:13,  3.31it/s][A
 80%|███████▉  | 176/221 [00:53<00:13,  3.31it/s][A
 80%|████████  | 177/221 [00:53<00:13,  3.31it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.31it/s][A
 81%|████████  | 179/221 [00:54<00:12,  3.31it/s][A
 81%|████████▏ | 180/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 181/221 [00:54<00:12,  3.31it/s][A
 82%|████████▏ | 182/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 183/221 [00:55<00:11,  3.31it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.31it/s][A
 84%|████████▎ | 185/221 [00:56<00:10,  3.31it/s][A
 84%|████████▍ | 186/221 [00:56<00:10,  3.31it/s][A
 85%|████████▍ | 187/221 [00:56<00:10,  3.31it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [00:57<00:09,  3.31it/s][A
 86%|████████▌ | 190/221 [00:57<00:09,  3.31it/s][A
 86%|████████▋ | 191/221 [00:57<00:09,  3.31it/s][A
 87%|████████▋ | 192/221 [00:58<00:08,  3.31it/s][A
 87%|████████▋ | 193/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 194/221 [00:58<00:08,  3.31it/s][A
 88%|████████▊ | 195/221 [00:59<00:07,  3.31it/s][A
 89%|████████▊ | 196/221 [00:59<00:07,  3.31it/s][A
 89%|████████▉ | 197/221 [00:59<00:07,  3.31it/s][A
 90%|████████▉ | 198/221 [00:59<00:06,  3.31it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:00<00:06,  3.31it/s][A
 91%|█████████ | 201/221 [01:00<00:06,  3.31it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 203/221 [01:01<00:05,  3.31it/s][A
 92%|█████████▏| 204/221 [01:01<00:05,  3.31it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.31it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▎| 207/221 [01:02<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:02<00:03,  3.31it/s][A
 95%|█████████▍| 209/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 210/221 [01:03<00:03,  3.31it/s][A
 95%|█████████▌| 211/221 [01:03<00:03,  3.31it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.31it/s][A
 96%|█████████▋| 213/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 214/221 [01:04<00:02,  3.31it/s][A
 97%|█████████▋| 215/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 216/221 [01:05<00:01,  3.31it/s][A
 98%|█████████▊| 217/221 [01:05<00:01,  3.31it/s][A
 99%|█████████▊| 218/221 [01:05<00:00,  3.31it/s][A
 99%|█████████▉| 219/221 [01:06<00:00,  3.31it/s][A
100%|█████████▉| 220/221 [01:06<00:00,  3.31it/s][A
100%|██████████| 221/221 [01:06<00:00,  3.31it/s][A100%|██████████| 221/221 [01:06<00:00,  3.30it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:42,  5.23it/s][A
  1%|          | 2/221 [00:01<02:04,  1.76it/s][A
  1%|▏         | 3/221 [00:01<01:24,  2.57it/s][A
  2%|▏         | 4/221 [00:01<01:08,  3.16it/s][A
  2%|▏         | 5/221 [00:01<01:02,  3.48it/s][A
  3%|▎         | 6/221 [00:01<00:54,  3.93it/s][A
  3%|▎         | 7/221 [00:02<00:53,  3.98it/s][A
  4%|▎         | 8/221 [00:02<01:08,  3.13it/s][A
  4%|▍         | 9/221 [00:03<01:23,  2.53it/s][A
  5%|▍         | 10/221 [00:03<01:19,  2.66it/s][A
  5%|▍         | 11/221 [00:03<01:03,  3.28it/s][A
  5%|▌         | 12/221 [00:03<01:02,  3.34it/s][A
  6%|▌         | 13/221 [00:04<01:24,  2.46it/s][A
  6%|▋         | 14/221 [00:04<01:05,  3.14it/s][A
  7%|▋         | 16/221 [00:04<00:51,  3.96it/s][A
  8%|▊         | 17/221 [00:06<01:32,  2.20it/s][A
  8%|▊         | 18/221 [00:06<01:25,  2.38it/s][A
  9%|▊         | 19/221 [00:06<01:12,  2.79it/s][A
  9%|▉         | 20/221 [00:06<00:58,  3.42it/s][A
 10%|▉         | 21/221 [00:06<00:52,  3.82it/s][A
 10%|▉         | 22/221 [00:07<00:47,  4.22it/s][A
 10%|█         | 23/221 [00:07<00:40,  4.85it/s][A
 11%|█         | 24/221 [00:07<00:35,  5.54it/s][A
 11%|█▏        | 25/221 [00:07<00:53,  3.63it/s][A
 12%|█▏        | 26/221 [00:08<00:54,  3.61it/s][A
 13%|█▎        | 28/221 [00:08<01:09,  2.79it/s][A
 13%|█▎        | 29/221 [00:09<01:05,  2.94it/s][A
 14%|█▎        | 30/221 [00:09<01:04,  2.94it/s][A
 14%|█▍        | 31/221 [00:10<01:09,  2.73it/s][A
 15%|█▍        | 33/221 [00:10<00:50,  3.70it/s][A
 15%|█▌        | 34/221 [00:10<00:44,  4.16it/s][A
 16%|█▌        | 35/221 [00:10<00:38,  4.84it/s][A
 16%|█▋        | 36/221 [00:10<00:47,  3.92it/s][A
 17%|█▋        | 37/221 [00:11<00:54,  3.38it/s][A
 17%|█▋        | 38/221 [00:11<00:59,  3.09it/s][A
 18%|█▊        | 39/221 [00:12<00:53,  3.43it/s][A
 18%|█▊        | 40/221 [00:12<00:57,  3.13it/s][A
 19%|█▊        | 41/221 [00:12<00:53,  3.39it/s][A
 19%|█▉        | 42/221 [00:12<00:46,  3.84it/s][A
 20%|█▉        | 44/221 [00:12<00:30,  5.88it/s][A
 20%|██        | 45/221 [00:13<00:41,  4.26it/s][A
 21%|██        | 46/221 [00:13<00:46,  3.74it/s][A
 21%|██▏       | 47/221 [00:14<00:51,  3.41it/s][A
 22%|██▏       | 48/221 [00:14<00:43,  4.02it/s][A
 22%|██▏       | 49/221 [00:14<00:39,  4.31it/s][A
 23%|██▎       | 50/221 [00:14<00:36,  4.68it/s][A
 23%|██▎       | 51/221 [00:14<00:36,  4.61it/s][A
 24%|██▎       | 52/221 [00:14<00:31,  5.32it/s][A
 24%|██▍       | 53/221 [00:15<00:35,  4.74it/s][A
 24%|██▍       | 54/221 [00:15<00:52,  3.18it/s][A
 25%|██▍       | 55/221 [00:15<00:46,  3.57it/s][A
 25%|██▌       | 56/221 [00:16<00:43,  3.83it/s][A
 26%|██▌       | 57/221 [00:16<00:41,  3.95it/s][A
 26%|██▌       | 58/221 [00:16<00:39,  4.08it/s][A
 27%|██▋       | 59/221 [00:16<00:33,  4.87it/s][A
 27%|██▋       | 60/221 [00:17<00:48,  3.32it/s][A
 28%|██▊       | 61/221 [00:17<00:44,  3.57it/s][A
 28%|██▊       | 62/221 [00:17<00:43,  3.66it/s][A
 29%|██▊       | 63/221 [00:18<00:50,  3.11it/s][A
 29%|██▉       | 64/221 [00:18<00:57,  2.71it/s][A
 29%|██▉       | 65/221 [00:19<01:03,  2.47it/s][A
 30%|██▉       | 66/221 [00:19<00:56,  2.77it/s][A
 30%|███       | 67/221 [00:19<01:05,  2.36it/s][A
 31%|███       | 68/221 [00:20<00:54,  2.83it/s][A
 31%|███       | 69/221 [00:20<00:48,  3.16it/s][A
 32%|███▏      | 70/221 [00:20<00:46,  3.22it/s][A
 32%|███▏      | 71/221 [00:20<00:45,  3.30it/s][A
 33%|███▎      | 72/221 [00:21<00:46,  3.21it/s][A
 33%|███▎      | 73/221 [00:21<00:54,  2.74it/s][A
 33%|███▎      | 74/221 [00:22<00:49,  2.99it/s][A
 34%|███▍      | 75/221 [00:22<00:48,  3.04it/s][A
 34%|███▍      | 76/221 [00:22<00:39,  3.64it/s][A
 35%|███▍      | 77/221 [00:22<00:38,  3.76it/s][A
 35%|███▌      | 78/221 [00:23<00:43,  3.25it/s][A
 36%|███▌      | 79/221 [00:23<01:00,  2.35it/s][A
 36%|███▌      | 80/221 [00:24<01:03,  2.22it/s][A
 37%|███▋      | 81/221 [00:24<00:54,  2.57it/s][A
 37%|███▋      | 82/221 [00:25<01:03,  2.18it/s][A
 38%|███▊      | 83/221 [00:25<01:10,  1.95it/s][A
 38%|███▊      | 84/221 [00:26<01:10,  1.94it/s][A
 38%|███▊      | 85/221 [00:26<01:02,  2.17it/s][A
 39%|███▉      | 86/221 [00:27<01:03,  2.12it/s][A
 39%|███▉      | 87/221 [00:28<01:21,  1.64it/s][A
 40%|███▉      | 88/221 [00:28<01:01,  2.16it/s][A
 40%|████      | 89/221 [00:28<00:51,  2.56it/s][A
 41%|████      | 90/221 [00:28<00:43,  3.01it/s][A
 42%|████▏     | 92/221 [00:29<00:32,  3.97it/s][A
 42%|████▏     | 93/221 [00:29<00:27,  4.67it/s][A
 43%|████▎     | 94/221 [00:29<00:27,  4.61it/s][A
 43%|████▎     | 95/221 [00:29<00:34,  3.69it/s][A
 43%|████▎     | 96/221 [00:29<00:31,  3.91it/s][A
 44%|████▍     | 97/221 [00:30<00:28,  4.41it/s][A
 44%|████▍     | 98/221 [00:30<00:38,  3.16it/s][A
 45%|████▍     | 99/221 [00:30<00:31,  3.87it/s][A
 45%|████▌     | 100/221 [00:30<00:28,  4.23it/s][A
 46%|████▌     | 101/221 [00:31<00:27,  4.43it/s][A
 46%|████▌     | 102/221 [00:31<00:28,  4.13it/s][A
 47%|████▋     | 104/221 [00:31<00:19,  5.94it/s][A
 48%|████▊     | 105/221 [00:31<00:20,  5.67it/s][A
 48%|████▊     | 106/221 [00:32<00:26,  4.40it/s][A
 48%|████▊     | 107/221 [00:32<00:26,  4.33it/s][A
 49%|████▉     | 108/221 [00:32<00:28,  3.92it/s][A
 49%|████▉     | 109/221 [00:33<00:42,  2.63it/s][A
 50%|████▉     | 110/221 [00:33<00:42,  2.62it/s][A
 50%|█████     | 111/221 [00:34<00:38,  2.86it/s][A
 51%|█████     | 112/221 [00:34<00:35,  3.09it/s][A
 51%|█████     | 113/221 [00:34<00:32,  3.34it/s][A
 52%|█████▏    | 115/221 [00:34<00:23,  4.57it/s][A
 53%|█████▎    | 117/221 [00:35<00:21,  4.93it/s][A
 53%|█████▎    | 118/221 [00:35<00:25,  4.02it/s][A
 54%|█████▍    | 119/221 [00:35<00:22,  4.53it/s][A
 54%|█████▍    | 120/221 [00:36<00:27,  3.74it/s][A
 55%|█████▍    | 121/221 [00:36<00:24,  4.03it/s][A
 55%|█████▌    | 122/221 [00:36<00:24,  3.99it/s][A
 56%|█████▌    | 123/221 [00:36<00:24,  3.94it/s][A
 56%|█████▌    | 124/221 [00:37<00:27,  3.56it/s][A
 57%|█████▋    | 125/221 [00:37<00:34,  2.80it/s][A
 57%|█████▋    | 126/221 [00:38<00:31,  2.98it/s][A
 58%|█████▊    | 128/221 [00:38<00:25,  3.69it/s][A
 58%|█████▊    | 129/221 [00:38<00:23,  3.90it/s][A
 59%|█████▉    | 130/221 [00:39<00:25,  3.53it/s][A
 59%|█████▉    | 131/221 [00:39<00:23,  3.90it/s][A
 60%|█████▉    | 132/221 [00:39<00:21,  4.15it/s][A
 60%|██████    | 133/221 [00:40<00:31,  2.76it/s][A
 61%|██████    | 134/221 [00:40<00:27,  3.17it/s][A
 61%|██████    | 135/221 [00:40<00:31,  2.73it/s][A
 62%|██████▏   | 136/221 [00:41<00:31,  2.69it/s][A
 62%|██████▏   | 137/221 [00:41<00:28,  2.90it/s][A
 62%|██████▏   | 138/221 [00:41<00:28,  2.89it/s][A
 63%|██████▎   | 139/221 [00:42<00:25,  3.18it/s][A
 63%|██████▎   | 140/221 [00:42<00:27,  2.95it/s][A
 64%|██████▍   | 141/221 [00:42<00:23,  3.42it/s][A
 64%|██████▍   | 142/221 [00:42<00:22,  3.49it/s][A
 65%|██████▌   | 144/221 [00:43<00:16,  4.53it/s][A
 66%|██████▌   | 145/221 [00:43<00:15,  5.00it/s][A
 66%|██████▌   | 146/221 [00:43<00:14,  5.06it/s][A
 67%|██████▋   | 147/221 [00:43<00:12,  5.72it/s][A
 67%|██████▋   | 148/221 [00:43<00:16,  4.53it/s][A
 67%|██████▋   | 149/221 [00:44<00:15,  4.65it/s][A
 68%|██████▊   | 150/221 [00:44<00:21,  3.29it/s][A
 68%|██████▊   | 151/221 [00:44<00:17,  3.94it/s][A
 69%|██████▉   | 152/221 [00:45<00:25,  2.69it/s][A
 69%|██████▉   | 153/221 [00:45<00:25,  2.62it/s][A
 70%|██████▉   | 154/221 [00:46<00:27,  2.42it/s][A
 70%|███████   | 155/221 [00:46<00:22,  2.91it/s][A
 71%|███████   | 156/221 [00:46<00:20,  3.18it/s][A
 71%|███████   | 157/221 [00:47<00:21,  3.01it/s][A
 71%|███████▏  | 158/221 [00:47<00:21,  2.88it/s][A
 72%|███████▏  | 159/221 [00:47<00:20,  2.96it/s][A
 72%|███████▏  | 160/221 [00:48<00:21,  2.90it/s][A
 73%|███████▎  | 161/221 [00:48<00:17,  3.48it/s][A
 73%|███████▎  | 162/221 [00:48<00:14,  4.19it/s][A
 74%|███████▍  | 163/221 [00:48<00:15,  3.72it/s][A
 74%|███████▍  | 164/221 [00:49<00:15,  3.73it/s][A
 75%|███████▍  | 165/221 [00:49<00:12,  4.48it/s][A
 75%|███████▌  | 166/221 [00:49<00:15,  3.57it/s][A
 76%|███████▌  | 167/221 [00:49<00:14,  3.64it/s][A
 76%|███████▌  | 168/221 [00:50<00:15,  3.44it/s][A
 76%|███████▋  | 169/221 [00:50<00:14,  3.60it/s][A
 77%|███████▋  | 170/221 [00:50<00:18,  2.80it/s][A
 77%|███████▋  | 171/221 [00:51<00:17,  2.79it/s][A
 78%|███████▊  | 172/221 [00:51<00:15,  3.22it/s][A
 79%|███████▊  | 174/221 [00:52<00:14,  3.34it/s][A
 79%|███████▉  | 175/221 [00:52<00:14,  3.25it/s][A
 80%|███████▉  | 176/221 [00:52<00:16,  2.80it/s][A
 80%|████████  | 177/221 [00:53<00:14,  3.09it/s][A
 81%|████████  | 178/221 [00:53<00:12,  3.47it/s][A
 81%|████████  | 179/221 [00:53<00:13,  3.14it/s][A
 81%|████████▏ | 180/221 [00:53<00:11,  3.70it/s][A
 82%|████████▏ | 182/221 [00:54<00:08,  4.55it/s][A
 83%|████████▎ | 183/221 [00:54<00:08,  4.73it/s][A
 83%|████████▎ | 184/221 [00:55<00:11,  3.14it/s][A
 84%|████████▎ | 185/221 [00:55<00:10,  3.34it/s][A
 84%|████████▍ | 186/221 [00:55<00:10,  3.21it/s][A
 85%|████████▍ | 187/221 [00:55<00:08,  3.94it/s][A
 85%|████████▌ | 188/221 [00:56<00:09,  3.32it/s][A
 86%|████████▌ | 189/221 [00:56<00:11,  2.73it/s][A
 86%|████████▌ | 190/221 [00:57<00:14,  2.18it/s][A
 86%|████████▋ | 191/221 [00:57<00:11,  2.61it/s][A
 87%|████████▋ | 192/221 [00:57<00:09,  3.11it/s][A
 87%|████████▋ | 193/221 [00:57<00:08,  3.40it/s][A
 88%|████████▊ | 194/221 [00:58<00:11,  2.45it/s][A
 88%|████████▊ | 195/221 [00:59<00:11,  2.23it/s][A
 89%|████████▊ | 196/221 [00:59<00:11,  2.14it/s][A
 89%|████████▉ | 197/221 [01:00<00:11,  2.17it/s][A
 90%|████████▉ | 198/221 [01:00<00:08,  2.68it/s][A
 90%|█████████ | 199/221 [01:00<00:06,  3.29it/s][A
 90%|█████████ | 200/221 [01:01<00:09,  2.29it/s][A
 91%|█████████ | 201/221 [01:01<00:07,  2.54it/s][A
 91%|█████████▏| 202/221 [01:01<00:05,  3.24it/s][A
 92%|█████████▏| 203/221 [01:01<00:04,  3.75it/s][A
 92%|█████████▏| 204/221 [01:02<00:05,  2.89it/s][A
 93%|█████████▎| 205/221 [01:02<00:04,  3.49it/s][A
 93%|█████████▎| 206/221 [01:02<00:04,  3.04it/s][A
 94%|█████████▎| 207/221 [01:03<00:03,  3.56it/s][A
 94%|█████████▍| 208/221 [01:03<00:04,  2.83it/s][A
 95%|█████████▍| 209/221 [01:04<00:04,  2.47it/s][A
 95%|█████████▌| 210/221 [01:04<00:03,  3.10it/s][A
 95%|█████████▌| 211/221 [01:04<00:03,  3.04it/s][A
 96%|█████████▌| 212/221 [01:04<00:02,  3.68it/s][A
 96%|█████████▋| 213/221 [01:05<00:03,  2.44it/s][A
 97%|█████████▋| 214/221 [01:05<00:02,  2.35it/s][A
 97%|█████████▋| 215/221 [01:06<00:02,  2.25it/s][A
 98%|█████████▊| 216/221 [01:06<00:01,  2.54it/s][A
 98%|█████████▊| 217/221 [01:07<00:01,  2.62it/s][A
 99%|█████████▊| 218/221 [01:07<00:01,  2.80it/s][A
 99%|█████████▉| 219/221 [01:07<00:00,  3.52it/s][A
100%|█████████▉| 220/221 [01:07<00:00,  3.86it/s][A
100%|██████████| 221/221 [01:08<00:00,  3.21it/s][A100%|██████████| 221/221 [01:08<00:00,  3.25it/s]
09/18/2024 19:11:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 649--===========

09/18/2024 19:11:37 - INFO - __main__ -   {'area_r1': 39.4, 'area_recall': '39.4/65.5/74.9', 'area_ravg': 59.9}
09/18/2024 19:11:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 649--===========

09/18/2024 19:11:37 - INFO - __main__ -   {'forward_r1': 36.1, 'forward_recall': '36.1/65.0/76.2', 'forward_ravg': 59.1}
09/18/2024 19:11:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 649--===========

09/18/2024 19:11:37 - INFO - __main__ -   {'area_video_r1': 38.6, 'area_video_recall': '38.6/66.3/76.8', 'area_video_ravg': 60.6}
09/18/2024 19:11:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 299=======

09/18/2024 19:11:37 - INFO - __main__ -   {'area_video_r1': 40.5, 'area_video_recall': '40.5/67.9/77.6', 'area_video_ravg': 62.0}
09/18/2024 19:11:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 649--===========

09/18/2024 19:11:37 - INFO - __main__ -   {'area_video_r1': 51.0, 'area_video_recall': '51.0/72.1/80.9', 'area_video_ravg': 68.0, 'area_video_back_r1': 50.1, 'area_video_back_recall': '50.1/71.8/77.7', 'area_video_back_ravg': 66.6}
09/18/2024 19:11:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 499=======

09/18/2024 19:11:37 - INFO - __main__ -   {'area_video_r1': 52.6, 'area_video_recall': '52.6/73.1/81.2', 'area_video_ravg': 69.0, 'area_video_back_r1': 49.5, 'area_video_back_recall': '49.5/71.5/78.2', 'area_video_back_ravg': 66.4}
09/18/2024 19:11:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 649--===========

09/18/2024 19:11:37 - INFO - __main__ -   {'video_r1': 28.2, 'video_recall': '28.2/52.3/63.0', 'video_ravg': 47.8}
09/18/2024 19:11:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 199=======

09/18/2024 19:11:37 - INFO - __main__ -   {'video_r1': 32.1, 'video_recall': '32.1/58.7/69.9', 'video_ravg': 53.6}
09/18/2024 19:11:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 649--===========

09/18/2024 19:11:37 - INFO - __main__ -   {'video_r1': 49.2, 'video_recall': '49.2/68.4/76.0', 'video_ravg': 64.6}
09/18/2024 19:11:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 49=======

09/18/2024 19:11:37 - INFO - __main__ -   {'video_r1': 52.8, 'video_recall': '52.8/72.1/77.7', 'video_ravg': 67.5}
09/18/2024 19:11:58 - INFO - __main__ -   {'loss_ret%tv%ta--finetune_area/loss_itc': 0.0, 'loss_ret%tv%ta--finetune_area/loss_itm': 0.0009678357164375484, 'loss_ret%tv%ta--finetune_area/loss_area': 1.1073551177978516, 'loss_ret%tv%ta--finetune_area/total_loss': 1.1083229780197144}
srun: Job step aborted: Waiting up to 92 seconds for job step to finish.
 22%|██▏       | 650/2910 [4:06:51<88:27:06, 140.90s/it] 22%|██▏       | 651/2910 [4:06:55<62:33:04, 99.68s/it]  22%|██▏       | 652/2910 [4:06:59<44:30:27, 70.96s/it]slurmstepd: error: *** STEP 7757441.0 ON lrdn3068 CANCELLED AT 2024-09-18T19:12:12 ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3188072 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3188073 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3188074 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3188075 closing signal SIGTERM
slurmstepd: error: *** JOB 7757441 ON lrdn3068 CANCELLED AT 2024-09-18T19:12:12 ***
