NODELIST=lrdn3065
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
20

31

DEVICE SET
DEVICE SET
DEVICE SET
DEVICE SET
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 01:33:14 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 01:33:14 - INFO - __main__ -   ==================model_configs==================

09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_model_type : vast
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_frozen_vision : False
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_frozen_audio : False
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_checkpointing : True
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_max_caption_len : 40
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_contra_dim : 512
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_vision_resolution : 224
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_audio_melbins : 64
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_audio_target_length : 1024
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_beam_size : 3
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_captioner_mode : False
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_generate_nums : 1
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_max_vision_sample_num : 8
09/19/2024 01:33:14 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
09/19/2024 01:33:14 - INFO - __main__ -   ==================run_configs==================

09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_checkpoint : 
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast//downstream/finetuneMSRVTT2
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_optim : adamw
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_weight_decay : 0.01
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_grad_norm : 2.0
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_resume : False
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_seed : 50
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_fp16 : True
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_bf16 : False
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_zero_shot : False
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_new_lr : 0
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_new_params_name : []
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_valid_freq : 10
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_dataset_mix_type : random
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_first_eval : True
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod120k
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_num_train_steps : 0
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_save_best : True
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_pin_mem : True
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_vision_resolution : 224
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_use_ddp : False
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_mode : training
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_log_steps : 100
09/19/2024 01:33:14 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
09/19/2024 01:33:14 - INFO - __main__ -   ==================data_configs==================

09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_type : annoindexed
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_training : True
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_name : msrvtt_ret
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_txt : datasets/annotations/msrvtt/descs_ret_train.json
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision : ../MSRVTT/videos/videos
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_audio : ../MSRVTT/audios
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_transforms : crop_flip
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_format : video_rawvideo
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_sample_num : 8
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_audio_sample_num : 1
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_task : ret%tv%ta
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_epoch : 3.6
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_n_workers : 8
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_train_batch_size : 64
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 16
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tvas
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
09/19/2024 01:33:14 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
09/19/2024 01:33:18 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/19/2024 01:33:18 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 158540 labels
ci sono 158540 labels
ci sono 158540 labels
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
ci sono 884 labelsci sono 884 labels
ci sono 884 labels

/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
ci sono 158540 labels
09/19/2024 01:33:25 - INFO - __main__ -   Create Dataset msrvtt_ret Success
09/19/2024 01:33:25 - INFO - __main__ -    loader ret%tv%ta--msrvtt_ret , ratio 8917 , bs_pergpu 16, n_workers 8
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 01:33:25 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 01:33:26 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 01:33:26 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 01:33:26 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/19/2024 01:33:26 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 884 labels
09/19/2024 01:33:26 - INFO - __main__ -   Create Dataset msrvtt_ret Success
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
09/19/2024 01:33:29 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 01:34:34 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 01:34:35 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 01:34:35 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 01:34:36 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 01:34:37 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 01:34:37 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 01:34:37 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 01:34:37 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 01:34:37 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 01:34:37 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 01:34:38 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 01:34:39 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 01:34:39 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/19/2024 01:34:39 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/19/2024 01:34:39 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.query.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'cls.predictions.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.value.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
09/19/2024 01:34:41 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['cls.predictions.transform.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.9.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.value.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.weight', 'cls.predictions.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'cls.predictions.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.key.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
09/19/2024 01:34:45 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod120k/ckpt/model_step_459.pt
09/19/2024 01:34:45 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod120k
09/19/2024 01:34:46 - INFO - __main__ -   Unexpected keys []
09/19/2024 01:34:46 - INFO - __main__ -   missing_keys  []
09/19/2024 01:34:47 - INFO - __main__ -   ==================learning_rate_settings==================

09/19/2024 01:34:47 - INFO - __main__ -     basic_lr : 2e-05
09/19/2024 01:34:47 - INFO - __main__ -     clip_lr_visual : 5e-07
09/19/2024 01:34:47 - INFO - __main__ -     clip_lr_visual_len : 245
09/19/2024 01:34:47 - INFO - __main__ -     new_lr : 0
09/19/2024 01:34:47 - INFO - __main__ -     new_params_name: []
09/19/2024 01:34:47 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 01:34:47 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:01<04:46,  1.30s/it]  1%|          | 2/221 [00:02<03:30,  1.04it/s]  1%|▏         | 3/221 [00:02<02:34,  1.41it/s]  2%|▏         | 4/221 [00:02<01:47,  2.01it/s]  2%|▏         | 5/221 [00:02<01:28,  2.45it/s]  3%|▎         | 6/221 [00:03<01:10,  3.04it/s]  3%|▎         | 7/221 [00:03<01:10,  3.04it/s]  4%|▎         | 8/221 [00:04<01:34,  2.24it/s]  4%|▍         | 9/221 [00:04<01:40,  2.12it/s]  5%|▍         | 10/221 [00:05<01:54,  1.85it/s]  5%|▍         | 11/221 [00:05<01:34,  2.21it/s]  5%|▌         | 12/221 [00:06<02:05,  1.67it/s]  6%|▌         | 13/221 [00:06<01:48,  1.93it/s]  6%|▋         | 14/221 [00:08<03:01,  1.14it/s]  7%|▋         | 15/221 [00:08<02:27,  1.39it/s]  7%|▋         | 16/221 [00:09<02:21,  1.45it/s]  8%|▊         | 17/221 [00:10<02:54,  1.17it/s]  8%|▊         | 18/221 [00:11<02:27,  1.37it/s]  9%|▊         | 19/221 [00:11<01:52,  1.80it/s]  9%|▉         | 20/221 [00:11<01:35,  2.10it/s] 10%|▉         | 21/221 [00:11<01:25,  2.34it/s] 10%|▉         | 22/221 [00:12<01:18,  2.54it/s] 11%|█         | 24/221 [00:12<00:54,  3.61it/s] 11%|█▏        | 25/221 [00:12<00:52,  3.74it/s] 12%|█▏        | 26/221 [00:13<01:19,  2.44it/s] 12%|█▏        | 27/221 [00:13<01:06,  2.91it/s] 13%|█▎        | 28/221 [00:14<01:40,  1.92it/s] 13%|█▎        | 29/221 [00:15<02:02,  1.57it/s] 14%|█▎        | 30/221 [00:16<01:49,  1.75it/s] 14%|█▍        | 31/221 [00:16<01:41,  1.88it/s] 14%|█▍        | 32/221 [00:16<01:17,  2.45it/s] 15%|█▍        | 33/221 [00:17<01:19,  2.37it/s] 15%|█▌        | 34/221 [00:17<01:10,  2.66it/s] 16%|█▌        | 35/221 [00:17<01:08,  2.70it/s] 16%|█▋        | 36/221 [00:18<01:16,  2.41it/s] 17%|█▋        | 37/221 [00:19<01:47,  1.71it/s] 17%|█▋        | 38/221 [00:19<01:51,  1.65it/s] 18%|█▊        | 39/221 [00:20<01:47,  1.69it/s] 18%|█▊        | 40/221 [00:20<01:46,  1.69it/s] 19%|█▊        | 41/221 [00:21<01:21,  2.21it/s] 19%|█▉        | 42/221 [00:21<01:37,  1.83it/s] 19%|█▉        | 43/221 [00:21<01:14,  2.38it/s] 20%|██        | 45/221 [00:23<01:45,  1.68it/s] 21%|██        | 46/221 [00:24<01:49,  1.60it/s] 21%|██▏       | 47/221 [00:25<02:05,  1.38it/s] 22%|██▏       | 48/221 [00:25<01:37,  1.78it/s] 22%|██▏       | 49/221 [00:26<01:37,  1.77it/s] 23%|██▎       | 50/221 [00:26<01:33,  1.83it/s] 23%|██▎       | 51/221 [00:26<01:12,  2.34it/s] 24%|██▎       | 52/221 [00:26<01:03,  2.67it/s] 24%|██▍       | 53/221 [00:27<00:54,  3.06it/s] 24%|██▍       | 54/221 [00:29<02:15,  1.23it/s] 25%|██▍       | 55/221 [00:29<01:55,  1.44it/s] 25%|██▌       | 56/221 [00:29<01:38,  1.67it/s] 26%|██▌       | 57/221 [00:30<01:24,  1.95it/s] 26%|██▌       | 58/221 [00:30<01:06,  2.44it/s] 27%|██▋       | 59/221 [00:30<01:01,  2.61it/s] 27%|██▋       | 60/221 [00:31<01:37,  1.65it/s] 28%|██▊       | 61/221 [00:32<01:25,  1.88it/s] 28%|██▊       | 62/221 [00:32<01:14,  2.14it/s] 29%|██▊       | 63/221 [00:32<01:11,  2.19it/s] 29%|██▉       | 64/221 [00:33<01:01,  2.54it/s] 29%|██▉       | 65/221 [00:33<00:58,  2.67it/s] 30%|██▉       | 66/221 [00:34<01:05,  2.38it/s] 30%|███       | 67/221 [00:34<01:20,  1.92it/s] 31%|███       | 68/221 [00:35<01:09,  2.21it/s] 31%|███       | 69/221 [00:36<01:39,  1.53it/s] 32%|███▏      | 70/221 [00:36<01:20,  1.89it/s] 32%|███▏      | 71/221 [00:37<01:32,  1.63it/s] 33%|███▎      | 72/221 [00:37<01:24,  1.77it/s] 33%|███▎      | 73/221 [00:38<01:25,  1.73it/s] 33%|███▎      | 74/221 [00:38<01:08,  2.16it/s] 34%|███▍      | 75/221 [00:39<01:21,  1.80it/s] 34%|███▍      | 76/221 [00:39<01:05,  2.21it/s] 35%|███▍      | 77/221 [00:39<01:04,  2.25it/s] 35%|███▌      | 78/221 [00:40<01:02,  2.27it/s] 36%|███▌      | 79/221 [00:41<01:19,  1.79it/s] 36%|███▌      | 80/221 [00:41<01:09,  2.04it/s] 37%|███▋      | 81/221 [00:41<01:07,  2.06it/s] 37%|███▋      | 82/221 [00:43<01:54,  1.22it/s] 38%|███▊      | 83/221 [00:44<01:43,  1.34it/s] 38%|███▊      | 84/221 [00:44<01:31,  1.49it/s] 39%|███▉      | 86/221 [00:45<01:10,  1.90it/s] 39%|███▉      | 87/221 [00:46<01:27,  1.53it/s] 40%|███▉      | 88/221 [00:47<01:33,  1.42it/s] 40%|████      | 89/221 [00:48<01:39,  1.32it/s] 41%|████      | 90/221 [00:48<01:26,  1.51it/s] 41%|████      | 91/221 [00:48<01:10,  1.85it/s] 42%|████▏     | 92/221 [00:49<01:02,  2.08it/s] 42%|████▏     | 93/221 [00:50<01:20,  1.59it/s] 43%|████▎     | 94/221 [00:50<01:09,  1.82it/s] 43%|████▎     | 95/221 [00:50<01:04,  1.96it/s] 43%|████▎     | 96/221 [00:51<01:01,  2.02it/s] 44%|████▍     | 97/221 [00:51<00:49,  2.51it/s] 44%|████▍     | 98/221 [00:51<00:50,  2.45it/s] 45%|████▍     | 99/221 [00:52<00:39,  3.06it/s] 45%|████▌     | 100/221 [00:52<00:42,  2.82it/s] 46%|████▌     | 101/221 [00:52<00:34,  3.45it/s] 46%|████▌     | 102/221 [00:53<00:55,  2.14it/s] 47%|████▋     | 103/221 [00:53<00:47,  2.48it/s] 47%|████▋     | 104/221 [00:53<00:39,  2.99it/s] 48%|████▊     | 105/221 [00:54<00:40,  2.84it/s] 48%|████▊     | 106/221 [00:55<01:10,  1.62it/s] 48%|████▊     | 107/221 [00:55<00:56,  2.03it/s] 49%|████▉     | 108/221 [00:56<00:47,  2.39it/s] 49%|████▉     | 109/221 [00:56<01:02,  1.79it/s] 50%|████▉     | 110/221 [00:57<01:00,  1.84it/s] 50%|█████     | 111/221 [00:58<01:10,  1.57it/s] 51%|█████     | 112/221 [00:58<00:56,  1.94it/s] 51%|█████     | 113/221 [00:59<00:56,  1.90it/s] 52%|█████▏    | 115/221 [00:59<00:35,  3.02it/s] 52%|█████▏    | 116/221 [01:06<03:26,  1.97s/it] 53%|█████▎    | 117/221 [01:06<02:44,  1.58s/it] 53%|█████▎    | 118/221 [01:07<02:11,  1.28s/it] 54%|█████▍    | 119/221 [01:07<01:48,  1.06s/it] 54%|█████▍    | 120/221 [01:08<01:32,  1.09it/s] 55%|█████▍    | 121/221 [01:08<01:08,  1.46it/s] 55%|█████▌    | 122/221 [01:08<00:56,  1.74it/s] 56%|█████▌    | 123/221 [01:08<00:45,  2.15it/s] 56%|█████▌    | 124/221 [01:09<00:40,  2.37it/s] 57%|█████▋    | 125/221 [01:09<00:47,  2.01it/s] 57%|█████▋    | 126/221 [01:10<00:50,  1.87it/s] 57%|█████▋    | 127/221 [01:11<00:59,  1.58it/s] 58%|█████▊    | 128/221 [01:12<01:02,  1.49it/s] 58%|█████▊    | 129/221 [01:12<00:49,  1.87it/s] 59%|█████▉    | 130/221 [01:12<00:47,  1.92it/s] 59%|█████▉    | 131/221 [01:12<00:38,  2.35it/s] 60%|█████▉    | 132/221 [01:13<00:35,  2.50it/s] 60%|██████    | 133/221 [01:13<00:42,  2.09it/s] 61%|██████    | 134/221 [01:14<00:37,  2.34it/s] 61%|██████    | 135/221 [01:14<00:39,  2.15it/s] 62%|██████▏   | 136/221 [01:15<00:45,  1.86it/s] 62%|██████▏   | 137/221 [01:15<00:40,  2.08it/s] 62%|██████▏   | 138/221 [01:16<00:44,  1.85it/s] 63%|██████▎   | 139/221 [01:17<00:53,  1.52it/s] 63%|██████▎   | 140/221 [01:18<00:49,  1.62it/s] 64%|██████▍   | 141/221 [01:18<00:44,  1.80it/s] 64%|██████▍   | 142/221 [01:18<00:42,  1.86it/s] 65%|██████▍   | 143/221 [01:19<00:47,  1.65it/s] 65%|██████▌   | 144/221 [01:20<00:43,  1.79it/s] 66%|██████▌   | 145/221 [01:20<00:32,  2.36it/s] 66%|██████▌   | 146/221 [01:20<00:26,  2.81it/s] 67%|██████▋   | 147/221 [01:20<00:25,  2.94it/s] 67%|██████▋   | 148/221 [01:21<00:30,  2.41it/s] 67%|██████▋   | 149/221 [01:21<00:26,  2.74it/s] 68%|██████▊   | 150/221 [01:21<00:25,  2.78it/s] 68%|██████▊   | 151/221 [01:24<01:05,  1.07it/s] 69%|██████▉   | 152/221 [01:27<01:48,  1.57s/it] 69%|██████▉   | 153/221 [01:27<01:26,  1.27s/it] 70%|██████▉   | 154/221 [01:28<01:10,  1.05s/it] 70%|███████   | 155/221 [01:28<00:54,  1.21it/s] 71%|███████   | 156/221 [01:29<00:44,  1.46it/s] 71%|███████   | 157/221 [01:31<01:13,  1.15s/it] 71%|███████▏  | 158/221 [01:31<00:58,  1.07it/s] 72%|███████▏  | 159/221 [01:31<00:45,  1.37it/s] 72%|███████▏  | 160/221 [01:32<00:38,  1.59it/s] 73%|███████▎  | 161/221 [01:32<00:29,  2.07it/s] 73%|███████▎  | 162/221 [01:33<00:32,  1.80it/s] 74%|███████▍  | 163/221 [01:33<00:29,  1.94it/s] 74%|███████▍  | 164/221 [01:34<00:32,  1.77it/s] 75%|███████▍  | 165/221 [01:34<00:25,  2.22it/s] 75%|███████▌  | 166/221 [01:35<00:30,  1.79it/s] 76%|███████▌  | 167/221 [01:35<00:27,  1.99it/s] 76%|███████▌  | 168/221 [01:37<00:44,  1.18it/s] 76%|███████▋  | 169/221 [01:37<00:35,  1.48it/s] 77%|███████▋  | 170/221 [01:38<00:33,  1.53it/s] 77%|███████▋  | 171/221 [01:38<00:33,  1.48it/s] 78%|███████▊  | 172/221 [01:39<00:27,  1.77it/s] 78%|███████▊  | 173/221 [01:39<00:23,  2.07it/s] 79%|███████▊  | 174/221 [01:39<00:21,  2.16it/s] 79%|███████▉  | 175/221 [01:40<00:23,  1.94it/s] 80%|███████▉  | 176/221 [01:40<00:21,  2.09it/s] 80%|████████  | 177/221 [01:41<00:18,  2.33it/s] 81%|████████  | 178/221 [01:41<00:17,  2.49it/s] 81%|████████  | 179/221 [01:42<00:20,  2.01it/s] 81%|████████▏ | 180/221 [01:42<00:16,  2.53it/s] 82%|████████▏ | 181/221 [01:42<00:14,  2.69it/s] 82%|████████▏ | 182/221 [01:43<00:13,  2.83it/s] 83%|████████▎ | 183/221 [01:43<00:13,  2.84it/s] 83%|████████▎ | 184/221 [01:44<00:15,  2.34it/s] 84%|████████▎ | 185/221 [01:44<00:13,  2.72it/s] 84%|████████▍ | 186/221 [01:45<00:18,  1.85it/s] 85%|████████▍ | 187/221 [01:45<00:15,  2.13it/s] 85%|████████▌ | 188/221 [01:46<00:15,  2.08it/s] 86%|████████▌ | 189/221 [01:46<00:16,  1.93it/s] 86%|████████▌ | 190/221 [01:47<00:17,  1.73it/s] 86%|████████▋ | 191/221 [01:47<00:13,  2.16it/s] 87%|████████▋ | 192/221 [01:47<00:11,  2.44it/s] 88%|████████▊ | 194/221 [01:49<00:17,  1.53it/s] 88%|████████▊ | 195/221 [01:49<00:14,  1.82it/s] 89%|████████▊ | 196/221 [01:51<00:16,  1.47it/s] 89%|████████▉ | 197/221 [01:51<00:13,  1.76it/s] 90%|████████▉ | 198/221 [01:51<00:12,  1.87it/s] 90%|█████████ | 199/221 [01:51<00:09,  2.22it/s] 90%|█████████ | 200/221 [01:52<00:09,  2.15it/s] 91%|█████████ | 201/221 [01:52<00:08,  2.23it/s] 91%|█████████▏| 202/221 [01:53<00:07,  2.47it/s] 92%|█████████▏| 203/221 [01:53<00:06,  2.67it/s] 92%|█████████▏| 204/221 [01:53<00:06,  2.50it/s] 93%|█████████▎| 205/221 [01:54<00:05,  2.97it/s] 93%|█████████▎| 206/221 [01:55<00:11,  1.34it/s] 94%|█████████▎| 207/221 [01:56<00:08,  1.59it/s] 94%|█████████▍| 208/221 [01:56<00:06,  1.92it/s] 95%|█████████▍| 209/221 [01:56<00:05,  2.00it/s] 95%|█████████▌| 210/221 [01:57<00:04,  2.50it/s] 95%|█████████▌| 211/221 [01:57<00:04,  2.02it/s] 96%|█████████▌| 212/221 [01:58<00:04,  2.22it/s] 96%|█████████▋| 213/221 [01:58<00:03,  2.29it/s] 97%|█████████▋| 214/221 [01:59<00:03,  2.09it/s] 97%|█████████▋| 215/221 [01:59<00:02,  2.23it/s] 98%|█████████▊| 216/221 [02:00<00:02,  2.06it/s] 98%|█████████▊| 217/221 [02:00<00:02,  1.73it/s] 99%|█████████▊| 218/221 [02:01<00:01,  1.60it/s] 99%|█████████▉| 219/221 [02:02<00:01,  1.78it/s]100%|█████████▉| 220/221 [02:07<00:01,  1.95s/it]100%|██████████| 221/221 [02:07<00:00,  1.46s/it]100%|██████████| 221/221 [02:07<00:00,  1.73it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<01:56,  1.88it/s]  1%|          | 2/221 [00:01<01:56,  1.88it/s]  1%|▏         | 3/221 [00:01<01:55,  1.88it/s]  2%|▏         | 4/221 [00:02<01:55,  1.88it/s]  2%|▏         | 5/221 [00:02<01:54,  1.88it/s]  3%|▎         | 6/221 [00:03<01:54,  1.88it/s]  3%|▎         | 7/221 [00:03<01:53,  1.88it/s]  4%|▎         | 8/221 [00:04<01:53,  1.88it/s]  4%|▍         | 9/221 [00:04<01:52,  1.88it/s]  5%|▍         | 10/221 [00:05<01:52,  1.88it/s]  5%|▍         | 11/221 [00:05<01:51,  1.88it/s]  5%|▌         | 12/221 [00:06<01:51,  1.88it/s]  6%|▌         | 13/221 [00:06<01:50,  1.88it/s]  6%|▋         | 14/221 [00:07<01:49,  1.88it/s]  7%|▋         | 15/221 [00:07<01:49,  1.88it/s]  7%|▋         | 16/221 [00:08<01:48,  1.88it/s]  8%|▊         | 17/221 [00:09<01:48,  1.88it/s]  8%|▊         | 18/221 [00:09<01:47,  1.88it/s]  9%|▊         | 19/221 [00:10<01:47,  1.88it/s]  9%|▉         | 20/221 [00:10<01:46,  1.88it/s] 10%|▉         | 21/221 [00:11<01:46,  1.88it/s] 10%|▉         | 22/221 [00:11<01:45,  1.88it/s] 10%|█         | 23/221 [00:12<01:45,  1.88it/s] 11%|█         | 24/221 [00:12<01:44,  1.88it/s] 11%|█▏        | 25/221 [00:13<01:44,  1.88it/s] 12%|█▏        | 26/221 [00:13<01:43,  1.88it/s] 12%|█▏        | 27/221 [00:14<01:43,  1.88it/s] 13%|█▎        | 28/221 [00:14<01:42,  1.88it/s] 13%|█▎        | 29/221 [00:15<01:42,  1.88it/s] 14%|█▎        | 30/221 [00:15<01:41,  1.88it/s] 14%|█▍        | 31/221 [00:16<01:41,  1.88it/s] 14%|█▍        | 32/221 [00:17<01:40,  1.88it/s] 15%|█▍        | 33/221 [00:17<01:39,  1.88it/s] 15%|█▌        | 34/221 [00:18<01:39,  1.88it/s] 16%|█▌        | 35/221 [00:18<01:38,  1.88it/s] 16%|█▋        | 36/221 [00:19<01:38,  1.88it/s] 17%|█▋        | 37/221 [00:19<01:37,  1.88it/s] 17%|█▋        | 38/221 [00:20<01:37,  1.88it/s] 18%|█▊        | 39/221 [00:20<01:36,  1.88it/s] 18%|█▊        | 40/221 [00:21<01:36,  1.88it/s] 19%|█▊        | 41/221 [00:21<01:35,  1.88it/s] 19%|█▉        | 42/221 [00:22<01:35,  1.88it/s] 19%|█▉        | 43/221 [00:22<01:34,  1.88it/s] 20%|█▉        | 44/221 [00:23<01:33,  1.88it/s] 20%|██        | 45/221 [00:23<01:33,  1.88it/s] 21%|██        | 46/221 [00:24<01:32,  1.88it/s] 21%|██▏       | 47/221 [00:24<01:32,  1.88it/s] 22%|██▏       | 48/221 [00:25<01:31,  1.88it/s] 22%|██▏       | 49/221 [00:26<01:31,  1.88it/s] 23%|██▎       | 50/221 [00:26<01:30,  1.88it/s] 23%|██▎       | 51/221 [00:27<01:30,  1.88it/s] 24%|██▎       | 52/221 [00:27<01:29,  1.88it/s] 24%|██▍       | 53/221 [00:28<01:29,  1.88it/s] 24%|██▍       | 54/221 [00:28<01:28,  1.88it/s] 25%|██▍       | 55/221 [00:29<01:28,  1.88it/s] 25%|██▌       | 56/221 [00:29<01:27,  1.88it/s] 26%|██▌       | 57/221 [00:30<01:27,  1.88it/s] 26%|██▌       | 58/221 [00:30<01:26,  1.88it/s] 27%|██▋       | 59/221 [00:31<01:25,  1.88it/s] 27%|██▋       | 60/221 [00:31<01:25,  1.88it/s] 28%|██▊       | 61/221 [00:32<01:25,  1.88it/s] 28%|██▊       | 62/221 [00:32<01:24,  1.88it/s] 29%|██▊       | 63/221 [00:33<01:24,  1.88it/s] 29%|██▉       | 64/221 [00:34<01:23,  1.88it/s] 29%|██▉       | 65/221 [00:34<01:22,  1.88it/s] 30%|██▉       | 66/221 [00:35<01:22,  1.88it/s] 30%|███       | 67/221 [00:35<01:21,  1.88it/s] 31%|███       | 68/221 [00:36<01:21,  1.88it/s] 31%|███       | 69/221 [00:36<01:20,  1.88it/s] 32%|███▏      | 70/221 [00:37<01:20,  1.88it/s] 32%|███▏      | 71/221 [00:37<01:19,  1.88it/s] 33%|███▎      | 72/221 [00:38<01:19,  1.88it/s] 33%|███▎      | 73/221 [00:38<01:18,  1.88it/s] 33%|███▎      | 74/221 [00:39<01:18,  1.88it/s] 34%|███▍      | 75/221 [00:39<01:17,  1.88it/s] 34%|███▍      | 76/221 [00:40<01:17,  1.88it/s] 35%|███▍      | 77/221 [00:40<01:16,  1.88it/s] 35%|███▌      | 78/221 [00:41<01:15,  1.88it/s] 36%|███▌      | 79/221 [00:41<01:15,  1.88it/s] 36%|███▌      | 80/221 [00:42<01:14,  1.88it/s] 37%|███▋      | 81/221 [00:43<01:14,  1.88it/s] 37%|███▋      | 82/221 [00:43<01:13,  1.88it/s] 38%|███▊      | 83/221 [00:44<01:13,  1.88it/s] 38%|███▊      | 84/221 [00:44<01:12,  1.88it/s] 38%|███▊      | 85/221 [00:45<01:12,  1.88it/s] 39%|███▉      | 86/221 [00:45<01:11,  1.88it/s] 39%|███▉      | 87/221 [00:46<01:11,  1.88it/s] 40%|███▉      | 88/221 [00:46<01:10,  1.88it/s] 40%|████      | 89/221 [00:47<01:10,  1.88it/s] 41%|████      | 90/221 [00:47<01:09,  1.89it/s] 41%|████      | 91/221 [00:48<01:08,  1.89it/s] 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s] 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s] 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s] 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s] 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s] 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s] 44%|████▍     | 98/221 [00:52<01:05,  1.89it/s] 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s] 45%|████▌     | 100/221 [00:53<01:04,  1.89it/s] 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s] 46%|████▌     | 102/221 [00:54<01:02,  1.89it/s] 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s] 47%|████▋     | 104/221 [00:55<01:01,  1.89it/s] 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s] 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s] 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s] 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s] 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s] 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s] 50%|█████     | 111/221 [00:58<00:58,  1.89it/s] 51%|█████     | 112/221 [00:59<00:57,  1.89it/s] 51%|█████     | 113/221 [00:59<00:57,  1.89it/s] 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s] 52%|█████▏    | 115/221 [01:01<00:56,  1.89it/s] 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s] 53%|█████▎    | 117/221 [01:02<00:55,  1.89it/s] 53%|█████▎    | 118/221 [01:02<00:54,  1.88it/s] 54%|█████▍    | 119/221 [01:03<00:54,  1.88it/s] 54%|█████▍    | 120/221 [01:03<00:53,  1.88it/s] 55%|█████▍    | 121/221 [01:04<00:53,  1.88it/s] 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s] 56%|█████▌    | 123/221 [01:05<00:52,  1.88it/s] 56%|█████▌    | 124/221 [01:05<00:51,  1.88it/s] 57%|█████▋    | 125/221 [01:06<00:50,  1.88it/s] 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s] 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s] 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s] 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s] 59%|█████▉    | 130/221 [01:09<00:48,  1.89it/s] 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s] 60%|█████▉    | 132/221 [01:10<00:47,  1.89it/s] 60%|██████    | 133/221 [01:10<00:46,  1.89it/s] 61%|██████    | 134/221 [01:11<00:46,  1.89it/s] 61%|██████    | 135/221 [01:11<00:45,  1.89it/s] 62%|██████▏   | 136/221 [01:12<00:44,  1.89it/s] 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s] 62%|██████▏   | 138/221 [01:13<00:43,  1.89it/s] 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s] 63%|██████▎   | 140/221 [01:14<00:42,  1.89it/s] 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s] 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s] 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s] 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s] 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s] 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s] 67%|██████▋   | 147/221 [01:18<00:39,  1.89it/s] 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s] 67%|██████▋   | 149/221 [01:19<00:38,  1.89it/s] 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s] 68%|██████▊   | 151/221 [01:20<00:37,  1.89it/s] 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s] 69%|██████▉   | 153/221 [01:21<00:35,  1.89it/s] 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s] 70%|███████   | 155/221 [01:22<00:34,  1.89it/s] 71%|███████   | 156/221 [01:22<00:34,  1.89it/s] 71%|███████   | 157/221 [01:23<00:33,  1.89it/s] 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s] 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s] 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s] 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s] 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s] 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s] 74%|███████▍  | 164/221 [01:27<00:30,  1.89it/s] 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s] 75%|███████▌  | 166/221 [01:28<00:29,  1.89it/s] 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s] 76%|███████▌  | 168/221 [01:29<00:28,  1.89it/s] 76%|███████▋  | 169/221 [01:29<00:27,  1.88it/s] 77%|███████▋  | 170/221 [01:30<00:27,  1.88it/s] 77%|███████▋  | 171/221 [01:30<00:26,  1.88it/s] 78%|███████▊  | 172/221 [01:31<00:26,  1.88it/s] 78%|███████▊  | 173/221 [01:31<00:25,  1.88it/s] 79%|███████▊  | 174/221 [01:32<00:24,  1.88it/s] 79%|███████▉  | 175/221 [01:32<00:24,  1.88it/s] 80%|███████▉  | 176/221 [01:33<00:23,  1.88it/s] 80%|████████  | 177/221 [01:33<00:23,  1.88it/s] 81%|████████  | 178/221 [01:34<00:22,  1.88it/s] 81%|████████  | 179/221 [01:34<00:22,  1.88it/s] 81%|████████▏ | 180/221 [01:35<00:21,  1.88it/s] 82%|████████▏ | 181/221 [01:36<00:21,  1.88it/s] 82%|████████▏ | 182/221 [01:36<00:20,  1.88it/s] 83%|████████▎ | 183/221 [01:37<00:20,  1.88it/s] 83%|████████▎ | 184/221 [01:37<00:19,  1.88it/s] 84%|████████▎ | 185/221 [01:38<00:19,  1.88it/s] 84%|████████▍ | 186/221 [01:38<00:18,  1.88it/s] 85%|████████▍ | 187/221 [01:39<00:18,  1.88it/s] 85%|████████▌ | 188/221 [01:39<00:17,  1.88it/s] 86%|████████▌ | 189/221 [01:40<00:17,  1.88it/s] 86%|████████▌ | 190/221 [01:40<00:16,  1.88it/s] 86%|████████▋ | 191/221 [01:41<00:15,  1.88it/s] 87%|████████▋ | 192/221 [01:41<00:15,  1.88it/s] 87%|████████▋ | 193/221 [01:42<00:14,  1.88it/s] 88%|████████▊ | 194/221 [01:42<00:14,  1.88it/s] 88%|████████▊ | 195/221 [01:43<00:13,  1.88it/s] 89%|████████▊ | 196/221 [01:44<00:13,  1.88it/s] 89%|████████▉ | 197/221 [01:44<00:12,  1.88it/s] 90%|████████▉ | 198/221 [01:45<00:12,  1.88it/s] 90%|█████████ | 199/221 [01:45<00:11,  1.88it/s] 90%|█████████ | 200/221 [01:46<00:11,  1.88it/s] 91%|█████████ | 201/221 [01:46<00:10,  1.88it/s] 91%|█████████▏| 202/221 [01:47<00:10,  1.88it/s] 92%|█████████▏| 203/221 [01:47<00:09,  1.88it/s] 92%|█████████▏| 204/221 [01:48<00:09,  1.88it/s] 93%|█████████▎| 205/221 [01:48<00:08,  1.88it/s] 93%|█████████▎| 206/221 [01:49<00:07,  1.88it/s] 94%|█████████▎| 207/221 [01:49<00:07,  1.88it/s] 94%|█████████▍| 208/221 [01:50<00:06,  1.88it/s] 95%|█████████▍| 209/221 [01:50<00:06,  1.88it/s] 95%|█████████▌| 210/221 [01:51<00:05,  1.88it/s] 95%|█████████▌| 211/221 [01:51<00:05,  1.88it/s] 96%|█████████▌| 212/221 [01:52<00:04,  1.88it/s] 96%|█████████▋| 213/221 [01:53<00:04,  1.88it/s] 97%|█████████▋| 214/221 [01:53<00:03,  1.88it/s] 97%|█████████▋| 215/221 [01:54<00:03,  1.88it/s] 98%|█████████▊| 216/221 [01:54<00:02,  1.88it/s] 98%|█████████▊| 217/221 [01:55<00:02,  1.88it/s] 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s] 99%|█████████▉| 219/221 [01:56<00:01,  1.89it/s]100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s]100%|██████████| 221/221 [01:57<00:00,  1.88it/s]100%|██████████| 221/221 [01:57<00:00,  1.88it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<01:06,  3.29it/s]  1%|          | 2/221 [00:01<03:05,  1.18it/s]  1%|▏         | 3/221 [00:01<01:51,  1.96it/s]  2%|▏         | 4/221 [00:02<01:42,  2.11it/s]  2%|▏         | 5/221 [00:02<01:50,  1.95it/s]  3%|▎         | 6/221 [00:02<01:30,  2.38it/s]  3%|▎         | 7/221 [00:03<01:28,  2.43it/s]  4%|▎         | 8/221 [00:04<01:59,  1.78it/s]  4%|▍         | 9/221 [00:04<02:03,  1.72it/s]  5%|▍         | 10/221 [00:05<01:58,  1.78it/s]  5%|▍         | 11/221 [00:05<01:48,  1.93it/s]  5%|▌         | 12/221 [00:06<01:40,  2.08it/s]  6%|▌         | 13/221 [00:07<02:34,  1.35it/s]  6%|▋         | 14/221 [00:07<02:07,  1.62it/s]  7%|▋         | 15/221 [00:07<01:41,  2.03it/s]  7%|▋         | 16/221 [00:08<01:38,  2.07it/s]  8%|▊         | 17/221 [00:10<03:26,  1.01s/it]  8%|▊         | 18/221 [00:11<03:01,  1.12it/s]  9%|▊         | 19/221 [00:11<02:28,  1.36it/s]  9%|▉         | 20/221 [00:12<02:03,  1.63it/s] 10%|▉         | 21/221 [00:12<01:45,  1.90it/s] 10%|▉         | 22/221 [00:12<01:28,  2.24it/s] 10%|█         | 23/221 [00:12<01:15,  2.62it/s] 11%|█         | 24/221 [00:13<01:04,  3.07it/s] 11%|█▏        | 25/221 [00:13<01:17,  2.52it/s] 12%|█▏        | 26/221 [00:14<01:33,  2.09it/s] 12%|█▏        | 27/221 [00:14<01:13,  2.65it/s] 13%|█▎        | 28/221 [00:15<02:13,  1.45it/s] 13%|█▎        | 29/221 [00:16<02:02,  1.57it/s] 14%|█▎        | 30/221 [00:16<02:02,  1.56it/s] 14%|█▍        | 31/221 [00:17<02:05,  1.51it/s] 14%|█▍        | 32/221 [00:17<01:38,  1.92it/s] 15%|█▍        | 33/221 [00:18<01:42,  1.84it/s] 15%|█▌        | 34/221 [00:18<01:24,  2.23it/s] 16%|█▌        | 35/221 [00:18<01:13,  2.53it/s] 16%|█▋        | 36/221 [00:19<01:23,  2.20it/s] 17%|█▋        | 37/221 [00:20<01:36,  1.91it/s] 17%|█▋        | 38/221 [00:20<01:45,  1.74it/s] 18%|█▊        | 39/221 [00:21<01:34,  1.92it/s] 18%|█▊        | 40/221 [00:22<01:52,  1.60it/s] 19%|█▊        | 41/221 [00:22<01:35,  1.89it/s] 19%|█▉        | 42/221 [00:22<01:20,  2.23it/s] 20%|█▉        | 44/221 [00:22<00:51,  3.46it/s] 20%|██        | 45/221 [00:23<01:07,  2.62it/s] 21%|██        | 46/221 [00:24<01:11,  2.46it/s] 21%|██▏       | 47/221 [00:24<01:21,  2.14it/s] 22%|██▏       | 48/221 [00:24<01:08,  2.54it/s] 22%|██▏       | 49/221 [00:25<01:15,  2.28it/s] 23%|██▎       | 50/221 [00:25<01:12,  2.35it/s] 23%|██▎       | 51/221 [00:26<01:07,  2.50it/s] 24%|██▎       | 52/221 [00:26<00:56,  3.00it/s] 24%|██▍       | 53/221 [00:26<01:02,  2.70it/s] 24%|██▍       | 54/221 [00:27<01:38,  1.69it/s] 25%|██▍       | 55/221 [00:28<01:26,  1.93it/s] 25%|██▌       | 56/221 [00:28<01:20,  2.05it/s] 26%|██▌       | 57/221 [00:29<01:12,  2.27it/s] 26%|██▌       | 58/221 [00:29<01:20,  2.04it/s] 27%|██▋       | 59/221 [00:29<01:05,  2.48it/s] 27%|██▋       | 60/221 [00:30<01:29,  1.80it/s] 28%|██▊       | 61/221 [00:31<01:22,  1.93it/s] 28%|██▊       | 62/221 [00:31<01:18,  2.04it/s] 29%|██▊       | 63/221 [00:32<01:39,  1.58it/s] 29%|██▉       | 64/221 [00:33<01:38,  1.59it/s] 29%|██▉       | 65/221 [00:34<01:47,  1.45it/s] 30%|██▉       | 66/221 [00:34<01:30,  1.72it/s] 30%|███       | 67/221 [00:35<01:57,  1.31it/s] 31%|███       | 68/221 [00:35<01:34,  1.62it/s] 31%|███       | 69/221 [00:36<01:33,  1.62it/s] 32%|███▏      | 70/221 [00:36<01:26,  1.76it/s] 32%|███▏      | 71/221 [00:37<01:22,  1.82it/s] 33%|███▎      | 72/221 [00:37<01:18,  1.91it/s] 33%|███▎      | 73/221 [00:38<01:25,  1.72it/s] 33%|███▎      | 74/221 [00:39<01:22,  1.79it/s] 34%|███▍      | 75/221 [00:39<01:16,  1.90it/s] 34%|███▍      | 76/221 [00:39<01:02,  2.33it/s] 35%|███▍      | 77/221 [00:40<00:56,  2.54it/s] 35%|███▌      | 78/221 [00:40<01:03,  2.25it/s] 36%|███▌      | 79/221 [00:41<01:30,  1.56it/s] 36%|███▌      | 80/221 [00:42<01:38,  1.44it/s] 37%|███▋      | 81/221 [00:43<01:31,  1.53it/s] 37%|███▋      | 82/221 [00:44<01:48,  1.28it/s] 38%|███▊      | 83/221 [00:45<02:09,  1.06it/s] 38%|███▊      | 84/221 [00:46<02:11,  1.04it/s] 38%|███▊      | 85/221 [00:46<01:46,  1.28it/s] 39%|███▉      | 86/221 [00:47<01:49,  1.23it/s] 39%|███▉      | 87/221 [00:49<02:35,  1.16s/it] 40%|███▉      | 88/221 [00:50<02:05,  1.06it/s] 40%|████      | 89/221 [00:50<01:46,  1.24it/s] 41%|████      | 90/221 [00:51<01:31,  1.43it/s] 41%|████      | 91/221 [00:51<01:15,  1.71it/s] 42%|████▏     | 92/221 [00:51<01:04,  1.99it/s] 42%|████▏     | 93/221 [00:52<00:56,  2.26it/s] 43%|████▎     | 94/221 [00:52<00:51,  2.44it/s] 43%|████▎     | 95/221 [00:53<01:03,  1.97it/s] 43%|████▎     | 96/221 [00:53<00:58,  2.15it/s] 44%|████▍     | 97/221 [00:53<00:50,  2.48it/s] 44%|████▍     | 98/221 [00:54<01:01,  2.02it/s] 45%|████▍     | 99/221 [00:54<00:50,  2.40it/s] 45%|████▌     | 100/221 [00:55<00:47,  2.53it/s] 46%|████▌     | 101/221 [00:55<00:42,  2.82it/s] 46%|████▌     | 102/221 [00:55<00:42,  2.81it/s] 47%|████▋     | 103/221 [00:55<00:36,  3.20it/s] 47%|████▋     | 104/221 [00:56<00:34,  3.44it/s] 48%|████▊     | 105/221 [00:56<00:33,  3.44it/s] 48%|████▊     | 106/221 [00:56<00:37,  3.03it/s] 48%|████▊     | 107/221 [00:57<00:37,  3.03it/s] 49%|████▉     | 108/221 [00:57<00:40,  2.80it/s] 49%|████▉     | 109/221 [00:59<01:22,  1.36it/s] 50%|████▉     | 110/221 [00:59<01:18,  1.41it/s] 50%|█████     | 111/221 [01:00<01:14,  1.48it/s] 51%|█████     | 112/221 [01:00<01:03,  1.73it/s] 51%|█████     | 113/221 [01:01<00:57,  1.88it/s] 52%|█████▏    | 115/221 [01:01<00:42,  2.51it/s] 53%|█████▎    | 117/221 [01:02<00:39,  2.66it/s] 53%|█████▎    | 118/221 [01:03<00:50,  2.04it/s] 54%|█████▍    | 119/221 [01:03<00:44,  2.31it/s] 54%|█████▍    | 120/221 [01:04<00:53,  1.89it/s] 55%|█████▍    | 121/221 [01:04<00:45,  2.19it/s] 55%|█████▌    | 122/221 [01:04<00:39,  2.50it/s] 56%|█████▌    | 123/221 [01:05<00:40,  2.45it/s] 56%|█████▌    | 124/221 [01:05<00:41,  2.31it/s] 57%|█████▋    | 125/221 [01:06<00:51,  1.87it/s] 57%|█████▋    | 126/221 [01:07<00:51,  1.84it/s] 57%|█████▋    | 127/221 [01:07<00:40,  2.33it/s] 58%|█████▊    | 128/221 [01:07<00:42,  2.18it/s] 58%|█████▊    | 129/221 [01:08<00:38,  2.41it/s] 59%|█████▉    | 130/221 [01:09<00:52,  1.72it/s] 59%|█████▉    | 131/221 [01:09<00:43,  2.08it/s] 60%|█████▉    | 132/221 [01:09<00:36,  2.42it/s] 60%|██████    | 133/221 [01:10<00:48,  1.83it/s] 61%|██████    | 134/221 [01:10<00:38,  2.28it/s] 61%|██████    | 135/221 [01:11<00:45,  1.90it/s] 62%|██████▏   | 136/221 [01:11<00:47,  1.77it/s] 62%|██████▏   | 137/221 [01:12<00:48,  1.75it/s] 62%|██████▏   | 138/221 [01:13<00:48,  1.73it/s] 63%|██████▎   | 139/221 [01:13<00:46,  1.77it/s] 63%|██████▎   | 140/221 [01:14<00:45,  1.76it/s] 64%|██████▍   | 141/221 [01:14<00:35,  2.23it/s] 64%|██████▍   | 142/221 [01:14<00:36,  2.14it/s] 65%|██████▍   | 143/221 [01:15<00:29,  2.65it/s] 65%|██████▌   | 144/221 [01:15<00:30,  2.56it/s] 66%|██████▌   | 145/221 [01:15<00:30,  2.51it/s] 66%|██████▌   | 146/221 [01:16<00:29,  2.50it/s] 67%|██████▋   | 147/221 [01:16<00:25,  2.86it/s] 67%|██████▋   | 148/221 [01:17<00:29,  2.44it/s] 67%|██████▋   | 149/221 [01:17<00:27,  2.65it/s] 68%|██████▊   | 150/221 [01:18<00:39,  1.81it/s] 68%|██████▊   | 151/221 [01:18<00:34,  2.06it/s] 69%|██████▉   | 152/221 [01:20<01:03,  1.08it/s] 69%|██████▉   | 153/221 [01:21<00:59,  1.14it/s] 70%|██████▉   | 154/221 [01:22<00:54,  1.22it/s] 70%|███████   | 155/221 [01:22<00:46,  1.42it/s] 71%|███████   | 156/221 [01:22<00:38,  1.67it/s] 71%|███████   | 157/221 [01:23<00:38,  1.68it/s] 71%|███████▏  | 158/221 [01:24<00:37,  1.67it/s] 72%|███████▏  | 159/221 [01:24<00:34,  1.81it/s] 72%|███████▏  | 160/221 [01:24<00:30,  1.97it/s] 73%|███████▎  | 161/221 [01:25<00:26,  2.23it/s] 73%|███████▎  | 162/221 [01:25<00:22,  2.59it/s] 74%|███████▍  | 163/221 [01:25<00:23,  2.46it/s] 74%|███████▍  | 164/221 [01:26<00:27,  2.07it/s] 75%|███████▍  | 165/221 [01:26<00:20,  2.70it/s] 75%|███████▌  | 166/221 [01:27<00:27,  2.01it/s] 76%|███████▌  | 167/221 [01:27<00:23,  2.26it/s] 76%|███████▌  | 168/221 [01:28<00:23,  2.25it/s] 76%|███████▋  | 169/221 [01:28<00:20,  2.59it/s] 77%|███████▋  | 170/221 [01:29<00:29,  1.73it/s] 77%|███████▋  | 171/221 [01:30<00:30,  1.64it/s] 78%|███████▊  | 172/221 [01:30<00:23,  2.06it/s] 78%|███████▊  | 173/221 [01:30<00:18,  2.62it/s] 79%|███████▊  | 174/221 [01:31<00:26,  1.76it/s] 79%|███████▉  | 175/221 [01:32<00:26,  1.73it/s] 80%|███████▉  | 176/221 [01:32<00:25,  1.78it/s] 80%|████████  | 177/221 [01:33<00:22,  1.93it/s] 81%|████████  | 178/221 [01:33<00:18,  2.28it/s] 81%|████████  | 179/221 [01:34<00:21,  1.95it/s] 81%|████████▏ | 180/221 [01:34<00:17,  2.28it/s] 82%|████████▏ | 181/221 [01:34<00:14,  2.75it/s] 82%|████████▏ | 182/221 [01:34<00:13,  2.88it/s] 83%|████████▎ | 183/221 [01:35<00:12,  3.12it/s] 83%|████████▎ | 184/221 [01:36<00:22,  1.68it/s] 84%|████████▎ | 185/221 [01:37<00:22,  1.58it/s] 84%|████████▍ | 186/221 [01:37<00:22,  1.53it/s] 85%|████████▍ | 187/221 [01:37<00:17,  1.99it/s] 85%|████████▌ | 188/221 [01:38<00:18,  1.81it/s] 86%|████████▌ | 189/221 [01:39<00:21,  1.48it/s] 86%|████████▌ | 190/221 [01:40<00:23,  1.29it/s] 86%|████████▋ | 191/221 [01:40<00:19,  1.56it/s] 87%|████████▋ | 192/221 [01:41<00:16,  1.79it/s] 87%|████████▋ | 193/221 [01:41<00:14,  2.00it/s] 88%|████████▊ | 194/221 [01:42<00:20,  1.34it/s] 88%|████████▊ | 195/221 [01:43<00:20,  1.29it/s] 89%|████████▊ | 196/221 [01:44<00:20,  1.23it/s] 89%|████████▉ | 197/221 [01:45<00:17,  1.34it/s] 90%|████████▉ | 198/221 [01:45<00:14,  1.64it/s] 90%|█████████ | 199/221 [01:45<00:10,  2.01it/s] 90%|█████████ | 200/221 [01:46<00:14,  1.45it/s] 91%|█████████ | 201/221 [01:47<00:12,  1.62it/s] 91%|█████████▏| 202/221 [01:47<00:09,  2.05it/s] 92%|█████████▏| 203/221 [01:47<00:07,  2.48it/s] 92%|█████████▏| 204/221 [01:48<00:09,  1.84it/s] 93%|█████████▎| 205/221 [01:48<00:07,  2.11it/s] 93%|█████████▎| 206/221 [01:49<00:08,  1.79it/s] 94%|█████████▎| 207/221 [01:50<00:06,  2.04it/s] 94%|█████████▍| 208/221 [01:50<00:07,  1.69it/s] 95%|█████████▍| 209/221 [01:51<00:08,  1.50it/s] 95%|█████████▌| 210/221 [01:52<00:06,  1.72it/s] 95%|█████████▌| 211/221 [01:52<00:05,  1.73it/s] 96%|█████████▌| 212/221 [01:53<00:04,  1.95it/s] 96%|█████████▋| 213/221 [01:54<00:05,  1.43it/s] 97%|█████████▋| 214/221 [01:54<00:04,  1.44it/s] 97%|█████████▋| 215/221 [01:55<00:04,  1.43it/s] 98%|█████████▊| 216/221 [01:56<00:03,  1.60it/s] 98%|█████████▊| 217/221 [01:56<00:02,  1.65it/s] 99%|█████████▊| 218/221 [01:56<00:01,  1.87it/s] 99%|█████████▉| 219/221 [01:57<00:00,  2.32it/s]100%|█████████▉| 220/221 [01:57<00:00,  2.43it/s]100%|██████████| 221/221 [01:58<00:00,  1.77it/s]100%|██████████| 221/221 [01:58<00:00,  1.87it/s]
09/19/2024 01:43:52 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_forward========

09/19/2024 01:43:52 - INFO - __main__ -   {'area_r1': 40.7, 'area_recall': '40.7/67.4/76.7', 'area_ravg': 61.6}
09/19/2024 01:43:52 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_backard========

09/19/2024 01:43:52 - INFO - __main__ -   {'forward_r1': 37.1, 'forward_recall': '37.1/65.4/76.2', 'forward_ravg': 59.6}
09/19/2024 01:43:52 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video========

09/19/2024 01:43:52 - INFO - __main__ -   {'area_video_r1': 39.0, 'area_video_recall': '39.0/68.0/77.6', 'area_video_ravg': 61.5}
09/19/2024 01:43:52 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itm_area========

09/19/2024 01:43:52 - INFO - __main__ -   {'area_video_r1': 55.3, 'area_video_recall': '55.3/76.1/83.8', 'area_video_ravg': 71.8, 'area_video_back_r1': 54.1, 'area_video_back_recall': '54.1/76.6/83.1', 'area_video_back_ravg': 71.3}
09/19/2024 01:43:52 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas========

09/19/2024 01:43:52 - INFO - __main__ -   {'video_r1': 28.1, 'video_recall': '28.1/52.8/63.5', 'video_ravg': 48.1}
09/19/2024 01:43:52 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas========

09/19/2024 01:43:52 - INFO - __main__ -   {'video_r1': 52.7, 'video_recall': '52.7/72.4/78.7', 'video_ravg': 67.9}
  0%|          | 0/8917 [00:00<?, ?it/s]/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/8917 [00:05<12:32:06,  5.06s/it]  0%|          | 2/8917 [00:08<10:29:56,  4.24s/it]  0%|          | 3/8917 [00:14<11:48:57,  4.77s/it]  0%|          | 4/8917 [00:17<10:31:55,  4.25s/it]  0%|          | 5/8917 [00:21<9:54:50,  4.00s/it]   0%|          | 6/8917 [00:24<9:30:15,  3.84s/it]  0%|          | 7/8917 [00:28<9:23:58,  3.80s/it]  0%|          | 8/8917 [00:32<9:19:09,  3.77s/it]  0%|          | 9/8917 [00:35<9:13:21,  3.73s/it]  0%|          | 10/8917 [00:39<9:08:28,  3.69s/it]  0%|          | 11/8917 [00:43<9:15:31,  3.74s/it]  0%|          | 12/8917 [00:46<9:05:40,  3.68s/it]  0%|          | 13/8917 [00:50<9:11:59,  3.72s/it]  0%|          | 14/8917 [00:53<8:53:04,  3.59s/it]  0%|          | 15/8917 [00:57<8:54:55,  3.61s/it]  0%|          | 16/8917 [01:01<9:04:54,  3.67s/it]  0%|          | 17/8917 [01:05<9:06:05,  3.68s/it]  0%|          | 18/8917 [01:08<8:59:34,  3.64s/it]  0%|          | 19/8917 [01:12<8:57:50,  3.63s/it]  0%|          | 20/8917 [01:15<9:05:34,  3.68s/it]  0%|          | 21/8917 [01:19<9:01:28,  3.65s/it]  0%|          | 22/8917 [01:23<8:59:00,  3.64s/it]  0%|          | 23/8917 [01:26<8:53:35,  3.60s/it]  0%|          | 24/8917 [01:30<8:42:56,  3.53s/it]  0%|          | 25/8917 [01:33<8:48:05,  3.56s/it]  0%|          | 26/8917 [01:37<8:58:27,  3.63s/it]  0%|          | 27/8917 [01:40<8:43:48,  3.54s/it]  0%|          | 28/8917 [01:44<8:50:29,  3.58s/it]  0%|          | 29/8917 [01:47<8:44:20,  3.54s/it]  0%|          | 30/8917 [01:51<8:42:05,  3.52s/it]  0%|          | 31/8917 [01:54<8:39:26,  3.51s/it]  0%|          | 32/8917 [01:58<8:44:39,  3.54s/it]  0%|          | 33/8917 [02:02<8:52:11,  3.59s/it]  0%|          | 34/8917 [02:05<8:41:08,  3.52s/it]  0%|          | 35/8917 [02:09<8:54:11,  3.61s/it]  0%|          | 36/8917 [02:12<8:50:04,  3.58s/it]  0%|          | 37/8917 [02:16<8:48:18,  3.57s/it]  0%|          | 38/8917 [02:19<8:38:52,  3.51s/it]  0%|          | 39/8917 [02:23<8:52:14,  3.60s/it]  0%|          | 40/8917 [02:27<9:01:52,  3.66s/it]  0%|          | 41/8917 [02:31<9:12:56,  3.74s/it]  0%|          | 42/8917 [02:35<9:13:25,  3.74s/it]  0%|          | 43/8917 [02:38<9:12:16,  3.73s/it]  0%|          | 44/8917 [02:42<9:19:00,  3.78s/it]  1%|          | 45/8917 [02:46<9:16:36,  3.76s/it]  1%|          | 46/8917 [02:49<8:53:56,  3.61s/it]  1%|          | 47/8917 [02:53<8:50:40,  3.59s/it]  1%|          | 48/8917 [02:56<8:58:18,  3.64s/it]  1%|          | 49/8917 [03:00<9:01:29,  3.66s/it]09/19/2024 01:46:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03780267760157585, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7270209789276123, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7648236751556396}
  1%|          | 50/8917 [03:04<8:56:29,  3.63s/it]  1%|          | 51/8917 [03:07<9:02:51,  3.67s/it]  1%|          | 52/8917 [03:11<8:56:47,  3.63s/it]  1%|          | 53/8917 [03:15<8:58:11,  3.64s/it]  1%|          | 54/8917 [03:18<9:03:56,  3.68s/it]  1%|          | 55/8917 [03:22<8:57:18,  3.64s/it]  1%|          | 56/8917 [03:25<8:50:17,  3.59s/it]  1%|          | 57/8917 [03:29<8:41:22,  3.53s/it]  1%|          | 58/8917 [03:32<8:44:21,  3.55s/it]  1%|          | 59/8917 [03:36<8:42:08,  3.54s/it]  1%|          | 60/8917 [03:40<9:01:02,  3.67s/it]  1%|          | 61/8917 [03:43<8:47:34,  3.57s/it]  1%|          | 62/8917 [03:47<8:44:48,  3.56s/it]  1%|          | 63/8917 [03:51<8:59:13,  3.65s/it]  1%|          | 64/8917 [03:54<8:57:53,  3.65s/it]  1%|          | 65/8917 [03:58<8:58:51,  3.65s/it]  1%|          | 66/8917 [04:02<8:52:48,  3.61s/it]  1%|          | 67/8917 [04:05<8:50:43,  3.60s/it]  1%|          | 68/8917 [04:09<8:53:14,  3.62s/it]  1%|          | 69/8917 [04:12<8:47:28,  3.58s/it]  1%|          | 70/8917 [04:16<8:50:16,  3.60s/it]  1%|          | 71/8917 [04:20<8:53:00,  3.62s/it]  1%|          | 72/8917 [04:23<8:53:12,  3.62s/it]  1%|          | 73/8917 [04:27<8:55:55,  3.64s/it]  1%|          | 74/8917 [04:30<8:39:16,  3.52s/it]  1%|          | 75/8917 [04:34<8:52:34,  3.61s/it]  1%|          | 76/8917 [04:37<8:49:17,  3.59s/it]  1%|          | 77/8917 [04:41<8:42:18,  3.55s/it]  1%|          | 78/8917 [04:45<8:58:11,  3.65s/it]  1%|          | 79/8917 [04:48<8:46:26,  3.57s/it]  1%|          | 80/8917 [04:52<8:37:31,  3.51s/it]  1%|          | 81/8917 [04:56<8:58:33,  3.66s/it]  1%|          | 82/8917 [04:59<8:48:22,  3.59s/it]  1%|          | 83/8917 [05:02<8:34:54,  3.50s/it]  1%|          | 84/8917 [05:06<8:38:10,  3.52s/it]  1%|          | 85/8917 [05:09<8:37:43,  3.52s/it]  1%|          | 86/8917 [05:13<8:45:35,  3.57s/it]  1%|          | 87/8917 [05:17<8:41:50,  3.55s/it]  1%|          | 88/8917 [05:20<8:51:23,  3.61s/it]  1%|          | 89/8917 [05:24<8:47:32,  3.59s/it]  1%|          | 90/8917 [05:27<8:41:54,  3.55s/it]  1%|          | 91/8917 [05:31<9:01:21,  3.68s/it]  1%|          | 92/8917 [05:35<8:47:43,  3.59s/it]  1%|          | 93/8917 [05:38<8:51:16,  3.61s/it]  1%|          | 94/8917 [05:42<9:02:26,  3.69s/it]  1%|          | 95/8917 [05:46<9:09:06,  3.73s/it]  1%|          | 96/8917 [05:50<9:15:48,  3.78s/it]  1%|          | 97/8917 [05:54<9:09:20,  3.74s/it]  1%|          | 98/8917 [05:57<8:57:49,  3.66s/it]  1%|          | 99/8917 [06:01<8:51:22,  3.62s/it]09/19/2024 01:49:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.016376694664359093, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0768237113952637, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.093200445175171}
  1%|          | 100/8917 [06:04<8:50:53,  3.61s/it]  1%|          | 101/8917 [06:08<8:48:22,  3.60s/it]  1%|          | 102/8917 [06:11<8:39:05,  3.53s/it]  1%|          | 103/8917 [06:15<8:52:25,  3.62s/it]  1%|          | 104/8917 [06:18<8:50:17,  3.61s/it]  1%|          | 105/8917 [06:22<9:04:50,  3.71s/it]  1%|          | 106/8917 [06:26<9:00:53,  3.68s/it]  1%|          | 107/8917 [06:30<9:01:48,  3.69s/it]  1%|          | 108/8917 [06:34<9:06:42,  3.72s/it]  1%|          | 109/8917 [06:37<9:00:53,  3.68s/it]  1%|          | 110/8917 [06:41<9:04:37,  3.71s/it]  1%|          | 111/8917 [06:45<9:07:46,  3.73s/it]  1%|▏         | 112/8917 [06:48<8:59:14,  3.67s/it]  1%|▏         | 113/8917 [06:52<8:55:16,  3.65s/it]  1%|▏         | 114/8917 [06:56<8:58:26,  3.67s/it]  1%|▏         | 115/8917 [06:59<9:02:09,  3.70s/it]  1%|▏         | 116/8917 [07:03<8:50:09,  3.61s/it]  1%|▏         | 117/8917 [07:07<8:59:46,  3.68s/it]  1%|▏         | 118/8917 [07:10<9:00:35,  3.69s/it]  1%|▏         | 119/8917 [07:14<8:51:17,  3.62s/it]  1%|▏         | 120/8917 [07:17<8:39:04,  3.54s/it]  1%|▏         | 121/8917 [07:21<8:50:33,  3.62s/it]  1%|▏         | 122/8917 [07:24<8:40:00,  3.55s/it]  1%|▏         | 123/8917 [07:28<8:46:09,  3.59s/it]  1%|▏         | 124/8917 [07:31<8:42:07,  3.56s/it]  1%|▏         | 125/8917 [07:35<8:50:32,  3.62s/it]  1%|▏         | 126/8917 [07:39<8:58:33,  3.68s/it]  1%|▏         | 127/8917 [07:43<8:51:14,  3.63s/it]  1%|▏         | 128/8917 [07:46<8:57:08,  3.67s/it]  1%|▏         | 129/8917 [07:50<8:47:15,  3.60s/it]  1%|▏         | 130/8917 [07:53<8:42:15,  3.57s/it]  1%|▏         | 131/8917 [07:57<8:45:33,  3.59s/it]  1%|▏         | 132/8917 [08:00<8:36:59,  3.53s/it]  1%|▏         | 133/8917 [08:04<8:45:29,  3.59s/it]  2%|▏         | 134/8917 [08:08<8:49:31,  3.62s/it]  2%|▏         | 135/8917 [08:11<8:46:22,  3.60s/it]  2%|▏         | 136/8917 [08:15<8:38:25,  3.54s/it]  2%|▏         | 137/8917 [08:18<8:38:47,  3.55s/it]  2%|▏         | 138/8917 [08:22<8:56:46,  3.67s/it]  2%|▏         | 139/8917 [08:26<8:48:08,  3.61s/it]  2%|▏         | 140/8917 [08:29<8:54:40,  3.66s/it]  2%|▏         | 141/8917 [08:33<8:56:00,  3.66s/it]  2%|▏         | 142/8917 [08:37<8:46:58,  3.60s/it]  2%|▏         | 143/8917 [08:40<8:35:06,  3.52s/it]  2%|▏         | 144/8917 [08:44<8:41:02,  3.56s/it]  2%|▏         | 145/8917 [08:47<8:55:04,  3.66s/it]  2%|▏         | 146/8917 [08:51<8:43:43,  3.58s/it]  2%|▏         | 147/8917 [08:55<8:59:10,  3.69s/it]  2%|▏         | 148/8917 [08:58<8:51:30,  3.64s/it]  2%|▏         | 149/8917 [09:02<8:55:10,  3.66s/it]09/19/2024 01:52:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.05807099863886833, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3704276084899902, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.4284985065460205}
  2%|▏         | 150/8917 [09:06<8:56:09,  3.67s/it]  2%|▏         | 151/8917 [09:09<8:42:00,  3.57s/it]  2%|▏         | 152/8917 [09:13<9:01:13,  3.70s/it]  2%|▏         | 153/8917 [09:17<9:03:28,  3.72s/it]  2%|▏         | 154/8917 [09:20<8:59:55,  3.70s/it]  2%|▏         | 155/8917 [09:24<8:57:43,  3.68s/it]  2%|▏         | 156/8917 [09:28<8:51:18,  3.64s/it]  2%|▏         | 157/8917 [09:31<8:45:53,  3.60s/it]  2%|▏         | 158/8917 [09:35<8:52:26,  3.65s/it]  2%|▏         | 159/8917 [09:39<8:53:05,  3.65s/it]  2%|▏         | 160/8917 [09:42<8:44:10,  3.59s/it]  2%|▏         | 161/8917 [09:46<8:44:48,  3.60s/it]  2%|▏         | 162/8917 [09:49<8:39:37,  3.56s/it]  2%|▏         | 163/8917 [09:52<8:29:49,  3.49s/it]  2%|▏         | 164/8917 [09:56<8:47:49,  3.62s/it]  2%|▏         | 165/8917 [10:00<8:53:08,  3.66s/it]  2%|▏         | 166/8917 [10:04<8:46:35,  3.61s/it]  2%|▏         | 167/8917 [10:07<8:43:00,  3.59s/it]  2%|▏         | 168/8917 [10:11<8:38:52,  3.56s/it]  2%|▏         | 169/8917 [10:14<8:26:38,  3.47s/it]  2%|▏         | 170/8917 [10:18<8:37:03,  3.55s/it]  2%|▏         | 171/8917 [10:21<8:36:12,  3.54s/it]  2%|▏         | 172/8917 [10:25<8:46:22,  3.61s/it]  2%|▏         | 173/8917 [10:28<8:43:21,  3.59s/it]  2%|▏         | 174/8917 [10:32<8:45:46,  3.61s/it]  2%|▏         | 175/8917 [10:36<8:42:36,  3.59s/it]  2%|▏         | 176/8917 [10:39<8:49:17,  3.63s/it]  2%|▏         | 177/8917 [10:43<8:46:33,  3.61s/it]  2%|▏         | 178/8917 [10:46<8:37:18,  3.55s/it]  2%|▏         | 179/8917 [10:50<8:38:22,  3.56s/it]  2%|▏         | 180/8917 [10:53<8:37:52,  3.56s/it]  2%|▏         | 181/8917 [10:57<8:33:45,  3.53s/it]  2%|▏         | 182/8917 [11:01<8:40:31,  3.58s/it]  2%|▏         | 183/8917 [11:04<8:43:36,  3.60s/it]  2%|▏         | 184/8917 [11:08<8:45:29,  3.61s/it]  2%|▏         | 185/8917 [11:12<8:46:47,  3.62s/it]  2%|▏         | 186/8917 [11:15<8:43:06,  3.59s/it]  2%|▏         | 187/8917 [11:19<8:55:20,  3.68s/it]  2%|▏         | 188/8917 [11:23<9:03:51,  3.74s/it]  2%|▏         | 189/8917 [11:27<9:06:16,  3.76s/it]  2%|▏         | 190/8917 [11:30<9:00:39,  3.72s/it]  2%|▏         | 191/8917 [11:34<8:54:36,  3.68s/it]  2%|▏         | 192/8917 [11:37<8:45:25,  3.61s/it]  2%|▏         | 193/8917 [11:41<8:53:57,  3.67s/it]  2%|▏         | 194/8917 [11:45<8:45:14,  3.61s/it]  2%|▏         | 195/8917 [11:48<8:45:39,  3.62s/it]  2%|▏         | 196/8917 [11:52<8:59:57,  3.71s/it]  2%|▏         | 197/8917 [11:56<8:57:22,  3.70s/it]  2%|▏         | 198/8917 [11:59<8:53:05,  3.67s/it]  2%|▏         | 199/8917 [12:03<8:51:25,  3.66s/it]09/19/2024 01:55:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.033428944647312164, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4451680183410645, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4785969257354736}
  2%|▏         | 200/8917 [12:07<8:48:45,  3.64s/it]  2%|▏         | 201/8917 [12:10<8:51:14,  3.66s/it]  2%|▏         | 202/8917 [12:14<8:40:27,  3.58s/it]  2%|▏         | 203/8917 [12:17<8:47:07,  3.63s/it]  2%|▏         | 204/8917 [12:21<8:43:07,  3.60s/it]  2%|▏         | 205/8917 [12:25<8:49:12,  3.64s/it]  2%|▏         | 206/8917 [12:28<8:46:22,  3.63s/it]  2%|▏         | 207/8917 [12:32<8:40:20,  3.58s/it]  2%|▏         | 208/8917 [12:35<8:36:40,  3.56s/it]  2%|▏         | 209/8917 [12:39<8:46:02,  3.62s/it]  2%|▏         | 210/8917 [12:43<8:46:29,  3.63s/it]  2%|▏         | 211/8917 [12:47<8:53:43,  3.68s/it]  2%|▏         | 212/8917 [12:50<9:01:37,  3.73s/it]  2%|▏         | 213/8917 [12:54<8:54:07,  3.68s/it]  2%|▏         | 214/8917 [12:57<8:46:45,  3.63s/it]  2%|▏         | 215/8917 [13:01<8:45:08,  3.62s/it]  2%|▏         | 216/8917 [13:04<8:29:10,  3.51s/it]  2%|▏         | 217/8917 [13:08<8:31:41,  3.53s/it]  2%|▏         | 218/8917 [13:12<8:42:56,  3.61s/it]  2%|▏         | 219/8917 [13:15<8:36:34,  3.56s/it]  2%|▏         | 220/8917 [13:19<8:40:37,  3.59s/it]  2%|▏         | 221/8917 [13:23<8:48:28,  3.65s/it]  2%|▏         | 222/8917 [13:26<8:38:39,  3.58s/it]  3%|▎         | 223/8917 [13:30<8:37:29,  3.57s/it]  3%|▎         | 224/8917 [13:33<8:40:32,  3.59s/it]  3%|▎         | 225/8917 [13:37<8:45:16,  3.63s/it]  3%|▎         | 226/8917 [13:40<8:40:54,  3.60s/it]  3%|▎         | 227/8917 [13:44<8:37:05,  3.57s/it]  3%|▎         | 228/8917 [13:47<8:33:52,  3.55s/it]  3%|▎         | 229/8917 [13:51<8:51:31,  3.67s/it]  3%|▎         | 230/8917 [13:55<8:42:53,  3.61s/it]  3%|▎         | 231/8917 [13:58<8:38:15,  3.58s/it]  3%|▎         | 232/8917 [14:02<8:33:49,  3.55s/it]  3%|▎         | 233/8917 [14:06<8:39:15,  3.59s/it]  3%|▎         | 234/8917 [14:09<8:50:59,  3.67s/it]  3%|▎         | 235/8917 [14:13<8:42:38,  3.61s/it]  3%|▎         | 236/8917 [14:16<8:33:43,  3.55s/it]  3%|▎         | 237/8917 [14:20<8:32:59,  3.55s/it]  3%|▎         | 238/8917 [14:24<8:44:26,  3.63s/it]  3%|▎         | 239/8917 [14:27<8:35:48,  3.57s/it]  3%|▎         | 240/8917 [14:31<8:48:20,  3.65s/it]  3%|▎         | 241/8917 [14:35<8:50:10,  3.67s/it]  3%|▎         | 242/8917 [14:38<8:40:18,  3.60s/it]  3%|▎         | 243/8917 [14:42<8:40:11,  3.60s/it]  3%|▎         | 244/8917 [14:45<8:37:30,  3.58s/it]  3%|▎         | 245/8917 [14:48<8:24:31,  3.49s/it]  3%|▎         | 246/8917 [14:52<8:40:29,  3.60s/it]  3%|▎         | 247/8917 [14:56<8:53:24,  3.69s/it]  3%|▎         | 248/8917 [15:00<8:41:21,  3.61s/it]  3%|▎         | 249/8917 [15:03<8:35:48,  3.57s/it]09/19/2024 01:58:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038329724222421646, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3981620073318481, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4364917278289795}
  3%|▎         | 250/8917 [15:07<8:41:32,  3.61s/it]  3%|▎         | 251/8917 [15:10<8:41:51,  3.61s/it]  3%|▎         | 252/8917 [15:14<8:40:40,  3.61s/it]  3%|▎         | 253/8917 [15:18<8:40:06,  3.60s/it]  3%|▎         | 254/8917 [15:21<8:39:08,  3.60s/it]  3%|▎         | 255/8917 [15:25<8:42:49,  3.62s/it]  3%|▎         | 256/8917 [15:28<8:34:37,  3.57s/it]  3%|▎         | 257/8917 [15:32<8:35:32,  3.57s/it]  3%|▎         | 258/8917 [15:35<8:33:44,  3.56s/it]  3%|▎         | 259/8917 [15:39<8:38:57,  3.60s/it]  3%|▎         | 260/8917 [15:43<8:41:09,  3.61s/it]  3%|▎         | 261/8917 [15:46<8:33:56,  3.56s/it]  3%|▎         | 262/8917 [15:50<8:38:35,  3.60s/it]  3%|▎         | 263/8917 [15:54<8:41:31,  3.62s/it]  3%|▎         | 264/8917 [15:57<8:38:14,  3.59s/it]  3%|▎         | 265/8917 [16:01<8:48:26,  3.66s/it]  3%|▎         | 266/8917 [16:04<8:34:10,  3.57s/it]  3%|▎         | 267/8917 [16:08<8:39:58,  3.61s/it]  3%|▎         | 268/8917 [16:12<8:38:07,  3.59s/it]  3%|▎         | 269/8917 [16:15<8:26:51,  3.52s/it]  3%|▎         | 270/8917 [16:19<8:38:18,  3.60s/it]  3%|▎         | 271/8917 [16:22<8:35:52,  3.58s/it]  3%|▎         | 272/8917 [16:26<8:33:05,  3.56s/it]  3%|▎         | 273/8917 [16:29<8:34:21,  3.57s/it]  3%|▎         | 274/8917 [16:33<8:41:03,  3.62s/it]  3%|▎         | 275/8917 [16:37<8:37:10,  3.59s/it]  3%|▎         | 276/8917 [16:40<8:48:57,  3.67s/it]  3%|▎         | 277/8917 [16:44<8:45:20,  3.65s/it]  3%|▎         | 278/8917 [16:48<8:43:53,  3.64s/it]  3%|▎         | 279/8917 [16:51<8:49:42,  3.68s/it]  3%|▎         | 280/8917 [16:55<8:46:57,  3.66s/it]  3%|▎         | 281/8917 [16:59<8:39:10,  3.61s/it]  3%|▎         | 282/8917 [17:02<8:35:53,  3.58s/it]  3%|▎         | 283/8917 [17:06<8:38:05,  3.60s/it]  3%|▎         | 284/8917 [17:09<8:33:54,  3.57s/it]  3%|▎         | 285/8917 [17:13<8:36:25,  3.59s/it]  3%|▎         | 286/8917 [17:17<8:48:03,  3.67s/it]  3%|▎         | 287/8917 [17:21<8:57:07,  3.73s/it]  3%|▎         | 288/8917 [17:24<8:51:50,  3.70s/it]  3%|▎         | 289/8917 [17:28<8:48:43,  3.68s/it]  3%|▎         | 290/8917 [17:31<8:49:30,  3.68s/it]  3%|▎         | 291/8917 [17:35<8:46:14,  3.66s/it]  3%|▎         | 292/8917 [17:39<8:47:06,  3.67s/it]  3%|▎         | 293/8917 [17:42<8:44:08,  3.65s/it]  3%|▎         | 294/8917 [17:46<8:43:35,  3.64s/it]  3%|▎         | 295/8917 [17:50<8:40:14,  3.62s/it]  3%|▎         | 296/8917 [17:53<8:45:02,  3.65s/it]  3%|▎         | 297/8917 [17:57<8:31:24,  3.56s/it]  3%|▎         | 298/8917 [18:00<8:37:41,  3.60s/it]  3%|▎         | 299/8917 [18:04<8:33:42,  3.58s/it]09/19/2024 02:01:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.037707772105932236, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.040199041366577, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.077906847000122}
srun: Job step aborted: Waiting up to 92 seconds for job step to finish.
  3%|▎         | 300/8917 [18:08<8:54:33,  3.72s/it]  3%|▎         | 301/8917 [18:12<8:53:39,  3.72s/it]  3%|▎         | 302/8917 [18:15<8:48:28,  3.68s/it]  3%|▎         | 303/8917 [18:19<8:35:50,  3.59s/it]  3%|▎         | 304/8917 [18:22<8:27:54,  3.54s/it]  3%|▎         | 305/8917 [18:26<8:30:39,  3.56s/it]  3%|▎         | 306/8917 [18:29<8:26:00,  3.53s/it]  3%|▎         | 307/8917 [18:33<8:36:04,  3.60s/it]slurmstepd: error: *** STEP 7763178.0 ON lrdn3065 CANCELLED AT 2024-09-19T02:02:26 ***
slurmstepd: error: *** JOB 7763178 ON lrdn3065 CANCELLED AT 2024-09-19T02:02:26 ***
WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3221313 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3221314 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3221315 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3221316 closing signal SIGTERM
