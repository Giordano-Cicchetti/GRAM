NODELIST=lrdn3411
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/distributed/launch.py:181: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use-env is set by default in torchrun.
If your script expects `--local-rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
3
10

2
DEVICE SET
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 0
DEVICE SET
DEVICE SET
DEVICE SET
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 1
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 3
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Added key: store_based_barrier_key:1 to store for rank: 2
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 02:04:04 - INFO - torch.distributed.distributed_c10d -   Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.
09/19/2024 02:04:04 - INFO - __main__ -   ==================model_configs==================

09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_model_type : vast
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_itm_ratio : 0.1
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_frozen_vision : False
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_frozen_audio : False
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_checkpointing : True
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_max_caption_len : 40
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_max_omni_caption_len : 70
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_max_subtitle_len : 70
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_contra_dim : 512
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_inherit_keys : ['vision_encoder_type', 'audio_encoder_type', 'audio_melbins', 'audio_target_length']
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_frame_embedding_type : adaptive
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_vision_resolution : 224
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_vision_encoder_type : evaclip01_giant
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_audio_encoder_type : beats
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_audio_melbins : 64
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_audio_target_length : 1024
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_beam_size : 3
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_captioner_mode : False
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_generate_nums : 1
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_ret_bidirection_evaluation : False
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_itm_rerank_num : 50
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_evaluation_type : evaluation_mm
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_default : ./config/vast/default_model_cfg.json
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_max_vision_sample_num : 8
09/19/2024 02:04:04 - INFO - __main__ -   model_cfg_max_audio_sample_num : 1
09/19/2024 02:04:04 - INFO - __main__ -   ==================run_configs==================

09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_checkpoint : 
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_output_dir : ./output/vast/pretrain_vast//downstream/finetuneMSRVTT3
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_gradient_accumulation_steps : 1
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_clip_lr : 5e-07
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_optim : adamw
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_learning_rate : 2e-05
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_betas : [0.9, 0.98]
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_weight_decay : 0.01
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_grad_norm : 2.0
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_warmup_ratio : 0.1
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_resume : False
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_seed : 50
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_fp16 : True
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_bf16 : False
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_zero_shot : False
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_scheduler : warmup_linear
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_new_lr : 0
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_new_params_name : []
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_valid_freq : 10
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_dataset_mix_type : random
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_remove_before_ckpt : True
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_first_eval : True
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_pretrain_dir : ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod120k
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_num_train_steps : 0
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_save_best : True
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_pin_mem : True
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_vision_resolution : 224
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_use_ddp : False
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_mode : training
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_log_steps : 100
09/19/2024 02:04:04 - INFO - __main__ -   run_cfg_default : ./config/vast/default_run_cfg.json
09/19/2024 02:04:04 - INFO - __main__ -   ==================data_configs==================

09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_type : annoindexed
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_training : True
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_name : msrvtt_ret
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_txt : datasets/annotations/msrvtt/descs_ret_train.json
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision : ../MSRVTT/videos/videos
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_audio : ../MSRVTT/audios
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_transforms : crop_flip
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_format : video_rawvideo
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_vision_sample_num : 8
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_audio_sample_num : 1
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_task : ret%tv%ta
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_epoch : 3.6
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_n_workers : 8
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_train_batch_size : 64
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_type : annoindexed
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_training : False
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_name : msrvtt_ret
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_txt : datasets/annotations/msrvtt/descs_ret_test.json
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision : ../MSRVTT/video_test
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_transforms : crop_flip
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_format : video_rawvideo
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio : ../MSRVTT/audio_test
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_vision_sample_num : 16
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_audio_sample_num : 1
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_task : ret%tvas
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_n_workers : 8
09/19/2024 02:04:04 - INFO - __main__ -   data_cfg_msrvtt_ret_val_batch_size : 64
wandb: Tracking run with wandb version 0.17.8
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
09/19/2024 02:04:07 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/19/2024 02:04:07 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 158540 labels
ci sono 158540 labels
ci sono 158540 labels
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
ci sono 884 labels
ci sono 884 labels
ci sono 884 labels
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'

Please 'pip install xformers'
09/19/2024 02:04:12 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/19/2024 02:04:12 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
09/19/2024 02:04:12 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
ci sono 158540 labels
09/19/2024 02:04:16 - INFO - __main__ -   Create Dataset msrvtt_ret Success
09/19/2024 02:04:16 - INFO - __main__ -    loader ret%tv%ta--msrvtt_ret , ratio 8917 , bs_pergpu 16, n_workers 8
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 02:04:23 - INFO - __main__ -   msrvtt_ret Using clip mean and std.
09/19/2024 02:04:23 - INFO - __main__ -   msrvtt_ret transforms crop_flip
ci sono 884 labels
09/19/2024 02:04:23 - INFO - __main__ -   Create Dataset msrvtt_ret Success
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
Please 'pip install xformers'
Please 'pip install xformers'
Please 'pip install xformers'
09/19/2024 02:04:28 - INFO - root -   Loaded EVA01-CLIP-g-14 model config.
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 02:05:13 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 02:05:13 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 02:05:14 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 02:05:16 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 02:05:16 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 02:05:17 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 02:05:17 - INFO - root -   Loading pretrained EVA01-CLIP-g-14 weights (./pretrained_weights/clip/EVA01_CLIP_g_14_psz14_s11B.pt).
09/19/2024 02:05:17 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 02:05:17 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 02:05:18 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
09/19/2024 02:05:19 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/19/2024 02:05:19 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
09/19/2024 02:05:20 - INFO - root -   incompatible_keys.missing_keys: []
09/19/2024 02:05:20 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.query.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'cls.predictions.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
09/19/2024 02:05:20 - INFO - model.audio_encoders.beats.beats -   BEATs Config: {'input_patch_size': 16, 'embed_dim': 512, 'conv_bias': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': 'gelu', 'layer_wise_gradient_decay_ratio': 1.0, 'layer_norm_first': False, 'deep_norm': True, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'conv_pos': 128, 'conv_pos_groups': 16, 'relative_position_embedding': True, 'num_buckets': 320, 'max_distance': 800, 'gru_rel_pos': True, 'finetuned_model': False, 'predictor_dropout': 0.1, 'predictor_class': 527}
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.6.crossattention.self.value.weight', 'cls.predictions.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.query.weight', 'cls.predictions.transform.dense.weight', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.key.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.value.weight', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.self.value.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.4.crossattention.output.dense.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.query.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.query.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.7.crossattention.self.value.bias', 'cls.predictions.transform.dense.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.3.crossattention.self.query.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
09/19/2024 02:05:22 - WARNING - model.text_encoders.bert.bert -   If you want to use `BertForMaskedLM` make sure `config.is_decoder=False` for bi-directional self-attention.
Some weights of BertForMaskedLM were not initialized from the model checkpoint at ./pretrained_weights/bert/bert-base-uncased and are newly initialized: ['encoder.layer.7.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.self.query.weight', 'encoder.layer.6.crossattention.output.dense.bias', 'encoder.layer.0.crossattention.self.value.bias', 'encoder.layer.7.crossattention.self.query.weight', 'encoder.layer.10.crossattention.output.dense.bias', 'encoder.layer.10.crossattention.output.LayerNorm.weight', 'encoder.layer.4.crossattention.output.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'encoder.layer.4.crossattention.self.key.bias', 'encoder.layer.4.crossattention.self.key.weight', 'encoder.layer.8.crossattention.self.query.bias', 'encoder.layer.10.crossattention.output.dense.weight', 'encoder.layer.8.crossattention.self.key.bias', 'encoder.layer.8.crossattention.self.query.weight', 'encoder.layer.9.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.key.weight', 'encoder.layer.0.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'encoder.layer.10.crossattention.self.value.bias', 'encoder.layer.10.crossattention.self.query.bias', 'encoder.layer.9.crossattention.output.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'encoder.layer.1.crossattention.self.query.bias', 'encoder.layer.6.crossattention.self.query.weight', 'encoder.layer.4.crossattention.self.query.weight', 'encoder.layer.3.crossattention.self.value.bias', 'encoder.layer.5.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.bias', 'encoder.layer.4.crossattention.output.dense.weight', 'encoder.layer.4.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.value.weight', 'encoder.layer.8.crossattention.output.dense.bias', 'encoder.layer.11.crossattention.self.key.weight', 'encoder.layer.8.crossattention.output.LayerNorm.bias', 'encoder.layer.0.crossattention.output.dense.bias', 'encoder.layer.1.crossattention.self.value.weight', 'encoder.layer.11.crossattention.self.value.weight', 'encoder.layer.5.crossattention.self.query.weight', 'encoder.layer.0.crossattention.self.query.weight', 'encoder.layer.1.crossattention.self.key.bias', 'encoder.layer.10.crossattention.self.key.bias', 'encoder.layer.0.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.weight', 'encoder.layer.0.crossattention.self.query.bias', 'encoder.layer.1.crossattention.output.LayerNorm.weight', 'encoder.layer.7.crossattention.output.LayerNorm.weight', 'encoder.layer.1.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.output.LayerNorm.bias', 'encoder.layer.7.crossattention.output.dense.weight', 'encoder.layer.2.crossattention.self.value.bias', 'encoder.layer.9.crossattention.self.query.weight', 'encoder.layer.9.crossattention.output.dense.weight', 'encoder.layer.5.crossattention.self.value.bias', 'encoder.layer.11.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.dense.bias', 'encoder.layer.3.crossattention.output.dense.bias', 'encoder.layer.7.crossattention.self.query.bias', 'encoder.layer.0.crossattention.self.key.weight', 'encoder.layer.1.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.key.weight', 'encoder.layer.10.crossattention.self.value.weight', 'encoder.layer.1.crossattention.output.dense.bias', 'encoder.layer.9.crossattention.self.key.bias', 'encoder.layer.5.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.dense.weight', 'encoder.layer.11.crossattention.self.value.bias', 'encoder.layer.4.crossattention.self.query.bias', 'encoder.layer.11.crossattention.output.dense.bias', 'encoder.layer.8.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.value.weight', 'encoder.layer.5.crossattention.output.dense.weight', 'encoder.layer.9.crossattention.self.query.bias', 'encoder.layer.2.crossattention.self.value.weight', 'encoder.layer.4.crossattention.self.value.bias', 'encoder.layer.11.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.weight', 'encoder.layer.3.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.output.dense.bias', 'encoder.layer.6.crossattention.self.value.bias', 'encoder.layer.2.crossattention.self.query.bias', 'encoder.layer.3.crossattention.self.value.weight', 'encoder.layer.7.crossattention.self.value.bias', 'encoder.layer.7.crossattention.output.LayerNorm.bias', 'encoder.layer.10.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.self.key.bias', 'encoder.layer.1.crossattention.self.value.bias', 'encoder.layer.5.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.value.weight', 'encoder.layer.3.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.self.query.bias', 'encoder.layer.4.crossattention.output.LayerNorm.weight', 'encoder.layer.6.crossattention.self.key.bias', 'encoder.layer.6.crossattention.self.value.weight', 'encoder.layer.2.crossattention.self.query.weight', 'encoder.layer.1.crossattention.output.LayerNorm.bias', 'encoder.layer.2.crossattention.output.LayerNorm.weight', 'encoder.layer.11.crossattention.self.key.bias', 'cls.predictions.transform.LayerNorm.weight', 'encoder.layer.11.crossattention.output.LayerNorm.bias', 'encoder.layer.1.crossattention.self.query.weight', 'encoder.layer.2.crossattention.output.LayerNorm.bias', 'encoder.layer.9.crossattention.output.dense.bias', 'cls.predictions.bias', 'encoder.layer.0.crossattention.output.dense.weight', 'encoder.layer.6.crossattention.self.query.bias', 'encoder.layer.7.crossattention.self.key.bias', 'encoder.layer.11.crossattention.output.LayerNorm.weight', 'encoder.layer.5.crossattention.self.key.weight', 'encoder.layer.2.crossattention.output.dense.weight', 'encoder.layer.3.crossattention.output.LayerNorm.bias', 'encoder.layer.6.crossattention.output.LayerNorm.weight', 'encoder.layer.0.crossattention.self.key.bias', 'encoder.layer.7.crossattention.self.key.weight', 'encoder.layer.11.crossattention.self.query.weight', 'encoder.layer.6.crossattention.self.key.weight', 'encoder.layer.9.crossattention.self.value.weight', 'encoder.layer.8.crossattention.self.key.weight', 'encoder.layer.3.crossattention.self.key.bias', 'encoder.layer.0.crossattention.output.LayerNorm.bias', 'encoder.layer.8.crossattention.output.LayerNorm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
09/19/2024 02:05:31 - INFO - __main__ -   load_from_pretrained: ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod120k/ckpt/model_step_459.pt
09/19/2024 02:05:31 - INFO - __main__ -   Load from pretrained dir ./output/vast/pretrain_vast/downstream/finetuneVolume256batchlossonlyvolume4Mod120k
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 02:05:33 - INFO - __main__ -   Unexpected keys []
09/19/2024 02:05:33 - INFO - __main__ -   missing_keys  []
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
09/19/2024 02:05:34 - INFO - __main__ -   ==================learning_rate_settings==================

09/19/2024 02:05:34 - INFO - __main__ -     basic_lr : 2e-05
09/19/2024 02:05:34 - INFO - __main__ -     clip_lr_visual : 5e-07
09/19/2024 02:05:34 - INFO - __main__ -     clip_lr_visual_len : 245
09/19/2024 02:05:34 - INFO - __main__ -     new_lr : 0
09/19/2024 02:05:34 - INFO - __main__ -     new_params_name: []
09/19/2024 02:05:34 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 02:05:34 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torch/utils/checkpoint.py:31: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn("None of the inputs have requires_grad=True. Gradients will be None")
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:01<04:47,  1.31s/it]  1%|          | 2/221 [00:02<03:30,  1.04it/s]  1%|▏         | 3/221 [00:02<02:34,  1.41it/s]  2%|▏         | 4/221 [00:02<01:47,  2.01it/s]  2%|▏         | 5/221 [00:02<01:28,  2.45it/s]  3%|▎         | 6/221 [00:03<01:10,  3.05it/s]  3%|▎         | 7/221 [00:03<01:10,  3.05it/s]  4%|▎         | 8/221 [00:04<01:34,  2.25it/s]  4%|▍         | 9/221 [00:04<01:39,  2.12it/s]  5%|▍         | 10/221 [00:05<01:54,  1.85it/s]  5%|▍         | 11/221 [00:05<01:34,  2.22it/s]  5%|▌         | 12/221 [00:06<02:04,  1.67it/s]  6%|▌         | 13/221 [00:06<01:47,  1.93it/s]  6%|▋         | 14/221 [00:08<03:01,  1.14it/s]  7%|▋         | 15/221 [00:08<02:27,  1.39it/s]  7%|▋         | 16/221 [00:09<02:21,  1.45it/s]  8%|▊         | 17/221 [00:10<02:53,  1.17it/s]  8%|▊         | 18/221 [00:11<02:27,  1.38it/s]  9%|▊         | 19/221 [00:11<01:52,  1.80it/s]  9%|▉         | 20/221 [00:11<01:35,  2.10it/s] 10%|▉         | 21/221 [00:11<01:25,  2.35it/s] 10%|▉         | 22/221 [00:12<01:18,  2.54it/s] 11%|█         | 24/221 [00:12<00:54,  3.61it/s] 11%|█▏        | 25/221 [00:12<00:52,  3.74it/s] 12%|█▏        | 26/221 [00:13<01:19,  2.44it/s] 12%|█▏        | 27/221 [00:13<01:06,  2.91it/s] 13%|█▎        | 28/221 [00:14<01:40,  1.92it/s] 13%|█▎        | 29/221 [00:15<02:02,  1.57it/s] 14%|█▎        | 30/221 [00:16<01:49,  1.75it/s] 14%|█▍        | 31/221 [00:16<01:41,  1.88it/s] 14%|█▍        | 32/221 [00:16<01:17,  2.45it/s] 15%|█▍        | 33/221 [00:17<01:19,  2.37it/s] 15%|█▌        | 34/221 [00:17<01:10,  2.66it/s] 16%|█▌        | 35/221 [00:17<01:08,  2.70it/s] 16%|█▋        | 36/221 [00:18<01:16,  2.41it/s] 17%|█▋        | 37/221 [00:19<01:47,  1.72it/s] 17%|█▋        | 38/221 [00:19<01:50,  1.65it/s] 18%|█▊        | 39/221 [00:20<01:47,  1.70it/s] 18%|█▊        | 40/221 [00:20<01:46,  1.70it/s] 19%|█▊        | 41/221 [00:21<01:21,  2.21it/s] 19%|█▉        | 42/221 [00:21<01:37,  1.84it/s] 19%|█▉        | 43/221 [00:21<01:14,  2.38it/s] 20%|██        | 45/221 [00:23<01:44,  1.68it/s] 21%|██        | 46/221 [00:24<01:49,  1.60it/s] 21%|██▏       | 47/221 [00:25<02:05,  1.39it/s] 22%|██▏       | 48/221 [00:25<01:36,  1.78it/s] 22%|██▏       | 49/221 [00:25<01:37,  1.77it/s] 23%|██▎       | 50/221 [00:26<01:33,  1.83it/s] 23%|██▎       | 51/221 [00:26<01:12,  2.35it/s] 24%|██▎       | 52/221 [00:26<01:03,  2.67it/s] 24%|██▍       | 53/221 [00:27<00:54,  3.06it/s] 24%|██▍       | 54/221 [00:29<02:15,  1.23it/s] 25%|██▍       | 55/221 [00:29<01:55,  1.44it/s] 25%|██▌       | 56/221 [00:29<01:38,  1.68it/s] 26%|██▌       | 57/221 [00:30<01:24,  1.95it/s] 26%|██▌       | 58/221 [00:30<01:06,  2.44it/s] 27%|██▋       | 59/221 [00:30<01:01,  2.62it/s] 27%|██▋       | 60/221 [00:31<01:37,  1.65it/s] 28%|██▊       | 61/221 [00:32<01:24,  1.88it/s] 28%|██▊       | 62/221 [00:32<01:14,  2.14it/s] 29%|██▊       | 63/221 [00:32<01:11,  2.20it/s] 29%|██▉       | 64/221 [00:33<01:01,  2.54it/s] 29%|██▉       | 65/221 [00:33<00:58,  2.67it/s] 30%|██▉       | 66/221 [00:33<01:05,  2.38it/s] 30%|███       | 67/221 [00:34<01:20,  1.92it/s] 31%|███       | 68/221 [00:35<01:08,  2.22it/s] 31%|███       | 69/221 [00:36<01:39,  1.53it/s] 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s] 32%|███▏      | 71/221 [00:37<01:32,  1.63it/s] 33%|███▎      | 72/221 [00:37<01:24,  1.77it/s] 33%|███▎      | 73/221 [00:38<01:25,  1.73it/s] 33%|███▎      | 74/221 [00:38<01:07,  2.16it/s] 34%|███▍      | 75/221 [00:39<01:21,  1.80it/s] 34%|███▍      | 76/221 [00:39<01:05,  2.21it/s] 35%|███▍      | 77/221 [00:39<01:04,  2.25it/s] 35%|███▌      | 78/221 [00:40<01:02,  2.28it/s] 36%|███▌      | 79/221 [00:41<01:19,  1.79it/s] 36%|███▌      | 80/221 [00:41<01:08,  2.05it/s] 37%|███▋      | 81/221 [00:41<01:07,  2.07it/s] 37%|███▋      | 82/221 [00:43<01:54,  1.22it/s] 38%|███▊      | 83/221 [00:44<01:42,  1.34it/s] 38%|███▊      | 84/221 [00:44<01:31,  1.49it/s] 39%|███▉      | 86/221 [00:45<01:10,  1.91it/s] 39%|███▉      | 87/221 [00:46<01:27,  1.53it/s] 40%|███▉      | 88/221 [00:47<01:33,  1.43it/s] 40%|████      | 89/221 [00:48<01:39,  1.33it/s] 41%|████      | 90/221 [00:48<01:26,  1.52it/s] 41%|████      | 91/221 [00:48<01:10,  1.85it/s] 42%|████▏     | 92/221 [00:49<01:02,  2.08it/s] 42%|████▏     | 93/221 [00:50<01:20,  1.60it/s] 43%|████▎     | 94/221 [00:50<01:09,  1.82it/s] 43%|████▎     | 95/221 [00:50<01:04,  1.96it/s] 43%|████▎     | 96/221 [00:51<01:01,  2.02it/s] 44%|████▍     | 97/221 [00:51<00:49,  2.51it/s] 44%|████▍     | 98/221 [00:51<00:50,  2.46it/s] 45%|████▍     | 99/221 [00:52<00:39,  3.06it/s] 45%|████▌     | 100/221 [00:52<00:42,  2.83it/s] 46%|████▌     | 101/221 [00:52<00:34,  3.45it/s] 46%|████▌     | 102/221 [00:53<00:55,  2.15it/s] 47%|████▋     | 103/221 [00:53<00:47,  2.48it/s] 47%|████▋     | 104/221 [00:53<00:39,  2.99it/s] 48%|████▊     | 105/221 [00:54<00:40,  2.85it/s] 48%|████▊     | 106/221 [00:55<01:10,  1.63it/s] 48%|████▊     | 107/221 [00:55<00:56,  2.03it/s] 49%|████▉     | 108/221 [00:55<00:47,  2.39it/s] 49%|████▉     | 109/221 [00:56<01:02,  1.80it/s] 50%|████▉     | 110/221 [00:57<01:00,  1.84it/s] 50%|█████     | 111/221 [00:58<01:10,  1.57it/s] 51%|█████     | 112/221 [00:58<00:56,  1.94it/s] 51%|█████     | 113/221 [00:58<00:56,  1.90it/s] 52%|█████▏    | 115/221 [00:59<00:35,  3.02it/s] 52%|█████▏    | 116/221 [01:06<03:26,  1.97s/it] 53%|█████▎    | 117/221 [01:06<02:43,  1.58s/it] 53%|█████▎    | 118/221 [01:07<02:11,  1.27s/it] 54%|█████▍    | 119/221 [01:07<01:48,  1.06s/it] 54%|█████▍    | 120/221 [01:08<01:32,  1.09it/s] 55%|█████▍    | 121/221 [01:08<01:08,  1.46it/s] 55%|█████▌    | 122/221 [01:08<00:56,  1.74it/s] 56%|█████▌    | 123/221 [01:08<00:45,  2.15it/s] 56%|█████▌    | 124/221 [01:09<00:40,  2.38it/s] 57%|█████▋    | 125/221 [01:09<00:47,  2.01it/s] 57%|█████▋    | 126/221 [01:10<00:50,  1.88it/s] 57%|█████▋    | 127/221 [01:11<00:59,  1.59it/s] 58%|█████▊    | 128/221 [01:11<01:02,  1.49it/s] 58%|█████▊    | 129/221 [01:12<00:49,  1.88it/s] 59%|█████▉    | 130/221 [01:12<00:47,  1.92it/s] 59%|█████▉    | 131/221 [01:12<00:38,  2.35it/s] 60%|█████▉    | 132/221 [01:13<00:35,  2.51it/s] 60%|██████    | 133/221 [01:13<00:42,  2.09it/s] 61%|██████    | 134/221 [01:14<00:37,  2.34it/s] 61%|██████    | 135/221 [01:14<00:39,  2.15it/s] 62%|██████▏   | 136/221 [01:15<00:45,  1.86it/s] 62%|██████▏   | 137/221 [01:15<00:40,  2.08it/s] 62%|██████▏   | 138/221 [01:16<00:44,  1.85it/s] 63%|██████▎   | 139/221 [01:17<00:53,  1.52it/s] 63%|██████▎   | 140/221 [01:17<00:49,  1.63it/s] 64%|██████▍   | 141/221 [01:18<00:44,  1.81it/s] 64%|██████▍   | 142/221 [01:18<00:42,  1.86it/s] 65%|██████▍   | 143/221 [01:19<00:47,  1.65it/s] 65%|██████▌   | 144/221 [01:20<00:42,  1.79it/s] 66%|██████▌   | 145/221 [01:20<00:32,  2.36it/s] 66%|██████▌   | 146/221 [01:20<00:26,  2.82it/s] 67%|██████▋   | 147/221 [01:20<00:25,  2.95it/s] 67%|██████▋   | 148/221 [01:21<00:30,  2.41it/s] 67%|██████▋   | 149/221 [01:21<00:26,  2.74it/s] 68%|██████▊   | 150/221 [01:21<00:25,  2.79it/s] 68%|██████▊   | 151/221 [01:24<01:05,  1.08it/s] 69%|██████▉   | 152/221 [01:27<01:47,  1.56s/it] 69%|██████▉   | 153/221 [01:27<01:25,  1.26s/it] 70%|██████▉   | 154/221 [01:28<01:09,  1.04s/it] 70%|███████   | 155/221 [01:28<00:54,  1.21it/s] 71%|███████   | 156/221 [01:28<00:44,  1.47it/s] 71%|███████   | 157/221 [01:31<01:13,  1.15s/it] 71%|███████▏  | 158/221 [01:31<00:58,  1.08it/s] 72%|███████▏  | 159/221 [01:31<00:45,  1.38it/s] 72%|███████▏  | 160/221 [01:32<00:38,  1.59it/s] 73%|███████▎  | 161/221 [01:32<00:28,  2.08it/s] 73%|███████▎  | 162/221 [01:33<00:32,  1.80it/s] 74%|███████▍  | 163/221 [01:33<00:29,  1.95it/s] 74%|███████▍  | 164/221 [01:34<00:31,  1.79it/s] 75%|███████▍  | 165/221 [01:34<00:25,  2.23it/s] 75%|███████▌  | 166/221 [01:35<00:30,  1.80it/s] 76%|███████▌  | 167/221 [01:35<00:26,  2.00it/s] 76%|███████▌  | 168/221 [01:37<00:44,  1.19it/s] 76%|███████▋  | 169/221 [01:37<00:34,  1.49it/s] 77%|███████▋  | 170/221 [01:38<00:33,  1.54it/s] 77%|███████▋  | 171/221 [01:38<00:33,  1.49it/s] 78%|███████▊  | 172/221 [01:39<00:27,  1.78it/s] 78%|███████▊  | 173/221 [01:39<00:22,  2.09it/s] 79%|███████▊  | 174/221 [01:39<00:21,  2.17it/s] 79%|███████▉  | 175/221 [01:40<00:23,  1.95it/s] 80%|███████▉  | 176/221 [01:40<00:21,  2.10it/s] 80%|████████  | 177/221 [01:41<00:18,  2.35it/s] 81%|████████  | 178/221 [01:41<00:17,  2.50it/s] 81%|████████  | 179/221 [01:42<00:20,  2.02it/s] 81%|████████▏ | 180/221 [01:42<00:16,  2.54it/s] 82%|████████▏ | 181/221 [01:42<00:14,  2.70it/s] 82%|████████▏ | 182/221 [01:42<00:13,  2.84it/s] 83%|████████▎ | 183/221 [01:43<00:13,  2.85it/s] 83%|████████▎ | 184/221 [01:43<00:15,  2.35it/s] 84%|████████▎ | 185/221 [01:44<00:13,  2.73it/s] 84%|████████▍ | 186/221 [01:45<00:18,  1.86it/s] 85%|████████▍ | 187/221 [01:45<00:15,  2.14it/s] 85%|████████▌ | 188/221 [01:45<00:15,  2.08it/s] 86%|████████▌ | 189/221 [01:46<00:16,  1.94it/s] 86%|████████▌ | 190/221 [01:47<00:17,  1.73it/s] 86%|████████▋ | 191/221 [01:47<00:13,  2.17it/s] 87%|████████▋ | 192/221 [01:47<00:11,  2.44it/s] 88%|████████▊ | 194/221 [01:49<00:17,  1.54it/s] 88%|████████▊ | 195/221 [01:49<00:14,  1.82it/s] 89%|████████▊ | 196/221 [01:50<00:16,  1.48it/s] 89%|████████▉ | 197/221 [01:51<00:13,  1.77it/s] 90%|████████▉ | 198/221 [01:51<00:12,  1.88it/s] 90%|█████████ | 199/221 [01:51<00:09,  2.23it/s] 90%|█████████ | 200/221 [01:52<00:09,  2.16it/s] 91%|█████████ | 201/221 [01:52<00:08,  2.23it/s] 91%|█████████▏| 202/221 [01:52<00:07,  2.47it/s] 92%|█████████▏| 203/221 [01:53<00:06,  2.67it/s] 92%|█████████▏| 204/221 [01:53<00:06,  2.51it/s] 93%|█████████▎| 205/221 [01:53<00:05,  2.98it/s] 93%|█████████▎| 206/221 [01:55<00:11,  1.34it/s] 94%|█████████▎| 207/221 [01:55<00:08,  1.60it/s] 94%|█████████▍| 208/221 [01:56<00:06,  1.93it/s] 95%|█████████▍| 209/221 [01:56<00:05,  2.01it/s] 95%|█████████▌| 210/221 [01:56<00:04,  2.51it/s] 95%|█████████▌| 211/221 [01:57<00:04,  2.03it/s] 96%|█████████▌| 212/221 [01:57<00:04,  2.23it/s] 96%|█████████▋| 213/221 [01:58<00:03,  2.30it/s] 97%|█████████▋| 214/221 [01:58<00:03,  2.09it/s] 97%|█████████▋| 215/221 [01:59<00:02,  2.24it/s] 98%|█████████▊| 216/221 [01:59<00:02,  2.07it/s] 98%|█████████▊| 217/221 [02:00<00:02,  1.74it/s] 99%|█████████▊| 218/221 [02:01<00:01,  1.60it/s] 99%|█████████▉| 219/221 [02:01<00:01,  1.78it/s]100%|█████████▉| 220/221 [02:06<00:01,  1.94s/it]100%|██████████| 221/221 [02:07<00:00,  1.45s/it]100%|██████████| 221/221 [02:07<00:00,  1.74it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<01:56,  1.89it/s]  1%|          | 2/221 [00:01<01:55,  1.89it/s]  1%|▏         | 3/221 [00:01<01:55,  1.89it/s]  2%|▏         | 4/221 [00:02<01:54,  1.89it/s]  2%|▏         | 5/221 [00:02<01:54,  1.89it/s]  3%|▎         | 6/221 [00:03<01:53,  1.89it/s]  3%|▎         | 7/221 [00:03<01:53,  1.89it/s]  4%|▎         | 8/221 [00:04<01:52,  1.89it/s]  4%|▍         | 9/221 [00:04<01:51,  1.89it/s]  5%|▍         | 10/221 [00:05<01:51,  1.89it/s]  5%|▍         | 11/221 [00:05<01:50,  1.89it/s]  5%|▌         | 12/221 [00:06<01:50,  1.89it/s]  6%|▌         | 13/221 [00:06<01:49,  1.89it/s]  6%|▋         | 14/221 [00:07<01:49,  1.89it/s]  7%|▋         | 15/221 [00:07<01:48,  1.89it/s]  7%|▋         | 16/221 [00:08<01:48,  1.89it/s]  8%|▊         | 17/221 [00:08<01:47,  1.89it/s]  8%|▊         | 18/221 [00:09<01:47,  1.89it/s]  9%|▊         | 19/221 [00:10<01:46,  1.89it/s]  9%|▉         | 20/221 [00:10<01:46,  1.89it/s] 10%|▉         | 21/221 [00:11<01:45,  1.89it/s] 10%|▉         | 22/221 [00:11<01:45,  1.89it/s] 10%|█         | 23/221 [00:12<01:44,  1.89it/s] 11%|█         | 24/221 [00:12<01:44,  1.89it/s] 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s] 12%|█▏        | 26/221 [00:13<01:42,  1.89it/s] 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s] 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s] 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s] 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s] 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s] 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s] 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s] 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s] 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s] 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s] 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s] 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s] 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s] 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s] 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s] 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s] 19%|█▉        | 43/221 [00:22<01:33,  1.89it/s] 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s] 20%|██        | 45/221 [00:23<01:32,  1.89it/s] 21%|██        | 46/221 [00:24<01:32,  1.89it/s] 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s] 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s] 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s] 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s] 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s] 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s] 24%|██▍       | 53/221 [00:27<01:28,  1.89it/s] 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s] 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s] 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s] 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s] 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s] 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s] 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s] 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s] 28%|██▊       | 62/221 [00:32<01:23,  1.89it/s] 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s] 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s] 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s] 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s] 30%|███       | 67/221 [00:35<01:21,  1.89it/s] 31%|███       | 68/221 [00:35<01:20,  1.89it/s] 31%|███       | 69/221 [00:36<01:20,  1.89it/s] 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s] 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s] 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s] 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s] 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s] 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s] 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s] 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s] 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s] 36%|███▌      | 79/221 [00:41<01:14,  1.89it/s] 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s] 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s] 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s] 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s] 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s] 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s] 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s] 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s] 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s] 40%|████      | 89/221 [00:46<01:09,  1.89it/s] 41%|████      | 90/221 [00:47<01:09,  1.89it/s] 41%|████      | 91/221 [00:48<01:08,  1.89it/s] 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s] 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s] 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s] 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s] 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s] 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s] 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s] 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s] 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s] 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s] 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s] 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s] 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s] 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s] 48%|████▊     | 106/221 [00:55<01:00,  1.89it/s] 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s] 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s] 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s] 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s] 50%|█████     | 111/221 [00:58<00:58,  1.89it/s] 51%|█████     | 112/221 [00:59<00:57,  1.89it/s] 51%|█████     | 113/221 [00:59<00:57,  1.89it/s] 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s] 52%|█████▏    | 115/221 [01:00<00:55,  1.89it/s] 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s] 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s] 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s] 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s] 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s] 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s] 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s] 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s] 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s] 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s] 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s] 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s] 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s] 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s] 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s] 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s] 60%|█████▉    | 132/221 [01:09<00:46,  1.89it/s] 60%|██████    | 133/221 [01:10<00:46,  1.89it/s] 61%|██████    | 134/221 [01:10<00:45,  1.89it/s] 61%|██████    | 135/221 [01:11<00:45,  1.89it/s] 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s] 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s] 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s] 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s] 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s] 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s] 64%|██████▍   | 142/221 [01:14<00:41,  1.89it/s] 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s] 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s] 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s] 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s] 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s] 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s] 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s] 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s] 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s] 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s] 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s] 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s] 70%|███████   | 155/221 [01:21<00:34,  1.89it/s] 71%|███████   | 156/221 [01:22<00:34,  1.89it/s] 71%|███████   | 157/221 [01:22<00:33,  1.89it/s] 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s] 72%|███████▏  | 159/221 [01:23<00:32,  1.89it/s] 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s] 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s] 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s] 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s] 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s] 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s] 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s] 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s] 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s] 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s] 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s] 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s] 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s] 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s] 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s] 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s] 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s] 80%|████████  | 177/221 [01:33<00:23,  1.89it/s] 81%|████████  | 178/221 [01:33<00:22,  1.89it/s] 81%|████████  | 179/221 [01:34<00:22,  1.89it/s] 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s] 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s] 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s] 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s] 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s] 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s] 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s] 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s] 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s] 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s] 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s] 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s] 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s] 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s] 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s] 88%|████████▊ | 195/221 [01:42<00:13,  1.89it/s] 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s] 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s] 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s] 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s] 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s] 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s] 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s] 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s] 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s] 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s] 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s] 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s] 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s] 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s] 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s] 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s] 96%|█████████▌| 212/221 [01:51<00:04,  1.89it/s] 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s] 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s] 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s] 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s] 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s] 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s] 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s]100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s]100%|██████████| 221/221 [01:56<00:00,  1.89it/s]100%|██████████| 221/221 [01:56<00:00,  1.89it/s]
  0%|          | 0/221 [00:00<?, ?it/s]  0%|          | 1/221 [00:00<01:06,  3.32it/s]  1%|          | 2/221 [00:01<03:04,  1.19it/s]  1%|▏         | 3/221 [00:01<01:50,  1.97it/s]  2%|▏         | 4/221 [00:02<01:42,  2.12it/s]  2%|▏         | 5/221 [00:02<01:50,  1.96it/s]  3%|▎         | 6/221 [00:02<01:29,  2.39it/s]  3%|▎         | 7/221 [00:03<01:27,  2.44it/s]  4%|▎         | 8/221 [00:04<01:58,  1.79it/s]  4%|▍         | 9/221 [00:04<02:02,  1.73it/s]  5%|▍         | 10/221 [00:05<01:57,  1.79it/s]  5%|▍         | 11/221 [00:05<01:48,  1.94it/s]  5%|▌         | 12/221 [00:06<01:39,  2.10it/s]  6%|▌         | 13/221 [00:07<02:33,  1.36it/s]  6%|▋         | 14/221 [00:07<02:06,  1.63it/s]  7%|▋         | 15/221 [00:07<01:40,  2.04it/s]  7%|▋         | 16/221 [00:08<01:38,  2.09it/s]  8%|▊         | 17/221 [00:10<03:25,  1.01s/it]  8%|▊         | 18/221 [00:11<03:00,  1.13it/s]  9%|▊         | 19/221 [00:11<02:27,  1.37it/s]  9%|▉         | 20/221 [00:11<02:02,  1.64it/s] 10%|▉         | 21/221 [00:12<01:44,  1.91it/s] 10%|▉         | 22/221 [00:12<01:28,  2.26it/s] 10%|█         | 23/221 [00:12<01:15,  2.63it/s] 11%|█         | 24/221 [00:12<01:03,  3.08it/s] 11%|█▏        | 25/221 [00:13<01:17,  2.53it/s] 12%|█▏        | 26/221 [00:14<01:33,  2.09it/s] 12%|█▏        | 27/221 [00:14<01:13,  2.66it/s] 13%|█▎        | 28/221 [00:15<02:12,  1.45it/s] 13%|█▎        | 29/221 [00:16<02:01,  1.58it/s] 14%|█▎        | 30/221 [00:16<02:02,  1.56it/s] 14%|█▍        | 31/221 [00:17<02:05,  1.51it/s] 14%|█▍        | 32/221 [00:17<01:38,  1.92it/s] 15%|█▍        | 33/221 [00:18<01:41,  1.85it/s] 15%|█▌        | 34/221 [00:18<01:23,  2.23it/s] 16%|█▌        | 35/221 [00:18<01:13,  2.54it/s] 16%|█▋        | 36/221 [00:19<01:23,  2.21it/s] 17%|█▋        | 37/221 [00:20<01:36,  1.91it/s] 17%|█▋        | 38/221 [00:20<01:45,  1.74it/s] 18%|█▊        | 39/221 [00:21<01:34,  1.92it/s] 18%|█▊        | 40/221 [00:22<01:52,  1.61it/s] 19%|█▊        | 41/221 [00:22<01:35,  1.89it/s] 19%|█▉        | 42/221 [00:22<01:19,  2.24it/s] 20%|█▉        | 44/221 [00:22<00:50,  3.47it/s] 20%|██        | 45/221 [00:23<01:06,  2.63it/s] 21%|██        | 46/221 [00:24<01:10,  2.47it/s] 21%|██▏       | 47/221 [00:24<01:21,  2.14it/s] 22%|██▏       | 48/221 [00:24<01:08,  2.54it/s] 22%|██▏       | 49/221 [00:25<01:15,  2.28it/s] 23%|██▎       | 50/221 [00:25<01:12,  2.36it/s] 23%|██▎       | 51/221 [00:26<01:07,  2.51it/s] 24%|██▎       | 52/221 [00:26<00:56,  3.01it/s] 24%|██▍       | 53/221 [00:26<01:01,  2.71it/s] 24%|██▍       | 54/221 [00:27<01:38,  1.69it/s] 25%|██▍       | 55/221 [00:28<01:25,  1.93it/s] 25%|██▌       | 56/221 [00:28<01:20,  2.05it/s] 26%|██▌       | 57/221 [00:28<01:12,  2.27it/s] 26%|██▌       | 58/221 [00:29<01:19,  2.04it/s] 27%|██▋       | 59/221 [00:29<01:05,  2.49it/s] 27%|██▋       | 60/221 [00:30<01:29,  1.80it/s] 28%|██▊       | 61/221 [00:31<01:22,  1.94it/s] 28%|██▊       | 62/221 [00:31<01:17,  2.04it/s] 29%|██▊       | 63/221 [00:32<01:39,  1.59it/s] 29%|██▉       | 64/221 [00:33<01:38,  1.60it/s] 29%|██▉       | 65/221 [00:33<01:47,  1.45it/s] 30%|██▉       | 66/221 [00:34<01:29,  1.72it/s] 30%|███       | 67/221 [00:35<01:57,  1.31it/s] 31%|███       | 68/221 [00:35<01:34,  1.63it/s] 31%|███       | 69/221 [00:36<01:33,  1.63it/s] 32%|███▏      | 70/221 [00:36<01:25,  1.76it/s] 32%|███▏      | 71/221 [00:37<01:22,  1.83it/s] 33%|███▎      | 72/221 [00:37<01:17,  1.92it/s] 33%|███▎      | 73/221 [00:38<01:25,  1.73it/s] 33%|███▎      | 74/221 [00:38<01:21,  1.79it/s] 34%|███▍      | 75/221 [00:39<01:16,  1.91it/s] 34%|███▍      | 76/221 [00:39<01:02,  2.34it/s] 35%|███▍      | 77/221 [00:39<00:56,  2.55it/s] 35%|███▌      | 78/221 [00:40<01:03,  2.25it/s] 36%|███▌      | 79/221 [00:41<01:30,  1.57it/s] 36%|███▌      | 80/221 [00:42<01:37,  1.44it/s] 37%|███▋      | 81/221 [00:42<01:31,  1.54it/s] 37%|███▋      | 82/221 [00:44<01:48,  1.29it/s] 38%|███▊      | 83/221 [00:45<02:09,  1.07it/s] 38%|███▊      | 84/221 [00:46<02:10,  1.05it/s] 38%|███▊      | 85/221 [00:46<01:45,  1.29it/s] 39%|███▉      | 86/221 [00:47<01:49,  1.23it/s] 39%|███▉      | 87/221 [00:49<02:34,  1.15s/it] 40%|███▉      | 88/221 [00:49<02:04,  1.07it/s] 40%|████      | 89/221 [00:50<01:46,  1.24it/s] 41%|████      | 90/221 [00:50<01:30,  1.44it/s] 41%|████      | 91/221 [00:51<01:15,  1.72it/s] 42%|████▏     | 92/221 [00:51<01:04,  2.00it/s] 42%|████▏     | 93/221 [00:51<00:56,  2.27it/s] 43%|████▎     | 94/221 [00:52<00:51,  2.46it/s] 43%|████▎     | 95/221 [00:52<01:03,  1.98it/s] 43%|████▎     | 96/221 [00:53<00:57,  2.16it/s] 44%|████▍     | 97/221 [00:53<00:49,  2.49it/s] 44%|████▍     | 98/221 [00:54<01:00,  2.03it/s] 45%|████▍     | 99/221 [00:54<00:50,  2.41it/s] 45%|████▌     | 100/221 [00:54<00:47,  2.54it/s] 46%|████▌     | 101/221 [00:55<00:42,  2.83it/s] 46%|████▌     | 102/221 [00:55<00:42,  2.83it/s] 47%|████▋     | 103/221 [00:55<00:36,  3.21it/s] 47%|████▋     | 104/221 [00:55<00:33,  3.45it/s] 48%|████▊     | 105/221 [00:56<00:33,  3.45it/s] 48%|████▊     | 106/221 [00:56<00:37,  3.05it/s] 48%|████▊     | 107/221 [00:56<00:37,  3.05it/s] 49%|████▉     | 108/221 [00:57<00:40,  2.82it/s] 49%|████▉     | 109/221 [00:58<01:21,  1.37it/s] 50%|████▉     | 110/221 [00:59<01:18,  1.42it/s] 50%|█████     | 111/221 [01:00<01:14,  1.48it/s] 51%|█████     | 112/221 [01:00<01:02,  1.74it/s] 51%|█████     | 113/221 [01:00<00:57,  1.89it/s] 52%|█████▏    | 115/221 [01:01<00:41,  2.52it/s] 53%|█████▎    | 117/221 [01:02<00:38,  2.67it/s] 53%|█████▎    | 118/221 [01:03<00:50,  2.05it/s] 54%|█████▍    | 119/221 [01:03<00:43,  2.32it/s] 54%|█████▍    | 120/221 [01:04<00:53,  1.90it/s] 55%|█████▍    | 121/221 [01:04<00:45,  2.20it/s] 55%|█████▌    | 122/221 [01:04<00:39,  2.51it/s] 56%|█████▌    | 123/221 [01:04<00:39,  2.46it/s] 56%|█████▌    | 124/221 [01:05<00:41,  2.32it/s] 57%|█████▋    | 125/221 [01:06<00:51,  1.88it/s] 57%|█████▋    | 126/221 [01:06<00:51,  1.85it/s] 57%|█████▋    | 127/221 [01:06<00:40,  2.34it/s] 58%|█████▊    | 128/221 [01:07<00:42,  2.19it/s] 58%|█████▊    | 129/221 [01:07<00:38,  2.42it/s] 59%|█████▉    | 130/221 [01:08<00:52,  1.73it/s] 59%|█████▉    | 131/221 [01:09<00:42,  2.09it/s] 60%|█████▉    | 132/221 [01:09<00:36,  2.43it/s] 60%|██████    | 133/221 [01:10<00:47,  1.84it/s] 61%|██████    | 134/221 [01:10<00:37,  2.29it/s] 61%|██████    | 135/221 [01:11<00:44,  1.91it/s] 62%|██████▏   | 136/221 [01:11<00:47,  1.78it/s] 62%|██████▏   | 137/221 [01:12<00:47,  1.75it/s] 62%|██████▏   | 138/221 [01:12<00:47,  1.74it/s] 63%|██████▎   | 139/221 [01:13<00:46,  1.78it/s] 63%|██████▎   | 140/221 [01:13<00:45,  1.77it/s] 64%|██████▍   | 141/221 [01:14<00:35,  2.24it/s] 64%|██████▍   | 142/221 [01:14<00:36,  2.15it/s] 65%|██████▍   | 143/221 [01:14<00:29,  2.66it/s] 65%|██████▌   | 144/221 [01:15<00:29,  2.58it/s] 66%|██████▌   | 145/221 [01:15<00:30,  2.52it/s] 66%|██████▌   | 146/221 [01:16<00:29,  2.51it/s] 67%|██████▋   | 147/221 [01:16<00:25,  2.88it/s] 67%|██████▋   | 148/221 [01:16<00:29,  2.45it/s] 67%|██████▋   | 149/221 [01:17<00:27,  2.66it/s] 68%|██████▊   | 150/221 [01:18<00:39,  1.82it/s] 68%|██████▊   | 151/221 [01:18<00:33,  2.07it/s] 69%|██████▉   | 152/221 [01:20<01:03,  1.09it/s] 69%|██████▉   | 153/221 [01:21<00:59,  1.15it/s] 70%|██████▉   | 154/221 [01:21<00:54,  1.23it/s] 70%|███████   | 155/221 [01:22<00:46,  1.42it/s] 71%|███████   | 156/221 [01:22<00:38,  1.68it/s] 71%|███████   | 157/221 [01:23<00:38,  1.68it/s] 71%|███████▏  | 158/221 [01:23<00:37,  1.68it/s] 72%|███████▏  | 159/221 [01:24<00:34,  1.81it/s] 72%|███████▏  | 160/221 [01:24<00:30,  1.98it/s] 73%|███████▎  | 161/221 [01:24<00:26,  2.24it/s] 73%|███████▎  | 162/221 [01:25<00:22,  2.60it/s] 74%|███████▍  | 163/221 [01:25<00:23,  2.46it/s] 74%|███████▍  | 164/221 [01:26<00:27,  2.07it/s] 75%|███████▍  | 165/221 [01:26<00:20,  2.71it/s] 75%|███████▌  | 166/221 [01:27<00:27,  2.02it/s] 76%|███████▌  | 167/221 [01:27<00:23,  2.26it/s] 76%|███████▌  | 168/221 [01:27<00:23,  2.25it/s] 76%|███████▋  | 169/221 [01:28<00:20,  2.60it/s] 77%|███████▋  | 170/221 [01:29<00:29,  1.74it/s] 77%|███████▋  | 171/221 [01:29<00:30,  1.65it/s] 78%|███████▊  | 172/221 [01:30<00:23,  2.07it/s] 78%|███████▊  | 173/221 [01:30<00:18,  2.63it/s] 79%|███████▊  | 174/221 [01:31<00:26,  1.77it/s] 79%|███████▉  | 175/221 [01:31<00:26,  1.74it/s] 80%|███████▉  | 176/221 [01:32<00:25,  1.78it/s] 80%|████████  | 177/221 [01:32<00:22,  1.94it/s] 81%|████████  | 178/221 [01:33<00:18,  2.28it/s] 81%|████████  | 179/221 [01:33<00:21,  1.96it/s] 81%|████████▏ | 180/221 [01:33<00:17,  2.29it/s] 82%|████████▏ | 181/221 [01:34<00:14,  2.76it/s] 82%|████████▏ | 182/221 [01:34<00:13,  2.89it/s] 83%|████████▎ | 183/221 [01:34<00:12,  3.13it/s] 83%|████████▎ | 184/221 [01:35<00:21,  1.69it/s] 84%|████████▎ | 185/221 [01:36<00:22,  1.59it/s] 84%|████████▍ | 186/221 [01:37<00:22,  1.53it/s] 85%|████████▍ | 187/221 [01:37<00:17,  1.99it/s] 85%|████████▌ | 188/221 [01:38<00:18,  1.81it/s] 86%|████████▌ | 189/221 [01:39<00:21,  1.48it/s] 86%|████████▌ | 190/221 [01:40<00:23,  1.30it/s] 86%|████████▋ | 191/221 [01:40<00:19,  1.57it/s] 87%|████████▋ | 192/221 [01:40<00:16,  1.80it/s] 87%|████████▋ | 193/221 [01:41<00:13,  2.01it/s] 88%|████████▊ | 194/221 [01:42<00:20,  1.35it/s] 88%|████████▊ | 195/221 [01:43<00:20,  1.29it/s] 89%|████████▊ | 196/221 [01:44<00:20,  1.23it/s] 89%|████████▉ | 197/221 [01:44<00:17,  1.34it/s] 90%|████████▉ | 198/221 [01:45<00:13,  1.65it/s] 90%|█████████ | 199/221 [01:45<00:10,  2.02it/s] 90%|█████████ | 200/221 [01:46<00:14,  1.46it/s] 91%|█████████ | 201/221 [01:46<00:12,  1.63it/s] 91%|█████████▏| 202/221 [01:47<00:09,  2.06it/s] 92%|█████████▏| 203/221 [01:47<00:07,  2.49it/s] 92%|█████████▏| 204/221 [01:48<00:09,  1.85it/s] 93%|█████████▎| 205/221 [01:48<00:07,  2.12it/s] 93%|█████████▎| 206/221 [01:49<00:08,  1.80it/s] 94%|█████████▎| 207/221 [01:49<00:06,  2.05it/s] 94%|█████████▍| 208/221 [01:50<00:07,  1.70it/s] 95%|█████████▍| 209/221 [01:51<00:07,  1.51it/s] 95%|█████████▌| 210/221 [01:51<00:06,  1.73it/s] 95%|█████████▌| 211/221 [01:52<00:05,  1.74it/s] 96%|█████████▌| 212/221 [01:52<00:04,  1.96it/s] 96%|█████████▋| 213/221 [01:53<00:05,  1.44it/s] 97%|█████████▋| 214/221 [01:54<00:04,  1.45it/s] 97%|█████████▋| 215/221 [01:55<00:04,  1.44it/s] 98%|█████████▊| 216/221 [01:55<00:03,  1.61it/s] 98%|█████████▊| 217/221 [01:56<00:02,  1.66it/s] 99%|█████████▊| 218/221 [01:56<00:01,  1.88it/s] 99%|█████████▉| 219/221 [01:56<00:00,  2.34it/s]100%|█████████▉| 220/221 [01:57<00:00,  2.44it/s]100%|██████████| 221/221 [01:57<00:00,  1.78it/s]100%|██████████| 221/221 [01:57<00:00,  1.87it/s]
09/19/2024 02:14:36 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_forward========

09/19/2024 02:14:36 - INFO - __main__ -   {'area_r1': 40.7, 'area_recall': '40.7/67.4/76.7', 'area_ravg': 61.6}
09/19/2024 02:14:36 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_backard========

09/19/2024 02:14:36 - INFO - __main__ -   {'forward_r1': 37.1, 'forward_recall': '37.1/65.4/76.2', 'forward_ravg': 59.6}
09/19/2024 02:14:36 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video========

09/19/2024 02:14:36 - INFO - __main__ -   {'area_video_r1': 39.0, 'area_video_recall': '39.0/68.0/77.6', 'area_video_ravg': 61.5}
09/19/2024 02:14:36 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itm_area========

09/19/2024 02:14:36 - INFO - __main__ -   {'area_video_r1': 55.3, 'area_video_recall': '55.3/76.1/83.8', 'area_video_ravg': 71.8, 'area_video_back_r1': 54.1, 'area_video_back_recall': '54.1/76.6/83.1', 'area_video_back_ravg': 71.3}
09/19/2024 02:14:36 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas========

09/19/2024 02:14:36 - INFO - __main__ -   {'video_r1': 28.1, 'video_recall': '28.1/52.8/63.5', 'video_ravg': 48.1}
09/19/2024 02:14:36 - INFO - __main__ -   ==== evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas========

09/19/2024 02:14:36 - INFO - __main__ -   {'video_r1': 52.7, 'video_recall': '52.7/72.4/78.7', 'video_ravg': 67.9}
  0%|          | 0/8917 [00:00<?, ?it/s]/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
/leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/utils/build_optimizer.py:171: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at ../torch/csrc/utils/python_arg_parser.cpp:1485.)
  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)
  0%|          | 1/8917 [00:04<12:12:52,  4.93s/it]  0%|          | 2/8917 [00:08<10:51:57,  4.39s/it]  0%|          | 3/8917 [00:12<9:49:55,  3.97s/it]   0%|          | 4/8917 [00:15<9:15:57,  3.74s/it]  0%|          | 5/8917 [00:19<9:17:14,  3.75s/it]  0%|          | 6/8917 [00:23<9:13:43,  3.73s/it]  0%|          | 7/8917 [00:27<9:14:56,  3.74s/it]  0%|          | 8/8917 [00:31<9:35:12,  3.87s/it]  0%|          | 9/8917 [00:34<9:30:46,  3.84s/it]  0%|          | 10/8917 [00:38<9:09:38,  3.70s/it]  0%|          | 11/8917 [00:42<9:29:19,  3.84s/it]  0%|          | 12/8917 [00:46<9:33:33,  3.86s/it]  0%|          | 13/8917 [00:49<9:19:53,  3.77s/it]  0%|          | 14/8917 [00:53<9:24:38,  3.81s/it]  0%|          | 15/8917 [00:57<9:14:24,  3.74s/it]  0%|          | 16/8917 [01:01<9:11:28,  3.72s/it]  0%|          | 17/8917 [01:04<9:12:57,  3.73s/it]  0%|          | 18/8917 [01:08<8:59:20,  3.64s/it]  0%|          | 19/8917 [01:11<8:57:23,  3.62s/it]  0%|          | 20/8917 [01:16<9:22:48,  3.80s/it]  0%|          | 21/8917 [01:19<9:12:53,  3.73s/it]  0%|          | 22/8917 [01:23<9:23:09,  3.80s/it]  0%|          | 23/8917 [01:27<9:24:01,  3.80s/it]  0%|          | 24/8917 [01:31<9:19:06,  3.77s/it]  0%|          | 25/8917 [01:34<9:12:30,  3.73s/it]  0%|          | 26/8917 [01:38<9:20:28,  3.78s/it]  0%|          | 27/8917 [01:42<9:18:22,  3.77s/it]  0%|          | 28/8917 [01:45<9:07:27,  3.70s/it]  0%|          | 29/8917 [01:49<9:20:17,  3.78s/it]  0%|          | 30/8917 [01:53<9:23:51,  3.81s/it]  0%|          | 31/8917 [01:57<9:30:54,  3.85s/it]  0%|          | 32/8917 [02:01<9:18:51,  3.77s/it]  0%|          | 33/8917 [02:04<9:13:44,  3.74s/it]  0%|          | 34/8917 [02:08<9:15:38,  3.75s/it]  0%|          | 35/8917 [02:12<9:27:28,  3.83s/it]  0%|          | 36/8917 [02:16<9:18:31,  3.77s/it]  0%|          | 37/8917 [02:19<9:09:09,  3.71s/it]  0%|          | 38/8917 [02:23<9:18:21,  3.77s/it]  0%|          | 39/8917 [02:27<9:13:47,  3.74s/it]  0%|          | 40/8917 [02:31<9:03:15,  3.67s/it]  0%|          | 41/8917 [02:34<9:14:09,  3.75s/it]  0%|          | 42/8917 [02:39<9:31:05,  3.86s/it]  0%|          | 43/8917 [02:42<9:21:51,  3.80s/it]  0%|          | 44/8917 [02:46<9:29:41,  3.85s/it]  1%|          | 45/8917 [02:50<9:21:34,  3.80s/it]  1%|          | 46/8917 [02:54<9:17:03,  3.77s/it]  1%|          | 47/8917 [02:57<9:16:16,  3.76s/it]  1%|          | 48/8917 [03:01<9:23:41,  3.81s/it]  1%|          | 49/8917 [03:05<9:23:23,  3.81s/it]09/19/2024 02:17:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03781096637248993, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7271559238433838, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7649668455123901}
  1%|          | 50/8917 [03:09<9:13:11,  3.74s/it]  1%|          | 51/8917 [03:12<9:10:21,  3.72s/it]  1%|          | 52/8917 [03:16<8:53:49,  3.61s/it]  1%|          | 53/8917 [03:19<8:54:00,  3.61s/it]  1%|          | 54/8917 [03:24<9:31:53,  3.87s/it]  1%|          | 55/8917 [03:28<9:31:56,  3.87s/it]  1%|          | 56/8917 [03:31<9:24:16,  3.82s/it]  1%|          | 57/8917 [03:35<9:13:47,  3.75s/it]  1%|          | 58/8917 [03:39<9:13:55,  3.75s/it]  1%|          | 59/8917 [03:43<9:17:39,  3.78s/it]  1%|          | 60/8917 [03:47<9:31:49,  3.87s/it]  1%|          | 61/8917 [03:50<9:18:19,  3.78s/it]  1%|          | 62/8917 [03:54<9:05:33,  3.70s/it]  1%|          | 63/8917 [03:57<9:05:43,  3.70s/it]  1%|          | 64/8917 [04:01<8:52:06,  3.61s/it]  1%|          | 65/8917 [04:05<9:01:34,  3.67s/it]  1%|          | 66/8917 [04:09<9:11:14,  3.74s/it]  1%|          | 67/8917 [04:12<8:58:12,  3.65s/it]  1%|          | 68/8917 [04:16<9:06:52,  3.71s/it]  1%|          | 69/8917 [04:20<9:07:42,  3.71s/it]  1%|          | 70/8917 [04:24<9:28:03,  3.85s/it]  1%|          | 71/8917 [04:27<9:23:26,  3.82s/it]  1%|          | 72/8917 [04:31<9:23:27,  3.82s/it]  1%|          | 73/8917 [04:35<9:18:31,  3.79s/it]  1%|          | 74/8917 [04:39<9:07:10,  3.71s/it]  1%|          | 75/8917 [04:43<9:24:06,  3.83s/it]  1%|          | 76/8917 [04:47<9:25:10,  3.84s/it]  1%|          | 77/8917 [04:50<9:07:14,  3.71s/it]  1%|          | 78/8917 [04:54<9:11:41,  3.74s/it]  1%|          | 79/8917 [04:57<9:10:54,  3.74s/it]  1%|          | 80/8917 [05:01<8:55:24,  3.64s/it]  1%|          | 81/8917 [05:05<9:03:42,  3.69s/it]  1%|          | 82/8917 [05:08<9:01:18,  3.68s/it]  1%|          | 83/8917 [05:12<9:04:06,  3.70s/it]  1%|          | 84/8917 [05:16<9:10:41,  3.74s/it]  1%|          | 85/8917 [05:20<9:14:53,  3.77s/it]  1%|          | 86/8917 [05:23<8:59:22,  3.66s/it]  1%|          | 87/8917 [05:27<9:10:18,  3.74s/it]  1%|          | 88/8917 [05:31<9:19:07,  3.80s/it]  1%|          | 89/8917 [05:35<9:14:28,  3.77s/it]  1%|          | 90/8917 [05:38<9:13:15,  3.76s/it]  1%|          | 91/8917 [05:42<9:19:44,  3.81s/it]  1%|          | 92/8917 [05:46<9:04:39,  3.70s/it]  1%|          | 93/8917 [05:50<9:06:24,  3.72s/it]  1%|          | 94/8917 [05:53<9:12:18,  3.76s/it]  1%|          | 95/8917 [05:57<9:13:06,  3.76s/it]  1%|          | 96/8917 [06:01<9:08:51,  3.73s/it]  1%|          | 97/8917 [06:05<9:10:36,  3.75s/it]  1%|          | 98/8917 [06:08<9:06:07,  3.72s/it]  1%|          | 99/8917 [06:12<9:00:39,  3.68s/it]09/19/2024 02:20:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01637582667171955, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.076507806777954, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.092883586883545}
  1%|          | 100/8917 [06:16<9:18:42,  3.80s/it]  1%|          | 101/8917 [06:19<9:00:54,  3.68s/it]  1%|          | 102/8917 [06:23<9:00:10,  3.68s/it]  1%|          | 103/8917 [06:27<9:09:04,  3.74s/it]  1%|          | 104/8917 [06:31<9:17:06,  3.79s/it]  1%|          | 105/8917 [06:35<9:21:48,  3.83s/it]  1%|          | 106/8917 [06:38<9:11:41,  3.76s/it]  1%|          | 107/8917 [06:42<9:22:02,  3.83s/it]  1%|          | 108/8917 [06:46<9:18:19,  3.80s/it]  1%|          | 109/8917 [06:50<9:19:21,  3.81s/it]  1%|          | 110/8917 [06:54<9:15:01,  3.78s/it]  1%|          | 111/8917 [06:57<9:06:57,  3.73s/it]  1%|▏         | 112/8917 [07:01<9:05:42,  3.72s/it]  1%|▏         | 113/8917 [07:05<9:10:19,  3.75s/it]  1%|▏         | 114/8917 [07:09<9:14:55,  3.78s/it]  1%|▏         | 115/8917 [07:12<9:16:59,  3.80s/it]  1%|▏         | 116/8917 [07:16<9:14:10,  3.78s/it]  1%|▏         | 117/8917 [07:20<9:05:56,  3.72s/it]  1%|▏         | 118/8917 [07:23<9:04:23,  3.71s/it]  1%|▏         | 119/8917 [07:27<9:04:44,  3.71s/it]  1%|▏         | 120/8917 [07:31<9:00:29,  3.69s/it]  1%|▏         | 121/8917 [07:35<9:06:25,  3.73s/it]  1%|▏         | 122/8917 [07:39<9:15:25,  3.79s/it]  1%|▏         | 123/8917 [07:42<9:19:25,  3.82s/it]  1%|▏         | 124/8917 [07:46<9:24:19,  3.85s/it]  1%|▏         | 125/8917 [07:50<9:14:55,  3.79s/it]  1%|▏         | 126/8917 [07:54<9:18:24,  3.81s/it]  1%|▏         | 127/8917 [07:58<9:27:36,  3.87s/it]  1%|▏         | 128/8917 [08:01<9:14:51,  3.79s/it]  1%|▏         | 129/8917 [08:06<9:25:17,  3.86s/it]  1%|▏         | 130/8917 [08:09<9:20:06,  3.82s/it]  1%|▏         | 131/8917 [08:13<9:09:35,  3.75s/it]  1%|▏         | 132/8917 [08:17<9:17:33,  3.81s/it]  1%|▏         | 133/8917 [08:20<9:09:04,  3.75s/it]  2%|▏         | 134/8917 [08:24<9:23:56,  3.85s/it]  2%|▏         | 135/8917 [08:28<9:12:18,  3.77s/it]  2%|▏         | 136/8917 [08:32<9:24:48,  3.86s/it]  2%|▏         | 137/8917 [08:36<9:20:37,  3.83s/it]  2%|▏         | 138/8917 [08:40<9:31:57,  3.91s/it]  2%|▏         | 139/8917 [08:43<9:14:33,  3.79s/it]  2%|▏         | 140/8917 [08:47<9:14:52,  3.79s/it]  2%|▏         | 141/8917 [08:51<9:16:53,  3.81s/it]  2%|▏         | 142/8917 [08:54<8:52:22,  3.64s/it]  2%|▏         | 143/8917 [08:58<8:54:18,  3.65s/it]  2%|▏         | 144/8917 [09:02<8:59:08,  3.69s/it]  2%|▏         | 145/8917 [09:06<9:05:44,  3.73s/it]  2%|▏         | 146/8917 [09:09<9:00:57,  3.70s/it]  2%|▏         | 147/8917 [09:13<8:59:41,  3.69s/it]  2%|▏         | 148/8917 [09:17<8:55:55,  3.67s/it]  2%|▏         | 149/8917 [09:20<9:03:12,  3.72s/it]09/19/2024 02:23:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.058080848306417465, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.3703174591064453, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.428398370742798}
  2%|▏         | 150/8917 [09:24<9:11:12,  3.77s/it]  2%|▏         | 151/8917 [09:28<9:07:34,  3.75s/it]  2%|▏         | 152/8917 [09:32<8:59:20,  3.69s/it]  2%|▏         | 153/8917 [09:35<9:05:03,  3.73s/it]  2%|▏         | 154/8917 [09:39<9:06:05,  3.74s/it]  2%|▏         | 155/8917 [09:43<9:05:15,  3.73s/it]  2%|▏         | 156/8917 [09:47<9:02:40,  3.72s/it]  2%|▏         | 157/8917 [09:51<9:21:17,  3.84s/it]  2%|▏         | 158/8917 [09:54<9:13:18,  3.79s/it]  2%|▏         | 159/8917 [09:58<9:12:04,  3.78s/it]  2%|▏         | 160/8917 [10:02<9:04:55,  3.73s/it]  2%|▏         | 161/8917 [10:05<8:57:04,  3.68s/it]  2%|▏         | 162/8917 [10:09<9:00:37,  3.71s/it]  2%|▏         | 163/8917 [10:13<9:17:07,  3.82s/it]  2%|▏         | 164/8917 [10:17<9:15:03,  3.80s/it]  2%|▏         | 165/8917 [10:20<8:55:40,  3.67s/it]  2%|▏         | 166/8917 [10:24<8:58:34,  3.69s/it]  2%|▏         | 167/8917 [10:28<8:51:38,  3.65s/it]  2%|▏         | 168/8917 [10:32<9:07:04,  3.75s/it]  2%|▏         | 169/8917 [10:35<9:05:51,  3.74s/it]  2%|▏         | 170/8917 [10:39<9:06:28,  3.75s/it]  2%|▏         | 171/8917 [10:43<9:04:43,  3.74s/it]  2%|▏         | 172/8917 [10:47<9:26:09,  3.88s/it]  2%|▏         | 173/8917 [10:51<9:28:48,  3.90s/it]  2%|▏         | 174/8917 [10:55<9:25:16,  3.88s/it]  2%|▏         | 175/8917 [10:59<9:20:53,  3.85s/it]  2%|▏         | 176/8917 [11:02<8:58:21,  3.70s/it]  2%|▏         | 177/8917 [11:06<9:11:32,  3.79s/it]  2%|▏         | 178/8917 [11:10<9:16:31,  3.82s/it]  2%|▏         | 179/8917 [11:13<9:12:11,  3.79s/it]  2%|▏         | 180/8917 [11:17<9:07:19,  3.76s/it]  2%|▏         | 181/8917 [11:21<9:13:48,  3.80s/it]  2%|▏         | 182/8917 [11:25<9:06:19,  3.75s/it]  2%|▏         | 183/8917 [11:28<9:04:53,  3.74s/it]  2%|▏         | 184/8917 [11:32<8:55:40,  3.68s/it]  2%|▏         | 185/8917 [11:36<8:50:33,  3.65s/it]  2%|▏         | 186/8917 [11:40<9:08:33,  3.77s/it]  2%|▏         | 187/8917 [11:44<9:19:08,  3.84s/it]  2%|▏         | 188/8917 [11:48<9:27:57,  3.90s/it]  2%|▏         | 189/8917 [11:52<9:36:20,  3.96s/it]  2%|▏         | 190/8917 [11:55<9:19:53,  3.85s/it]  2%|▏         | 191/8917 [11:59<9:03:13,  3.74s/it]  2%|▏         | 192/8917 [12:03<9:25:49,  3.89s/it]  2%|▏         | 193/8917 [12:07<9:18:24,  3.84s/it]  2%|▏         | 194/8917 [12:11<9:22:59,  3.87s/it]  2%|▏         | 195/8917 [12:14<9:04:48,  3.75s/it]  2%|▏         | 196/8917 [12:18<9:16:49,  3.83s/it]  2%|▏         | 197/8917 [12:22<9:07:32,  3.77s/it]  2%|▏         | 198/8917 [12:26<9:14:42,  3.82s/it]  2%|▏         | 199/8917 [12:29<9:03:33,  3.74s/it]09/19/2024 02:27:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03346187248826027, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.446260690689087, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4797226190567017}
  2%|▏         | 200/8917 [12:33<9:07:18,  3.77s/it]  2%|▏         | 201/8917 [12:37<9:01:33,  3.73s/it]  2%|▏         | 202/8917 [12:40<8:45:55,  3.62s/it]  2%|▏         | 203/8917 [12:44<8:39:47,  3.58s/it]  2%|▏         | 204/8917 [12:47<8:46:23,  3.62s/it]  2%|▏         | 205/8917 [12:51<8:43:46,  3.61s/it]  2%|▏         | 206/8917 [12:55<8:50:35,  3.65s/it]  2%|▏         | 207/8917 [12:59<9:11:48,  3.80s/it]  2%|▏         | 208/8917 [13:03<9:08:11,  3.78s/it]  2%|▏         | 209/8917 [13:06<9:12:45,  3.81s/it]  2%|▏         | 210/8917 [13:10<9:19:04,  3.85s/it]  2%|▏         | 211/8917 [13:14<9:28:22,  3.92s/it]  2%|▏         | 212/8917 [13:18<9:15:06,  3.83s/it]  2%|▏         | 213/8917 [13:22<9:15:42,  3.83s/it]  2%|▏         | 214/8917 [13:26<9:21:19,  3.87s/it]  2%|▏         | 215/8917 [13:29<9:04:49,  3.76s/it]  2%|▏         | 216/8917 [13:33<9:02:44,  3.74s/it]  2%|▏         | 217/8917 [13:37<9:01:33,  3.73s/it]  2%|▏         | 218/8917 [13:40<8:58:58,  3.72s/it]  2%|▏         | 219/8917 [13:44<8:55:14,  3.69s/it]  2%|▏         | 220/8917 [13:48<8:49:58,  3.66s/it]  2%|▏         | 221/8917 [13:51<8:41:30,  3.60s/it]  2%|▏         | 222/8917 [13:55<9:02:27,  3.74s/it]  3%|▎         | 223/8917 [13:59<9:15:59,  3.84s/it]  3%|▎         | 224/8917 [14:03<9:06:26,  3.77s/it]  3%|▎         | 225/8917 [14:06<8:57:49,  3.71s/it]  3%|▎         | 226/8917 [14:10<8:55:19,  3.70s/it]  3%|▎         | 227/8917 [14:14<8:52:41,  3.68s/it]  3%|▎         | 228/8917 [14:17<8:46:40,  3.64s/it]  3%|▎         | 229/8917 [14:21<9:02:22,  3.75s/it]  3%|▎         | 230/8917 [14:25<8:51:33,  3.67s/it]  3%|▎         | 231/8917 [14:28<8:50:04,  3.66s/it]  3%|▎         | 232/8917 [14:32<8:43:48,  3.62s/it]  3%|▎         | 233/8917 [14:35<8:37:59,  3.58s/it]  3%|▎         | 234/8917 [14:40<9:04:16,  3.76s/it]  3%|▎         | 235/8917 [14:44<9:19:59,  3.87s/it]  3%|▎         | 236/8917 [14:48<9:16:19,  3.85s/it]  3%|▎         | 237/8917 [14:51<9:07:34,  3.79s/it]  3%|▎         | 238/8917 [14:55<9:11:13,  3.81s/it]  3%|▎         | 239/8917 [14:59<9:08:05,  3.79s/it]  3%|▎         | 240/8917 [15:03<9:13:23,  3.83s/it]  3%|▎         | 241/8917 [15:07<9:20:24,  3.88s/it]  3%|▎         | 242/8917 [15:10<9:05:35,  3.77s/it]  3%|▎         | 243/8917 [15:14<9:08:06,  3.79s/it]  3%|▎         | 244/8917 [15:18<9:00:07,  3.74s/it]  3%|▎         | 245/8917 [15:21<8:56:14,  3.71s/it]  3%|▎         | 246/8917 [15:25<9:01:15,  3.75s/it]  3%|▎         | 247/8917 [15:29<9:15:11,  3.84s/it]  3%|▎         | 248/8917 [15:33<8:57:55,  3.72s/it]  3%|▎         | 249/8917 [15:36<8:52:40,  3.69s/it]09/19/2024 02:30:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03760576620697975, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4088175296783447, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4464232921600342}
  3%|▎         | 250/8917 [15:40<9:01:50,  3.75s/it]  3%|▎         | 251/8917 [15:44<8:56:01,  3.71s/it]  3%|▎         | 252/8917 [15:48<9:15:08,  3.84s/it]  3%|▎         | 253/8917 [15:52<9:07:08,  3.79s/it]  3%|▎         | 254/8917 [15:55<8:55:57,  3.71s/it]  3%|▎         | 255/8917 [15:59<8:56:23,  3.72s/it]  3%|▎         | 256/8917 [16:03<8:56:30,  3.72s/it]  3%|▎         | 257/8917 [16:06<8:50:16,  3.67s/it]  3%|▎         | 258/8917 [16:10<8:58:09,  3.73s/it]  3%|▎         | 259/8917 [16:14<8:56:59,  3.72s/it]  3%|▎         | 260/8917 [16:18<9:03:13,  3.77s/it]  3%|▎         | 261/8917 [16:21<8:53:55,  3.70s/it]  3%|▎         | 262/8917 [16:25<8:50:56,  3.68s/it]  3%|▎         | 263/8917 [16:28<8:45:03,  3.64s/it]  3%|▎         | 264/8917 [16:32<8:41:49,  3.62s/it]  3%|▎         | 265/8917 [16:36<9:11:03,  3.82s/it]  3%|▎         | 266/8917 [16:40<9:02:33,  3.76s/it]  3%|▎         | 267/8917 [16:44<9:10:46,  3.82s/it]  3%|▎         | 268/8917 [16:48<9:07:20,  3.80s/it]  3%|▎         | 269/8917 [16:51<9:11:05,  3.82s/it]  3%|▎         | 270/8917 [16:56<9:25:35,  3.92s/it]  3%|▎         | 271/8917 [16:59<9:14:45,  3.85s/it]  3%|▎         | 272/8917 [17:03<8:55:11,  3.71s/it]  3%|▎         | 273/8917 [17:06<8:43:32,  3.63s/it]  3%|▎         | 274/8917 [17:10<9:02:52,  3.77s/it]  3%|▎         | 275/8917 [17:14<8:49:22,  3.68s/it]  3%|▎         | 276/8917 [17:17<8:57:15,  3.73s/it]  3%|▎         | 277/8917 [17:21<8:49:08,  3.67s/it]  3%|▎         | 278/8917 [17:25<8:48:28,  3.67s/it]  3%|▎         | 279/8917 [17:29<9:06:01,  3.79s/it]  3%|▎         | 280/8917 [17:33<9:14:30,  3.85s/it]  3%|▎         | 281/8917 [17:37<9:18:34,  3.88s/it]  3%|▎         | 282/8917 [17:41<9:30:23,  3.96s/it]  3%|▎         | 283/8917 [17:45<9:16:59,  3.87s/it]  3%|▎         | 284/8917 [17:48<9:01:54,  3.77s/it]  3%|▎         | 285/8917 [17:52<9:06:07,  3.80s/it]  3%|▎         | 286/8917 [17:55<8:54:08,  3.71s/it]  3%|▎         | 287/8917 [17:59<9:07:45,  3.81s/it]  3%|▎         | 288/8917 [18:03<9:01:49,  3.77s/it]  3%|▎         | 289/8917 [18:07<9:12:59,  3.85s/it]  3%|▎         | 290/8917 [18:11<9:19:18,  3.89s/it]  3%|▎         | 291/8917 [18:15<8:59:50,  3.75s/it]  3%|▎         | 292/8917 [18:18<8:45:31,  3.66s/it]  3%|▎         | 293/8917 [18:22<8:42:43,  3.64s/it]  3%|▎         | 294/8917 [18:26<8:54:31,  3.72s/it]  3%|▎         | 295/8917 [18:30<9:08:23,  3.82s/it]  3%|▎         | 296/8917 [18:33<9:02:40,  3.78s/it]  3%|▎         | 297/8917 [18:37<8:49:29,  3.69s/it]  3%|▎         | 298/8917 [18:40<8:47:18,  3.67s/it]  3%|▎         | 299/8917 [18:44<8:51:15,  3.70s/it]09/19/2024 02:33:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03754960000514984, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.0380988121032715, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.075648307800293}
  3%|▎         | 300/8917 [18:48<9:02:38,  3.78s/it]  3%|▎         | 301/8917 [18:52<8:57:22,  3.74s/it]  3%|▎         | 302/8917 [18:56<9:07:30,  3.81s/it]  3%|▎         | 303/8917 [18:59<9:02:50,  3.78s/it]  3%|▎         | 304/8917 [19:03<9:01:46,  3.77s/it]  3%|▎         | 305/8917 [19:07<8:58:47,  3.75s/it]  3%|▎         | 306/8917 [19:11<9:13:21,  3.86s/it]  3%|▎         | 307/8917 [19:14<8:55:50,  3.73s/it]  3%|▎         | 308/8917 [19:18<8:36:11,  3.60s/it]  3%|▎         | 309/8917 [19:22<8:47:11,  3.67s/it]  3%|▎         | 310/8917 [19:25<8:50:13,  3.70s/it]  3%|▎         | 311/8917 [19:30<9:12:53,  3.85s/it]  3%|▎         | 312/8917 [19:33<8:57:02,  3.74s/it]  4%|▎         | 313/8917 [19:37<9:07:02,  3.81s/it]  4%|▎         | 314/8917 [19:41<8:56:58,  3.74s/it]  4%|▎         | 315/8917 [19:45<9:07:11,  3.82s/it]  4%|▎         | 316/8917 [19:48<8:54:49,  3.73s/it]  4%|▎         | 317/8917 [19:52<9:03:30,  3.79s/it]  4%|▎         | 318/8917 [19:56<9:07:32,  3.82s/it]  4%|▎         | 319/8917 [20:00<9:00:52,  3.77s/it]  4%|▎         | 320/8917 [20:03<8:52:43,  3.72s/it]  4%|▎         | 321/8917 [20:07<8:47:57,  3.69s/it]  4%|▎         | 322/8917 [20:11<9:06:20,  3.81s/it]  4%|▎         | 323/8917 [20:15<9:07:06,  3.82s/it]  4%|▎         | 324/8917 [20:18<8:58:43,  3.76s/it]  4%|▎         | 325/8917 [20:22<8:47:32,  3.68s/it]  4%|▎         | 326/8917 [20:25<8:43:17,  3.65s/it]  4%|▎         | 327/8917 [20:30<9:02:11,  3.79s/it]  4%|▎         | 328/8917 [20:33<9:01:16,  3.78s/it]  4%|▎         | 329/8917 [20:37<9:07:44,  3.83s/it]  4%|▎         | 330/8917 [20:41<9:21:45,  3.93s/it]  4%|▎         | 331/8917 [20:45<9:19:32,  3.91s/it]  4%|▎         | 332/8917 [20:48<8:49:55,  3.70s/it]  4%|▎         | 333/8917 [20:52<8:59:11,  3.77s/it]  4%|▎         | 334/8917 [20:56<8:51:16,  3.71s/it]  4%|▍         | 335/8917 [21:00<8:52:10,  3.72s/it]  4%|▍         | 336/8917 [21:04<9:07:24,  3.83s/it]  4%|▍         | 337/8917 [21:08<9:11:10,  3.85s/it]  4%|▍         | 338/8917 [21:11<9:05:40,  3.82s/it]  4%|▍         | 339/8917 [21:16<9:17:55,  3.90s/it]  4%|▍         | 340/8917 [21:19<9:13:52,  3.87s/it]  4%|▍         | 341/8917 [21:23<9:15:23,  3.89s/it]  4%|▍         | 342/8917 [21:27<8:55:22,  3.75s/it]  4%|▍         | 343/8917 [21:30<8:52:43,  3.73s/it]  4%|▍         | 344/8917 [21:34<8:37:06,  3.62s/it]  4%|▍         | 345/8917 [21:37<8:28:38,  3.56s/it]  4%|▍         | 346/8917 [21:41<8:26:17,  3.54s/it]  4%|▍         | 347/8917 [21:45<8:45:26,  3.68s/it]  4%|▍         | 348/8917 [21:49<9:08:16,  3.84s/it]  4%|▍         | 349/8917 [21:53<9:01:59,  3.80s/it]09/19/2024 02:36:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03582276403903961, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7640042304992676, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.799826979637146}
  4%|▍         | 350/8917 [21:56<8:59:57,  3.78s/it]  4%|▍         | 351/8917 [22:00<8:56:33,  3.76s/it]  4%|▍         | 352/8917 [22:04<8:55:06,  3.75s/it]  4%|▍         | 353/8917 [22:07<8:38:26,  3.63s/it]  4%|▍         | 354/8917 [22:11<8:37:37,  3.63s/it]  4%|▍         | 355/8917 [22:14<8:37:48,  3.63s/it]  4%|▍         | 356/8917 [22:18<8:43:44,  3.67s/it]  4%|▍         | 357/8917 [22:22<8:40:37,  3.65s/it]  4%|▍         | 358/8917 [22:26<8:59:38,  3.78s/it]  4%|▍         | 359/8917 [22:30<9:05:15,  3.82s/it]  4%|▍         | 360/8917 [22:33<8:52:04,  3.73s/it]  4%|▍         | 361/8917 [22:37<8:52:30,  3.73s/it]  4%|▍         | 362/8917 [22:41<8:59:46,  3.79s/it]  4%|▍         | 363/8917 [22:45<8:52:42,  3.74s/it]  4%|▍         | 364/8917 [22:48<9:02:11,  3.80s/it]  4%|▍         | 365/8917 [22:52<9:01:19,  3.80s/it]  4%|▍         | 366/8917 [22:56<9:06:11,  3.83s/it]  4%|▍         | 367/8917 [23:00<8:55:46,  3.76s/it]  4%|▍         | 368/8917 [23:04<8:54:02,  3.75s/it]  4%|▍         | 369/8917 [23:07<8:59:17,  3.79s/it]  4%|▍         | 370/8917 [23:11<8:47:16,  3.70s/it]  4%|▍         | 371/8917 [23:15<8:58:46,  3.78s/it]  4%|▍         | 372/8917 [23:18<8:48:42,  3.71s/it]  4%|▍         | 373/8917 [23:22<8:43:01,  3.67s/it]  4%|▍         | 374/8917 [23:26<8:51:00,  3.73s/it]  4%|▍         | 375/8917 [23:30<8:52:49,  3.74s/it]  4%|▍         | 376/8917 [23:33<8:46:43,  3.70s/it]  4%|▍         | 377/8917 [23:37<8:58:08,  3.78s/it]  4%|▍         | 378/8917 [23:41<9:07:55,  3.85s/it]  4%|▍         | 379/8917 [23:45<9:06:21,  3.84s/it]  4%|▍         | 380/8917 [23:49<9:02:33,  3.81s/it]  4%|▍         | 381/8917 [23:52<8:53:40,  3.75s/it]  4%|▍         | 382/8917 [23:56<8:49:36,  3.72s/it]  4%|▍         | 383/8917 [24:00<8:54:57,  3.76s/it]  4%|▍         | 384/8917 [24:03<8:45:01,  3.69s/it]  4%|▍         | 385/8917 [24:07<8:40:00,  3.66s/it]  4%|▍         | 386/8917 [24:11<8:47:10,  3.71s/it]  4%|▍         | 387/8917 [24:15<8:49:24,  3.72s/it]  4%|▍         | 388/8917 [24:18<8:49:32,  3.73s/it]  4%|▍         | 389/8917 [24:22<8:52:22,  3.75s/it]  4%|▍         | 390/8917 [24:26<8:53:54,  3.76s/it]  4%|▍         | 391/8917 [24:30<9:01:07,  3.81s/it]  4%|▍         | 392/8917 [24:34<9:12:24,  3.89s/it]  4%|▍         | 393/8917 [24:37<9:00:11,  3.80s/it]  4%|▍         | 394/8917 [24:41<8:57:02,  3.78s/it]  4%|▍         | 395/8917 [24:45<9:12:41,  3.89s/it]  4%|▍         | 396/8917 [24:49<8:59:02,  3.80s/it]  4%|▍         | 397/8917 [24:53<8:59:10,  3.80s/it]  4%|▍         | 398/8917 [24:57<8:59:34,  3.80s/it]  4%|▍         | 399/8917 [25:00<8:53:40,  3.76s/it]09/19/2024 02:39:38 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026154885068535805, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6995830535888672, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7257379293441772}
  4%|▍         | 400/8917 [25:04<9:00:39,  3.81s/it]  4%|▍         | 401/8917 [25:08<8:51:45,  3.75s/it]  5%|▍         | 402/8917 [25:11<8:50:34,  3.74s/it]  5%|▍         | 403/8917 [25:15<8:36:53,  3.64s/it]  5%|▍         | 404/8917 [25:19<9:01:09,  3.81s/it]  5%|▍         | 405/8917 [25:23<8:54:25,  3.77s/it]  5%|▍         | 406/8917 [25:26<8:47:41,  3.72s/it]  5%|▍         | 407/8917 [25:30<8:42:18,  3.68s/it]  5%|▍         | 408/8917 [25:33<8:33:35,  3.62s/it]  5%|▍         | 409/8917 [25:37<8:32:48,  3.62s/it]  5%|▍         | 410/8917 [25:41<8:35:31,  3.64s/it]  5%|▍         | 411/8917 [25:44<8:31:18,  3.61s/it]  5%|▍         | 412/8917 [25:48<8:49:08,  3.73s/it]  5%|▍         | 413/8917 [25:52<8:44:07,  3.70s/it]  5%|▍         | 414/8917 [25:55<8:27:53,  3.58s/it]  5%|▍         | 415/8917 [25:59<8:53:19,  3.76s/it]  5%|▍         | 416/8917 [26:04<9:14:19,  3.91s/it]  5%|▍         | 417/8917 [26:08<9:12:39,  3.90s/it]  5%|▍         | 418/8917 [26:11<8:58:28,  3.80s/it]  5%|▍         | 419/8917 [26:14<8:40:35,  3.68s/it]  5%|▍         | 420/8917 [26:18<8:50:43,  3.75s/it]  5%|▍         | 421/8917 [26:22<8:37:53,  3.66s/it]  5%|▍         | 422/8917 [26:25<8:36:30,  3.65s/it]  5%|▍         | 423/8917 [26:29<8:43:46,  3.70s/it]  5%|▍         | 424/8917 [26:33<8:47:20,  3.73s/it]  5%|▍         | 425/8917 [26:37<8:37:36,  3.66s/it]  5%|▍         | 426/8917 [26:40<8:35:04,  3.64s/it]  5%|▍         | 427/8917 [26:44<8:44:22,  3.71s/it]  5%|▍         | 428/8917 [26:48<8:40:29,  3.68s/it]  5%|▍         | 429/8917 [26:51<8:43:22,  3.70s/it]  5%|▍         | 430/8917 [26:55<8:52:25,  3.76s/it]  5%|▍         | 431/8917 [26:59<8:45:29,  3.72s/it]  5%|▍         | 432/8917 [27:03<8:48:35,  3.74s/it]  5%|▍         | 433/8917 [27:06<8:45:53,  3.72s/it]  5%|▍         | 434/8917 [27:10<8:49:20,  3.74s/it]  5%|▍         | 435/8917 [27:14<9:00:32,  3.82s/it]  5%|▍         | 436/8917 [27:18<9:02:52,  3.84s/it]  5%|▍         | 437/8917 [27:22<9:02:10,  3.84s/it]  5%|▍         | 438/8917 [27:25<8:36:33,  3.66s/it]  5%|▍         | 439/8917 [27:29<8:43:13,  3.70s/it]  5%|▍         | 440/8917 [27:33<8:56:57,  3.80s/it]  5%|▍         | 441/8917 [27:37<8:57:50,  3.81s/it]  5%|▍         | 442/8917 [27:40<8:44:23,  3.71s/it]  5%|▍         | 443/8917 [27:44<8:59:46,  3.82s/it]  5%|▍         | 444/8917 [27:48<9:06:36,  3.87s/it]  5%|▍         | 445/8917 [27:52<8:57:05,  3.80s/it]  5%|▌         | 446/8917 [27:56<9:00:33,  3.83s/it]  5%|▌         | 447/8917 [28:00<8:59:27,  3.82s/it]  5%|▌         | 448/8917 [28:03<8:52:53,  3.78s/it]  5%|▌         | 449/8917 [28:07<8:55:35,  3.79s/it]09/19/2024 02:42:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02907009981572628, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7492228746414185, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7782930135726929}
  5%|▌         | 450/8917 [28:11<8:53:33,  3.78s/it]  5%|▌         | 451/8917 [28:15<9:00:55,  3.83s/it]  5%|▌         | 452/8917 [28:19<8:59:22,  3.82s/it]  5%|▌         | 453/8917 [28:22<8:55:33,  3.80s/it]  5%|▌         | 454/8917 [28:26<8:51:05,  3.77s/it]  5%|▌         | 455/8917 [28:30<8:54:17,  3.79s/it]  5%|▌         | 456/8917 [28:34<9:09:25,  3.90s/it]  5%|▌         | 457/8917 [28:38<9:03:04,  3.85s/it]  5%|▌         | 458/8917 [28:41<8:43:43,  3.71s/it]  5%|▌         | 459/8917 [28:45<8:48:01,  3.75s/it]  5%|▌         | 460/8917 [28:49<8:42:10,  3.70s/it]  5%|▌         | 461/8917 [28:52<8:42:19,  3.71s/it]  5%|▌         | 462/8917 [28:56<8:36:18,  3.66s/it]  5%|▌         | 463/8917 [29:00<8:43:59,  3.72s/it]  5%|▌         | 464/8917 [29:03<8:39:36,  3.69s/it]  5%|▌         | 465/8917 [29:08<9:00:12,  3.83s/it]  5%|▌         | 466/8917 [29:11<8:38:28,  3.68s/it]  5%|▌         | 467/8917 [29:15<8:51:45,  3.78s/it]  5%|▌         | 468/8917 [29:19<8:50:51,  3.77s/it]  5%|▌         | 469/8917 [29:22<8:50:14,  3.77s/it]  5%|▌         | 470/8917 [29:26<8:49:47,  3.76s/it]  5%|▌         | 471/8917 [29:30<8:35:08,  3.66s/it]  5%|▌         | 472/8917 [29:33<8:31:46,  3.64s/it]  5%|▌         | 473/8917 [29:37<8:37:06,  3.67s/it]  5%|▌         | 474/8917 [29:41<8:39:11,  3.69s/it]  5%|▌         | 475/8917 [29:45<8:50:49,  3.77s/it]  5%|▌         | 476/8917 [29:48<8:39:55,  3.70s/it]  5%|▌         | 477/8917 [29:52<8:37:15,  3.68s/it]  5%|▌         | 478/8917 [29:55<8:25:25,  3.59s/it]  5%|▌         | 479/8917 [29:59<8:25:52,  3.60s/it]  5%|▌         | 480/8917 [30:03<8:38:34,  3.69s/it]  5%|▌         | 481/8917 [30:07<8:57:13,  3.82s/it]  5%|▌         | 482/8917 [30:11<8:53:42,  3.80s/it]  5%|▌         | 483/8917 [30:14<8:46:49,  3.75s/it]  5%|▌         | 484/8917 [30:18<8:46:50,  3.75s/it]  5%|▌         | 485/8917 [30:22<8:41:56,  3.71s/it]  5%|▌         | 486/8917 [30:26<8:53:52,  3.80s/it]  5%|▌         | 487/8917 [30:29<8:50:33,  3.78s/it]  5%|▌         | 488/8917 [30:33<8:47:33,  3.76s/it]  5%|▌         | 489/8917 [30:37<8:44:10,  3.73s/it]  5%|▌         | 490/8917 [30:40<8:44:40,  3.74s/it]  6%|▌         | 491/8917 [30:44<8:35:24,  3.67s/it]  6%|▌         | 492/8917 [30:47<8:30:15,  3.63s/it]  6%|▌         | 493/8917 [30:51<8:34:13,  3.66s/it]  6%|▌         | 494/8917 [30:55<8:37:58,  3.69s/it]  6%|▌         | 495/8917 [30:59<8:33:35,  3.66s/it]  6%|▌         | 496/8917 [31:02<8:34:28,  3.67s/it]  6%|▌         | 497/8917 [31:06<8:39:51,  3.70s/it]  6%|▌         | 498/8917 [31:10<8:36:44,  3.68s/it]  6%|▌         | 499/8917 [31:13<8:35:29,  3.67s/it]09/19/2024 02:45:50 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 02:45:50 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:54,  1.93it/s][A
  1%|          | 2/221 [00:01<03:00,  1.21it/s][A
  1%|▏         | 3/221 [00:02<02:29,  1.46it/s][A
  2%|▏         | 4/221 [00:02<01:38,  2.19it/s][A
  2%|▏         | 5/221 [00:02<01:11,  3.04it/s][A
  3%|▎         | 6/221 [00:02<01:12,  2.96it/s][A
  3%|▎         | 7/221 [00:03<01:31,  2.35it/s][A
  4%|▎         | 8/221 [00:03<01:24,  2.51it/s][A
  4%|▍         | 9/221 [00:04<01:31,  2.33it/s][A
  5%|▍         | 10/221 [00:04<01:15,  2.80it/s][A
  5%|▍         | 11/221 [00:04<01:04,  3.27it/s][A
  5%|▌         | 12/221 [00:05<02:07,  1.63it/s][A
  6%|▌         | 13/221 [00:06<01:47,  1.94it/s][A
  6%|▋         | 14/221 [00:06<01:34,  2.18it/s][A
  7%|▋         | 15/221 [00:06<01:32,  2.23it/s][A
  7%|▋         | 16/221 [00:07<01:34,  2.16it/s][A
  8%|▊         | 17/221 [00:07<01:36,  2.12it/s][A
  8%|▊         | 18/221 [00:08<02:15,  1.50it/s][A
  9%|▊         | 19/221 [00:09<02:09,  1.55it/s][A
  9%|▉         | 20/221 [00:09<01:42,  1.96it/s][A
 10%|▉         | 21/221 [00:10<01:28,  2.25it/s][A
 10%|▉         | 22/221 [00:10<01:31,  2.18it/s][A
 11%|█         | 24/221 [00:10<01:01,  3.19it/s][A
 11%|█▏        | 25/221 [00:11<01:02,  3.13it/s][A
 12%|█▏        | 26/221 [00:11<01:03,  3.08it/s][A
 13%|█▎        | 28/221 [00:12<01:02,  3.07it/s][A
 13%|█▎        | 29/221 [00:12<01:05,  2.93it/s][A
 14%|█▎        | 30/221 [00:13<01:26,  2.22it/s][A
 14%|█▍        | 31/221 [00:13<01:25,  2.22it/s][A
 15%|█▍        | 33/221 [00:14<01:07,  2.79it/s][A
 15%|█▌        | 34/221 [00:14<00:57,  3.28it/s][A
 16%|█▌        | 35/221 [00:14<00:57,  3.21it/s][A
 16%|█▋        | 36/221 [00:15<00:59,  3.10it/s][A
 17%|█▋        | 37/221 [00:15<01:17,  2.38it/s][A
 17%|█▋        | 38/221 [00:16<01:26,  2.13it/s][A
 18%|█▊        | 39/221 [00:16<01:09,  2.61it/s][A
 18%|█▊        | 40/221 [00:17<01:18,  2.32it/s][A
 19%|█▊        | 41/221 [00:17<01:03,  2.85it/s][A
 19%|█▉        | 42/221 [00:17<01:08,  2.62it/s][A
 19%|█▉        | 43/221 [00:17<01:03,  2.79it/s][A
 20%|█▉        | 44/221 [00:18<00:54,  3.25it/s][A
 20%|██        | 45/221 [00:19<02:01,  1.45it/s][A
 21%|██        | 46/221 [00:20<01:43,  1.69it/s][A
 21%|██▏       | 47/221 [00:21<02:13,  1.31it/s][A
 22%|██▏       | 48/221 [00:21<01:38,  1.76it/s][A
 22%|██▏       | 49/221 [00:21<01:23,  2.07it/s][A
 23%|██▎       | 50/221 [00:21<01:08,  2.50it/s][A
 23%|██▎       | 51/221 [00:22<01:00,  2.82it/s][A
 24%|██▎       | 52/221 [00:22<00:50,  3.33it/s][A
 24%|██▍       | 53/221 [00:22<00:46,  3.57it/s][A
 24%|██▍       | 54/221 [00:23<01:38,  1.70it/s][A
 25%|██▍       | 55/221 [00:24<01:41,  1.64it/s][A
 25%|██▌       | 56/221 [00:24<01:23,  1.98it/s][A
 26%|██▌       | 57/221 [00:25<01:13,  2.24it/s][A
 26%|██▌       | 58/221 [00:25<00:56,  2.89it/s][A
 27%|██▋       | 59/221 [00:25<00:48,  3.35it/s][A
 27%|██▋       | 60/221 [00:26<01:08,  2.35it/s][A
 28%|██▊       | 61/221 [00:26<01:02,  2.57it/s][A
 28%|██▊       | 62/221 [00:26<01:05,  2.41it/s][A
 29%|██▉       | 64/221 [00:27<01:00,  2.61it/s][A
 29%|██▉       | 65/221 [00:27<00:58,  2.64it/s][A
 30%|██▉       | 66/221 [00:28<01:06,  2.35it/s][A
 30%|███       | 67/221 [00:28<01:04,  2.40it/s][A
 31%|███       | 68/221 [00:29<00:57,  2.67it/s][A
 31%|███       | 69/221 [00:29<01:13,  2.07it/s][A
 32%|███▏      | 70/221 [00:30<00:57,  2.63it/s][A
 32%|███▏      | 71/221 [00:30<01:08,  2.17it/s][A
 33%|███▎      | 72/221 [00:31<01:14,  2.00it/s][A
 33%|███▎      | 73/221 [00:32<01:26,  1.71it/s][A
 33%|███▎      | 74/221 [00:32<01:14,  1.97it/s][A
 34%|███▍      | 75/221 [00:32<01:12,  2.01it/s][A
 34%|███▍      | 76/221 [00:33<00:58,  2.46it/s][A
 35%|███▍      | 77/221 [00:33<01:14,  1.93it/s][A
 35%|███▌      | 78/221 [00:33<00:57,  2.47it/s][A
 36%|███▌      | 79/221 [00:35<01:31,  1.56it/s][A
 37%|███▋      | 81/221 [00:35<01:13,  1.91it/s][A
 37%|███▋      | 82/221 [00:36<01:23,  1.67it/s][A
 38%|███▊      | 83/221 [00:37<01:36,  1.43it/s][A
 38%|███▊      | 84/221 [00:37<01:15,  1.82it/s][A
 39%|███▉      | 86/221 [00:38<01:00,  2.22it/s][A
 39%|███▉      | 87/221 [00:39<01:29,  1.50it/s][A
 40%|███▉      | 88/221 [00:40<01:28,  1.50it/s][A
 40%|████      | 89/221 [00:41<01:35,  1.38it/s][A
 41%|████      | 90/221 [00:41<01:23,  1.57it/s][A
 41%|████      | 91/221 [00:42<01:05,  1.98it/s][A
 42%|████▏     | 92/221 [00:42<01:08,  1.88it/s][A
 42%|████▏     | 93/221 [00:43<01:06,  1.92it/s][A
 43%|████▎     | 94/221 [00:43<01:15,  1.68it/s][A
 43%|████▎     | 95/221 [00:44<01:05,  1.92it/s][A
 43%|████▎     | 96/221 [00:45<01:32,  1.35it/s][A
 44%|████▍     | 97/221 [00:45<01:12,  1.70it/s][A
 44%|████▍     | 98/221 [00:46<01:15,  1.62it/s][A
 45%|████▍     | 99/221 [00:46<01:03,  1.92it/s][A
 45%|████▌     | 100/221 [00:47<00:59,  2.02it/s][A
 46%|████▌     | 101/221 [00:47<00:46,  2.58it/s][A
 46%|████▌     | 102/221 [00:49<01:41,  1.17it/s][A
 47%|████▋     | 103/221 [00:49<01:15,  1.55it/s][A
 47%|████▋     | 104/221 [00:49<01:03,  1.83it/s][A
 48%|████▊     | 105/221 [00:50<01:04,  1.79it/s][A
 48%|████▊     | 106/221 [00:51<01:18,  1.47it/s][A
 48%|████▊     | 107/221 [00:51<01:02,  1.81it/s][A
 49%|████▉     | 108/221 [00:51<00:55,  2.05it/s][A
 49%|████▉     | 109/221 [00:52<00:48,  2.30it/s][A
 50%|████▉     | 110/221 [00:52<00:38,  2.91it/s][A
 50%|█████     | 111/221 [00:52<00:39,  2.78it/s][A
 51%|█████     | 112/221 [00:53<00:39,  2.79it/s][A
 51%|█████     | 113/221 [00:53<00:34,  3.10it/s][A
 52%|█████▏    | 115/221 [00:53<00:35,  2.99it/s][A
 52%|█████▏    | 116/221 [00:55<00:56,  1.84it/s][A
 53%|█████▎    | 117/221 [00:55<00:54,  1.91it/s][A
 53%|█████▎    | 118/221 [00:55<00:50,  2.05it/s][A
 54%|█████▍    | 119/221 [00:56<00:48,  2.10it/s][A
 54%|█████▍    | 120/221 [00:56<00:48,  2.08it/s][A
 55%|█████▍    | 121/221 [00:57<00:46,  2.15it/s][A
 55%|█████▌    | 122/221 [00:57<00:41,  2.40it/s][A
 56%|█████▌    | 123/221 [00:58<00:41,  2.34it/s][A
 56%|█████▌    | 124/221 [00:58<00:35,  2.75it/s][A
 57%|█████▋    | 125/221 [00:59<00:53,  1.80it/s][A
 57%|█████▋    | 126/221 [01:00<00:57,  1.64it/s][A
 57%|█████▋    | 127/221 [01:00<00:56,  1.66it/s][A
 58%|█████▊    | 128/221 [01:01<01:00,  1.54it/s][A
 58%|█████▊    | 129/221 [01:01<00:47,  1.95it/s][A
 59%|█████▉    | 130/221 [01:02<00:53,  1.69it/s][A
 59%|█████▉    | 131/221 [01:02<00:45,  2.00it/s][A
 60%|█████▉    | 132/221 [01:03<01:06,  1.35it/s][A
 60%|██████    | 133/221 [01:04<00:52,  1.66it/s][A
 61%|██████    | 134/221 [01:05<01:14,  1.16it/s][A
 61%|██████    | 135/221 [01:06<01:05,  1.32it/s][A
 62%|██████▏   | 136/221 [01:06<00:59,  1.43it/s][A
 62%|██████▏   | 137/221 [01:06<00:45,  1.86it/s][A
 62%|██████▏   | 138/221 [01:08<00:57,  1.43it/s][A
 63%|██████▎   | 139/221 [01:08<00:48,  1.70it/s][A
 63%|██████▎   | 140/221 [01:08<00:48,  1.66it/s][A
 64%|██████▍   | 141/221 [01:10<00:58,  1.36it/s][A
 64%|██████▍   | 142/221 [01:10<00:54,  1.45it/s][A
 65%|██████▍   | 143/221 [01:11<00:53,  1.46it/s][A
 65%|██████▌   | 144/221 [01:11<00:46,  1.64it/s][A
 66%|██████▌   | 145/221 [01:11<00:36,  2.09it/s][A
 66%|██████▌   | 146/221 [01:12<00:29,  2.56it/s][A
 67%|██████▋   | 148/221 [01:13<00:42,  1.70it/s][A
 67%|██████▋   | 149/221 [01:14<00:42,  1.68it/s][A
 68%|██████▊   | 150/221 [01:14<00:38,  1.83it/s][A
 68%|██████▊   | 151/221 [01:15<00:35,  1.95it/s][A
 69%|██████▉   | 152/221 [01:15<00:41,  1.68it/s][A
 69%|██████▉   | 153/221 [01:16<00:31,  2.13it/s][A
 70%|██████▉   | 154/221 [01:16<00:29,  2.28it/s][A
 70%|███████   | 155/221 [01:16<00:27,  2.38it/s][A
 71%|███████   | 156/221 [01:17<00:26,  2.49it/s][A
 71%|███████   | 157/221 [01:24<02:38,  2.47s/it][A
 71%|███████▏  | 158/221 [01:25<02:04,  1.98s/it][A
 72%|███████▏  | 159/221 [01:25<01:29,  1.44s/it][A
 72%|███████▏  | 160/221 [01:25<01:05,  1.07s/it][A
 73%|███████▎  | 161/221 [01:26<00:52,  1.14it/s][A
 73%|███████▎  | 162/221 [01:26<00:38,  1.55it/s][A
 74%|███████▍  | 163/221 [01:26<00:36,  1.60it/s][A
 74%|███████▍  | 164/221 [01:27<00:29,  1.95it/s][A
 75%|███████▍  | 165/221 [01:27<00:31,  1.79it/s][A
 75%|███████▌  | 166/221 [01:28<00:39,  1.39it/s][A
 76%|███████▌  | 167/221 [01:29<00:37,  1.42it/s][A
 76%|███████▌  | 168/221 [01:33<01:25,  1.62s/it][A
 76%|███████▋  | 169/221 [01:33<01:03,  1.21s/it][A
 77%|███████▋  | 170/221 [01:34<00:49,  1.03it/s][A
 77%|███████▋  | 171/221 [01:34<00:43,  1.14it/s][A
 78%|███████▊  | 172/221 [01:35<00:39,  1.25it/s][A
 78%|███████▊  | 173/221 [01:36<00:37,  1.27it/s][A
 79%|███████▊  | 174/221 [01:36<00:27,  1.68it/s][A
 79%|███████▉  | 175/221 [01:36<00:25,  1.80it/s][A
 80%|███████▉  | 176/221 [01:36<00:21,  2.05it/s][A
 80%|████████  | 177/221 [01:37<00:18,  2.33it/s][A
 81%|████████  | 178/221 [01:37<00:19,  2.26it/s][A
 81%|████████  | 179/221 [01:38<00:20,  2.03it/s][A
 81%|████████▏ | 180/221 [01:38<00:16,  2.47it/s][A
 82%|████████▏ | 182/221 [01:39<00:12,  3.11it/s][A
 83%|████████▎ | 183/221 [01:39<00:15,  2.39it/s][A
 83%|████████▎ | 184/221 [01:40<00:18,  2.01it/s][A
 84%|████████▍ | 186/221 [01:40<00:14,  2.49it/s][A
 85%|████████▍ | 187/221 [01:41<00:12,  2.63it/s][A
 85%|████████▌ | 188/221 [01:41<00:12,  2.74it/s][A
 86%|████████▌ | 189/221 [01:41<00:11,  2.82it/s][A
 86%|████████▌ | 190/221 [01:42<00:12,  2.50it/s][A
 86%|████████▋ | 191/221 [01:42<00:10,  2.80it/s][A
 87%|████████▋ | 192/221 [01:43<00:11,  2.63it/s][A
 87%|████████▋ | 193/221 [01:43<00:10,  2.78it/s][A
 88%|████████▊ | 194/221 [01:45<00:19,  1.35it/s][A
 88%|████████▊ | 195/221 [01:45<00:14,  1.75it/s][A
 89%|████████▊ | 196/221 [01:45<00:13,  1.92it/s][A
 89%|████████▉ | 197/221 [01:46<00:11,  2.03it/s][A
 90%|████████▉ | 198/221 [01:46<00:10,  2.15it/s][A
 90%|█████████ | 199/221 [01:46<00:09,  2.20it/s][A
 90%|█████████ | 200/221 [01:47<00:10,  2.00it/s][A
 91%|█████████ | 201/221 [01:47<00:09,  2.06it/s][A
 91%|█████████▏| 202/221 [01:48<00:09,  2.00it/s][A
 92%|█████████▏| 203/221 [01:49<00:09,  1.88it/s][A
 92%|█████████▏| 204/221 [01:49<00:09,  1.83it/s][A
 93%|█████████▎| 205/221 [01:49<00:06,  2.32it/s][A
 93%|█████████▎| 206/221 [01:50<00:08,  1.87it/s][A
 94%|█████████▍| 208/221 [01:51<00:05,  2.44it/s][A
 95%|█████████▍| 209/221 [01:51<00:04,  2.88it/s][A
 95%|█████████▌| 211/221 [01:52<00:03,  2.57it/s][A
 96%|█████████▌| 212/221 [01:52<00:03,  2.30it/s][A
 96%|█████████▋| 213/221 [01:52<00:02,  2.74it/s][A
 97%|█████████▋| 214/221 [01:53<00:02,  2.61it/s][A
 97%|█████████▋| 215/221 [01:53<00:02,  2.47it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  2.22it/s][A
 98%|█████████▊| 217/221 [01:57<00:05,  1.28s/it][A
 99%|█████████▊| 218/221 [01:58<00:03,  1.17s/it][A
 99%|█████████▉| 219/221 [01:59<00:01,  1.09it/s][A
100%|█████████▉| 220/221 [02:00<00:01,  1.23s/it][A
100%|██████████| 221/221 [02:01<00:00,  1.04it/s][A100%|██████████| 221/221 [02:01<00:00,  1.82it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:33,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:05,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:05<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:33<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:50,  4.32it/s][A
  1%|          | 2/221 [00:02<04:30,  1.24s/it][A
  1%|▏         | 3/221 [00:02<02:55,  1.24it/s][A
  2%|▏         | 5/221 [00:02<01:23,  2.58it/s][A
  3%|▎         | 6/221 [00:03<02:09,  1.66it/s][A
  3%|▎         | 7/221 [00:04<02:29,  1.43it/s][A
  4%|▎         | 8/221 [00:05<02:31,  1.41it/s][A
  4%|▍         | 9/221 [00:05<02:02,  1.73it/s][A
  5%|▍         | 10/221 [00:06<02:02,  1.72it/s][A
  5%|▌         | 12/221 [00:06<01:34,  2.20it/s][A
  6%|▌         | 13/221 [00:07<01:34,  2.19it/s][A
  6%|▋         | 14/221 [00:07<01:41,  2.05it/s][A
  7%|▋         | 15/221 [00:08<01:37,  2.10it/s][A
  7%|▋         | 16/221 [00:09<01:54,  1.78it/s][A
  8%|▊         | 17/221 [00:09<01:41,  2.00it/s][A
  8%|▊         | 18/221 [00:09<01:31,  2.22it/s][A
  9%|▊         | 19/221 [00:10<01:38,  2.06it/s][A
 10%|▉         | 21/221 [00:10<01:11,  2.81it/s][A
 10%|▉         | 22/221 [00:11<01:29,  2.23it/s][A
 11%|█         | 24/221 [00:13<01:56,  1.69it/s][A
 11%|█▏        | 25/221 [00:13<01:56,  1.69it/s][A
 12%|█▏        | 26/221 [00:14<02:11,  1.48it/s][A
 12%|█▏        | 27/221 [00:14<01:45,  1.84it/s][A
 13%|█▎        | 28/221 [00:15<02:18,  1.39it/s][A
 13%|█▎        | 29/221 [00:16<02:04,  1.54it/s][A
 14%|█▎        | 30/221 [00:16<01:39,  1.92it/s][A
 14%|█▍        | 31/221 [00:17<01:48,  1.75it/s][A
 14%|█▍        | 32/221 [00:17<01:50,  1.72it/s][A
 15%|█▍        | 33/221 [00:18<02:06,  1.49it/s][A
 15%|█▌        | 34/221 [00:19<01:47,  1.75it/s][A
 16%|█▌        | 35/221 [00:19<01:26,  2.15it/s][A
 16%|█▋        | 36/221 [00:19<01:21,  2.26it/s][A
 17%|█▋        | 37/221 [00:20<01:14,  2.48it/s][A
 17%|█▋        | 38/221 [00:20<01:37,  1.88it/s][A
 18%|█▊        | 39/221 [00:21<01:32,  1.97it/s][A
 18%|█▊        | 40/221 [00:22<01:44,  1.74it/s][A
 19%|█▊        | 41/221 [00:22<01:21,  2.20it/s][A
 19%|█▉        | 42/221 [00:23<01:49,  1.63it/s][A
 19%|█▉        | 43/221 [00:23<01:37,  1.83it/s][A
 20%|█▉        | 44/221 [00:24<01:43,  1.71it/s][A
 20%|██        | 45/221 [00:25<02:37,  1.12it/s][A
 21%|██        | 46/221 [00:26<02:15,  1.29it/s][A
 21%|██▏       | 47/221 [00:26<01:57,  1.48it/s][A
 22%|██▏       | 48/221 [00:27<01:33,  1.84it/s][A
 22%|██▏       | 49/221 [00:27<01:15,  2.27it/s][A
 23%|██▎       | 50/221 [00:28<01:32,  1.85it/s][A
 23%|██▎       | 51/221 [00:28<01:28,  1.93it/s][A
 24%|██▎       | 52/221 [00:28<01:22,  2.05it/s][A
 24%|██▍       | 53/221 [00:30<01:55,  1.46it/s][A
 24%|██▍       | 54/221 [00:31<02:17,  1.22it/s][A
 25%|██▍       | 55/221 [00:32<02:49,  1.02s/it][A
 25%|██▌       | 56/221 [00:33<02:18,  1.20it/s][A
 26%|██▌       | 57/221 [00:34<02:34,  1.06it/s][A
 26%|██▌       | 58/221 [00:34<01:55,  1.41it/s][A
 27%|██▋       | 59/221 [00:34<01:39,  1.62it/s][A
 27%|██▋       | 60/221 [00:35<01:36,  1.67it/s][A
 28%|██▊       | 61/221 [00:36<01:54,  1.39it/s][A
 28%|██▊       | 62/221 [00:37<01:48,  1.47it/s][A
 29%|██▊       | 63/221 [00:37<01:57,  1.34it/s][A
 29%|██▉       | 64/221 [00:38<01:49,  1.43it/s][A
 29%|██▉       | 65/221 [00:39<01:47,  1.45it/s][A
 30%|██▉       | 66/221 [00:39<01:29,  1.72it/s][A
 30%|███       | 67/221 [00:39<01:20,  1.91it/s][A
 31%|███       | 68/221 [00:40<01:17,  1.97it/s][A
 31%|███       | 69/221 [00:41<01:43,  1.48it/s][A
 32%|███▏      | 70/221 [00:41<01:18,  1.93it/s][A
 32%|███▏      | 71/221 [00:42<01:17,  1.94it/s][A
 33%|███▎      | 72/221 [00:43<01:38,  1.51it/s][A
 33%|███▎      | 73/221 [00:43<01:37,  1.51it/s][A
 33%|███▎      | 74/221 [00:44<01:35,  1.54it/s][A
 34%|███▍      | 75/221 [00:45<01:47,  1.36it/s][A
 34%|███▍      | 76/221 [00:45<01:35,  1.52it/s][A
 35%|███▍      | 77/221 [00:46<01:39,  1.45it/s][A
 35%|███▌      | 78/221 [00:47<01:35,  1.50it/s][A
 36%|███▌      | 80/221 [00:47<01:12,  1.94it/s][A
 37%|███▋      | 81/221 [00:48<01:05,  2.14it/s][A
 37%|███▋      | 82/221 [00:48<00:58,  2.36it/s][A
 38%|███▊      | 83/221 [00:48<00:52,  2.62it/s][A
 38%|███▊      | 84/221 [00:49<00:57,  2.36it/s][A
 38%|███▊      | 85/221 [00:50<01:09,  1.95it/s][A
 39%|███▉      | 86/221 [00:50<01:12,  1.85it/s][A
 39%|███▉      | 87/221 [00:51<01:14,  1.80it/s][A
 40%|███▉      | 88/221 [00:52<01:24,  1.58it/s][A
 40%|████      | 89/221 [00:52<01:31,  1.45it/s][A
 41%|████      | 90/221 [00:53<01:11,  1.84it/s][A
 41%|████      | 91/221 [00:53<01:15,  1.72it/s][A
 42%|████▏     | 92/221 [00:54<01:05,  1.98it/s][A
 42%|████▏     | 93/221 [00:54<01:04,  1.99it/s][A
 43%|████▎     | 94/221 [00:54<00:58,  2.16it/s][A
 43%|████▎     | 95/221 [00:55<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:56<01:15,  1.66it/s][A
 44%|████▍     | 97/221 [00:57<01:24,  1.47it/s][A
 44%|████▍     | 98/221 [00:58<01:42,  1.20it/s][A
 45%|████▍     | 99/221 [00:58<01:23,  1.47it/s][A
 45%|████▌     | 100/221 [00:59<01:11,  1.68it/s][A
 46%|████▌     | 101/221 [00:59<01:10,  1.69it/s][A
 46%|████▌     | 102/221 [01:00<01:00,  1.98it/s][A
 47%|████▋     | 103/221 [01:01<01:15,  1.56it/s][A
 47%|████▋     | 104/221 [01:02<01:34,  1.24it/s][A
 48%|████▊     | 105/221 [01:02<01:24,  1.37it/s][A
 48%|████▊     | 106/221 [01:03<01:21,  1.40it/s][A
 48%|████▊     | 107/221 [01:04<01:21,  1.40it/s][A
 49%|████▉     | 108/221 [01:04<01:13,  1.54it/s][A
 49%|████▉     | 109/221 [01:05<01:11,  1.57it/s][A
 50%|████▉     | 110/221 [01:06<01:21,  1.36it/s][A
 50%|█████     | 111/221 [01:07<01:29,  1.23it/s][A
 51%|█████     | 112/221 [01:07<01:15,  1.44it/s][A
 51%|█████     | 113/221 [01:08<01:07,  1.59it/s][A
 52%|█████▏    | 114/221 [01:08<01:06,  1.61it/s][A
 52%|█████▏    | 115/221 [01:08<00:52,  2.04it/s][A
 52%|█████▏    | 116/221 [01:09<00:54,  1.94it/s][A
 53%|█████▎    | 117/221 [01:09<00:50,  2.05it/s][A
 53%|█████▎    | 118/221 [01:10<00:48,  2.14it/s][A
 54%|█████▍    | 119/221 [01:10<00:52,  1.95it/s][A
 54%|█████▍    | 120/221 [01:11<00:48,  2.09it/s][A
 55%|█████▍    | 121/221 [01:11<00:46,  2.15it/s][A
 55%|█████▌    | 122/221 [01:12<00:45,  2.16it/s][A
 56%|█████▌    | 123/221 [01:12<00:44,  2.20it/s][A
 56%|█████▌    | 124/221 [01:13<00:45,  2.12it/s][A
 57%|█████▋    | 125/221 [01:14<00:58,  1.63it/s][A
 57%|█████▋    | 126/221 [01:15<01:08,  1.39it/s][A
 57%|█████▋    | 127/221 [01:15<01:09,  1.36it/s][A
 58%|█████▊    | 128/221 [01:16<01:05,  1.42it/s][A
 58%|█████▊    | 129/221 [01:16<00:56,  1.62it/s][A
 59%|█████▉    | 130/221 [01:17<00:47,  1.91it/s][A
 59%|█████▉    | 131/221 [01:17<00:51,  1.75it/s][A
 60%|█████▉    | 132/221 [01:18<00:39,  2.24it/s][A
 60%|██████    | 133/221 [01:18<00:40,  2.16it/s][A
 61%|██████    | 134/221 [01:18<00:32,  2.66it/s][A
 61%|██████    | 135/221 [01:19<00:42,  2.04it/s][A
 62%|██████▏   | 136/221 [01:20<00:44,  1.92it/s][A
 62%|██████▏   | 137/221 [01:20<00:43,  1.93it/s][A
 62%|██████▏   | 138/221 [01:21<00:57,  1.44it/s][A
 63%|██████▎   | 139/221 [01:23<01:13,  1.12it/s][A
 63%|██████▎   | 140/221 [01:23<01:01,  1.31it/s][A
 64%|██████▍   | 141/221 [01:24<00:54,  1.46it/s][A
 64%|██████▍   | 142/221 [01:25<01:06,  1.18it/s][A
 65%|██████▍   | 143/221 [01:25<00:51,  1.51it/s][A
 65%|██████▌   | 144/221 [01:25<00:43,  1.76it/s][A
 66%|██████▌   | 145/221 [01:26<00:35,  2.11it/s][A
 66%|██████▌   | 146/221 [01:27<00:53,  1.40it/s][A
 67%|██████▋   | 148/221 [01:27<00:34,  2.12it/s][A
 67%|██████▋   | 149/221 [01:28<00:45,  1.58it/s][A
 68%|██████▊   | 150/221 [01:29<00:39,  1.82it/s][A
 68%|██████▊   | 151/221 [01:29<00:37,  1.87it/s][A
 69%|██████▉   | 152/221 [01:30<00:35,  1.97it/s][A
 69%|██████▉   | 153/221 [01:30<00:35,  1.92it/s][A
 70%|██████▉   | 154/221 [01:30<00:31,  2.15it/s][A
 70%|███████   | 155/221 [01:31<00:30,  2.15it/s][A
 71%|███████   | 156/221 [01:31<00:26,  2.45it/s][A
 71%|███████   | 157/221 [01:32<00:27,  2.31it/s][A
 71%|███████▏  | 158/221 [01:32<00:27,  2.27it/s][A
 72%|███████▏  | 159/221 [01:33<00:29,  2.10it/s][A
 73%|███████▎  | 161/221 [01:33<00:19,  3.11it/s][A
 73%|███████▎  | 162/221 [01:34<00:24,  2.37it/s][A
 74%|███████▍  | 163/221 [01:34<00:23,  2.49it/s][A
 74%|███████▍  | 164/221 [01:35<00:33,  1.70it/s][A
 75%|███████▍  | 165/221 [01:36<00:40,  1.37it/s][A
 75%|███████▌  | 166/221 [01:37<00:34,  1.58it/s][A
 76%|███████▌  | 167/221 [01:37<00:30,  1.76it/s][A
 76%|███████▌  | 168/221 [01:38<00:40,  1.32it/s][A
 76%|███████▋  | 169/221 [01:39<00:41,  1.26it/s][A
 77%|███████▋  | 170/221 [01:40<00:39,  1.28it/s][A
 77%|███████▋  | 171/221 [01:40<00:32,  1.55it/s][A
 78%|███████▊  | 172/221 [01:41<00:29,  1.66it/s][A
 78%|███████▊  | 173/221 [01:42<00:34,  1.40it/s][A
 79%|███████▊  | 174/221 [01:43<00:39,  1.20it/s][A
 79%|███████▉  | 175/221 [01:43<00:34,  1.35it/s][A
 80%|███████▉  | 176/221 [01:44<00:38,  1.16it/s][A
 80%|████████  | 177/221 [01:45<00:28,  1.52it/s][A
 81%|████████  | 178/221 [01:46<00:38,  1.12it/s][A
 81%|████████  | 179/221 [01:47<00:32,  1.30it/s][A
 81%|████████▏ | 180/221 [01:47<00:26,  1.53it/s][A
 82%|████████▏ | 181/221 [01:47<00:20,  1.94it/s][A
 82%|████████▏ | 182/221 [01:47<00:17,  2.18it/s][A
 83%|████████▎ | 183/221 [01:48<00:20,  1.87it/s][A
 83%|████████▎ | 184/221 [01:49<00:22,  1.66it/s][A
 84%|████████▍ | 186/221 [01:49<00:12,  2.81it/s][A
 85%|████████▍ | 187/221 [01:50<00:13,  2.51it/s][A
 85%|████████▌ | 188/221 [01:50<00:15,  2.19it/s][A
 86%|████████▌ | 190/221 [01:52<00:17,  1.75it/s][A
 86%|████████▋ | 191/221 [01:52<00:17,  1.67it/s][A
 87%|████████▋ | 192/221 [01:53<00:17,  1.69it/s][A
 87%|████████▋ | 193/221 [01:54<00:16,  1.67it/s][A
 88%|████████▊ | 194/221 [01:54<00:14,  1.92it/s][A
 88%|████████▊ | 195/221 [01:54<00:11,  2.24it/s][A
 89%|████████▉ | 197/221 [01:55<00:10,  2.39it/s][A 89%|████████▉ | 197/221 [01:55<00:14,  1.71it/s]
09/19/2024 02:54:49 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 499--===========

09/19/2024 02:54:49 - INFO - __main__ -   {'area_r1': 43.9, 'area_recall': '43.9/71.2/81.3', 'area_ravg': 65.5}
09/19/2024 02:54:49 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 499--===========

09/19/2024 02:54:49 - INFO - __main__ -   {'forward_r1': 47.4, 'forward_recall': '47.4/74.8/84.7', 'forward_ravg': 69.0}
09/19/2024 02:54:49 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 499--===========

09/19/2024 02:54:49 - INFO - __main__ -   {'area_video_r1': 46.9, 'area_video_recall': '46.9/75.9/85.6', 'area_video_ravg': 69.5}
09/19/2024 02:54:49 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 499=======

09/19/2024 02:54:49 - INFO - __main__ -   {'area_video_r1': 46.9, 'area_video_recall': '46.9/75.9/85.6', 'area_video_ravg': 69.5}
09/19/2024 02:54:49 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 499--===========

09/19/2024 02:54:49 - INFO - __main__ -   {'area_video_r1': 58.6, 'area_video_recall': '58.6/79.2/85.7', 'area_video_ravg': 74.5, 'area_video_back_r1': 57.9, 'area_video_back_recall': '57.9/81.2/89.7', 'area_video_back_ravg': 76.3}
09/19/2024 02:54:49 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 499=======

09/19/2024 02:54:49 - INFO - __main__ -   {'area_video_r1': 58.6, 'area_video_recall': '58.6/79.2/85.7', 'area_video_ravg': 74.5, 'area_video_back_r1': 57.9, 'area_video_back_recall': '57.9/81.2/89.7', 'area_video_back_ravg': 76.3}
09/19/2024 02:54:49 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 499--===========

09/19/2024 02:54:49 - INFO - __main__ -   {'video_r1': 31.7, 'video_recall': '31.7/54.9/64.8', 'video_ravg': 50.5}
09/19/2024 02:54:49 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 499=======

09/19/2024 02:54:49 - INFO - __main__ -   {'video_r1': 31.7, 'video_recall': '31.7/54.9/64.8', 'video_ravg': 50.5}
09/19/2024 02:54:49 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 499--===========

09/19/2024 02:54:49 - INFO - __main__ -   {'video_r1': 51.6, 'video_recall': '51.6/70.0/75.6', 'video_ravg': 65.7}
09/19/2024 02:54:49 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 499=======

09/19/2024 02:54:49 - INFO - __main__ -   {'video_r1': 51.6, 'video_recall': '51.6/70.0/75.6', 'video_ravg': 65.7}
09/19/2024 02:55:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.040186941623687744, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.1346633434295654, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.1748502254486084}
  6%|▌         | 500/8917 [40:56<414:50:15, 177.43s/it]  6%|▌         | 501/8917 [41:00<292:40:54, 125.20s/it]  6%|▌         | 502/8917 [41:03<207:19:26, 88.69s/it]   6%|▌         | 503/8917 [41:07<147:46:15, 63.23s/it]  6%|▌         | 504/8917 [41:11<106:15:05, 45.47s/it]  6%|▌         | 505/8917 [41:14<76:43:02, 32.83s/it]   6%|▌         | 506/8917 [41:18<56:30:16, 24.18s/it]  6%|▌         | 507/8917 [41:22<42:18:40, 18.11s/it]  6%|▌         | 508/8917 [41:26<32:08:25, 13.76s/it]  6%|▌         | 509/8917 [41:30<25:07:30, 10.76s/it]  6%|▌         | 510/8917 [41:34<20:26:26,  8.75s/it]  6%|▌         | 511/8917 [41:37<16:50:42,  7.21s/it]  6%|▌         | 512/8917 [41:41<14:26:42,  6.19s/it]  6%|▌         | 513/8917 [41:45<12:43:26,  5.45s/it]  6%|▌         | 514/8917 [41:48<11:31:29,  4.94s/it]  6%|▌         | 515/8917 [41:52<10:32:54,  4.52s/it]  6%|▌         | 516/8917 [41:56<10:16:04,  4.40s/it]  6%|▌         | 517/8917 [42:00<9:44:01,  4.17s/it]   6%|▌         | 518/8917 [42:03<9:17:49,  3.98s/it]  6%|▌         | 519/8917 [42:07<8:52:40,  3.81s/it]  6%|▌         | 520/8917 [42:11<8:53:32,  3.81s/it]  6%|▌         | 521/8917 [42:14<8:42:24,  3.73s/it]  6%|▌         | 522/8917 [42:18<8:50:57,  3.79s/it]  6%|▌         | 523/8917 [42:22<8:51:05,  3.80s/it]  6%|▌         | 524/8917 [42:26<8:46:20,  3.76s/it]  6%|▌         | 525/8917 [42:29<8:35:02,  3.68s/it]  6%|▌         | 526/8917 [42:33<8:46:20,  3.76s/it]  6%|▌         | 527/8917 [42:37<8:44:24,  3.75s/it]  6%|▌         | 528/8917 [42:41<8:52:45,  3.81s/it]  6%|▌         | 529/8917 [42:44<8:40:16,  3.72s/it]  6%|▌         | 530/8917 [42:48<8:47:27,  3.77s/it]  6%|▌         | 531/8917 [42:52<8:40:14,  3.72s/it]  6%|▌         | 532/8917 [42:56<8:51:04,  3.80s/it]  6%|▌         | 533/8917 [42:59<8:36:49,  3.70s/it]  6%|▌         | 534/8917 [43:03<8:47:30,  3.78s/it]  6%|▌         | 535/8917 [43:07<8:44:26,  3.75s/it]  6%|▌         | 536/8917 [43:11<8:48:15,  3.78s/it]  6%|▌         | 537/8917 [43:14<8:42:26,  3.74s/it]  6%|▌         | 538/8917 [43:18<8:49:30,  3.79s/it]  6%|▌         | 539/8917 [43:22<8:41:18,  3.73s/it]  6%|▌         | 540/8917 [43:25<8:25:07,  3.62s/it]  6%|▌         | 541/8917 [43:29<8:28:09,  3.64s/it]  6%|▌         | 542/8917 [43:32<8:22:18,  3.60s/it]  6%|▌         | 543/8917 [43:36<8:26:37,  3.63s/it]  6%|▌         | 544/8917 [43:40<8:32:09,  3.67s/it]  6%|▌         | 545/8917 [43:43<8:32:23,  3.67s/it]  6%|▌         | 546/8917 [43:47<8:29:18,  3.65s/it]  6%|▌         | 547/8917 [43:51<8:43:55,  3.76s/it]  6%|▌         | 548/8917 [43:55<8:31:39,  3.67s/it]  6%|▌         | 549/8917 [43:58<8:27:33,  3.64s/it]09/19/2024 02:58:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.034222375601530075, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.5526747703552246, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5868971347808838}
  6%|▌         | 550/8917 [44:02<8:22:29,  3.60s/it]  6%|▌         | 551/8917 [44:05<8:16:03,  3.56s/it]  6%|▌         | 552/8917 [44:08<8:11:12,  3.52s/it]  6%|▌         | 553/8917 [44:13<8:42:20,  3.75s/it]  6%|▌         | 554/8917 [44:16<8:31:01,  3.67s/it]  6%|▌         | 555/8917 [44:20<8:28:29,  3.65s/it]  6%|▌         | 556/8917 [44:24<8:35:20,  3.70s/it]  6%|▌         | 557/8917 [44:27<8:37:59,  3.72s/it]  6%|▋         | 558/8917 [44:31<8:29:34,  3.66s/it]  6%|▋         | 559/8917 [44:34<8:16:01,  3.56s/it]  6%|▋         | 560/8917 [44:38<8:43:32,  3.76s/it]  6%|▋         | 561/8917 [44:42<8:46:41,  3.78s/it]  6%|▋         | 562/8917 [44:46<8:54:28,  3.84s/it]  6%|▋         | 563/8917 [44:50<8:38:08,  3.72s/it]  6%|▋         | 564/8917 [44:53<8:25:18,  3.63s/it]  6%|▋         | 565/8917 [44:57<8:36:45,  3.71s/it]  6%|▋         | 566/8917 [45:01<8:58:25,  3.87s/it]  6%|▋         | 567/8917 [45:05<8:45:36,  3.78s/it]  6%|▋         | 568/8917 [45:08<8:35:12,  3.70s/it]  6%|▋         | 569/8917 [45:12<8:46:38,  3.79s/it]  6%|▋         | 570/8917 [45:16<8:34:19,  3.70s/it]  6%|▋         | 571/8917 [45:20<8:50:00,  3.81s/it]  6%|▋         | 572/8917 [45:23<8:38:02,  3.72s/it]  6%|▋         | 573/8917 [45:27<8:22:38,  3.61s/it]  6%|▋         | 574/8917 [45:31<8:35:37,  3.71s/it]  6%|▋         | 575/8917 [45:34<8:27:40,  3.65s/it]  6%|▋         | 576/8917 [45:38<8:37:01,  3.72s/it]  6%|▋         | 577/8917 [45:42<8:31:47,  3.68s/it]  6%|▋         | 578/8917 [45:45<8:29:50,  3.67s/it]  6%|▋         | 579/8917 [45:49<8:21:39,  3.61s/it]  7%|▋         | 580/8917 [45:52<8:22:24,  3.62s/it]  7%|▋         | 581/8917 [45:56<8:33:54,  3.70s/it]  7%|▋         | 582/8917 [46:00<8:35:18,  3.71s/it]  7%|▋         | 583/8917 [46:04<8:46:50,  3.79s/it]  7%|▋         | 584/8917 [46:08<9:00:06,  3.89s/it]  7%|▋         | 585/8917 [46:12<8:48:12,  3.80s/it]  7%|▋         | 586/8917 [46:16<8:43:27,  3.77s/it]  7%|▋         | 587/8917 [46:19<8:50:19,  3.82s/it]  7%|▋         | 588/8917 [46:23<8:44:21,  3.78s/it]  7%|▋         | 589/8917 [46:27<8:51:03,  3.83s/it]  7%|▋         | 590/8917 [46:31<8:47:54,  3.80s/it]  7%|▋         | 591/8917 [46:35<8:49:42,  3.82s/it]  7%|▋         | 592/8917 [46:38<8:39:15,  3.74s/it]  7%|▋         | 593/8917 [46:42<8:34:06,  3.71s/it]  7%|▋         | 594/8917 [46:46<8:44:17,  3.78s/it]  7%|▋         | 595/8917 [46:49<8:23:02,  3.63s/it]  7%|▋         | 596/8917 [46:53<8:48:08,  3.81s/it]  7%|▋         | 597/8917 [46:57<8:37:08,  3.73s/it]  7%|▋         | 598/8917 [47:01<8:34:41,  3.71s/it]  7%|▋         | 599/8917 [47:04<8:34:11,  3.71s/it]09/19/2024 03:01:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.039187874644994736, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.5272314548492432, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5664193630218506}
  7%|▋         | 600/8917 [47:08<8:23:51,  3.63s/it]  7%|▋         | 601/8917 [47:12<8:34:08,  3.71s/it]  7%|▋         | 602/8917 [47:15<8:29:14,  3.67s/it]  7%|▋         | 603/8917 [47:19<8:31:46,  3.69s/it]  7%|▋         | 604/8917 [47:23<8:35:41,  3.72s/it]  7%|▋         | 605/8917 [47:26<8:30:09,  3.68s/it]  7%|▋         | 606/8917 [47:30<8:32:43,  3.70s/it]  7%|▋         | 607/8917 [47:34<8:38:07,  3.74s/it]  7%|▋         | 608/8917 [47:38<8:46:39,  3.80s/it]  7%|▋         | 609/8917 [47:42<8:47:41,  3.81s/it]  7%|▋         | 610/8917 [47:45<8:45:27,  3.80s/it]  7%|▋         | 611/8917 [47:49<8:48:10,  3.82s/it]  7%|▋         | 612/8917 [47:53<8:46:53,  3.81s/it]  7%|▋         | 613/8917 [47:57<8:44:20,  3.79s/it]  7%|▋         | 614/8917 [48:01<8:41:56,  3.77s/it]  7%|▋         | 615/8917 [48:04<8:40:17,  3.76s/it]  7%|▋         | 616/8917 [48:08<8:32:36,  3.71s/it]  7%|▋         | 617/8917 [48:12<8:35:30,  3.73s/it]  7%|▋         | 618/8917 [48:16<8:46:38,  3.81s/it]  7%|▋         | 619/8917 [48:19<8:41:32,  3.77s/it]  7%|▋         | 620/8917 [48:23<8:56:07,  3.88s/it]  7%|▋         | 621/8917 [48:27<8:57:57,  3.89s/it]  7%|▋         | 622/8917 [48:31<8:45:56,  3.80s/it]  7%|▋         | 623/8917 [48:35<8:40:16,  3.76s/it]  7%|▋         | 624/8917 [48:39<8:45:51,  3.80s/it]  7%|▋         | 625/8917 [48:42<8:36:32,  3.74s/it]  7%|▋         | 626/8917 [48:46<8:52:26,  3.85s/it]  7%|▋         | 627/8917 [48:50<8:45:47,  3.81s/it]  7%|▋         | 628/8917 [48:53<8:35:19,  3.73s/it]  7%|▋         | 629/8917 [48:57<8:42:22,  3.78s/it]  7%|▋         | 630/8917 [49:01<8:51:43,  3.85s/it]  7%|▋         | 631/8917 [49:05<8:51:53,  3.85s/it]  7%|▋         | 632/8917 [49:09<8:37:57,  3.75s/it]  7%|▋         | 633/8917 [49:13<8:42:45,  3.79s/it]  7%|▋         | 634/8917 [49:17<8:54:26,  3.87s/it]  7%|▋         | 635/8917 [49:20<8:51:35,  3.85s/it]  7%|▋         | 636/8917 [49:24<8:44:31,  3.80s/it]  7%|▋         | 637/8917 [49:28<8:34:29,  3.73s/it]  7%|▋         | 638/8917 [49:32<8:43:38,  3.79s/it]  7%|▋         | 639/8917 [49:35<8:30:29,  3.70s/it]  7%|▋         | 640/8917 [49:39<8:20:13,  3.63s/it]  7%|▋         | 641/8917 [49:43<8:33:20,  3.72s/it]  7%|▋         | 642/8917 [49:46<8:36:00,  3.74s/it]  7%|▋         | 643/8917 [49:50<8:32:58,  3.72s/it]  7%|▋         | 644/8917 [49:54<8:43:09,  3.79s/it]  7%|▋         | 645/8917 [49:58<8:39:37,  3.77s/it]  7%|▋         | 646/8917 [50:01<8:27:23,  3.68s/it]  7%|▋         | 647/8917 [50:05<8:30:13,  3.70s/it]  7%|▋         | 648/8917 [50:09<8:38:39,  3.76s/it]  7%|▋         | 649/8917 [50:13<8:39:25,  3.77s/it]09/19/2024 03:04:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03377234563231468, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3868852853775024, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4206576347351074}
  7%|▋         | 650/8917 [50:16<8:40:42,  3.78s/it]  7%|▋         | 651/8917 [50:20<8:43:18,  3.80s/it]  7%|▋         | 652/8917 [50:24<8:42:41,  3.79s/it]  7%|▋         | 653/8917 [50:28<8:36:51,  3.75s/it]  7%|▋         | 654/8917 [50:31<8:28:15,  3.69s/it]  7%|▋         | 655/8917 [50:35<8:21:19,  3.64s/it]  7%|▋         | 656/8917 [50:39<8:29:22,  3.70s/it]  7%|▋         | 657/8917 [50:42<8:34:53,  3.74s/it]  7%|▋         | 658/8917 [50:46<8:34:41,  3.74s/it]  7%|▋         | 659/8917 [50:50<8:39:24,  3.77s/it]  7%|▋         | 660/8917 [50:54<8:35:50,  3.75s/it]  7%|▋         | 661/8917 [50:57<8:30:05,  3.71s/it]  7%|▋         | 662/8917 [51:01<8:40:04,  3.78s/it]  7%|▋         | 663/8917 [51:05<8:45:01,  3.82s/it]  7%|▋         | 664/8917 [51:09<8:37:39,  3.76s/it]  7%|▋         | 665/8917 [51:12<8:34:02,  3.74s/it]  7%|▋         | 666/8917 [51:16<8:40:35,  3.79s/it]  7%|▋         | 667/8917 [51:20<8:28:43,  3.70s/it]  7%|▋         | 668/8917 [51:23<8:24:09,  3.67s/it]  8%|▊         | 669/8917 [51:27<8:22:05,  3.65s/it]  8%|▊         | 670/8917 [51:31<8:16:10,  3.61s/it]  8%|▊         | 671/8917 [51:34<8:21:36,  3.65s/it]  8%|▊         | 672/8917 [51:38<8:07:02,  3.54s/it]  8%|▊         | 673/8917 [51:41<8:13:51,  3.59s/it]  8%|▊         | 674/8917 [51:45<8:16:55,  3.62s/it]  8%|▊         | 675/8917 [51:49<8:25:24,  3.68s/it]  8%|▊         | 676/8917 [51:52<8:19:43,  3.64s/it]  8%|▊         | 677/8917 [51:56<8:28:53,  3.71s/it]  8%|▊         | 678/8917 [52:00<8:21:41,  3.65s/it]  8%|▊         | 679/8917 [52:03<8:22:59,  3.66s/it]  8%|▊         | 680/8917 [52:08<8:40:17,  3.79s/it]  8%|▊         | 681/8917 [52:11<8:33:02,  3.74s/it]  8%|▊         | 682/8917 [52:15<8:32:16,  3.73s/it]  8%|▊         | 683/8917 [52:19<8:30:44,  3.72s/it]  8%|▊         | 684/8917 [52:22<8:26:04,  3.69s/it]  8%|▊         | 685/8917 [52:26<8:28:44,  3.71s/it]  8%|▊         | 686/8917 [52:30<8:37:49,  3.77s/it]  8%|▊         | 687/8917 [52:34<8:42:16,  3.81s/it]  8%|▊         | 688/8917 [52:38<8:40:56,  3.80s/it]  8%|▊         | 689/8917 [52:41<8:31:48,  3.73s/it]  8%|▊         | 690/8917 [52:45<8:39:28,  3.79s/it]  8%|▊         | 691/8917 [52:49<8:40:34,  3.80s/it]  8%|▊         | 692/8917 [52:53<8:40:42,  3.80s/it]  8%|▊         | 693/8917 [52:56<8:27:02,  3.70s/it]  8%|▊         | 694/8917 [53:00<8:25:56,  3.69s/it]  8%|▊         | 695/8917 [53:03<8:12:02,  3.59s/it]  8%|▊         | 696/8917 [53:07<8:15:27,  3.62s/it]  8%|▊         | 697/8917 [53:10<8:12:45,  3.60s/it]  8%|▊         | 698/8917 [53:15<8:37:28,  3.78s/it]  8%|▊         | 699/8917 [53:19<8:54:56,  3.91s/it]09/19/2024 03:07:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029233811423182487, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3240045309066772, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3532383441925049}
  8%|▊         | 700/8917 [53:23<8:57:14,  3.92s/it]  8%|▊         | 701/8917 [53:26<8:43:34,  3.82s/it]  8%|▊         | 702/8917 [53:30<8:29:40,  3.72s/it]  8%|▊         | 703/8917 [53:34<8:29:31,  3.72s/it]  8%|▊         | 704/8917 [53:37<8:21:21,  3.66s/it]  8%|▊         | 705/8917 [53:41<8:26:31,  3.70s/it]  8%|▊         | 706/8917 [53:45<8:52:55,  3.89s/it]  8%|▊         | 707/8917 [53:49<8:50:15,  3.88s/it]  8%|▊         | 708/8917 [53:53<8:51:41,  3.89s/it]  8%|▊         | 709/8917 [53:57<8:38:48,  3.79s/it]  8%|▊         | 710/8917 [54:00<8:30:35,  3.73s/it]  8%|▊         | 711/8917 [54:04<8:27:30,  3.71s/it]  8%|▊         | 712/8917 [54:08<8:37:16,  3.78s/it]  8%|▊         | 713/8917 [54:12<8:41:49,  3.82s/it]  8%|▊         | 714/8917 [54:15<8:32:04,  3.75s/it]  8%|▊         | 715/8917 [54:19<8:14:29,  3.62s/it]  8%|▊         | 716/8917 [54:22<8:11:20,  3.59s/it]  8%|▊         | 717/8917 [54:26<8:19:10,  3.65s/it]  8%|▊         | 718/8917 [54:29<8:15:11,  3.62s/it]  8%|▊         | 719/8917 [54:33<8:11:21,  3.60s/it]  8%|▊         | 720/8917 [54:37<8:11:30,  3.60s/it]  8%|▊         | 721/8917 [54:41<8:30:54,  3.74s/it]  8%|▊         | 722/8917 [54:44<8:31:50,  3.75s/it]  8%|▊         | 723/8917 [54:49<8:53:38,  3.91s/it]  8%|▊         | 724/8917 [54:52<8:41:44,  3.82s/it]  8%|▊         | 725/8917 [54:56<8:45:46,  3.85s/it]  8%|▊         | 726/8917 [55:00<8:40:09,  3.81s/it]  8%|▊         | 727/8917 [55:04<8:35:06,  3.77s/it]  8%|▊         | 728/8917 [55:07<8:39:44,  3.81s/it]  8%|▊         | 729/8917 [55:11<8:20:28,  3.67s/it]  8%|▊         | 730/8917 [55:15<8:24:36,  3.70s/it]  8%|▊         | 731/8917 [55:18<8:29:26,  3.73s/it]  8%|▊         | 732/8917 [55:22<8:30:07,  3.74s/it]  8%|▊         | 733/8917 [55:26<8:19:07,  3.66s/it]  8%|▊         | 734/8917 [55:29<8:25:29,  3.71s/it]  8%|▊         | 735/8917 [55:33<8:20:26,  3.67s/it]  8%|▊         | 736/8917 [55:37<8:25:22,  3.71s/it]  8%|▊         | 737/8917 [55:40<8:20:45,  3.67s/it]  8%|▊         | 738/8917 [55:44<8:22:13,  3.68s/it]  8%|▊         | 739/8917 [55:48<8:28:32,  3.73s/it]  8%|▊         | 740/8917 [55:52<8:31:07,  3.75s/it]  8%|▊         | 741/8917 [55:56<8:32:32,  3.76s/it]  8%|▊         | 742/8917 [55:59<8:38:19,  3.80s/it]  8%|▊         | 743/8917 [56:03<8:37:19,  3.80s/it]  8%|▊         | 744/8917 [56:07<8:23:59,  3.70s/it]  8%|▊         | 745/8917 [56:10<8:21:46,  3.68s/it]  8%|▊         | 746/8917 [56:14<8:24:52,  3.71s/it]  8%|▊         | 747/8917 [56:18<8:31:34,  3.76s/it]  8%|▊         | 748/8917 [56:21<8:20:31,  3.68s/it]  8%|▊         | 749/8917 [56:25<8:31:57,  3.76s/it]09/19/2024 03:11:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.045962464064359665, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 2.2446203231811523, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 2.2905828952789307}
  8%|▊         | 750/8917 [56:30<8:44:58,  3.86s/it]  8%|▊         | 751/8917 [56:33<8:22:29,  3.69s/it]  8%|▊         | 752/8917 [56:37<8:29:50,  3.75s/it]  8%|▊         | 753/8917 [56:40<8:24:49,  3.71s/it]  8%|▊         | 754/8917 [56:44<8:22:06,  3.69s/it]  8%|▊         | 755/8917 [56:48<8:31:36,  3.76s/it]  8%|▊         | 756/8917 [56:52<8:33:13,  3.77s/it]  8%|▊         | 757/8917 [56:55<8:30:31,  3.75s/it]  9%|▊         | 758/8917 [56:59<8:28:48,  3.74s/it]  9%|▊         | 759/8917 [57:03<8:36:27,  3.80s/it]  9%|▊         | 760/8917 [57:07<8:38:05,  3.81s/it]  9%|▊         | 761/8917 [57:11<8:34:50,  3.79s/it]  9%|▊         | 762/8917 [57:15<8:38:39,  3.82s/it]  9%|▊         | 763/8917 [57:18<8:37:39,  3.81s/it]  9%|▊         | 764/8917 [57:22<8:27:01,  3.73s/it]  9%|▊         | 765/8917 [57:25<8:17:45,  3.66s/it]  9%|▊         | 766/8917 [57:29<8:25:05,  3.72s/it]  9%|▊         | 767/8917 [57:33<8:33:20,  3.78s/it]  9%|▊         | 768/8917 [57:37<8:38:56,  3.82s/it]  9%|▊         | 769/8917 [57:41<8:34:27,  3.79s/it]  9%|▊         | 770/8917 [57:44<8:28:03,  3.74s/it]  9%|▊         | 771/8917 [57:48<8:21:34,  3.69s/it]  9%|▊         | 772/8917 [57:52<8:32:57,  3.78s/it]  9%|▊         | 773/8917 [57:55<8:17:38,  3.67s/it]  9%|▊         | 774/8917 [57:59<8:31:05,  3.77s/it]  9%|▊         | 775/8917 [58:03<8:33:11,  3.78s/it]  9%|▊         | 776/8917 [58:07<8:37:21,  3.81s/it]  9%|▊         | 777/8917 [58:11<8:27:16,  3.74s/it]  9%|▊         | 778/8917 [58:14<8:25:44,  3.73s/it]  9%|▊         | 779/8917 [58:18<8:20:19,  3.69s/it]  9%|▊         | 780/8917 [58:21<8:10:51,  3.62s/it]  9%|▉         | 781/8917 [58:26<8:39:22,  3.83s/it]  9%|▉         | 782/8917 [58:29<8:29:36,  3.76s/it]  9%|▉         | 783/8917 [58:34<8:49:30,  3.91s/it]  9%|▉         | 784/8917 [58:37<8:49:24,  3.91s/it]  9%|▉         | 785/8917 [58:41<8:39:11,  3.83s/it]  9%|▉         | 786/8917 [58:45<8:34:36,  3.80s/it]  9%|▉         | 787/8917 [58:49<8:32:07,  3.78s/it]  9%|▉         | 788/8917 [58:52<8:11:48,  3.63s/it]  9%|▉         | 789/8917 [58:55<8:10:06,  3.62s/it]  9%|▉         | 790/8917 [59:00<8:31:34,  3.78s/it]  9%|▉         | 791/8917 [59:03<8:18:45,  3.68s/it]  9%|▉         | 792/8917 [59:07<8:19:14,  3.69s/it]  9%|▉         | 793/8917 [59:11<8:36:13,  3.81s/it]  9%|▉         | 794/8917 [59:15<8:34:02,  3.80s/it]  9%|▉         | 795/8917 [59:18<8:35:52,  3.81s/it]  9%|▉         | 796/8917 [59:22<8:31:58,  3.78s/it]  9%|▉         | 797/8917 [59:26<8:24:07,  3.73s/it]  9%|▉         | 798/8917 [59:30<8:26:56,  3.75s/it]  9%|▉         | 799/8917 [59:33<8:26:32,  3.74s/it]09/19/2024 03:14:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04474399611353874, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.8431503772735596, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.8878943920135498}
  9%|▉         | 800/8917 [59:37<8:26:42,  3.75s/it]  9%|▉         | 801/8917 [59:41<8:24:12,  3.73s/it]  9%|▉         | 802/8917 [59:45<8:36:16,  3.82s/it]  9%|▉         | 803/8917 [59:48<8:14:44,  3.66s/it]  9%|▉         | 804/8917 [59:52<8:07:59,  3.61s/it]  9%|▉         | 805/8917 [59:55<8:14:58,  3.66s/it]  9%|▉         | 806/8917 [59:59<8:15:43,  3.67s/it]  9%|▉         | 807/8917 [1:00:02<8:08:21,  3.61s/it]  9%|▉         | 808/8917 [1:00:06<8:08:47,  3.62s/it]  9%|▉         | 809/8917 [1:00:10<8:07:03,  3.60s/it]  9%|▉         | 810/8917 [1:00:13<8:10:54,  3.63s/it]  9%|▉         | 811/8917 [1:00:17<8:21:52,  3.71s/it]  9%|▉         | 812/8917 [1:00:21<8:22:44,  3.72s/it]  9%|▉         | 813/8917 [1:00:25<8:15:49,  3.67s/it]  9%|▉         | 814/8917 [1:00:28<8:24:00,  3.73s/it]  9%|▉         | 815/8917 [1:00:32<8:28:43,  3.77s/it]  9%|▉         | 816/8917 [1:00:36<8:18:20,  3.69s/it]  9%|▉         | 817/8917 [1:00:40<8:21:38,  3.72s/it]  9%|▉         | 818/8917 [1:00:43<8:15:36,  3.67s/it]  9%|▉         | 819/8917 [1:00:47<8:12:18,  3.65s/it]  9%|▉         | 820/8917 [1:00:51<8:17:44,  3.69s/it]  9%|▉         | 821/8917 [1:00:54<8:09:58,  3.63s/it]  9%|▉         | 822/8917 [1:00:58<8:24:53,  3.74s/it]  9%|▉         | 823/8917 [1:01:02<8:22:15,  3.72s/it]  9%|▉         | 824/8917 [1:01:05<8:09:05,  3.63s/it]  9%|▉         | 825/8917 [1:01:09<8:17:50,  3.69s/it]  9%|▉         | 826/8917 [1:01:13<8:26:43,  3.76s/it]  9%|▉         | 827/8917 [1:01:17<8:21:19,  3.72s/it]  9%|▉         | 828/8917 [1:01:20<8:29:15,  3.78s/it]  9%|▉         | 829/8917 [1:01:24<8:22:36,  3.73s/it]  9%|▉         | 830/8917 [1:01:28<8:24:20,  3.74s/it]  9%|▉         | 831/8917 [1:01:31<8:06:36,  3.61s/it]  9%|▉         | 832/8917 [1:01:35<8:24:11,  3.74s/it]  9%|▉         | 833/8917 [1:01:39<8:21:53,  3.73s/it]  9%|▉         | 834/8917 [1:01:42<8:15:37,  3.68s/it]  9%|▉         | 835/8917 [1:01:46<8:15:41,  3.68s/it]  9%|▉         | 836/8917 [1:01:50<8:22:20,  3.73s/it]  9%|▉         | 837/8917 [1:01:54<8:25:16,  3.75s/it]  9%|▉         | 838/8917 [1:01:58<8:27:22,  3.77s/it]  9%|▉         | 839/8917 [1:02:02<8:35:19,  3.83s/it]  9%|▉         | 840/8917 [1:02:05<8:34:35,  3.82s/it]  9%|▉         | 841/8917 [1:02:09<8:23:56,  3.74s/it]  9%|▉         | 842/8917 [1:02:13<8:20:48,  3.72s/it]  9%|▉         | 843/8917 [1:02:17<8:31:15,  3.80s/it]  9%|▉         | 844/8917 [1:02:20<8:26:31,  3.76s/it]  9%|▉         | 845/8917 [1:02:24<8:35:24,  3.83s/it]  9%|▉         | 846/8917 [1:02:28<8:20:20,  3.72s/it]  9%|▉         | 847/8917 [1:02:32<8:34:31,  3.83s/it] 10%|▉         | 848/8917 [1:02:36<8:36:56,  3.84s/it] 10%|▉         | 849/8917 [1:02:39<8:28:36,  3.78s/it]09/19/2024 03:17:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028064608573913574, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3200626373291016, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3481272459030151}
 10%|▉         | 850/8917 [1:02:43<8:32:18,  3.81s/it] 10%|▉         | 851/8917 [1:02:47<8:21:38,  3.73s/it] 10%|▉         | 852/8917 [1:02:50<8:23:24,  3.75s/it] 10%|▉         | 853/8917 [1:02:55<8:36:00,  3.84s/it] 10%|▉         | 854/8917 [1:02:58<8:14:34,  3.68s/it] 10%|▉         | 855/8917 [1:03:02<8:27:01,  3.77s/it] 10%|▉         | 856/8917 [1:03:05<8:18:56,  3.71s/it] 10%|▉         | 857/8917 [1:03:09<8:14:24,  3.68s/it] 10%|▉         | 858/8917 [1:03:12<8:00:37,  3.58s/it] 10%|▉         | 859/8917 [1:03:16<7:54:48,  3.54s/it] 10%|▉         | 860/8917 [1:03:19<7:53:48,  3.53s/it] 10%|▉         | 861/8917 [1:03:23<8:01:11,  3.58s/it] 10%|▉         | 862/8917 [1:03:27<8:09:42,  3.65s/it] 10%|▉         | 863/8917 [1:03:31<8:11:56,  3.66s/it] 10%|▉         | 864/8917 [1:03:34<8:20:40,  3.73s/it] 10%|▉         | 865/8917 [1:03:38<8:32:19,  3.82s/it] 10%|▉         | 866/8917 [1:03:42<8:38:05,  3.86s/it] 10%|▉         | 867/8917 [1:03:46<8:20:15,  3.73s/it] 10%|▉         | 868/8917 [1:03:49<8:16:28,  3.70s/it] 10%|▉         | 869/8917 [1:03:53<8:09:41,  3.65s/it] 10%|▉         | 870/8917 [1:03:57<8:12:14,  3.67s/it] 10%|▉         | 871/8917 [1:04:00<8:01:29,  3.59s/it] 10%|▉         | 872/8917 [1:04:04<8:05:51,  3.62s/it] 10%|▉         | 873/8917 [1:04:08<8:15:19,  3.69s/it] 10%|▉         | 874/8917 [1:04:11<8:17:02,  3.71s/it] 10%|▉         | 875/8917 [1:04:15<8:17:26,  3.71s/it] 10%|▉         | 876/8917 [1:04:19<8:09:45,  3.65s/it] 10%|▉         | 877/8917 [1:04:22<8:10:31,  3.66s/it] 10%|▉         | 878/8917 [1:04:26<8:25:00,  3.77s/it] 10%|▉         | 879/8917 [1:04:30<8:29:49,  3.81s/it] 10%|▉         | 880/8917 [1:04:34<8:23:11,  3.76s/it] 10%|▉         | 881/8917 [1:04:38<8:33:11,  3.83s/it] 10%|▉         | 882/8917 [1:04:42<8:29:11,  3.80s/it] 10%|▉         | 883/8917 [1:04:45<8:15:57,  3.70s/it] 10%|▉         | 884/8917 [1:04:49<8:32:14,  3.83s/it] 10%|▉         | 885/8917 [1:04:53<8:40:03,  3.88s/it] 10%|▉         | 886/8917 [1:04:57<8:30:03,  3.81s/it] 10%|▉         | 887/8917 [1:05:01<8:46:46,  3.94s/it] 10%|▉         | 888/8917 [1:05:05<8:39:43,  3.88s/it] 10%|▉         | 889/8917 [1:05:09<8:34:31,  3.85s/it] 10%|▉         | 890/8917 [1:05:13<8:40:11,  3.89s/it] 10%|▉         | 891/8917 [1:05:16<8:30:20,  3.82s/it] 10%|█         | 892/8917 [1:05:20<8:31:04,  3.82s/it] 10%|█         | 893/8917 [1:05:24<8:23:21,  3.76s/it] 10%|█         | 894/8917 [1:05:28<8:34:22,  3.85s/it] 10%|█         | 895/8917 [1:05:31<8:19:31,  3.74s/it] 10%|█         | 896/8917 [1:05:35<8:20:48,  3.75s/it] 10%|█         | 897/8917 [1:05:39<8:22:22,  3.76s/it] 10%|█         | 898/8917 [1:05:42<8:14:38,  3.70s/it] 10%|█         | 899/8917 [1:05:47<8:43:12,  3.92s/it]09/19/2024 03:20:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02544914186000824, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.469860315322876, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4953094720840454}
 10%|█         | 900/8917 [1:05:50<8:27:19,  3.80s/it] 10%|█         | 901/8917 [1:05:54<8:18:00,  3.73s/it] 10%|█         | 902/8917 [1:05:58<8:23:30,  3.77s/it] 10%|█         | 903/8917 [1:06:01<8:16:44,  3.72s/it] 10%|█         | 904/8917 [1:06:05<8:31:02,  3.83s/it] 10%|█         | 905/8917 [1:06:09<8:23:14,  3.77s/it] 10%|█         | 906/8917 [1:06:13<8:15:00,  3.71s/it] 10%|█         | 907/8917 [1:06:16<8:19:17,  3.74s/it] 10%|█         | 908/8917 [1:06:20<8:10:51,  3.68s/it] 10%|█         | 909/8917 [1:06:24<8:16:10,  3.72s/it] 10%|█         | 910/8917 [1:06:27<8:13:54,  3.70s/it] 10%|█         | 911/8917 [1:06:31<8:04:57,  3.63s/it] 10%|█         | 912/8917 [1:06:35<8:07:40,  3.66s/it] 10%|█         | 913/8917 [1:06:38<8:12:25,  3.69s/it] 10%|█         | 914/8917 [1:06:42<8:20:21,  3.75s/it] 10%|█         | 915/8917 [1:06:46<8:17:16,  3.73s/it] 10%|█         | 916/8917 [1:06:49<8:11:02,  3.68s/it] 10%|█         | 917/8917 [1:06:53<8:15:16,  3.71s/it] 10%|█         | 918/8917 [1:06:57<8:06:32,  3.65s/it] 10%|█         | 919/8917 [1:07:01<8:09:04,  3.67s/it] 10%|█         | 920/8917 [1:07:04<8:10:06,  3.68s/it] 10%|█         | 921/8917 [1:07:08<8:20:21,  3.75s/it] 10%|█         | 922/8917 [1:07:12<8:04:53,  3.64s/it] 10%|█         | 923/8917 [1:07:15<8:03:24,  3.63s/it] 10%|█         | 924/8917 [1:07:19<7:56:57,  3.58s/it] 10%|█         | 925/8917 [1:07:22<8:07:21,  3.66s/it] 10%|█         | 926/8917 [1:07:26<8:11:28,  3.69s/it] 10%|█         | 927/8917 [1:07:30<8:03:48,  3.63s/it] 10%|█         | 928/8917 [1:07:33<7:57:11,  3.58s/it] 10%|█         | 929/8917 [1:07:37<8:09:12,  3.67s/it] 10%|█         | 930/8917 [1:07:41<8:08:06,  3.67s/it] 10%|█         | 931/8917 [1:07:45<8:20:19,  3.76s/it] 10%|█         | 932/8917 [1:07:48<8:16:30,  3.73s/it] 10%|█         | 933/8917 [1:07:53<8:38:38,  3.90s/it] 10%|█         | 934/8917 [1:07:56<8:24:33,  3.79s/it] 10%|█         | 935/8917 [1:08:00<8:07:12,  3.66s/it] 10%|█         | 936/8917 [1:08:03<8:02:16,  3.63s/it] 11%|█         | 937/8917 [1:08:07<8:02:41,  3.63s/it] 11%|█         | 938/8917 [1:08:10<8:01:42,  3.62s/it] 11%|█         | 939/8917 [1:08:14<8:08:56,  3.68s/it] 11%|█         | 940/8917 [1:08:18<8:19:57,  3.76s/it] 11%|█         | 941/8917 [1:08:22<8:25:00,  3.80s/it] 11%|█         | 942/8917 [1:08:25<8:03:26,  3.64s/it] 11%|█         | 943/8917 [1:08:29<8:09:24,  3.68s/it] 11%|█         | 944/8917 [1:08:33<8:24:19,  3.80s/it] 11%|█         | 945/8917 [1:08:37<8:28:30,  3.83s/it] 11%|█         | 946/8917 [1:08:41<8:17:47,  3.75s/it] 11%|█         | 947/8917 [1:08:44<8:26:20,  3.81s/it] 11%|█         | 948/8917 [1:08:48<8:12:04,  3.70s/it] 11%|█         | 949/8917 [1:08:52<8:07:08,  3.67s/it]09/19/2024 03:23:29 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.033198073506355286, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.115940809249878, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1491389274597168}
 11%|█         | 950/8917 [1:08:56<8:24:01,  3.80s/it] 11%|█         | 951/8917 [1:09:00<8:28:53,  3.83s/it] 11%|█         | 952/8917 [1:09:03<8:31:13,  3.85s/it] 11%|█         | 953/8917 [1:09:07<8:09:22,  3.69s/it] 11%|█         | 954/8917 [1:09:11<8:29:43,  3.84s/it] 11%|█         | 955/8917 [1:09:15<8:35:28,  3.88s/it] 11%|█         | 956/8917 [1:09:19<8:37:24,  3.90s/it] 11%|█         | 957/8917 [1:09:22<8:22:28,  3.79s/it] 11%|█         | 958/8917 [1:09:26<8:10:58,  3.70s/it] 11%|█         | 959/8917 [1:09:30<8:19:02,  3.76s/it] 11%|█         | 960/8917 [1:09:34<8:24:55,  3.81s/it] 11%|█         | 961/8917 [1:09:37<8:14:56,  3.73s/it] 11%|█         | 962/8917 [1:09:41<8:17:03,  3.75s/it] 11%|█         | 963/8917 [1:09:45<8:28:59,  3.84s/it] 11%|█         | 964/8917 [1:09:49<8:14:20,  3.73s/it] 11%|█         | 965/8917 [1:09:52<8:09:22,  3.69s/it] 11%|█         | 966/8917 [1:09:56<8:17:24,  3.75s/it] 11%|█         | 967/8917 [1:10:00<8:20:01,  3.77s/it] 11%|█         | 968/8917 [1:10:03<7:56:07,  3.59s/it] 11%|█         | 969/8917 [1:10:07<8:06:44,  3.67s/it] 11%|█         | 970/8917 [1:10:11<8:17:51,  3.76s/it] 11%|█         | 971/8917 [1:10:15<8:18:09,  3.76s/it] 11%|█         | 972/8917 [1:10:18<8:05:45,  3.67s/it] 11%|█         | 973/8917 [1:10:22<7:59:39,  3.62s/it] 11%|█         | 974/8917 [1:10:26<8:10:22,  3.70s/it] 11%|█         | 975/8917 [1:10:29<8:09:14,  3.70s/it] 11%|█         | 976/8917 [1:10:33<8:08:39,  3.69s/it] 11%|█         | 977/8917 [1:10:37<8:25:07,  3.82s/it] 11%|█         | 978/8917 [1:10:41<8:25:36,  3.82s/it] 11%|█         | 979/8917 [1:10:44<8:20:17,  3.78s/it] 11%|█         | 980/8917 [1:10:48<8:18:50,  3.77s/it] 11%|█         | 981/8917 [1:10:52<8:10:57,  3.71s/it] 11%|█         | 982/8917 [1:10:55<7:55:30,  3.60s/it] 11%|█         | 983/8917 [1:10:59<7:55:56,  3.60s/it] 11%|█         | 984/8917 [1:11:03<8:12:41,  3.73s/it] 11%|█         | 985/8917 [1:11:07<8:17:50,  3.77s/it] 11%|█         | 986/8917 [1:11:11<8:22:37,  3.80s/it] 11%|█         | 987/8917 [1:11:14<8:20:51,  3.79s/it] 11%|█         | 988/8917 [1:11:18<8:29:54,  3.86s/it] 11%|█         | 989/8917 [1:11:22<8:21:59,  3.80s/it] 11%|█         | 990/8917 [1:11:26<8:30:56,  3.87s/it] 11%|█         | 991/8917 [1:11:29<8:10:48,  3.72s/it] 11%|█         | 992/8917 [1:11:33<8:04:07,  3.67s/it] 11%|█         | 993/8917 [1:11:37<8:13:42,  3.74s/it] 11%|█         | 994/8917 [1:11:40<8:08:41,  3.70s/it] 11%|█         | 995/8917 [1:11:44<7:59:49,  3.63s/it] 11%|█         | 996/8917 [1:11:48<7:59:54,  3.64s/it] 11%|█         | 997/8917 [1:11:51<8:09:32,  3.71s/it] 11%|█         | 998/8917 [1:11:55<8:05:40,  3.68s/it] 11%|█         | 999/8917 [1:11:59<8:11:34,  3.73s/it]09/19/2024 03:26:35 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 03:26:35 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:34,  6.37it/s][A
  1%|          | 2/221 [00:00<01:11,  3.05it/s][A
  1%|▏         | 3/221 [00:02<03:25,  1.06it/s][A
  2%|▏         | 4/221 [00:02<02:12,  1.64it/s][A
  2%|▏         | 5/221 [00:02<01:32,  2.34it/s][A
  3%|▎         | 6/221 [00:02<01:31,  2.36it/s][A
  3%|▎         | 7/221 [00:03<01:28,  2.42it/s][A
  4%|▎         | 8/221 [00:03<01:15,  2.81it/s][A
  4%|▍         | 9/221 [00:03<01:16,  2.77it/s][A
  5%|▍         | 10/221 [00:04<01:24,  2.49it/s][A
  5%|▍         | 11/221 [00:04<01:06,  3.14it/s][A
  5%|▌         | 12/221 [00:05<01:41,  2.06it/s][A
  6%|▌         | 13/221 [00:05<01:22,  2.52it/s][A
  6%|▋         | 14/221 [00:05<01:09,  2.99it/s][A
  7%|▋         | 15/221 [00:06<01:12,  2.85it/s][A
  7%|▋         | 16/221 [00:06<01:27,  2.34it/s][A
  8%|▊         | 17/221 [00:07<02:03,  1.65it/s][A
  8%|▊         | 18/221 [00:09<02:41,  1.25it/s][A
  9%|▊         | 19/221 [00:09<02:30,  1.34it/s][A
 10%|▉         | 21/221 [00:10<01:37,  2.04it/s][A
 10%|▉         | 22/221 [00:10<01:34,  2.11it/s][A
 11%|█         | 24/221 [00:10<01:08,  2.89it/s][A
 11%|█▏        | 25/221 [00:11<01:03,  3.07it/s][A
 12%|█▏        | 26/221 [00:11<01:15,  2.59it/s][A
 13%|█▎        | 28/221 [00:12<01:08,  2.83it/s][A
 13%|█▎        | 29/221 [00:12<01:25,  2.25it/s][A
 14%|█▎        | 30/221 [00:13<01:44,  1.82it/s][A
 14%|█▍        | 31/221 [00:14<01:42,  1.85it/s][A
 15%|█▍        | 33/221 [00:14<01:14,  2.52it/s][A
 16%|█▌        | 35/221 [00:15<00:55,  3.34it/s][A
 16%|█▋        | 36/221 [00:15<00:52,  3.51it/s][A
 17%|█▋        | 37/221 [00:15<01:08,  2.68it/s][A
 17%|█▋        | 38/221 [00:16<01:30,  2.02it/s][A
 18%|█▊        | 39/221 [00:16<01:12,  2.51it/s][A
 18%|█▊        | 40/221 [00:17<01:09,  2.60it/s][A
 19%|█▊        | 41/221 [00:17<00:59,  3.04it/s][A
 19%|█▉        | 42/221 [00:17<00:58,  3.04it/s][A
 19%|█▉        | 43/221 [00:17<00:49,  3.59it/s][A
 20%|█▉        | 44/221 [00:18<00:42,  4.20it/s][A
 20%|██        | 45/221 [00:24<06:09,  2.10s/it][A
 21%|██        | 46/221 [00:25<04:40,  1.60s/it][A
 21%|██▏       | 47/221 [00:26<04:25,  1.53s/it][A
 22%|██▏       | 48/221 [00:26<03:18,  1.15s/it][A
 22%|██▏       | 49/221 [00:26<02:32,  1.13it/s][A
 23%|██▎       | 50/221 [00:27<01:56,  1.47it/s][A
 23%|██▎       | 51/221 [00:27<01:31,  1.87it/s][A
 24%|██▎       | 52/221 [00:27<01:10,  2.38it/s][A
 24%|██▍       | 53/221 [00:27<01:06,  2.52it/s][A
 24%|██▍       | 54/221 [00:28<01:30,  1.84it/s][A
 25%|██▍       | 55/221 [00:32<04:00,  1.45s/it][A
 25%|██▌       | 56/221 [00:32<03:00,  1.09s/it][A
 26%|██▌       | 57/221 [00:32<02:22,  1.15it/s][A
 27%|██▋       | 59/221 [00:33<01:22,  1.96it/s][A
 27%|██▋       | 60/221 [00:33<01:20,  2.00it/s][A
 28%|██▊       | 61/221 [00:33<01:07,  2.38it/s][A
 28%|██▊       | 62/221 [00:34<01:01,  2.60it/s][A
 29%|██▊       | 63/221 [00:34<01:00,  2.59it/s][A
 29%|██▉       | 64/221 [00:35<01:15,  2.09it/s][A
 29%|██▉       | 65/221 [00:35<01:02,  2.51it/s][A
 30%|██▉       | 66/221 [00:35<01:10,  2.19it/s][A
 30%|███       | 67/221 [00:36<01:05,  2.34it/s][A
 31%|███       | 68/221 [00:36<00:59,  2.59it/s][A
 31%|███       | 69/221 [00:37<01:20,  1.88it/s][A
 32%|███▏      | 70/221 [00:37<01:01,  2.47it/s][A
 32%|███▏      | 71/221 [00:37<01:00,  2.48it/s][A
 33%|███▎      | 72/221 [00:38<01:05,  2.28it/s][A
 33%|███▎      | 73/221 [00:39<01:14,  1.98it/s][A
 33%|███▎      | 74/221 [00:39<00:58,  2.50it/s][A
 34%|███▍      | 75/221 [00:39<00:56,  2.57it/s][A
 34%|███▍      | 76/221 [00:40<00:59,  2.44it/s][A
 35%|███▍      | 77/221 [00:40<01:14,  1.92it/s][A
 35%|███▌      | 78/221 [00:41<00:58,  2.43it/s][A
 36%|███▌      | 79/221 [00:42<01:28,  1.60it/s][A
 37%|███▋      | 81/221 [00:43<01:36,  1.46it/s][A
 37%|███▋      | 82/221 [00:49<04:43,  2.04s/it][A
 38%|███▊      | 83/221 [00:50<04:09,  1.81s/it][A
 38%|███▊      | 84/221 [00:51<03:07,  1.37s/it][A
 38%|███▊      | 85/221 [00:51<02:18,  1.02s/it][A
 39%|███▉      | 86/221 [00:51<01:58,  1.14it/s][A
 39%|███▉      | 87/221 [00:52<01:59,  1.12it/s][A
 40%|███▉      | 88/221 [00:53<01:46,  1.25it/s][A
 40%|████      | 89/221 [00:54<01:40,  1.31it/s][A
 41%|████      | 90/221 [00:54<01:24,  1.55it/s][A
 41%|████      | 91/221 [00:54<01:06,  1.96it/s][A
 42%|████▏     | 92/221 [00:54<00:59,  2.16it/s][A
 42%|████▏     | 93/221 [00:55<01:04,  1.98it/s][A
 43%|████▎     | 94/221 [00:56<01:12,  1.76it/s][A
 43%|████▎     | 95/221 [00:56<01:03,  2.00it/s][A
 43%|████▎     | 96/221 [00:58<02:04,  1.00it/s][A
 44%|████▍     | 97/221 [00:58<01:32,  1.35it/s][A
 44%|████▍     | 98/221 [00:59<01:30,  1.36it/s][A
 45%|████▍     | 99/221 [00:59<01:12,  1.69it/s][A
 45%|████▌     | 100/221 [01:00<01:10,  1.73it/s][A
 46%|████▌     | 101/221 [01:00<00:57,  2.08it/s][A
 46%|████▌     | 102/221 [01:03<02:05,  1.05s/it][A
 47%|████▋     | 103/221 [01:03<01:31,  1.28it/s][A
 47%|████▋     | 104/221 [01:03<01:14,  1.57it/s][A
 48%|████▊     | 105/221 [01:04<01:11,  1.62it/s][A
 48%|████▊     | 106/221 [01:04<01:14,  1.53it/s][A
 48%|████▊     | 107/221 [01:05<01:05,  1.73it/s][A
 49%|████▉     | 108/221 [01:05<00:57,  1.95it/s][A
 49%|████▉     | 109/221 [01:05<00:45,  2.45it/s][A
 50%|█████     | 111/221 [01:06<00:34,  3.15it/s][A
 51%|█████     | 112/221 [01:06<00:35,  3.04it/s][A
 51%|█████     | 113/221 [01:06<00:33,  3.22it/s][A
 52%|█████▏    | 115/221 [01:07<00:25,  4.14it/s][A
 52%|█████▏    | 116/221 [01:09<01:30,  1.16it/s][A
 53%|█████▎    | 117/221 [01:10<01:19,  1.31it/s][A
 53%|█████▎    | 118/221 [01:10<01:07,  1.53it/s][A
 54%|█████▍    | 119/221 [01:11<01:01,  1.66it/s][A
 54%|█████▍    | 120/221 [01:11<00:54,  1.84it/s][A
 55%|█████▍    | 121/221 [01:11<00:46,  2.15it/s][A
 55%|█████▌    | 122/221 [01:12<00:42,  2.33it/s][A
 56%|█████▌    | 123/221 [01:13<01:05,  1.49it/s][A
 56%|█████▌    | 124/221 [01:13<00:50,  1.93it/s][A
 57%|█████▋    | 125/221 [01:14<00:54,  1.77it/s][A
 57%|█████▋    | 126/221 [01:14<00:52,  1.80it/s][A
 57%|█████▋    | 127/221 [01:15<00:52,  1.80it/s][A
 58%|█████▊    | 128/221 [01:16<00:52,  1.76it/s][A
 58%|█████▊    | 129/221 [01:16<00:45,  2.04it/s][A
 59%|█████▉    | 130/221 [01:16<00:42,  2.16it/s][A
 59%|█████▉    | 131/221 [01:17<00:41,  2.18it/s][A
 60%|█████▉    | 132/221 [01:18<00:51,  1.74it/s][A
 60%|██████    | 133/221 [01:18<00:44,  1.98it/s][A
 61%|██████    | 134/221 [01:21<01:51,  1.28s/it][A
 61%|██████    | 135/221 [01:22<01:42,  1.19s/it][A
 62%|██████▏   | 136/221 [01:22<01:21,  1.04it/s][A
 62%|██████▏   | 137/221 [01:23<01:04,  1.30it/s][A
 62%|██████▏   | 138/221 [01:24<01:10,  1.17it/s][A
 63%|██████▎   | 139/221 [01:24<00:59,  1.38it/s][A
 63%|██████▎   | 140/221 [01:25<00:54,  1.50it/s][A
 64%|██████▍   | 141/221 [01:25<00:49,  1.63it/s][A
 64%|██████▍   | 142/221 [01:26<00:46,  1.70it/s][A
 65%|██████▍   | 143/221 [01:27<00:53,  1.45it/s][A
 65%|██████▌   | 144/221 [01:27<00:42,  1.81it/s][A
 66%|██████▌   | 146/221 [01:27<00:25,  2.95it/s][A
 67%|██████▋   | 148/221 [01:28<00:33,  2.17it/s][A
 67%|██████▋   | 149/221 [01:29<00:30,  2.39it/s][A
 68%|██████▊   | 150/221 [01:29<00:29,  2.40it/s][A
 68%|██████▊   | 151/221 [01:29<00:26,  2.66it/s][A
 69%|██████▉   | 152/221 [01:30<00:24,  2.81it/s][A
 69%|██████▉   | 153/221 [01:30<00:20,  3.33it/s][A
 70%|██████▉   | 154/221 [01:30<00:21,  3.12it/s][A
 70%|███████   | 155/221 [01:31<00:23,  2.76it/s][A
 71%|███████   | 156/221 [01:31<00:20,  3.14it/s][A
 71%|███████   | 157/221 [01:37<02:04,  1.95s/it][A
 71%|███████▏  | 158/221 [01:37<01:33,  1.48s/it][A
 72%|███████▏  | 159/221 [01:37<01:07,  1.08s/it][A
 72%|███████▏  | 160/221 [01:37<00:49,  1.22it/s][A
 73%|███████▎  | 161/221 [01:38<00:39,  1.53it/s][A
 74%|███████▍  | 163/221 [01:38<00:26,  2.19it/s][A
 74%|███████▍  | 164/221 [01:38<00:23,  2.44it/s][A
 75%|███████▍  | 165/221 [01:40<00:34,  1.62it/s][A
 75%|███████▌  | 166/221 [01:41<00:47,  1.15it/s][A
 76%|███████▌  | 167/221 [01:42<00:40,  1.32it/s][A
 76%|███████▌  | 168/221 [01:48<02:13,  2.52s/it][A
 76%|███████▋  | 169/221 [01:49<01:37,  1.88s/it][A
 77%|███████▋  | 170/221 [01:49<01:13,  1.45s/it][A
 77%|███████▋  | 171/221 [01:50<00:56,  1.12s/it][A
 78%|███████▊  | 172/221 [01:50<00:46,  1.06it/s][A
 78%|███████▊  | 173/221 [01:51<00:40,  1.19it/s][A
 79%|███████▊  | 174/221 [01:51<00:31,  1.48it/s][A
 79%|███████▉  | 175/221 [01:51<00:27,  1.69it/s][A
 80%|███████▉  | 176/221 [01:52<00:23,  1.90it/s][A
 80%|████████  | 177/221 [01:52<00:19,  2.32it/s][A
 81%|████████  | 178/221 [01:52<00:16,  2.55it/s][A
 81%|████████  | 179/221 [01:53<00:17,  2.35it/s][A
 81%|████████▏ | 180/221 [01:53<00:14,  2.90it/s][A
 82%|████████▏ | 182/221 [01:53<00:12,  3.07it/s][A
 83%|████████▎ | 183/221 [01:55<00:20,  1.84it/s][A
 83%|████████▎ | 184/221 [01:55<00:21,  1.71it/s][A
 84%|████████▍ | 186/221 [01:56<00:15,  2.23it/s][A
 85%|████████▍ | 187/221 [01:56<00:14,  2.39it/s][A
 85%|████████▌ | 188/221 [01:57<00:13,  2.52it/s][A
 86%|████████▌ | 189/221 [01:57<00:10,  3.04it/s][A
 86%|████████▌ | 190/221 [01:57<00:10,  2.98it/s][A
 86%|████████▋ | 191/221 [01:57<00:09,  3.06it/s][A
 87%|████████▋ | 192/221 [01:58<00:13,  2.19it/s][A
 87%|████████▋ | 193/221 [01:58<00:10,  2.71it/s][A
 88%|████████▊ | 194/221 [01:59<00:11,  2.29it/s][A
 88%|████████▊ | 195/221 [01:59<00:11,  2.35it/s][A
 89%|████████▊ | 196/221 [01:59<00:08,  2.87it/s][A
 89%|████████▉ | 197/221 [02:00<00:08,  2.88it/s][A
 90%|████████▉ | 198/221 [02:00<00:09,  2.35it/s][A
 90%|█████████ | 199/221 [02:01<00:07,  2.86it/s][A
 90%|█████████ | 200/221 [02:01<00:07,  2.84it/s][A
 91%|█████████ | 201/221 [02:01<00:07,  2.58it/s][A
 91%|█████████▏| 202/221 [02:02<00:07,  2.52it/s][A
 92%|█████████▏| 203/221 [02:03<00:11,  1.54it/s][A
 92%|█████████▏| 204/221 [02:03<00:09,  1.78it/s][A
 93%|█████████▎| 205/221 [02:04<00:06,  2.30it/s][A
 93%|█████████▎| 206/221 [02:04<00:06,  2.18it/s][A
 94%|█████████▍| 208/221 [02:04<00:04,  3.01it/s][A
 95%|█████████▍| 209/221 [02:05<00:03,  3.47it/s][A
 95%|█████████▌| 211/221 [02:06<00:04,  2.42it/s][A
 96%|█████████▌| 212/221 [02:06<00:03,  2.38it/s][A
 96%|█████████▋| 213/221 [02:07<00:03,  2.44it/s][A
 97%|█████████▋| 214/221 [02:07<00:03,  2.30it/s][A
 97%|█████████▋| 215/221 [02:08<00:02,  2.24it/s][A
 98%|█████████▊| 216/221 [02:08<00:02,  2.31it/s][A
 98%|█████████▊| 217/221 [02:10<00:03,  1.22it/s][A
 99%|█████████▊| 218/221 [02:10<00:02,  1.37it/s][A
 99%|█████████▉| 219/221 [02:11<00:01,  1.71it/s][A
100%|█████████▉| 220/221 [02:13<00:01,  1.25s/it][A
100%|██████████| 221/221 [02:14<00:00,  1.01it/s][A100%|██████████| 221/221 [02:14<00:00,  1.65it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:38,  5.74it/s][A
  1%|          | 2/221 [00:00<02:01,  1.80it/s][A
  1%|▏         | 3/221 [00:01<01:54,  1.90it/s][A
  2%|▏         | 4/221 [00:01<01:38,  2.21it/s][A
  2%|▏         | 5/221 [00:02<02:23,  1.50it/s][A
  3%|▎         | 6/221 [00:03<01:51,  1.93it/s][A
  3%|▎         | 7/221 [00:03<02:03,  1.74it/s][A
  4%|▎         | 8/221 [00:04<01:45,  2.03it/s][A
  4%|▍         | 9/221 [00:04<01:45,  2.00it/s][A
  5%|▍         | 10/221 [00:04<01:34,  2.24it/s][A
  5%|▍         | 11/221 [00:05<01:31,  2.29it/s][A
  5%|▌         | 12/221 [00:05<01:42,  2.03it/s][A
  6%|▌         | 13/221 [00:06<01:24,  2.45it/s][A
  6%|▋         | 14/221 [00:06<01:36,  2.15it/s][A
  7%|▋         | 15/221 [00:07<01:28,  2.33it/s][A
  7%|▋         | 16/221 [00:07<01:42,  2.00it/s][A
  8%|▊         | 17/221 [00:08<01:41,  2.01it/s][A
  8%|▊         | 18/221 [00:08<01:31,  2.21it/s][A
  9%|▊         | 19/221 [00:09<01:38,  2.05it/s][A
  9%|▉         | 20/221 [00:09<01:24,  2.39it/s][A
 10%|▉         | 21/221 [00:09<01:29,  2.24it/s][A
 10%|▉         | 22/221 [00:11<02:28,  1.34it/s][A
 10%|█         | 23/221 [00:11<01:52,  1.77it/s][A
 11%|█         | 24/221 [00:11<01:29,  2.21it/s][A
 11%|█▏        | 25/221 [00:12<01:46,  1.84it/s][A
 12%|█▏        | 26/221 [00:12<01:41,  1.92it/s][A
 12%|█▏        | 27/221 [00:13<01:31,  2.12it/s][A
 13%|█▎        | 28/221 [00:14<02:21,  1.36it/s][A
 13%|█▎        | 29/221 [00:15<02:51,  1.12it/s][A
 14%|█▎        | 30/221 [00:16<02:42,  1.18it/s][A
 14%|█▍        | 31/221 [00:17<02:19,  1.36it/s][A
 14%|█▍        | 32/221 [00:17<02:06,  1.50it/s][A
 15%|█▍        | 33/221 [00:18<01:53,  1.66it/s][A
 15%|█▌        | 34/221 [00:18<02:00,  1.55it/s][A
 16%|█▌        | 35/221 [00:19<01:34,  1.96it/s][A
 16%|█▋        | 36/221 [00:19<01:18,  2.35it/s][A
 17%|█▋        | 37/221 [00:19<01:25,  2.16it/s][A
 17%|█▋        | 38/221 [00:20<01:27,  2.08it/s][A
 18%|█▊        | 39/221 [00:20<01:34,  1.92it/s][A
 18%|█▊        | 40/221 [00:21<01:41,  1.78it/s][A
 19%|█▊        | 41/221 [00:21<01:24,  2.13it/s][A
 19%|█▉        | 42/221 [00:22<01:08,  2.60it/s][A
 19%|█▉        | 43/221 [00:22<01:03,  2.78it/s][A
 20%|█▉        | 44/221 [00:22<00:52,  3.35it/s][A
 20%|██        | 45/221 [00:22<00:48,  3.59it/s][A
 21%|██        | 46/221 [00:23<00:58,  3.02it/s][A
 21%|██▏       | 47/221 [00:23<01:17,  2.24it/s][A
 22%|██▏       | 48/221 [00:24<01:09,  2.48it/s][A
 22%|██▏       | 49/221 [00:24<01:14,  2.30it/s][A
 23%|██▎       | 50/221 [00:25<01:20,  2.13it/s][A
 23%|██▎       | 51/221 [00:25<01:21,  2.10it/s][A
 24%|██▎       | 52/221 [00:26<01:09,  2.43it/s][A
 24%|██▍       | 53/221 [00:26<01:14,  2.27it/s][A
 24%|██▍       | 54/221 [00:27<01:19,  2.11it/s][A
 25%|██▍       | 55/221 [00:27<01:18,  2.12it/s][A
 25%|██▌       | 56/221 [00:27<01:11,  2.32it/s][A
 26%|██▌       | 57/221 [00:28<01:07,  2.42it/s][A
 26%|██▌       | 58/221 [00:28<00:57,  2.85it/s][A
 27%|██▋       | 60/221 [00:29<00:54,  2.96it/s][A
 28%|██▊       | 61/221 [00:29<00:52,  3.02it/s][A
 28%|██▊       | 62/221 [00:29<00:56,  2.81it/s][A
 29%|██▊       | 63/221 [00:30<01:14,  2.12it/s][A
 29%|██▉       | 64/221 [00:32<01:53,  1.38it/s][A
 29%|██▉       | 65/221 [00:32<01:46,  1.46it/s][A
 30%|██▉       | 66/221 [00:33<01:34,  1.64it/s][A
 30%|███       | 67/221 [00:34<02:20,  1.10it/s][A
 31%|███       | 68/221 [00:34<01:45,  1.45it/s][A
 31%|███       | 69/221 [00:34<01:18,  1.94it/s][A
 32%|███▏      | 70/221 [00:35<01:14,  2.03it/s][A
 32%|███▏      | 71/221 [00:35<01:14,  2.02it/s][A
 33%|███▎      | 72/221 [00:36<01:00,  2.47it/s][A
 33%|███▎      | 73/221 [00:36<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:37<01:15,  1.96it/s][A
 34%|███▍      | 75/221 [00:37<01:02,  2.34it/s][A
 34%|███▍      | 76/221 [00:38<01:02,  2.33it/s][A
 35%|███▍      | 77/221 [00:38<01:07,  2.15it/s][A
 35%|███▌      | 78/221 [00:39<01:13,  1.95it/s][A
 36%|███▌      | 79/221 [00:40<01:38,  1.44it/s][A
 36%|███▌      | 80/221 [00:40<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:41<01:30,  1.55it/s][A
 37%|███▋      | 82/221 [00:41<01:20,  1.74it/s][A
 38%|███▊      | 83/221 [00:42<01:31,  1.50it/s][A
 38%|███▊      | 84/221 [00:44<02:01,  1.13it/s][A
 38%|███▊      | 85/221 [00:44<01:46,  1.28it/s][A
 39%|███▉      | 86/221 [00:45<01:35,  1.41it/s][A
 39%|███▉      | 87/221 [00:45<01:19,  1.68it/s][A
 40%|███▉      | 88/221 [00:45<01:15,  1.76it/s][A
 40%|████      | 89/221 [00:46<01:29,  1.48it/s][A
 41%|████      | 90/221 [00:47<01:18,  1.68it/s][A
 41%|████      | 91/221 [00:47<01:02,  2.08it/s][A
 42%|████▏     | 92/221 [00:47<00:58,  2.20it/s][A
 43%|████▎     | 94/221 [00:48<00:41,  3.08it/s][A
 43%|████▎     | 95/221 [00:48<00:48,  2.59it/s][A
 43%|████▎     | 96/221 [00:49<00:52,  2.39it/s][A
 44%|████▍     | 97/221 [00:49<00:53,  2.31it/s][A
 44%|████▍     | 98/221 [00:50<01:13,  1.68it/s][A
 45%|████▍     | 99/221 [00:51<01:17,  1.58it/s][A
 45%|████▌     | 100/221 [00:52<01:28,  1.37it/s][A
 46%|████▌     | 101/221 [00:53<01:44,  1.15it/s][A
 46%|████▌     | 102/221 [00:54<01:48,  1.09it/s][A
 47%|████▋     | 103/221 [00:54<01:21,  1.46it/s][A
 47%|████▋     | 104/221 [00:55<01:21,  1.44it/s][A
 48%|████▊     | 105/221 [00:55<01:05,  1.78it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.90it/s][A
 48%|████▊     | 107/221 [00:56<00:53,  2.12it/s][A
 49%|████▉     | 108/221 [00:57<01:01,  1.83it/s][A
 49%|████▉     | 109/221 [00:57<00:55,  2.02it/s][A
 50%|████▉     | 110/221 [00:58<01:00,  1.83it/s][A
 50%|█████     | 111/221 [00:59<01:13,  1.49it/s][A
 51%|█████     | 112/221 [00:59<01:01,  1.76it/s][A
 51%|█████     | 113/221 [01:00<00:54,  1.99it/s][A
 52%|█████▏    | 114/221 [01:00<00:53,  2.00it/s][A
 52%|█████▏    | 115/221 [01:01<00:59,  1.77it/s][A
 52%|█████▏    | 116/221 [01:01<00:50,  2.07it/s][A
 53%|█████▎    | 117/221 [01:02<00:54,  1.91it/s][A
 53%|█████▎    | 118/221 [01:02<00:52,  1.98it/s][A
 54%|█████▍    | 119/221 [01:03<00:54,  1.87it/s][A
 54%|█████▍    | 120/221 [01:03<00:48,  2.08it/s][A
 55%|█████▍    | 121/221 [01:04<00:56,  1.77it/s][A
 55%|█████▌    | 122/221 [01:04<00:54,  1.83it/s][A
 56%|█████▌    | 123/221 [01:05<00:52,  1.86it/s][A
 56%|█████▌    | 124/221 [01:06<01:04,  1.49it/s][A
 57%|█████▋    | 125/221 [01:07<01:08,  1.40it/s][A
 57%|█████▋    | 126/221 [01:07<00:54,  1.75it/s][A
 57%|█████▋    | 127/221 [01:08<00:56,  1.67it/s][A
 58%|█████▊    | 128/221 [01:08<00:55,  1.67it/s][A
 58%|█████▊    | 129/221 [01:09<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:09<00:40,  2.22it/s][A
 59%|█████▉    | 131/221 [01:09<00:45,  1.96it/s][A
 60%|█████▉    | 132/221 [01:10<00:41,  2.14it/s][A
 60%|██████    | 133/221 [01:10<00:35,  2.51it/s][A
 61%|██████    | 134/221 [01:11<00:44,  1.95it/s][A
 61%|██████    | 135/221 [01:11<00:39,  2.20it/s][A
 62%|██████▏   | 136/221 [01:12<00:37,  2.27it/s][A
 62%|██████▏   | 137/221 [01:12<00:41,  2.05it/s][A
 62%|██████▏   | 138/221 [01:13<00:43,  1.90it/s][A
 63%|██████▎   | 139/221 [01:14<00:57,  1.44it/s][A
 63%|██████▎   | 140/221 [01:15<00:57,  1.40it/s][A
 64%|██████▍   | 141/221 [01:15<00:51,  1.54it/s][A
 64%|██████▍   | 142/221 [01:16<00:49,  1.59it/s][A
 65%|██████▍   | 143/221 [01:16<00:43,  1.77it/s][A
 65%|██████▌   | 144/221 [01:17<00:39,  1.93it/s][A
 66%|██████▌   | 145/221 [01:17<00:30,  2.46it/s][A
 66%|██████▌   | 146/221 [01:17<00:29,  2.57it/s][A
 67%|██████▋   | 147/221 [01:17<00:22,  3.30it/s][A
 67%|██████▋   | 148/221 [01:17<00:22,  3.19it/s][A
 67%|██████▋   | 149/221 [01:18<00:23,  3.07it/s][A
 68%|██████▊   | 150/221 [01:18<00:25,  2.76it/s][A
 68%|██████▊   | 151/221 [01:18<00:20,  3.38it/s][A
 69%|██████▉   | 152/221 [01:19<00:32,  2.10it/s][A
 69%|██████▉   | 153/221 [01:20<00:28,  2.42it/s][A
 70%|██████▉   | 154/221 [01:20<00:31,  2.13it/s][A
 70%|███████   | 155/221 [01:21<00:31,  2.13it/s][A
 71%|███████   | 156/221 [01:21<00:26,  2.49it/s][A
 71%|███████   | 157/221 [01:21<00:27,  2.31it/s][A
 71%|███████▏  | 158/221 [01:22<00:28,  2.24it/s][A
 72%|███████▏  | 159/221 [01:22<00:23,  2.66it/s][A
 72%|███████▏  | 160/221 [01:22<00:21,  2.81it/s][A
 73%|███████▎  | 161/221 [01:23<00:18,  3.22it/s][A
 73%|███████▎  | 162/221 [01:23<00:15,  3.91it/s][A
 74%|███████▍  | 163/221 [01:24<00:27,  2.09it/s][A
 74%|███████▍  | 164/221 [01:24<00:21,  2.60it/s][A
 75%|███████▍  | 165/221 [01:24<00:21,  2.59it/s][A
 75%|███████▌  | 166/221 [01:26<00:35,  1.54it/s][A
 76%|███████▌  | 167/221 [01:26<00:30,  1.77it/s][A
 76%|███████▌  | 168/221 [01:26<00:27,  1.94it/s][A
 76%|███████▋  | 169/221 [01:27<00:24,  2.12it/s][A
 77%|███████▋  | 170/221 [01:27<00:24,  2.11it/s][A
 77%|███████▋  | 171/221 [01:28<00:29,  1.71it/s][A
 78%|███████▊  | 172/221 [01:29<00:27,  1.79it/s][A
 78%|███████▊  | 173/221 [01:29<00:25,  1.88it/s][A
 79%|███████▊  | 174/221 [01:29<00:21,  2.20it/s][A
 79%|███████▉  | 175/221 [01:30<00:21,  2.19it/s][A
 80%|███████▉  | 176/221 [01:30<00:18,  2.39it/s][A
 80%|████████  | 177/221 [01:31<00:21,  2.05it/s][A
 81%|████████  | 178/221 [01:31<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:32<00:19,  2.12it/s][A
 81%|████████▏ | 180/221 [01:32<00:17,  2.37it/s][A
 82%|████████▏ | 181/221 [01:32<00:17,  2.23it/s][A
 82%|████████▏ | 182/221 [01:33<00:19,  1.99it/s][A
 83%|████████▎ | 183/221 [01:34<00:22,  1.70it/s][A
 83%|████████▎ | 184/221 [01:36<00:33,  1.10it/s][A
 84%|████████▎ | 185/221 [01:36<00:27,  1.32it/s][A
 84%|████████▍ | 186/221 [01:36<00:22,  1.57it/s][A
 85%|████████▍ | 187/221 [01:37<00:22,  1.50it/s][A
 85%|████████▌ | 188/221 [01:38<00:29,  1.11it/s][A
 86%|████████▌ | 189/221 [01:39<00:23,  1.36it/s][A
 86%|████████▌ | 190/221 [01:39<00:21,  1.42it/s][A
 86%|████████▋ | 191/221 [01:40<00:23,  1.26it/s][A
 87%|████████▋ | 192/221 [01:41<00:19,  1.48it/s][A
 87%|████████▋ | 193/221 [01:41<00:15,  1.82it/s][A
 88%|████████▊ | 194/221 [01:42<00:15,  1.72it/s][A
 88%|████████▊ | 195/221 [01:43<00:17,  1.53it/s][A
 89%|████████▊ | 196/221 [01:43<00:12,  1.93it/s][A
 89%|████████▉ | 197/221 [01:43<00:13,  1.81it/s][A
 90%|████████▉ | 198/221 [01:44<00:13,  1.68it/s][A
 90%|█████████ | 199/221 [01:44<00:11,  1.95it/s][A
 90%|█████████ | 200/221 [01:45<00:13,  1.60it/s][A
 91%|█████████ | 201/221 [01:46<00:12,  1.58it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.78it/s][A
 92%|█████████▏| 203/221 [01:47<00:08,  2.04it/s][A
 92%|█████████▏| 204/221 [01:47<00:06,  2.49it/s][A
 93%|█████████▎| 205/221 [01:47<00:05,  2.83it/s][A
 93%|█████████▎| 206/221 [01:48<00:05,  2.67it/s][A
 94%|█████████▎| 207/221 [01:48<00:05,  2.56it/s][A
 94%|█████████▍| 208/221 [01:49<00:07,  1.73it/s][A
 95%|█████████▍| 209/221 [01:49<00:05,  2.15it/s][A
 95%|█████████▌| 210/221 [01:49<00:04,  2.74it/s][A
 95%|█████████▌| 211/221 [01:50<00:03,  2.57it/s][A
 96%|█████████▌| 212/221 [01:50<00:03,  2.90it/s][A
 96%|█████████▋| 213/221 [01:51<00:03,  2.09it/s][A
 97%|█████████▋| 214/221 [01:52<00:03,  1.76it/s][A
 97%|█████████▋| 215/221 [01:52<00:03,  1.85it/s][A
 98%|█████████▊| 216/221 [01:53<00:02,  1.86it/s][A
 98%|█████████▊| 217/221 [01:53<00:01,  2.05it/s][A
 99%|█████████▊| 218/221 [01:53<00:01,  2.58it/s][A
 99%|█████████▉| 219/221 [01:53<00:00,  3.01it/s][A
100%|█████████▉| 220/221 [01:54<00:00,  3.13it/s][A
100%|██████████| 221/221 [01:55<00:00,  1.62it/s][A100%|██████████| 221/221 [01:55<00:00,  1.91it/s]
09/19/2024 03:35:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 999--===========

09/19/2024 03:35:34 - INFO - __main__ -   {'area_r1': 43.4, 'area_recall': '43.4/72.2/80.8', 'area_ravg': 65.5}
09/19/2024 03:35:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 999--===========

09/19/2024 03:35:34 - INFO - __main__ -   {'forward_r1': 49.1, 'forward_recall': '49.1/77.4/85.7', 'forward_ravg': 70.7}
09/19/2024 03:35:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 999--===========

09/19/2024 03:35:34 - INFO - __main__ -   {'area_video_r1': 47.2, 'area_video_recall': '47.2/77.5/85.4', 'area_video_ravg': 70.0}
09/19/2024 03:35:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 999=======

09/19/2024 03:35:34 - INFO - __main__ -   {'area_video_r1': 47.2, 'area_video_recall': '47.2/77.5/85.4', 'area_video_ravg': 70.0}
09/19/2024 03:35:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 999--===========

09/19/2024 03:35:34 - INFO - __main__ -   {'area_video_r1': 59.6, 'area_video_recall': '59.6/80.8/86.5', 'area_video_ravg': 75.6, 'area_video_back_r1': 58.7, 'area_video_back_recall': '58.7/82.1/89.6', 'area_video_back_ravg': 76.8}
09/19/2024 03:35:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 999=======

09/19/2024 03:35:34 - INFO - __main__ -   {'area_video_r1': 59.6, 'area_video_recall': '59.6/80.8/86.5', 'area_video_ravg': 75.6, 'area_video_back_r1': 58.7, 'area_video_back_recall': '58.7/82.1/89.6', 'area_video_back_ravg': 76.8}
09/19/2024 03:35:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 999--===========

09/19/2024 03:35:34 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 03:35:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 03:35:34 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 03:35:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 999--===========

09/19/2024 03:35:34 - INFO - __main__ -   {'video_r1': 58.3, 'video_recall': '58.3/79.1/85.2', 'video_ravg': 74.2}
09/19/2024 03:35:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 999=======

09/19/2024 03:35:34 - INFO - __main__ -   {'video_r1': 58.3, 'video_recall': '58.3/79.1/85.2', 'video_ravg': 74.2}
09/19/2024 03:36:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.017038565129041672, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3044734001159668, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.32151198387146}
 11%|█         | 1000/8917 [1:21:45<392:14:03, 178.36s/it] 11%|█         | 1001/8917 [1:21:48<276:40:20, 125.82s/it] 11%|█         | 1002/8917 [1:21:51<195:57:32, 89.13s/it]  11%|█         | 1003/8917 [1:21:55<139:39:20, 63.53s/it] 11%|█▏        | 1004/8917 [1:21:59<100:07:40, 45.55s/it] 11%|█▏        | 1005/8917 [1:22:02<72:25:32, 32.95s/it]  11%|█▏        | 1006/8917 [1:22:06<53:10:06, 24.19s/it] 11%|█▏        | 1007/8917 [1:22:10<39:38:59, 18.05s/it] 11%|█▏        | 1008/8917 [1:22:13<29:59:44, 13.65s/it] 11%|█▏        | 1009/8917 [1:22:17<23:36:26, 10.75s/it] 11%|█▏        | 1010/8917 [1:22:21<18:57:14,  8.63s/it] 11%|█▏        | 1011/8917 [1:22:25<15:41:08,  7.14s/it] 11%|█▏        | 1012/8917 [1:22:28<13:22:42,  6.09s/it] 11%|█▏        | 1013/8917 [1:22:32<11:40:41,  5.32s/it] 11%|█▏        | 1014/8917 [1:22:36<10:40:30,  4.86s/it] 11%|█▏        | 1015/8917 [1:22:39<9:57:52,  4.54s/it]  11%|█▏        | 1016/8917 [1:22:43<9:26:45,  4.30s/it] 11%|█▏        | 1017/8917 [1:22:47<9:03:26,  4.13s/it] 11%|█▏        | 1018/8917 [1:22:50<8:39:44,  3.95s/it] 11%|█▏        | 1019/8917 [1:22:54<8:27:44,  3.86s/it] 11%|█▏        | 1020/8917 [1:22:58<8:34:01,  3.91s/it] 11%|█▏        | 1021/8917 [1:23:02<8:25:50,  3.84s/it] 11%|█▏        | 1022/8917 [1:23:05<8:04:45,  3.68s/it] 11%|█▏        | 1023/8917 [1:23:09<8:30:53,  3.88s/it] 11%|█▏        | 1024/8917 [1:23:13<8:19:50,  3.80s/it] 11%|█▏        | 1025/8917 [1:23:16<8:09:45,  3.72s/it] 12%|█▏        | 1026/8917 [1:23:20<7:58:21,  3.64s/it] 12%|█▏        | 1027/8917 [1:23:24<8:07:54,  3.71s/it] 12%|█▏        | 1028/8917 [1:23:28<8:09:52,  3.73s/it] 12%|█▏        | 1029/8917 [1:23:32<8:26:58,  3.86s/it] 12%|█▏        | 1030/8917 [1:23:35<8:15:09,  3.77s/it] 12%|█▏        | 1031/8917 [1:23:39<7:54:51,  3.61s/it] 12%|█▏        | 1032/8917 [1:23:42<7:58:49,  3.64s/it] 12%|█▏        | 1033/8917 [1:23:46<8:00:35,  3.66s/it] 12%|█▏        | 1034/8917 [1:23:49<7:54:28,  3.61s/it] 12%|█▏        | 1035/8917 [1:23:53<7:59:09,  3.65s/it] 12%|█▏        | 1036/8917 [1:23:57<7:59:21,  3.65s/it] 12%|█▏        | 1037/8917 [1:24:01<8:05:38,  3.70s/it] 12%|█▏        | 1038/8917 [1:24:05<8:27:32,  3.86s/it] 12%|█▏        | 1039/8917 [1:24:09<8:27:20,  3.86s/it] 12%|█▏        | 1040/8917 [1:24:12<8:12:02,  3.75s/it] 12%|█▏        | 1041/8917 [1:24:16<8:00:33,  3.66s/it] 12%|█▏        | 1042/8917 [1:24:19<7:54:21,  3.61s/it] 12%|█▏        | 1043/8917 [1:24:23<7:58:16,  3.64s/it] 12%|█▏        | 1044/8917 [1:24:26<7:55:14,  3.62s/it] 12%|█▏        | 1045/8917 [1:24:30<7:55:37,  3.63s/it] 12%|█▏        | 1046/8917 [1:24:34<7:52:37,  3.60s/it] 12%|█▏        | 1047/8917 [1:24:37<7:59:22,  3.65s/it] 12%|█▏        | 1048/8917 [1:24:41<7:59:55,  3.66s/it] 12%|█▏        | 1049/8917 [1:24:45<8:01:56,  3.68s/it]09/19/2024 03:39:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0313122533261776, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.491248369216919, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5225605964660645}
 12%|█▏        | 1050/8917 [1:24:49<8:06:46,  3.71s/it] 12%|█▏        | 1051/8917 [1:24:52<7:58:51,  3.65s/it] 12%|█▏        | 1052/8917 [1:24:56<8:00:39,  3.67s/it] 12%|█▏        | 1053/8917 [1:25:00<8:01:32,  3.67s/it] 12%|█▏        | 1054/8917 [1:25:03<8:03:32,  3.69s/it] 12%|█▏        | 1055/8917 [1:25:07<7:54:52,  3.62s/it] 12%|█▏        | 1056/8917 [1:25:10<7:51:25,  3.60s/it] 12%|█▏        | 1057/8917 [1:25:14<8:14:04,  3.77s/it] 12%|█▏        | 1058/8917 [1:25:18<8:10:15,  3.74s/it] 12%|█▏        | 1059/8917 [1:25:22<8:02:38,  3.69s/it] 12%|█▏        | 1060/8917 [1:25:25<7:53:24,  3.62s/it] 12%|█▏        | 1061/8917 [1:25:29<7:59:20,  3.66s/it] 12%|█▏        | 1062/8917 [1:25:32<7:50:29,  3.59s/it] 12%|█▏        | 1063/8917 [1:25:36<7:51:26,  3.60s/it] 12%|█▏        | 1064/8917 [1:25:40<7:51:42,  3.60s/it] 12%|█▏        | 1065/8917 [1:25:43<7:47:14,  3.57s/it] 12%|█▏        | 1066/8917 [1:25:47<7:47:08,  3.57s/it] 12%|█▏        | 1067/8917 [1:25:50<7:53:25,  3.62s/it] 12%|█▏        | 1068/8917 [1:25:54<7:55:50,  3.64s/it] 12%|█▏        | 1069/8917 [1:25:58<8:14:47,  3.78s/it] 12%|█▏        | 1070/8917 [1:26:02<8:06:18,  3.72s/it] 12%|█▏        | 1071/8917 [1:26:05<8:04:08,  3.70s/it] 12%|█▏        | 1072/8917 [1:26:09<7:52:54,  3.62s/it] 12%|█▏        | 1073/8917 [1:26:13<7:57:37,  3.65s/it] 12%|█▏        | 1074/8917 [1:26:16<7:59:21,  3.67s/it] 12%|█▏        | 1075/8917 [1:26:20<7:51:41,  3.61s/it] 12%|█▏        | 1076/8917 [1:26:24<8:05:01,  3.71s/it] 12%|█▏        | 1077/8917 [1:26:28<8:10:07,  3.75s/it] 12%|█▏        | 1078/8917 [1:26:31<8:04:00,  3.70s/it] 12%|█▏        | 1079/8917 [1:26:35<8:10:24,  3.75s/it] 12%|█▏        | 1080/8917 [1:26:39<8:10:51,  3.76s/it] 12%|█▏        | 1081/8917 [1:26:42<8:05:48,  3.72s/it] 12%|█▏        | 1082/8917 [1:26:46<8:10:08,  3.75s/it] 12%|█▏        | 1083/8917 [1:26:50<8:18:05,  3.81s/it] 12%|█▏        | 1084/8917 [1:26:54<8:14:35,  3.79s/it] 12%|█▏        | 1085/8917 [1:26:58<8:09:03,  3.75s/it] 12%|█▏        | 1086/8917 [1:27:01<8:02:23,  3.70s/it] 12%|█▏        | 1087/8917 [1:27:05<8:09:46,  3.75s/it] 12%|█▏        | 1088/8917 [1:27:09<8:06:50,  3.73s/it] 12%|█▏        | 1089/8917 [1:27:13<8:11:25,  3.77s/it] 12%|█▏        | 1090/8917 [1:27:16<8:07:09,  3.73s/it] 12%|█▏        | 1091/8917 [1:27:20<8:17:02,  3.81s/it] 12%|█▏        | 1092/8917 [1:27:24<8:13:45,  3.79s/it] 12%|█▏        | 1093/8917 [1:27:27<8:05:47,  3.73s/it] 12%|█▏        | 1094/8917 [1:27:31<8:07:46,  3.74s/it] 12%|█▏        | 1095/8917 [1:27:35<8:08:21,  3.75s/it] 12%|█▏        | 1096/8917 [1:27:39<8:17:15,  3.81s/it] 12%|█▏        | 1097/8917 [1:27:42<8:03:29,  3.71s/it] 12%|█▏        | 1098/8917 [1:27:46<7:49:37,  3.60s/it] 12%|█▏        | 1099/8917 [1:27:50<7:52:28,  3.63s/it]09/19/2024 03:42:27 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028581736609339714, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7584856748580933, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7870674133300781}
 12%|█▏        | 1100/8917 [1:27:53<7:50:50,  3.61s/it] 12%|█▏        | 1101/8917 [1:27:57<8:15:13,  3.80s/it] 12%|█▏        | 1102/8917 [1:28:01<8:08:59,  3.75s/it] 12%|█▏        | 1103/8917 [1:28:05<8:07:46,  3.75s/it] 12%|█▏        | 1104/8917 [1:28:08<8:02:33,  3.71s/it] 12%|█▏        | 1105/8917 [1:28:12<8:08:34,  3.75s/it] 12%|█▏        | 1106/8917 [1:28:16<8:02:17,  3.70s/it] 12%|█▏        | 1107/8917 [1:28:20<8:09:55,  3.76s/it] 12%|█▏        | 1108/8917 [1:28:23<8:11:49,  3.78s/it] 12%|█▏        | 1109/8917 [1:28:27<8:03:41,  3.72s/it] 12%|█▏        | 1110/8917 [1:28:31<8:02:54,  3.71s/it] 12%|█▏        | 1111/8917 [1:28:35<8:10:55,  3.77s/it] 12%|█▏        | 1112/8917 [1:28:38<8:06:59,  3.74s/it] 12%|█▏        | 1113/8917 [1:28:42<8:07:18,  3.75s/it] 12%|█▏        | 1114/8917 [1:28:46<8:03:43,  3.72s/it] 13%|█▎        | 1115/8917 [1:28:49<7:59:29,  3.69s/it] 13%|█▎        | 1116/8917 [1:28:53<8:01:38,  3.70s/it] 13%|█▎        | 1117/8917 [1:28:57<8:06:24,  3.74s/it] 13%|█▎        | 1118/8917 [1:29:00<7:55:46,  3.66s/it] 13%|█▎        | 1119/8917 [1:29:04<7:57:49,  3.68s/it] 13%|█▎        | 1120/8917 [1:29:08<8:08:25,  3.76s/it] 13%|█▎        | 1121/8917 [1:29:12<8:09:54,  3.77s/it] 13%|█▎        | 1122/8917 [1:29:16<8:10:01,  3.77s/it] 13%|█▎        | 1123/8917 [1:29:19<8:06:35,  3.75s/it] 13%|█▎        | 1124/8917 [1:29:23<8:17:29,  3.83s/it] 13%|█▎        | 1125/8917 [1:29:27<8:09:41,  3.77s/it] 13%|█▎        | 1126/8917 [1:29:31<8:18:54,  3.84s/it] 13%|█▎        | 1127/8917 [1:29:35<8:13:21,  3.80s/it] 13%|█▎        | 1128/8917 [1:29:39<8:17:55,  3.84s/it] 13%|█▎        | 1129/8917 [1:29:42<8:16:42,  3.83s/it] 13%|█▎        | 1130/8917 [1:29:46<8:03:37,  3.73s/it] 13%|█▎        | 1131/8917 [1:29:49<7:46:05,  3.59s/it] 13%|█▎        | 1132/8917 [1:29:53<7:58:02,  3.68s/it] 13%|█▎        | 1133/8917 [1:29:57<8:00:03,  3.70s/it] 13%|█▎        | 1134/8917 [1:30:00<7:54:19,  3.66s/it] 13%|█▎        | 1135/8917 [1:30:04<8:00:38,  3.71s/it] 13%|█▎        | 1136/8917 [1:30:08<7:58:37,  3.69s/it] 13%|█▎        | 1137/8917 [1:30:12<8:02:38,  3.72s/it] 13%|█▎        | 1138/8917 [1:30:16<8:11:55,  3.79s/it] 13%|█▎        | 1139/8917 [1:30:19<8:04:28,  3.74s/it] 13%|█▎        | 1140/8917 [1:30:23<8:16:59,  3.83s/it] 13%|█▎        | 1141/8917 [1:30:27<8:11:25,  3.79s/it] 13%|█▎        | 1142/8917 [1:30:30<7:52:04,  3.64s/it] 13%|█▎        | 1143/8917 [1:30:34<7:51:43,  3.64s/it] 13%|█▎        | 1144/8917 [1:30:37<7:48:26,  3.62s/it] 13%|█▎        | 1145/8917 [1:30:41<7:57:22,  3.69s/it] 13%|█▎        | 1146/8917 [1:30:45<8:00:01,  3.71s/it] 13%|█▎        | 1147/8917 [1:30:49<7:53:12,  3.65s/it] 13%|█▎        | 1148/8917 [1:30:52<7:48:24,  3.62s/it] 13%|█▎        | 1149/8917 [1:30:56<8:08:59,  3.78s/it]09/19/2024 03:45:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04269861429929733, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.389937400817871, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.432636022567749}
 13%|█▎        | 1150/8917 [1:31:00<8:00:15,  3.71s/it] 13%|█▎        | 1151/8917 [1:31:04<8:14:58,  3.82s/it] 13%|█▎        | 1152/8917 [1:31:07<8:03:30,  3.74s/it] 13%|█▎        | 1153/8917 [1:31:11<7:48:31,  3.62s/it] 13%|█▎        | 1154/8917 [1:31:15<7:56:48,  3.69s/it] 13%|█▎        | 1155/8917 [1:31:18<7:51:18,  3.64s/it] 13%|█▎        | 1156/8917 [1:31:22<7:57:49,  3.69s/it] 13%|█▎        | 1157/8917 [1:31:26<7:58:38,  3.70s/it] 13%|█▎        | 1158/8917 [1:31:29<7:59:14,  3.71s/it] 13%|█▎        | 1159/8917 [1:31:34<8:14:18,  3.82s/it] 13%|█▎        | 1160/8917 [1:31:37<8:01:43,  3.73s/it] 13%|█▎        | 1161/8917 [1:31:40<7:46:13,  3.61s/it] 13%|█▎        | 1162/8917 [1:31:45<8:10:58,  3.80s/it] 13%|█▎        | 1163/8917 [1:31:49<8:28:16,  3.93s/it] 13%|█▎        | 1164/8917 [1:31:53<8:18:25,  3.86s/it] 13%|█▎        | 1165/8917 [1:31:56<8:04:56,  3.75s/it] 13%|█▎        | 1166/8917 [1:32:00<8:09:41,  3.79s/it] 13%|█▎        | 1167/8917 [1:32:04<8:06:49,  3.77s/it] 13%|█▎        | 1168/8917 [1:32:08<8:24:46,  3.91s/it] 13%|█▎        | 1169/8917 [1:32:11<8:07:52,  3.78s/it] 13%|█▎        | 1170/8917 [1:32:15<8:05:44,  3.76s/it] 13%|█▎        | 1171/8917 [1:32:19<8:01:59,  3.73s/it] 13%|█▎        | 1172/8917 [1:32:23<8:06:14,  3.77s/it] 13%|█▎        | 1173/8917 [1:32:26<8:07:46,  3.78s/it] 13%|█▎        | 1174/8917 [1:32:30<8:10:46,  3.80s/it] 13%|█▎        | 1175/8917 [1:32:34<8:11:28,  3.81s/it] 13%|█▎        | 1176/8917 [1:32:38<8:12:42,  3.82s/it] 13%|█▎        | 1177/8917 [1:32:42<8:21:02,  3.88s/it] 13%|█▎        | 1178/8917 [1:32:45<8:06:28,  3.77s/it] 13%|█▎        | 1179/8917 [1:32:49<8:06:18,  3.77s/it] 13%|█▎        | 1180/8917 [1:32:53<7:58:51,  3.71s/it] 13%|█▎        | 1181/8917 [1:32:57<7:59:10,  3.72s/it] 13%|█▎        | 1182/8917 [1:33:00<7:57:37,  3.70s/it] 13%|█▎        | 1183/8917 [1:33:04<8:00:59,  3.73s/it] 13%|█▎        | 1184/8917 [1:33:08<8:05:12,  3.76s/it] 13%|█▎        | 1185/8917 [1:33:12<8:14:10,  3.83s/it] 13%|█▎        | 1186/8917 [1:33:16<8:12:51,  3.83s/it] 13%|█▎        | 1187/8917 [1:33:19<8:10:37,  3.81s/it] 13%|█▎        | 1188/8917 [1:33:23<8:07:58,  3.79s/it] 13%|█▎        | 1189/8917 [1:33:27<8:08:14,  3.79s/it] 13%|█▎        | 1190/8917 [1:33:31<8:04:52,  3.77s/it] 13%|█▎        | 1191/8917 [1:33:35<8:13:04,  3.83s/it] 13%|█▎        | 1192/8917 [1:33:39<8:14:43,  3.84s/it] 13%|█▎        | 1193/8917 [1:33:42<8:11:44,  3.82s/it] 13%|█▎        | 1194/8917 [1:33:46<8:01:49,  3.74s/it] 13%|█▎        | 1195/8917 [1:33:50<8:06:39,  3.78s/it] 13%|█▎        | 1196/8917 [1:33:53<8:02:08,  3.75s/it] 13%|█▎        | 1197/8917 [1:33:58<8:22:24,  3.90s/it] 13%|█▎        | 1198/8917 [1:34:01<8:09:51,  3.81s/it] 13%|█▎        | 1199/8917 [1:34:05<8:06:53,  3.79s/it]09/19/2024 03:48:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020343203097581863, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3214468955993652, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3417900800704956}
 13%|█▎        | 1200/8917 [1:34:09<7:59:16,  3.73s/it] 13%|█▎        | 1201/8917 [1:34:12<7:59:20,  3.73s/it] 13%|█▎        | 1202/8917 [1:34:16<7:56:34,  3.71s/it] 13%|█▎        | 1203/8917 [1:34:20<7:54:46,  3.69s/it] 14%|█▎        | 1204/8917 [1:34:23<7:57:41,  3.72s/it] 14%|█▎        | 1205/8917 [1:34:27<7:49:49,  3.66s/it] 14%|█▎        | 1206/8917 [1:34:30<7:43:37,  3.61s/it] 14%|█▎        | 1207/8917 [1:34:34<7:47:26,  3.64s/it] 14%|█▎        | 1208/8917 [1:34:38<7:46:53,  3.63s/it] 14%|█▎        | 1209/8917 [1:34:41<7:50:18,  3.66s/it] 14%|█▎        | 1210/8917 [1:34:45<7:55:11,  3.70s/it] 14%|█▎        | 1211/8917 [1:34:49<8:07:36,  3.80s/it] 14%|█▎        | 1212/8917 [1:34:53<8:00:42,  3.74s/it] 14%|█▎        | 1213/8917 [1:34:56<7:53:35,  3.69s/it] 14%|█▎        | 1214/8917 [1:35:00<7:47:35,  3.64s/it] 14%|█▎        | 1215/8917 [1:35:04<7:48:38,  3.65s/it] 14%|█▎        | 1216/8917 [1:35:07<7:46:14,  3.63s/it] 14%|█▎        | 1217/8917 [1:35:11<7:53:19,  3.69s/it] 14%|█▎        | 1218/8917 [1:35:15<7:57:16,  3.72s/it] 14%|█▎        | 1219/8917 [1:35:19<8:02:56,  3.76s/it] 14%|█▎        | 1220/8917 [1:35:22<7:55:42,  3.71s/it] 14%|█▎        | 1221/8917 [1:35:26<7:58:46,  3.73s/it] 14%|█▎        | 1222/8917 [1:35:30<8:01:07,  3.75s/it] 14%|█▎        | 1223/8917 [1:35:34<8:02:28,  3.76s/it] 14%|█▎        | 1224/8917 [1:35:38<8:11:40,  3.83s/it] 14%|█▎        | 1225/8917 [1:35:41<7:57:15,  3.72s/it] 14%|█▎        | 1226/8917 [1:35:45<7:48:18,  3.65s/it] 14%|█▍        | 1227/8917 [1:35:48<7:38:58,  3.58s/it] 14%|█▍        | 1228/8917 [1:35:52<7:43:58,  3.62s/it] 14%|█▍        | 1229/8917 [1:35:55<7:41:08,  3.60s/it] 14%|█▍        | 1230/8917 [1:35:59<7:54:52,  3.71s/it] 14%|█▍        | 1231/8917 [1:36:03<8:06:27,  3.80s/it] 14%|█▍        | 1232/8917 [1:36:07<8:04:23,  3.78s/it] 14%|█▍        | 1233/8917 [1:36:11<8:22:45,  3.93s/it] 14%|█▍        | 1234/8917 [1:36:15<8:20:16,  3.91s/it] 14%|█▍        | 1235/8917 [1:36:19<8:11:42,  3.84s/it] 14%|█▍        | 1236/8917 [1:36:23<8:09:05,  3.82s/it] 14%|█▍        | 1237/8917 [1:36:26<8:09:36,  3.83s/it] 14%|█▍        | 1238/8917 [1:36:30<8:10:57,  3.84s/it] 14%|█▍        | 1239/8917 [1:36:34<8:02:56,  3.77s/it] 14%|█▍        | 1240/8917 [1:36:38<7:59:35,  3.75s/it] 14%|█▍        | 1241/8917 [1:36:41<8:03:16,  3.78s/it] 14%|█▍        | 1242/8917 [1:36:45<7:55:04,  3.71s/it] 14%|█▍        | 1243/8917 [1:36:49<8:03:55,  3.78s/it] 14%|█▍        | 1244/8917 [1:36:52<7:50:13,  3.68s/it] 14%|█▍        | 1245/8917 [1:36:56<7:52:24,  3.69s/it] 14%|█▍        | 1246/8917 [1:37:00<8:04:08,  3.79s/it] 14%|█▍        | 1247/8917 [1:37:04<8:03:46,  3.78s/it] 14%|█▍        | 1248/8917 [1:37:08<8:13:02,  3.86s/it] 14%|█▍        | 1249/8917 [1:37:11<8:00:10,  3.76s/it]09/19/2024 03:51:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030418304726481438, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2342183589935303, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2646366357803345}
 14%|█▍        | 1250/8917 [1:37:15<8:03:27,  3.78s/it] 14%|█▍        | 1251/8917 [1:37:19<8:04:16,  3.79s/it] 14%|█▍        | 1252/8917 [1:37:23<7:54:49,  3.72s/it] 14%|█▍        | 1253/8917 [1:37:26<7:54:26,  3.71s/it] 14%|█▍        | 1254/8917 [1:37:30<7:58:43,  3.75s/it] 14%|█▍        | 1255/8917 [1:37:34<7:56:17,  3.73s/it] 14%|█▍        | 1256/8917 [1:37:37<7:46:01,  3.65s/it] 14%|█▍        | 1257/8917 [1:37:41<7:58:03,  3.74s/it] 14%|█▍        | 1258/8917 [1:37:45<8:05:41,  3.80s/it] 14%|█▍        | 1259/8917 [1:37:49<8:03:11,  3.79s/it] 14%|█▍        | 1260/8917 [1:37:53<8:05:26,  3.80s/it] 14%|█▍        | 1261/8917 [1:37:57<8:00:22,  3.76s/it] 14%|█▍        | 1262/8917 [1:38:00<7:58:33,  3.75s/it] 14%|█▍        | 1263/8917 [1:38:04<7:53:42,  3.71s/it] 14%|█▍        | 1264/8917 [1:38:08<8:04:25,  3.80s/it] 14%|█▍        | 1265/8917 [1:38:12<8:14:11,  3.88s/it] 14%|█▍        | 1266/8917 [1:38:16<8:06:32,  3.82s/it] 14%|█▍        | 1267/8917 [1:38:19<8:00:30,  3.77s/it] 14%|█▍        | 1268/8917 [1:38:23<7:50:03,  3.69s/it] 14%|█▍        | 1269/8917 [1:38:27<7:57:04,  3.74s/it] 14%|█▍        | 1270/8917 [1:38:30<7:49:57,  3.69s/it] 14%|█▍        | 1271/8917 [1:38:34<7:45:08,  3.65s/it] 14%|█▍        | 1272/8917 [1:38:37<7:49:23,  3.68s/it] 14%|█▍        | 1273/8917 [1:38:42<8:02:39,  3.79s/it] 14%|█▍        | 1274/8917 [1:38:45<8:03:20,  3.79s/it] 14%|█▍        | 1275/8917 [1:38:49<7:51:14,  3.70s/it] 14%|█▍        | 1276/8917 [1:38:53<7:53:27,  3.72s/it] 14%|█▍        | 1277/8917 [1:38:56<7:44:05,  3.64s/it] 14%|█▍        | 1278/8917 [1:39:00<8:01:59,  3.79s/it] 14%|█▍        | 1279/8917 [1:39:04<7:57:46,  3.75s/it] 14%|█▍        | 1280/8917 [1:39:07<7:49:20,  3.69s/it] 14%|█▍        | 1281/8917 [1:39:11<7:42:54,  3.64s/it] 14%|█▍        | 1282/8917 [1:39:15<8:01:15,  3.78s/it] 14%|█▍        | 1283/8917 [1:39:19<8:06:14,  3.82s/it] 14%|█▍        | 1284/8917 [1:39:23<8:00:41,  3.78s/it] 14%|█▍        | 1285/8917 [1:39:26<7:54:17,  3.73s/it] 14%|█▍        | 1286/8917 [1:39:30<7:51:51,  3.71s/it] 14%|█▍        | 1287/8917 [1:39:34<7:53:42,  3.73s/it] 14%|█▍        | 1288/8917 [1:39:37<7:56:24,  3.75s/it] 14%|█▍        | 1289/8917 [1:39:41<7:52:57,  3.72s/it] 14%|█▍        | 1290/8917 [1:39:45<7:47:43,  3.68s/it] 14%|█▍        | 1291/8917 [1:39:48<7:50:10,  3.70s/it] 14%|█▍        | 1292/8917 [1:39:52<7:59:51,  3.78s/it] 15%|█▍        | 1293/8917 [1:39:56<8:00:14,  3.78s/it] 15%|█▍        | 1294/8917 [1:40:00<7:47:36,  3.68s/it] 15%|█▍        | 1295/8917 [1:40:03<7:32:38,  3.56s/it] 15%|█▍        | 1296/8917 [1:40:07<7:33:42,  3.57s/it] 15%|█▍        | 1297/8917 [1:40:10<7:49:04,  3.69s/it] 15%|█▍        | 1298/8917 [1:40:14<7:56:01,  3.75s/it] 15%|█▍        | 1299/8917 [1:40:18<7:55:54,  3.75s/it]09/19/2024 03:54:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03465455770492554, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6811590194702148, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7158136367797852}
 15%|█▍        | 1300/8917 [1:40:22<7:54:56,  3.74s/it] 15%|█▍        | 1301/8917 [1:40:26<7:55:56,  3.75s/it] 15%|█▍        | 1302/8917 [1:40:29<7:54:25,  3.74s/it] 15%|█▍        | 1303/8917 [1:40:33<7:41:14,  3.63s/it] 15%|█▍        | 1304/8917 [1:40:37<7:57:04,  3.76s/it] 15%|█▍        | 1305/8917 [1:40:41<8:01:07,  3.79s/it] 15%|█▍        | 1306/8917 [1:40:44<7:50:15,  3.71s/it] 15%|█▍        | 1307/8917 [1:40:48<7:52:45,  3.73s/it] 15%|█▍        | 1308/8917 [1:40:52<7:56:53,  3.76s/it] 15%|█▍        | 1309/8917 [1:40:55<7:52:25,  3.73s/it] 15%|█▍        | 1310/8917 [1:40:59<7:37:46,  3.61s/it] 15%|█▍        | 1311/8917 [1:41:02<7:27:32,  3.53s/it] 15%|█▍        | 1312/8917 [1:41:06<7:45:04,  3.67s/it] 15%|█▍        | 1313/8917 [1:41:10<7:51:53,  3.72s/it] 15%|█▍        | 1314/8917 [1:41:13<7:46:13,  3.68s/it] 15%|█▍        | 1315/8917 [1:41:18<8:05:19,  3.83s/it] 15%|█▍        | 1316/8917 [1:41:21<8:00:23,  3.79s/it] 15%|█▍        | 1317/8917 [1:41:25<7:58:40,  3.78s/it] 15%|█▍        | 1318/8917 [1:41:29<7:56:06,  3.76s/it] 15%|█▍        | 1319/8917 [1:41:33<8:06:50,  3.84s/it] 15%|█▍        | 1320/8917 [1:41:37<8:04:21,  3.83s/it] 15%|█▍        | 1321/8917 [1:41:40<7:53:33,  3.74s/it] 15%|█▍        | 1322/8917 [1:41:44<7:52:01,  3.73s/it] 15%|█▍        | 1323/8917 [1:41:48<7:54:52,  3.75s/it] 15%|█▍        | 1324/8917 [1:41:51<7:49:34,  3.71s/it] 15%|█▍        | 1325/8917 [1:41:55<7:43:55,  3.67s/it] 15%|█▍        | 1326/8917 [1:41:59<7:46:35,  3.69s/it] 15%|█▍        | 1327/8917 [1:42:02<7:40:10,  3.64s/it] 15%|█▍        | 1328/8917 [1:42:06<7:56:03,  3.76s/it] 15%|█▍        | 1329/8917 [1:42:10<8:03:21,  3.82s/it] 15%|█▍        | 1330/8917 [1:42:14<8:00:18,  3.80s/it] 15%|█▍        | 1331/8917 [1:42:18<7:56:16,  3.77s/it] 15%|█▍        | 1332/8917 [1:42:21<7:52:43,  3.74s/it] 15%|█▍        | 1333/8917 [1:42:25<7:43:28,  3.67s/it] 15%|█▍        | 1334/8917 [1:42:28<7:44:35,  3.68s/it] 15%|█▍        | 1335/8917 [1:42:32<7:47:41,  3.70s/it] 15%|█▍        | 1336/8917 [1:42:36<7:44:21,  3.68s/it] 15%|█▍        | 1337/8917 [1:42:40<7:54:00,  3.75s/it] 15%|█▌        | 1338/8917 [1:42:43<7:45:24,  3.68s/it] 15%|█▌        | 1339/8917 [1:42:47<7:48:00,  3.71s/it] 15%|█▌        | 1340/8917 [1:42:51<7:47:06,  3.70s/it] 15%|█▌        | 1341/8917 [1:42:54<7:42:41,  3.66s/it] 15%|█▌        | 1342/8917 [1:42:58<7:44:16,  3.68s/it] 15%|█▌        | 1343/8917 [1:43:02<7:43:36,  3.67s/it] 15%|█▌        | 1344/8917 [1:43:05<7:36:21,  3.62s/it] 15%|█▌        | 1345/8917 [1:43:09<7:44:31,  3.68s/it] 15%|█▌        | 1346/8917 [1:43:13<7:42:17,  3.66s/it] 15%|█▌        | 1347/8917 [1:43:16<7:41:00,  3.65s/it] 15%|█▌        | 1348/8917 [1:43:20<7:45:58,  3.69s/it] 15%|█▌        | 1349/8917 [1:43:24<7:40:06,  3.65s/it]09/19/2024 03:58:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02776133082807064, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6306509971618652, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.6584123373031616}
 15%|█▌        | 1350/8917 [1:43:27<7:36:58,  3.62s/it] 15%|█▌        | 1351/8917 [1:43:31<7:46:56,  3.70s/it] 15%|█▌        | 1352/8917 [1:43:35<7:57:51,  3.79s/it] 15%|█▌        | 1353/8917 [1:43:39<7:59:05,  3.80s/it] 15%|█▌        | 1354/8917 [1:43:43<7:56:03,  3.78s/it] 15%|█▌        | 1355/8917 [1:43:46<7:54:38,  3.77s/it] 15%|█▌        | 1356/8917 [1:43:50<8:08:34,  3.88s/it] 15%|█▌        | 1357/8917 [1:43:54<7:56:38,  3.78s/it] 15%|█▌        | 1358/8917 [1:43:58<7:53:45,  3.76s/it] 15%|█▌        | 1359/8917 [1:44:01<7:53:17,  3.76s/it] 15%|█▌        | 1360/8917 [1:44:05<7:47:47,  3.71s/it] 15%|█▌        | 1361/8917 [1:44:09<7:52:19,  3.75s/it] 15%|█▌        | 1362/8917 [1:44:12<7:36:28,  3.63s/it] 15%|█▌        | 1363/8917 [1:44:16<7:36:09,  3.62s/it] 15%|█▌        | 1364/8917 [1:44:19<7:28:12,  3.56s/it] 15%|█▌        | 1365/8917 [1:44:23<7:38:53,  3.65s/it] 15%|█▌        | 1366/8917 [1:44:27<7:56:18,  3.78s/it] 15%|█▌        | 1367/8917 [1:44:31<7:51:30,  3.75s/it] 15%|█▌        | 1368/8917 [1:44:35<7:58:01,  3.80s/it] 15%|█▌        | 1369/8917 [1:44:39<7:54:50,  3.77s/it] 15%|█▌        | 1370/8917 [1:44:42<7:49:13,  3.73s/it] 15%|█▌        | 1371/8917 [1:44:46<7:54:46,  3.77s/it] 15%|█▌        | 1372/8917 [1:44:50<7:51:12,  3.75s/it] 15%|█▌        | 1373/8917 [1:44:54<8:02:04,  3.83s/it] 15%|█▌        | 1374/8917 [1:44:57<7:55:41,  3.78s/it] 15%|█▌        | 1375/8917 [1:45:01<7:42:28,  3.68s/it] 15%|█▌        | 1376/8917 [1:45:04<7:34:29,  3.62s/it] 15%|█▌        | 1377/8917 [1:45:08<7:35:21,  3.62s/it] 15%|█▌        | 1378/8917 [1:45:12<7:39:07,  3.65s/it] 15%|█▌        | 1379/8917 [1:45:15<7:41:49,  3.68s/it] 15%|█▌        | 1380/8917 [1:45:19<7:33:00,  3.61s/it] 15%|█▌        | 1381/8917 [1:45:23<7:52:07,  3.76s/it] 15%|█▌        | 1382/8917 [1:45:27<7:46:18,  3.71s/it] 16%|█▌        | 1383/8917 [1:45:31<7:54:57,  3.78s/it] 16%|█▌        | 1384/8917 [1:45:34<7:58:33,  3.81s/it] 16%|█▌        | 1385/8917 [1:45:38<8:07:12,  3.88s/it] 16%|█▌        | 1386/8917 [1:45:42<7:48:03,  3.73s/it] 16%|█▌        | 1387/8917 [1:45:46<8:00:11,  3.83s/it] 16%|█▌        | 1388/8917 [1:45:49<7:45:44,  3.71s/it] 16%|█▌        | 1389/8917 [1:45:53<7:43:28,  3.69s/it] 16%|█▌        | 1390/8917 [1:45:56<7:34:30,  3.62s/it] 16%|█▌        | 1391/8917 [1:46:00<7:45:44,  3.71s/it] 16%|█▌        | 1392/8917 [1:46:04<7:46:22,  3.72s/it] 16%|█▌        | 1393/8917 [1:46:08<7:44:35,  3.70s/it] 16%|█▌        | 1394/8917 [1:46:12<7:49:13,  3.74s/it] 16%|█▌        | 1395/8917 [1:46:15<7:41:42,  3.68s/it] 16%|█▌        | 1396/8917 [1:46:19<8:01:18,  3.84s/it] 16%|█▌        | 1397/8917 [1:46:23<7:55:03,  3.79s/it] 16%|█▌        | 1398/8917 [1:46:27<7:56:31,  3.80s/it] 16%|█▌        | 1399/8917 [1:46:30<7:45:58,  3.72s/it]09/19/2024 04:01:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02638986147940159, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.608557939529419, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.6349477767944336}
 16%|█▌        | 1400/8917 [1:46:34<7:44:12,  3.71s/it] 16%|█▌        | 1401/8917 [1:46:38<7:43:01,  3.70s/it] 16%|█▌        | 1402/8917 [1:46:41<7:36:35,  3.65s/it] 16%|█▌        | 1403/8917 [1:46:45<7:34:15,  3.63s/it] 16%|█▌        | 1404/8917 [1:46:48<7:33:10,  3.62s/it] 16%|█▌        | 1405/8917 [1:46:52<7:32:23,  3.61s/it] 16%|█▌        | 1406/8917 [1:46:56<7:34:19,  3.63s/it] 16%|█▌        | 1407/8917 [1:47:00<7:43:43,  3.70s/it] 16%|█▌        | 1408/8917 [1:47:03<7:43:38,  3.70s/it] 16%|█▌        | 1409/8917 [1:47:07<7:36:30,  3.65s/it] 16%|█▌        | 1410/8917 [1:47:11<7:41:44,  3.69s/it] 16%|█▌        | 1411/8917 [1:47:14<7:43:37,  3.71s/it] 16%|█▌        | 1412/8917 [1:47:18<7:50:07,  3.76s/it] 16%|█▌        | 1413/8917 [1:47:22<8:03:18,  3.86s/it] 16%|█▌        | 1414/8917 [1:47:26<8:04:10,  3.87s/it] 16%|█▌        | 1415/8917 [1:47:30<7:43:49,  3.71s/it] 16%|█▌        | 1416/8917 [1:47:33<7:48:29,  3.75s/it] 16%|█▌        | 1417/8917 [1:47:37<7:40:48,  3.69s/it] 16%|█▌        | 1418/8917 [1:47:41<7:50:42,  3.77s/it] 16%|█▌        | 1419/8917 [1:47:45<7:46:27,  3.73s/it] 16%|█▌        | 1420/8917 [1:47:49<7:54:35,  3.80s/it] 16%|█▌        | 1421/8917 [1:47:52<7:57:16,  3.82s/it] 16%|█▌        | 1422/8917 [1:47:56<7:47:05,  3.74s/it] 16%|█▌        | 1423/8917 [1:47:59<7:35:31,  3.65s/it] 16%|█▌        | 1424/8917 [1:48:03<7:38:34,  3.67s/it] 16%|█▌        | 1425/8917 [1:48:07<7:39:30,  3.68s/it] 16%|█▌        | 1426/8917 [1:48:11<7:52:36,  3.79s/it] 16%|█▌        | 1427/8917 [1:48:14<7:46:15,  3.74s/it] 16%|█▌        | 1428/8917 [1:48:18<7:46:30,  3.74s/it] 16%|█▌        | 1429/8917 [1:48:22<8:03:45,  3.88s/it] 16%|█▌        | 1430/8917 [1:48:26<7:54:47,  3.80s/it] 16%|█▌        | 1431/8917 [1:48:30<7:45:16,  3.73s/it] 16%|█▌        | 1432/8917 [1:48:34<7:54:07,  3.80s/it] 16%|█▌        | 1433/8917 [1:48:37<7:44:33,  3.72s/it] 16%|█▌        | 1434/8917 [1:48:41<7:42:30,  3.71s/it] 16%|█▌        | 1435/8917 [1:48:45<7:43:41,  3.72s/it] 16%|█▌        | 1436/8917 [1:48:48<7:43:22,  3.72s/it] 16%|█▌        | 1437/8917 [1:48:52<7:41:11,  3.70s/it] 16%|█▌        | 1438/8917 [1:48:56<7:41:49,  3.70s/it] 16%|█▌        | 1439/8917 [1:49:00<7:54:38,  3.81s/it] 16%|█▌        | 1440/8917 [1:49:03<7:54:47,  3.81s/it] 16%|█▌        | 1441/8917 [1:49:07<7:38:38,  3.68s/it] 16%|█▌        | 1442/8917 [1:49:11<7:49:35,  3.77s/it] 16%|█▌        | 1443/8917 [1:49:14<7:45:35,  3.74s/it] 16%|█▌        | 1444/8917 [1:49:18<7:40:04,  3.69s/it] 16%|█▌        | 1445/8917 [1:49:22<7:43:43,  3.72s/it] 16%|█▌        | 1446/8917 [1:49:25<7:39:56,  3.69s/it] 16%|█▌        | 1447/8917 [1:49:29<7:35:58,  3.66s/it] 16%|█▌        | 1448/8917 [1:49:33<7:30:08,  3.62s/it] 16%|█▌        | 1449/8917 [1:49:36<7:26:54,  3.59s/it]09/19/2024 04:04:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02184280939400196, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.253709316253662, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2755521535873413}
 16%|█▋        | 1450/8917 [1:49:40<7:41:20,  3.71s/it] 16%|█▋        | 1451/8917 [1:49:44<7:47:41,  3.76s/it] 16%|█▋        | 1452/8917 [1:49:48<7:51:53,  3.79s/it] 16%|█▋        | 1453/8917 [1:49:52<7:53:10,  3.80s/it] 16%|█▋        | 1454/8917 [1:49:55<7:41:40,  3.71s/it] 16%|█▋        | 1455/8917 [1:49:59<7:43:58,  3.73s/it] 16%|█▋        | 1456/8917 [1:50:03<7:49:53,  3.78s/it] 16%|█▋        | 1457/8917 [1:50:07<7:47:45,  3.76s/it] 16%|█▋        | 1458/8917 [1:50:10<7:29:38,  3.62s/it] 16%|█▋        | 1459/8917 [1:50:14<7:39:55,  3.70s/it] 16%|█▋        | 1460/8917 [1:50:17<7:36:47,  3.68s/it] 16%|█▋        | 1461/8917 [1:50:21<7:35:34,  3.67s/it] 16%|█▋        | 1462/8917 [1:50:25<7:33:42,  3.65s/it] 16%|█▋        | 1463/8917 [1:50:28<7:36:46,  3.68s/it] 16%|█▋        | 1464/8917 [1:50:32<7:45:06,  3.74s/it] 16%|█▋        | 1465/8917 [1:50:36<7:37:33,  3.68s/it] 16%|█▋        | 1466/8917 [1:50:40<7:41:44,  3.72s/it] 16%|█▋        | 1467/8917 [1:50:43<7:42:15,  3.72s/it] 16%|█▋        | 1468/8917 [1:50:47<7:33:46,  3.66s/it] 16%|█▋        | 1469/8917 [1:50:50<7:21:02,  3.55s/it] 16%|█▋        | 1470/8917 [1:50:54<7:35:43,  3.67s/it] 16%|█▋        | 1471/8917 [1:50:58<7:34:36,  3.66s/it] 17%|█▋        | 1472/8917 [1:51:01<7:36:53,  3.68s/it] 17%|█▋        | 1473/8917 [1:51:05<7:44:11,  3.74s/it] 17%|█▋        | 1474/8917 [1:51:09<7:47:04,  3.77s/it] 17%|█▋        | 1475/8917 [1:51:13<7:42:00,  3.72s/it] 17%|█▋        | 1476/8917 [1:51:17<7:55:50,  3.84s/it] 17%|█▋        | 1477/8917 [1:51:20<7:45:32,  3.75s/it] 17%|█▋        | 1478/8917 [1:51:24<7:55:03,  3.83s/it] 17%|█▋        | 1479/8917 [1:51:28<7:44:05,  3.74s/it] 17%|█▋        | 1480/8917 [1:51:32<7:39:27,  3.71s/it] 17%|█▋        | 1481/8917 [1:51:35<7:38:28,  3.70s/it] 17%|█▋        | 1482/8917 [1:51:39<7:34:19,  3.67s/it] 17%|█▋        | 1483/8917 [1:51:43<7:32:44,  3.65s/it] 17%|█▋        | 1484/8917 [1:51:46<7:35:31,  3.68s/it] 17%|█▋        | 1485/8917 [1:51:50<7:35:27,  3.68s/it] 17%|█▋        | 1486/8917 [1:51:53<7:30:20,  3.64s/it] 17%|█▋        | 1487/8917 [1:51:57<7:28:06,  3.62s/it] 17%|█▋        | 1488/8917 [1:52:01<7:55:06,  3.84s/it] 17%|█▋        | 1489/8917 [1:52:05<7:56:32,  3.85s/it] 17%|█▋        | 1490/8917 [1:52:09<8:01:06,  3.89s/it] 17%|█▋        | 1491/8917 [1:52:13<7:37:53,  3.70s/it] 17%|█▋        | 1492/8917 [1:52:16<7:28:56,  3.63s/it] 17%|█▋        | 1493/8917 [1:52:20<7:40:48,  3.72s/it] 17%|█▋        | 1494/8917 [1:52:24<7:45:51,  3.77s/it] 17%|█▋        | 1495/8917 [1:52:27<7:38:16,  3.70s/it] 17%|█▋        | 1496/8917 [1:52:31<7:39:19,  3.71s/it] 17%|█▋        | 1497/8917 [1:52:35<7:34:25,  3.67s/it] 17%|█▋        | 1498/8917 [1:52:38<7:30:45,  3.65s/it] 17%|█▋        | 1499/8917 [1:52:42<7:27:51,  3.62s/it]09/19/2024 04:07:18 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 04:07:18 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:10,  3.14it/s][A
  1%|          | 2/221 [00:00<01:07,  3.24it/s][A
  1%|▏         | 3/221 [00:01<01:53,  1.92it/s][A
  2%|▏         | 5/221 [00:01<00:59,  3.62it/s][A
  3%|▎         | 6/221 [00:01<00:51,  4.17it/s][A
  3%|▎         | 7/221 [00:01<00:43,  4.94it/s][A
  4%|▎         | 8/221 [00:02<00:52,  4.04it/s][A
  4%|▍         | 9/221 [00:02<00:57,  3.68it/s][A
  5%|▍         | 10/221 [00:02<01:03,  3.34it/s][A
  5%|▌         | 12/221 [00:04<01:29,  2.32it/s][A
  6%|▌         | 13/221 [00:04<01:19,  2.61it/s][A
  6%|▋         | 14/221 [00:04<01:10,  2.95it/s][A
  7%|▋         | 15/221 [00:04<01:12,  2.83it/s][A
  7%|▋         | 16/221 [00:05<01:32,  2.23it/s][A
  8%|▊         | 17/221 [00:07<02:32,  1.34it/s][A
  8%|▊         | 18/221 [00:07<02:39,  1.27it/s][A
  9%|▊         | 19/221 [00:08<02:10,  1.54it/s][A
 10%|▉         | 21/221 [00:08<01:24,  2.36it/s][A
 10%|▉         | 22/221 [00:08<01:19,  2.52it/s][A
 11%|█         | 24/221 [00:09<00:55,  3.52it/s][A
 11%|█▏        | 25/221 [00:09<00:53,  3.65it/s][A
 12%|█▏        | 26/221 [00:09<00:58,  3.31it/s][A
 13%|█▎        | 28/221 [00:10<00:52,  3.68it/s][A
 13%|█▎        | 29/221 [00:11<01:23,  2.31it/s][A
 14%|█▎        | 30/221 [00:11<01:21,  2.34it/s][A
 14%|█▍        | 31/221 [00:12<01:36,  1.97it/s][A
 15%|█▍        | 33/221 [00:12<01:11,  2.61it/s][A
 16%|█▌        | 35/221 [00:13<00:54,  3.42it/s][A
 16%|█▋        | 36/221 [00:13<00:49,  3.77it/s][A
 17%|█▋        | 37/221 [00:13<01:03,  2.90it/s][A
 17%|█▋        | 38/221 [00:14<01:35,  1.92it/s][A
 18%|█▊        | 39/221 [00:15<01:17,  2.35it/s][A
 18%|█▊        | 40/221 [00:15<01:14,  2.43it/s][A
 19%|█▊        | 41/221 [00:15<01:01,  2.92it/s][A
 19%|█▉        | 42/221 [00:15<00:58,  3.06it/s][A
 20%|█▉        | 44/221 [00:16<00:38,  4.54it/s][A
 20%|██        | 45/221 [00:19<02:54,  1.01it/s][A
 21%|██        | 46/221 [00:19<02:24,  1.21it/s][A
 21%|██▏       | 47/221 [00:21<03:16,  1.13s/it][A
 22%|██▏       | 48/221 [00:21<02:28,  1.16it/s][A
 22%|██▏       | 49/221 [00:22<01:57,  1.46it/s][A
 23%|██▎       | 50/221 [00:22<01:48,  1.58it/s][A
 23%|██▎       | 51/221 [00:22<01:28,  1.92it/s][A
 24%|██▍       | 53/221 [00:23<00:54,  3.10it/s][A
 24%|██▍       | 54/221 [00:23<01:09,  2.39it/s][A
 25%|██▍       | 55/221 [00:25<01:57,  1.41it/s][A
 25%|██▌       | 56/221 [00:25<01:36,  1.71it/s][A
 26%|██▌       | 57/221 [00:25<01:18,  2.08it/s][A
 27%|██▋       | 59/221 [00:25<00:48,  3.31it/s][A
 27%|██▋       | 60/221 [00:27<01:23,  1.92it/s][A
 28%|██▊       | 61/221 [00:27<01:14,  2.14it/s][A
 28%|██▊       | 62/221 [00:27<01:10,  2.27it/s][A
 29%|██▊       | 63/221 [00:28<01:04,  2.47it/s][A
 29%|██▉       | 64/221 [00:28<01:22,  1.91it/s][A
 30%|██▉       | 66/221 [00:29<01:02,  2.47it/s][A
 30%|███       | 67/221 [00:29<00:54,  2.82it/s][A
 31%|███       | 68/221 [00:29<00:46,  3.32it/s][A
 31%|███       | 69/221 [00:30<01:07,  2.26it/s][A
 32%|███▏      | 71/221 [00:30<00:46,  3.22it/s][A
 33%|███▎      | 72/221 [00:31<00:43,  3.40it/s][A
 33%|███▎      | 73/221 [00:32<01:04,  2.30it/s][A
 33%|███▎      | 74/221 [00:32<00:53,  2.77it/s][A
 34%|███▍      | 75/221 [00:32<01:06,  2.21it/s][A
 34%|███▍      | 76/221 [00:33<01:01,  2.35it/s][A
 35%|███▍      | 77/221 [00:34<01:40,  1.44it/s][A
 36%|███▌      | 79/221 [00:35<01:25,  1.67it/s][A
 37%|███▋      | 81/221 [00:36<01:12,  1.93it/s][A
 37%|███▋      | 82/221 [00:39<02:36,  1.13s/it][A
 38%|███▊      | 83/221 [00:40<02:22,  1.03s/it][A
 38%|███▊      | 84/221 [00:40<01:52,  1.22it/s][A
 39%|███▉      | 86/221 [00:41<01:22,  1.64it/s][A
 39%|███▉      | 87/221 [00:43<02:15,  1.01s/it][A
 40%|███▉      | 88/221 [00:43<01:52,  1.19it/s][A
 40%|████      | 89/221 [00:44<01:42,  1.29it/s][A
 41%|████      | 90/221 [00:45<01:35,  1.38it/s][A
 42%|████▏     | 92/221 [00:45<01:00,  2.13it/s][A
 42%|████▏     | 93/221 [00:45<01:04,  1.99it/s][A
 43%|████▎     | 94/221 [00:47<01:26,  1.47it/s][A
 43%|████▎     | 95/221 [00:47<01:15,  1.68it/s][A
 43%|████▎     | 96/221 [00:48<01:23,  1.50it/s][A
 44%|████▍     | 97/221 [00:48<01:04,  1.91it/s][A
 44%|████▍     | 98/221 [00:49<01:10,  1.75it/s][A
 45%|████▍     | 99/221 [00:49<00:56,  2.16it/s][A
 45%|████▌     | 100/221 [00:49<00:52,  2.30it/s][A
 46%|████▌     | 101/221 [00:49<00:43,  2.77it/s][A
 46%|████▌     | 102/221 [00:51<01:25,  1.39it/s][A
 47%|████▋     | 103/221 [00:51<01:04,  1.84it/s][A
 47%|████▋     | 104/221 [00:51<00:52,  2.22it/s][A
 48%|████▊     | 105/221 [00:52<00:50,  2.28it/s][A
 48%|████▊     | 106/221 [00:53<01:16,  1.50it/s][A
 48%|████▊     | 107/221 [00:53<01:06,  1.72it/s][A
 49%|████▉     | 108/221 [00:54<00:56,  1.99it/s][A
 49%|████▉     | 109/221 [00:54<00:50,  2.20it/s][A
 50%|████▉     | 110/221 [00:54<00:40,  2.71it/s][A
 50%|█████     | 111/221 [00:55<00:41,  2.67it/s][A
 51%|█████     | 112/221 [00:55<00:42,  2.54it/s][A
 51%|█████     | 113/221 [00:55<00:38,  2.81it/s][A
 52%|█████▏    | 115/221 [00:56<00:30,  3.46it/s][A
 52%|█████▏    | 116/221 [00:56<00:34,  3.01it/s][A
 53%|█████▎    | 117/221 [00:57<00:39,  2.66it/s][A
 53%|█████▎    | 118/221 [00:57<00:39,  2.61it/s][A
 54%|█████▍    | 119/221 [00:58<00:47,  2.14it/s][A
 54%|█████▍    | 120/221 [00:58<00:40,  2.46it/s][A
 55%|█████▍    | 121/221 [00:58<00:38,  2.59it/s][A
 55%|█████▌    | 122/221 [00:59<00:36,  2.73it/s][A
 56%|█████▌    | 123/221 [00:59<00:46,  2.12it/s][A
 56%|█████▌    | 124/221 [01:00<00:38,  2.50it/s][A
 57%|█████▋    | 125/221 [01:01<00:53,  1.78it/s][A
 57%|█████▋    | 126/221 [01:03<01:37,  1.02s/it][A
 57%|█████▋    | 127/221 [01:04<01:33,  1.00it/s][A
 58%|█████▊    | 128/221 [01:04<01:20,  1.15it/s][A
 58%|█████▊    | 129/221 [01:04<01:00,  1.52it/s][A
 59%|█████▉    | 130/221 [01:05<00:51,  1.77it/s][A
 59%|█████▉    | 131/221 [01:06<01:03,  1.42it/s][A
 60%|█████▉    | 132/221 [01:07<01:15,  1.18it/s][A
 60%|██████    | 133/221 [01:07<01:00,  1.45it/s][A
 61%|██████    | 134/221 [01:09<01:33,  1.07s/it][A
 61%|██████    | 135/221 [01:12<02:04,  1.45s/it][A
 62%|██████▏   | 136/221 [01:12<01:36,  1.13s/it][A
 62%|██████▏   | 137/221 [01:12<01:10,  1.18it/s][A
 62%|██████▏   | 138/221 [01:14<01:41,  1.22s/it][A
 63%|██████▎   | 139/221 [01:15<01:18,  1.04it/s][A
 63%|██████▎   | 140/221 [01:15<01:06,  1.22it/s][A
 64%|██████▍   | 141/221 [01:15<00:55,  1.44it/s][A
 64%|██████▍   | 142/221 [01:16<00:47,  1.65it/s][A
 65%|██████▍   | 143/221 [01:16<00:41,  1.87it/s][A
 65%|██████▌   | 144/221 [01:16<00:32,  2.34it/s][A
 66%|██████▌   | 146/221 [01:17<00:20,  3.68it/s][A
 67%|██████▋   | 148/221 [01:18<00:25,  2.86it/s][A
 67%|██████▋   | 149/221 [01:18<00:27,  2.58it/s][A
 68%|██████▊   | 150/221 [01:19<00:28,  2.49it/s][A
 68%|██████▊   | 151/221 [01:19<00:28,  2.44it/s][A
 69%|██████▉   | 152/221 [01:19<00:27,  2.50it/s][A
 69%|██████▉   | 153/221 [01:19<00:22,  3.01it/s][A
 70%|██████▉   | 154/221 [01:20<00:24,  2.68it/s][A
 70%|███████   | 155/221 [01:20<00:25,  2.61it/s][A
 71%|███████   | 156/221 [01:21<00:21,  3.07it/s][A
 71%|███████   | 157/221 [01:29<02:59,  2.81s/it][A
 71%|███████▏  | 158/221 [01:30<02:11,  2.08s/it][A
 72%|███████▏  | 159/221 [01:30<01:33,  1.50s/it][A
 72%|███████▏  | 160/221 [01:30<01:07,  1.11s/it][A
 73%|███████▎  | 161/221 [01:30<00:50,  1.20it/s][A
 74%|███████▍  | 163/221 [01:30<00:30,  1.92it/s][A
 74%|███████▍  | 164/221 [01:31<00:25,  2.26it/s][A
 75%|███████▍  | 165/221 [01:31<00:28,  1.98it/s][A
 75%|███████▌  | 166/221 [01:33<00:40,  1.36it/s][A
 76%|███████▌  | 167/221 [01:33<00:31,  1.74it/s][A
 76%|███████▌  | 168/221 [01:40<02:08,  2.43s/it][A
 76%|███████▋  | 169/221 [01:40<01:34,  1.81s/it][A
 77%|███████▋  | 170/221 [01:41<01:11,  1.41s/it][A
 77%|███████▋  | 171/221 [01:41<00:58,  1.17s/it][A
 78%|███████▊  | 172/221 [01:41<00:43,  1.14it/s][A
 78%|███████▊  | 173/221 [01:42<00:35,  1.34it/s][A
 79%|███████▊  | 174/221 [01:42<00:27,  1.73it/s][A
 79%|███████▉  | 175/221 [01:42<00:24,  1.92it/s][A
 80%|███████▉  | 176/221 [01:43<00:25,  1.78it/s][A
 80%|████████  | 177/221 [01:43<00:20,  2.13it/s][A
 81%|████████  | 178/221 [01:44<00:23,  1.86it/s][A
 81%|████████  | 179/221 [01:45<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:45<00:17,  2.38it/s][A
 82%|████████▏ | 182/221 [01:45<00:14,  2.73it/s][A
 83%|████████▎ | 183/221 [01:46<00:13,  2.72it/s][A
 83%|████████▎ | 184/221 [01:47<00:18,  2.01it/s][A
 84%|████████▎ | 185/221 [01:47<00:14,  2.48it/s][A
 84%|████████▍ | 186/221 [01:47<00:14,  2.40it/s][A
 85%|████████▍ | 187/221 [01:48<00:13,  2.56it/s][A
 85%|████████▌ | 188/221 [01:48<00:10,  3.07it/s][A
 86%|████████▌ | 189/221 [01:48<00:09,  3.35it/s][A
 86%|████████▌ | 190/221 [01:49<00:12,  2.44it/s][A
 86%|████████▋ | 191/221 [01:49<00:11,  2.72it/s][A
 87%|████████▋ | 192/221 [01:49<00:08,  3.25it/s][A
 88%|████████▊ | 194/221 [01:51<00:15,  1.72it/s][A
 88%|████████▊ | 195/221 [01:51<00:12,  2.03it/s][A
 89%|████████▊ | 196/221 [01:51<00:10,  2.40it/s][A
 89%|████████▉ | 197/221 [01:51<00:08,  2.73it/s][A
 90%|████████▉ | 198/221 [01:52<00:08,  2.78it/s][A
 90%|█████████ | 199/221 [01:52<00:07,  3.14it/s][A
 90%|█████████ | 200/221 [01:53<00:08,  2.55it/s][A
 91%|█████████ | 201/221 [01:53<00:08,  2.24it/s][A
 91%|█████████▏| 202/221 [01:54<00:07,  2.38it/s][A
 92%|█████████▏| 203/221 [01:55<00:13,  1.29it/s][A
 92%|█████████▏| 204/221 [01:55<00:10,  1.69it/s][A
 93%|█████████▎| 205/221 [01:55<00:07,  2.15it/s][A
 93%|█████████▎| 206/221 [01:56<00:09,  1.63it/s][A
 94%|█████████▍| 208/221 [01:57<00:05,  2.41it/s][A
 95%|█████████▌| 210/221 [01:57<00:02,  3.67it/s][A
 95%|█████████▌| 211/221 [01:58<00:03,  2.64it/s][A
 96%|█████████▌| 212/221 [01:58<00:04,  2.21it/s][A
 96%|█████████▋| 213/221 [01:59<00:03,  2.35it/s][A
 97%|█████████▋| 214/221 [01:59<00:03,  2.10it/s][A
 97%|█████████▋| 215/221 [02:00<00:02,  2.20it/s][A
 98%|█████████▊| 216/221 [02:00<00:02,  2.02it/s][A
 98%|█████████▊| 217/221 [02:06<00:07,  1.98s/it][A
 99%|█████████▊| 218/221 [02:07<00:04,  1.56s/it][A
 99%|█████████▉| 219/221 [02:07<00:02,  1.15s/it][A
100%|█████████▉| 220/221 [02:09<00:01,  1.60s/it][A
100%|██████████| 221/221 [02:10<00:00,  1.20s/it][A100%|██████████| 221/221 [02:10<00:00,  1.70it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:34,  6.39it/s][A
  1%|          | 2/221 [00:00<01:34,  2.31it/s][A
  1%|▏         | 3/221 [00:01<01:27,  2.49it/s][A
  2%|▏         | 4/221 [00:01<01:41,  2.13it/s][A
  2%|▏         | 5/221 [00:02<01:59,  1.80it/s][A
  3%|▎         | 6/221 [00:02<01:43,  2.08it/s][A
  3%|▎         | 7/221 [00:02<01:23,  2.58it/s][A
  4%|▎         | 8/221 [00:03<02:05,  1.70it/s][A
  4%|▍         | 9/221 [00:05<03:16,  1.08it/s][A
  5%|▍         | 10/221 [00:05<02:35,  1.36it/s][A
  5%|▍         | 11/221 [00:06<02:50,  1.23it/s][A
  5%|▌         | 12/221 [00:07<02:33,  1.36it/s][A
  6%|▌         | 13/221 [00:08<02:21,  1.47it/s][A
  6%|▋         | 14/221 [00:08<02:06,  1.63it/s][A
  7%|▋         | 15/221 [00:08<01:49,  1.89it/s][A
  7%|▋         | 16/221 [00:09<01:51,  1.84it/s][A
  8%|▊         | 17/221 [00:09<01:51,  1.82it/s][A
  8%|▊         | 18/221 [00:10<01:36,  2.11it/s][A
  9%|▊         | 19/221 [00:10<01:43,  1.94it/s][A
  9%|▉         | 20/221 [00:11<01:24,  2.38it/s][A
 10%|▉         | 21/221 [00:11<01:27,  2.29it/s][A
 10%|▉         | 22/221 [00:11<01:20,  2.47it/s][A
 10%|█         | 23/221 [00:12<01:04,  3.05it/s][A
 11%|█         | 24/221 [00:12<00:57,  3.45it/s][A
 11%|█▏        | 25/221 [00:12<01:13,  2.67it/s][A
 12%|█▏        | 26/221 [00:13<01:17,  2.51it/s][A
 12%|█▏        | 27/221 [00:14<01:48,  1.79it/s][A
 13%|█▎        | 28/221 [00:15<02:27,  1.31it/s][A
 13%|█▎        | 29/221 [00:16<02:56,  1.09it/s][A
 14%|█▎        | 30/221 [00:17<02:55,  1.09it/s][A
 14%|█▍        | 31/221 [00:18<02:31,  1.26it/s][A
 14%|█▍        | 32/221 [00:18<02:13,  1.41it/s][A
 15%|█▍        | 33/221 [00:19<01:59,  1.58it/s][A
 15%|█▌        | 34/221 [00:19<01:43,  1.80it/s][A
 16%|█▌        | 35/221 [00:19<01:26,  2.16it/s][A
 16%|█▋        | 36/221 [00:20<02:07,  1.45it/s][A
 17%|█▋        | 37/221 [00:21<01:52,  1.63it/s][A
 17%|█▋        | 38/221 [00:21<01:46,  1.73it/s][A
 18%|█▊        | 39/221 [00:22<01:39,  1.83it/s][A
 18%|█▊        | 40/221 [00:24<02:41,  1.12it/s][A
 19%|█▊        | 41/221 [00:24<02:16,  1.32it/s][A
 19%|█▉        | 42/221 [00:24<01:44,  1.72it/s][A
 19%|█▉        | 43/221 [00:25<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:25<01:23,  2.13it/s][A
 20%|██        | 45/221 [00:25<01:08,  2.58it/s][A
 21%|██        | 46/221 [00:25<01:03,  2.75it/s][A
 21%|██▏       | 47/221 [00:26<01:10,  2.48it/s][A
 22%|██▏       | 48/221 [00:26<00:57,  2.99it/s][A
 22%|██▏       | 49/221 [00:26<01:00,  2.85it/s][A
 23%|██▎       | 50/221 [00:28<01:50,  1.55it/s][A
 23%|██▎       | 51/221 [00:29<02:02,  1.39it/s][A
 24%|██▎       | 52/221 [00:29<01:37,  1.74it/s][A
 24%|██▍       | 53/221 [00:29<01:21,  2.07it/s][A
 24%|██▍       | 54/221 [00:30<01:14,  2.23it/s][A
 25%|██▍       | 55/221 [00:30<01:00,  2.73it/s][A
 25%|██▌       | 56/221 [00:30<01:03,  2.59it/s][A
 26%|██▌       | 57/221 [00:30<00:55,  2.94it/s][A
 26%|██▌       | 58/221 [00:31<00:53,  3.02it/s][A
 27%|██▋       | 60/221 [00:31<00:52,  3.09it/s][A
 28%|██▊       | 61/221 [00:32<01:03,  2.51it/s][A
 28%|██▊       | 62/221 [00:33<01:11,  2.22it/s][A
 29%|██▊       | 63/221 [00:33<01:22,  1.92it/s][A
 29%|██▉       | 64/221 [00:35<01:56,  1.35it/s][A
 29%|██▉       | 65/221 [00:36<02:20,  1.11it/s][A
 30%|██▉       | 66/221 [00:36<01:50,  1.41it/s][A
 30%|███       | 67/221 [00:38<02:24,  1.07it/s][A
 31%|███       | 68/221 [00:38<01:51,  1.37it/s][A
 31%|███       | 69/221 [00:38<01:26,  1.77it/s][A
 32%|███▏      | 70/221 [00:38<01:16,  1.97it/s][A
 32%|███▏      | 71/221 [00:39<01:03,  2.38it/s][A
 33%|███▎      | 72/221 [00:39<01:08,  2.18it/s][A
 33%|███▎      | 73/221 [00:40<01:10,  2.10it/s][A
 33%|███▎      | 74/221 [00:40<01:16,  1.91it/s][A
 34%|███▍      | 75/221 [00:41<01:10,  2.07it/s][A
 34%|███▍      | 76/221 [00:41<01:03,  2.27it/s][A
 35%|███▍      | 77/221 [00:42<01:08,  2.11it/s][A
 35%|███▌      | 78/221 [00:42<00:54,  2.64it/s][A
 36%|███▌      | 79/221 [00:43<01:19,  1.78it/s][A
 36%|███▌      | 80/221 [00:43<01:02,  2.26it/s][A
 37%|███▋      | 81/221 [00:43<01:04,  2.18it/s][A
 37%|███▋      | 82/221 [00:44<00:56,  2.45it/s][A
 38%|███▊      | 83/221 [00:44<00:59,  2.31it/s][A
 38%|███▊      | 84/221 [00:45<01:20,  1.71it/s][A
 38%|███▊      | 85/221 [00:46<01:31,  1.49it/s][A
 39%|███▉      | 86/221 [00:46<01:20,  1.67it/s][A
 39%|███▉      | 87/221 [00:47<01:22,  1.63it/s][A
 40%|███▉      | 88/221 [00:48<01:21,  1.64it/s][A
 40%|████      | 89/221 [00:48<01:19,  1.66it/s][A
 41%|████      | 90/221 [00:49<01:13,  1.79it/s][A
 41%|████      | 91/221 [00:49<01:00,  2.15it/s][A
 42%|████▏     | 92/221 [00:50<01:27,  1.47it/s][A
 42%|████▏     | 93/221 [00:50<01:06,  1.91it/s][A
 43%|████▎     | 94/221 [00:51<01:02,  2.02it/s][A
 43%|████▎     | 95/221 [00:51<01:00,  2.08it/s][A
 43%|████▎     | 96/221 [00:52<00:55,  2.27it/s][A
 44%|████▍     | 97/221 [00:52<00:53,  2.31it/s][A
 44%|████▍     | 98/221 [00:53<01:34,  1.30it/s][A
 45%|████▍     | 99/221 [00:54<01:21,  1.50it/s][A
 45%|████▌     | 100/221 [00:55<01:20,  1.50it/s][A
 46%|████▌     | 101/221 [00:55<01:24,  1.42it/s][A
 46%|████▌     | 102/221 [00:56<01:22,  1.44it/s][A
 47%|████▋     | 103/221 [00:56<01:01,  1.92it/s][A
 47%|████▋     | 104/221 [00:57<00:54,  2.15it/s][A
 48%|████▊     | 105/221 [00:57<00:48,  2.40it/s][A
 48%|████▊     | 106/221 [00:57<00:39,  2.93it/s][A
 48%|████▊     | 107/221 [00:57<00:42,  2.66it/s][A
 49%|████▉     | 108/221 [00:58<00:53,  2.12it/s][A
 49%|████▉     | 109/221 [00:59<00:59,  1.87it/s][A
 50%|████▉     | 110/221 [01:01<02:09,  1.17s/it][A
 50%|█████     | 111/221 [01:02<01:45,  1.04it/s][A
 51%|█████     | 112/221 [01:02<01:23,  1.31it/s][A
 51%|█████     | 113/221 [01:03<01:08,  1.58it/s][A
 52%|█████▏    | 114/221 [01:03<00:52,  2.06it/s][A
 52%|█████▏    | 115/221 [01:04<01:25,  1.24it/s][A
 52%|█████▏    | 116/221 [01:04<01:06,  1.59it/s][A
 53%|█████▎    | 117/221 [01:05<01:09,  1.50it/s][A
 53%|█████▎    | 118/221 [01:06<01:02,  1.64it/s][A
 54%|█████▍    | 119/221 [01:06<00:55,  1.84it/s][A
 54%|█████▍    | 120/221 [01:07<00:52,  1.93it/s][A
 55%|█████▍    | 121/221 [01:07<00:50,  1.99it/s][A
 55%|█████▌    | 122/221 [01:07<00:43,  2.25it/s][A
 56%|█████▌    | 123/221 [01:08<00:45,  2.17it/s][A
 56%|█████▌    | 124/221 [01:08<00:43,  2.23it/s][A
 57%|█████▋    | 125/221 [01:09<00:52,  1.82it/s][A
 57%|█████▋    | 126/221 [01:09<00:42,  2.25it/s][A
 57%|█████▋    | 127/221 [01:10<00:49,  1.90it/s][A
 58%|█████▊    | 128/221 [01:11<00:52,  1.77it/s][A
 58%|█████▊    | 129/221 [01:11<00:45,  2.03it/s][A
 59%|█████▉    | 130/221 [01:12<00:49,  1.83it/s][A
 59%|█████▉    | 131/221 [01:12<00:44,  2.04it/s][A
 60%|█████▉    | 132/221 [01:12<00:39,  2.26it/s][A
 60%|██████    | 133/221 [01:13<00:34,  2.52it/s][A
 61%|██████    | 134/221 [01:14<00:53,  1.63it/s][A
 61%|██████    | 135/221 [01:14<00:44,  1.92it/s][A
 62%|██████▏   | 136/221 [01:14<00:40,  2.11it/s][A
 62%|██████▏   | 137/221 [01:15<00:39,  2.11it/s][A
 62%|██████▏   | 138/221 [01:15<00:41,  1.99it/s][A
 63%|██████▎   | 139/221 [01:16<00:44,  1.85it/s][A
 63%|██████▎   | 140/221 [01:17<00:42,  1.92it/s][A
 64%|██████▍   | 141/221 [01:17<00:36,  2.19it/s][A
 64%|██████▍   | 142/221 [01:17<00:35,  2.24it/s][A
 65%|██████▍   | 143/221 [01:18<00:30,  2.57it/s][A
 65%|██████▌   | 144/221 [01:18<00:40,  1.91it/s][A
 66%|██████▌   | 145/221 [01:18<00:31,  2.43it/s][A
 67%|██████▋   | 147/221 [01:19<00:23,  3.11it/s][A
 67%|██████▋   | 148/221 [01:19<00:25,  2.89it/s][A
 67%|██████▋   | 149/221 [01:20<00:28,  2.56it/s][A
 68%|██████▊   | 150/221 [01:21<00:35,  2.02it/s][A
 68%|██████▊   | 151/221 [01:21<00:29,  2.36it/s][A
 69%|██████▉   | 152/221 [01:22<00:46,  1.49it/s][A
 69%|██████▉   | 153/221 [01:22<00:37,  1.81it/s][A
 70%|██████▉   | 154/221 [01:23<00:40,  1.67it/s][A
 70%|███████   | 155/221 [01:24<00:36,  1.82it/s][A
 71%|███████   | 156/221 [01:24<00:29,  2.19it/s][A
 71%|███████   | 157/221 [01:24<00:31,  2.04it/s][A
 71%|███████▏  | 158/221 [01:25<00:37,  1.67it/s][A
 72%|███████▏  | 159/221 [01:25<00:28,  2.15it/s][A
 72%|███████▏  | 160/221 [01:26<00:28,  2.16it/s][A
 73%|███████▎  | 161/221 [01:27<00:32,  1.86it/s][A
 74%|███████▍  | 163/221 [01:27<00:24,  2.42it/s][A
 74%|███████▍  | 164/221 [01:27<00:20,  2.80it/s][A
 75%|███████▍  | 165/221 [01:28<00:23,  2.36it/s][A
 75%|███████▌  | 166/221 [01:30<00:43,  1.26it/s][A
 76%|███████▌  | 167/221 [01:30<00:37,  1.46it/s][A
 76%|███████▌  | 168/221 [01:30<00:29,  1.78it/s][A
 76%|███████▋  | 169/221 [01:31<00:25,  2.02it/s][A
 77%|███████▋  | 170/221 [01:31<00:24,  2.06it/s][A
 77%|███████▋  | 171/221 [01:32<00:33,  1.49it/s][A
 78%|███████▊  | 172/221 [01:33<00:30,  1.63it/s][A
 78%|███████▊  | 173/221 [01:33<00:26,  1.82it/s][A
 79%|███████▊  | 174/221 [01:33<00:21,  2.16it/s][A
 79%|███████▉  | 175/221 [01:34<00:19,  2.34it/s][A
 80%|███████▉  | 176/221 [01:35<00:28,  1.60it/s][A
 80%|████████  | 177/221 [01:35<00:26,  1.68it/s][A
 81%|████████  | 178/221 [01:36<00:23,  1.86it/s][A
 81%|████████  | 179/221 [01:36<00:19,  2.16it/s][A
 81%|████████▏ | 180/221 [01:36<00:16,  2.47it/s][A
 82%|████████▏ | 181/221 [01:37<00:24,  1.60it/s][A
 82%|████████▏ | 182/221 [01:39<00:29,  1.31it/s][A
 83%|████████▎ | 183/221 [01:39<00:25,  1.51it/s][A
 83%|████████▎ | 184/221 [01:40<00:27,  1.35it/s][A
 84%|████████▎ | 185/221 [01:41<00:29,  1.21it/s][A
 84%|████████▍ | 186/221 [01:41<00:22,  1.53it/s][A
 85%|████████▍ | 187/221 [01:42<00:25,  1.33it/s][A
 85%|████████▌ | 188/221 [01:43<00:30,  1.09it/s][A
 86%|████████▌ | 189/221 [01:44<00:23,  1.35it/s][A
 86%|████████▌ | 190/221 [01:44<00:21,  1.45it/s][A
 86%|████████▋ | 191/221 [01:46<00:25,  1.20it/s][A
 87%|████████▋ | 192/221 [01:46<00:20,  1.44it/s][A
 87%|████████▋ | 193/221 [01:46<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:47<00:14,  1.82it/s][A
 88%|████████▊ | 195/221 [01:48<00:18,  1.41it/s][A
 89%|████████▊ | 196/221 [01:48<00:16,  1.53it/s][A
 89%|████████▉ | 197/221 [01:49<00:15,  1.58it/s][A
 90%|████████▉ | 198/221 [01:49<00:11,  1.97it/s][A
 90%|█████████ | 199/221 [01:49<00:09,  2.31it/s][A
 90%|█████████ | 200/221 [01:50<00:13,  1.55it/s][A
 91%|█████████ | 201/221 [01:51<00:12,  1.64it/s][A
 91%|█████████▏| 202/221 [01:51<00:11,  1.72it/s][A
 92%|█████████▏| 203/221 [01:52<00:10,  1.64it/s][A
 92%|█████████▏| 204/221 [01:52<00:08,  2.05it/s][A
 93%|█████████▎| 205/221 [01:53<00:06,  2.39it/s][A
 93%|█████████▎| 206/221 [01:53<00:06,  2.25it/s][A
 94%|█████████▎| 207/221 [01:53<00:05,  2.40it/s][A
 94%|█████████▍| 208/221 [01:56<00:11,  1.09it/s][A
 95%|█████████▍| 209/221 [01:56<00:08,  1.45it/s][A
 95%|█████████▌| 211/221 [01:56<00:04,  2.02it/s][A
 96%|█████████▌| 212/221 [01:57<00:04,  2.24it/s][A
 96%|█████████▋| 213/221 [01:58<00:04,  1.67it/s][A
 97%|█████████▋| 214/221 [01:58<00:03,  1.94it/s][A
 97%|█████████▋| 215/221 [01:58<00:02,  2.08it/s][A
 98%|█████████▊| 216/221 [01:59<00:02,  1.87it/s][A
 98%|█████████▊| 217/221 [01:59<00:02,  1.94it/s][A
 99%|█████████▊| 218/221 [02:00<00:01,  2.04it/s][A
 99%|█████████▉| 219/221 [02:00<00:00,  2.30it/s][A
100%|█████████▉| 220/221 [02:00<00:00,  2.65it/s][A
100%|██████████| 221/221 [02:04<00:00,  1.33s/it][A100%|██████████| 221/221 [02:04<00:00,  1.78it/s]
09/19/2024 04:16:15 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 1499--===========

09/19/2024 04:16:15 - INFO - __main__ -   {'area_r1': 43.9, 'area_recall': '43.9/70.1/80.8', 'area_ravg': 64.9}
09/19/2024 04:16:15 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 1499--===========

09/19/2024 04:16:15 - INFO - __main__ -   {'forward_r1': 49.8, 'forward_recall': '49.8/77.3/86.8', 'forward_ravg': 71.3}
09/19/2024 04:16:15 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 1499--===========

09/19/2024 04:16:15 - INFO - __main__ -   {'area_video_r1': 49.4, 'area_video_recall': '49.4/78.4/87.0', 'area_video_ravg': 71.6}
09/19/2024 04:16:15 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 1499=======

09/19/2024 04:16:15 - INFO - __main__ -   {'area_video_r1': 49.4, 'area_video_recall': '49.4/78.4/87.0', 'area_video_ravg': 71.6}
09/19/2024 04:16:15 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 1499--===========

09/19/2024 04:16:15 - INFO - __main__ -   {'area_video_r1': 60.9, 'area_video_recall': '60.9/81.1/87.0', 'area_video_ravg': 76.3, 'area_video_back_r1': 59.3, 'area_video_back_recall': '59.3/83.6/90.2', 'area_video_back_ravg': 77.7}
09/19/2024 04:16:15 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 1499=======

09/19/2024 04:16:15 - INFO - __main__ -   {'area_video_r1': 60.9, 'area_video_recall': '60.9/81.1/87.0', 'area_video_ravg': 76.3, 'area_video_back_r1': 59.3, 'area_video_back_recall': '59.3/83.6/90.2', 'area_video_back_ravg': 77.7}
09/19/2024 04:16:15 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 1499--===========

09/19/2024 04:16:15 - INFO - __main__ -   {'video_r1': 33.0, 'video_recall': '33.0/60.4/71.6', 'video_ravg': 55.0}
09/19/2024 04:16:15 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 04:16:15 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 04:16:15 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 1499--===========

09/19/2024 04:16:15 - INFO - __main__ -   {'video_r1': 60.0, 'video_recall': '60.0/79.1/84.8', 'video_ravg': 74.6}
09/19/2024 04:16:15 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 1499=======

09/19/2024 04:16:15 - INFO - __main__ -   {'video_r1': 60.0, 'video_recall': '60.0/79.1/84.8', 'video_ravg': 74.6}
09/19/2024 04:16:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.035675499588251114, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7167706489562988, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.752446174621582}
 17%|█▋        | 1500/8917 [2:02:20<362:21:44, 175.88s/it] 17%|█▋        | 1501/8917 [2:02:23<255:35:10, 124.07s/it] 17%|█▋        | 1502/8917 [2:02:26<181:00:32, 87.88s/it]  17%|█▋        | 1503/8917 [2:02:30<128:49:45, 62.56s/it] 17%|█▋        | 1504/8917 [2:02:34<92:31:48, 44.94s/it]  17%|█▋        | 1505/8917 [2:02:37<67:00:12, 32.54s/it] 17%|█▋        | 1506/8917 [2:02:41<49:13:42, 23.91s/it] 17%|█▋        | 1507/8917 [2:02:45<36:40:00, 17.81s/it] 17%|█▋        | 1508/8917 [2:02:48<28:06:47, 13.66s/it] 17%|█▋        | 1509/8917 [2:02:52<22:08:51, 10.76s/it] 17%|█▋        | 1510/8917 [2:02:56<17:31:41,  8.52s/it] 17%|█▋        | 1511/8917 [2:02:59<14:31:26,  7.06s/it] 17%|█▋        | 1512/8917 [2:03:04<12:43:36,  6.19s/it] 17%|█▋        | 1513/8917 [2:03:07<11:16:49,  5.48s/it] 17%|█▋        | 1514/8917 [2:03:11<9:56:33,  4.83s/it]  17%|█▋        | 1515/8917 [2:03:14<9:10:10,  4.46s/it] 17%|█▋        | 1516/8917 [2:03:18<8:45:13,  4.26s/it] 17%|█▋        | 1517/8917 [2:03:22<8:38:30,  4.20s/it] 17%|█▋        | 1518/8917 [2:03:26<8:25:01,  4.10s/it] 17%|█▋        | 1519/8917 [2:03:30<8:12:17,  3.99s/it] 17%|█▋        | 1520/8917 [2:03:34<8:02:34,  3.91s/it] 17%|█▋        | 1521/8917 [2:03:37<7:51:17,  3.82s/it] 17%|█▋        | 1522/8917 [2:03:41<7:57:49,  3.88s/it] 17%|█▋        | 1523/8917 [2:03:45<7:44:36,  3.77s/it] 17%|█▋        | 1524/8917 [2:03:49<7:48:16,  3.80s/it] 17%|█▋        | 1525/8917 [2:03:52<7:50:56,  3.82s/it] 17%|█▋        | 1526/8917 [2:03:56<7:50:49,  3.82s/it] 17%|█▋        | 1527/8917 [2:04:00<7:34:13,  3.69s/it] 17%|█▋        | 1528/8917 [2:04:03<7:38:34,  3.72s/it] 17%|█▋        | 1529/8917 [2:04:07<7:35:00,  3.70s/it] 17%|█▋        | 1530/8917 [2:04:11<7:29:34,  3.65s/it] 17%|█▋        | 1531/8917 [2:04:14<7:38:49,  3.73s/it] 17%|█▋        | 1532/8917 [2:04:18<7:29:02,  3.65s/it] 17%|█▋        | 1533/8917 [2:04:22<7:31:53,  3.67s/it] 17%|█▋        | 1534/8917 [2:04:26<7:44:35,  3.78s/it] 17%|█▋        | 1535/8917 [2:04:30<7:46:08,  3.79s/it] 17%|█▋        | 1536/8917 [2:04:33<7:42:57,  3.76s/it] 17%|█▋        | 1537/8917 [2:04:37<7:40:21,  3.74s/it] 17%|█▋        | 1538/8917 [2:04:41<7:37:54,  3.72s/it] 17%|█▋        | 1539/8917 [2:04:44<7:37:46,  3.72s/it] 17%|█▋        | 1540/8917 [2:04:48<7:27:24,  3.64s/it] 17%|█▋        | 1541/8917 [2:04:52<7:45:28,  3.79s/it] 17%|█▋        | 1542/8917 [2:04:55<7:35:47,  3.71s/it] 17%|█▋        | 1543/8917 [2:04:59<7:31:58,  3.68s/it] 17%|█▋        | 1544/8917 [2:05:03<7:26:21,  3.63s/it] 17%|█▋        | 1545/8917 [2:05:06<7:26:07,  3.63s/it] 17%|█▋        | 1546/8917 [2:05:10<7:21:26,  3.59s/it] 17%|█▋        | 1547/8917 [2:05:13<7:28:47,  3.65s/it] 17%|█▋        | 1548/8917 [2:05:17<7:42:38,  3.77s/it] 17%|█▋        | 1549/8917 [2:05:21<7:47:04,  3.80s/it]09/19/2024 04:19:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026262283325195312, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.5007894039154053, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5270516872406006}
 17%|█▋        | 1550/8917 [2:05:25<7:39:12,  3.74s/it] 17%|█▋        | 1551/8917 [2:05:29<7:40:22,  3.75s/it] 17%|█▋        | 1552/8917 [2:05:32<7:31:27,  3.68s/it] 17%|█▋        | 1553/8917 [2:05:36<7:21:08,  3.59s/it] 17%|█▋        | 1554/8917 [2:05:39<7:25:33,  3.63s/it] 17%|█▋        | 1555/8917 [2:05:43<7:32:40,  3.69s/it] 17%|█▋        | 1556/8917 [2:05:47<7:37:18,  3.73s/it] 17%|█▋        | 1557/8917 [2:05:51<7:29:59,  3.67s/it] 17%|█▋        | 1558/8917 [2:05:55<7:40:52,  3.76s/it] 17%|█▋        | 1559/8917 [2:05:58<7:37:57,  3.73s/it] 17%|█▋        | 1560/8917 [2:06:02<7:41:08,  3.76s/it] 18%|█▊        | 1561/8917 [2:06:06<7:42:03,  3.77s/it] 18%|█▊        | 1562/8917 [2:06:10<7:40:22,  3.76s/it] 18%|█▊        | 1563/8917 [2:06:13<7:46:00,  3.80s/it] 18%|█▊        | 1564/8917 [2:06:17<7:40:36,  3.76s/it] 18%|█▊        | 1565/8917 [2:06:21<7:39:11,  3.75s/it] 18%|█▊        | 1566/8917 [2:06:24<7:24:55,  3.63s/it] 18%|█▊        | 1567/8917 [2:06:28<7:30:04,  3.67s/it] 18%|█▊        | 1568/8917 [2:06:32<7:28:56,  3.67s/it] 18%|█▊        | 1569/8917 [2:06:36<7:45:24,  3.80s/it] 18%|█▊        | 1570/8917 [2:06:40<7:47:13,  3.82s/it] 18%|█▊        | 1571/8917 [2:06:44<7:54:12,  3.87s/it] 18%|█▊        | 1572/8917 [2:06:47<7:45:58,  3.81s/it] 18%|█▊        | 1573/8917 [2:06:51<7:39:45,  3.76s/it] 18%|█▊        | 1574/8917 [2:06:54<7:32:45,  3.70s/it] 18%|█▊        | 1575/8917 [2:06:58<7:38:04,  3.74s/it] 18%|█▊        | 1576/8917 [2:07:02<7:33:48,  3.71s/it] 18%|█▊        | 1577/8917 [2:07:06<7:36:03,  3.73s/it] 18%|█▊        | 1578/8917 [2:07:09<7:39:18,  3.76s/it] 18%|█▊        | 1579/8917 [2:07:13<7:31:37,  3.69s/it] 18%|█▊        | 1580/8917 [2:07:17<7:27:37,  3.66s/it] 18%|█▊        | 1581/8917 [2:07:20<7:29:29,  3.68s/it] 18%|█▊        | 1582/8917 [2:07:24<7:29:48,  3.68s/it] 18%|█▊        | 1583/8917 [2:07:28<7:49:18,  3.84s/it] 18%|█▊        | 1584/8917 [2:07:32<7:42:54,  3.79s/it] 18%|█▊        | 1585/8917 [2:07:36<7:49:49,  3.84s/it] 18%|█▊        | 1586/8917 [2:07:40<7:48:46,  3.84s/it] 18%|█▊        | 1587/8917 [2:07:43<7:45:01,  3.81s/it] 18%|█▊        | 1588/8917 [2:07:47<7:30:11,  3.69s/it] 18%|█▊        | 1589/8917 [2:07:51<7:44:08,  3.80s/it] 18%|█▊        | 1590/8917 [2:07:55<7:38:34,  3.76s/it] 18%|█▊        | 1591/8917 [2:07:58<7:38:47,  3.76s/it] 18%|█▊        | 1592/8917 [2:08:02<7:29:31,  3.68s/it] 18%|█▊        | 1593/8917 [2:08:06<7:32:51,  3.71s/it] 18%|█▊        | 1594/8917 [2:08:10<7:40:35,  3.77s/it] 18%|█▊        | 1595/8917 [2:08:14<8:00:58,  3.94s/it] 18%|█▊        | 1596/8917 [2:08:17<7:45:59,  3.82s/it] 18%|█▊        | 1597/8917 [2:08:21<7:45:11,  3.81s/it] 18%|█▊        | 1598/8917 [2:08:25<7:31:55,  3.70s/it] 18%|█▊        | 1599/8917 [2:08:29<7:38:07,  3.76s/it]09/19/2024 04:23:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03301401063799858, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3713703155517578, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4043843746185303}
 18%|█▊        | 1600/8917 [2:08:32<7:37:29,  3.75s/it] 18%|█▊        | 1601/8917 [2:08:36<7:28:58,  3.68s/it] 18%|█▊        | 1602/8917 [2:08:40<7:31:13,  3.70s/it] 18%|█▊        | 1603/8917 [2:08:43<7:37:32,  3.75s/it] 18%|█▊        | 1604/8917 [2:08:47<7:44:37,  3.81s/it] 18%|█▊        | 1605/8917 [2:08:51<7:40:42,  3.78s/it] 18%|█▊        | 1606/8917 [2:08:54<7:21:42,  3.63s/it] 18%|█▊        | 1607/8917 [2:08:58<7:41:39,  3.79s/it] 18%|█▊        | 1608/8917 [2:09:02<7:36:05,  3.74s/it] 18%|█▊        | 1609/8917 [2:09:06<7:27:46,  3.68s/it] 18%|█▊        | 1610/8917 [2:09:10<7:37:44,  3.76s/it] 18%|█▊        | 1611/8917 [2:09:14<7:48:41,  3.85s/it] 18%|█▊        | 1612/8917 [2:09:17<7:45:12,  3.82s/it] 18%|█▊        | 1613/8917 [2:09:21<7:41:35,  3.79s/it] 18%|█▊        | 1614/8917 [2:09:25<7:36:43,  3.75s/it] 18%|█▊        | 1615/8917 [2:09:29<7:41:51,  3.80s/it] 18%|█▊        | 1616/8917 [2:09:33<7:44:13,  3.81s/it] 18%|█▊        | 1617/8917 [2:09:36<7:37:47,  3.76s/it] 18%|█▊        | 1618/8917 [2:09:40<7:50:56,  3.87s/it] 18%|█▊        | 1619/8917 [2:09:44<7:32:59,  3.72s/it] 18%|█▊        | 1620/8917 [2:09:48<7:39:54,  3.78s/it] 18%|█▊        | 1621/8917 [2:09:52<7:44:28,  3.82s/it] 18%|█▊        | 1622/8917 [2:09:55<7:34:19,  3.74s/it] 18%|█▊        | 1623/8917 [2:09:59<7:46:26,  3.84s/it] 18%|█▊        | 1624/8917 [2:10:03<7:39:24,  3.78s/it] 18%|█▊        | 1625/8917 [2:10:06<7:21:22,  3.63s/it] 18%|█▊        | 1626/8917 [2:10:10<7:26:59,  3.68s/it] 18%|█▊        | 1627/8917 [2:10:14<7:37:18,  3.76s/it] 18%|█▊        | 1628/8917 [2:10:18<7:34:22,  3.74s/it] 18%|█▊        | 1629/8917 [2:10:21<7:24:40,  3.66s/it] 18%|█▊        | 1630/8917 [2:10:25<7:23:16,  3.65s/it] 18%|█▊        | 1631/8917 [2:10:29<7:40:35,  3.79s/it] 18%|█▊        | 1632/8917 [2:10:32<7:38:19,  3.77s/it] 18%|█▊        | 1633/8917 [2:10:36<7:41:27,  3.80s/it] 18%|█▊        | 1634/8917 [2:10:40<7:42:50,  3.81s/it] 18%|█▊        | 1635/8917 [2:10:44<7:32:55,  3.73s/it] 18%|█▊        | 1636/8917 [2:10:47<7:31:10,  3.72s/it] 18%|█▊        | 1637/8917 [2:10:51<7:36:53,  3.77s/it] 18%|█▊        | 1638/8917 [2:10:55<7:43:12,  3.82s/it] 18%|█▊        | 1639/8917 [2:10:59<7:35:39,  3.76s/it] 18%|█▊        | 1640/8917 [2:11:03<7:34:11,  3.74s/it] 18%|█▊        | 1641/8917 [2:11:06<7:37:27,  3.77s/it] 18%|█▊        | 1642/8917 [2:11:10<7:20:56,  3.64s/it] 18%|█▊        | 1643/8917 [2:11:13<7:25:22,  3.67s/it] 18%|█▊        | 1644/8917 [2:11:17<7:31:11,  3.72s/it] 18%|█▊        | 1645/8917 [2:11:21<7:30:35,  3.72s/it] 18%|█▊        | 1646/8917 [2:11:25<7:32:47,  3.74s/it] 18%|█▊        | 1647/8917 [2:11:29<7:32:30,  3.73s/it] 18%|█▊        | 1648/8917 [2:11:32<7:34:02,  3.75s/it] 18%|█▊        | 1649/8917 [2:11:36<7:33:14,  3.74s/it]09/19/2024 04:26:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03479456156492233, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.791095495223999, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.825890064239502}
 19%|█▊        | 1650/8917 [2:11:40<7:24:12,  3.67s/it] 19%|█▊        | 1651/8917 [2:11:44<7:40:27,  3.80s/it] 19%|█▊        | 1652/8917 [2:11:47<7:34:14,  3.75s/it] 19%|█▊        | 1653/8917 [2:11:51<7:33:37,  3.75s/it] 19%|█▊        | 1654/8917 [2:11:54<7:22:26,  3.66s/it] 19%|█▊        | 1655/8917 [2:11:58<7:29:08,  3.71s/it] 19%|█▊        | 1656/8917 [2:12:02<7:30:01,  3.72s/it] 19%|█▊        | 1657/8917 [2:12:06<7:28:15,  3.70s/it] 19%|█▊        | 1658/8917 [2:12:10<7:48:52,  3.88s/it] 19%|█▊        | 1659/8917 [2:12:14<7:43:21,  3.83s/it] 19%|█▊        | 1660/8917 [2:12:17<7:29:35,  3.72s/it] 19%|█▊        | 1661/8917 [2:12:21<7:23:50,  3.67s/it] 19%|█▊        | 1662/8917 [2:12:24<7:24:50,  3.68s/it] 19%|█▊        | 1663/8917 [2:12:28<7:23:55,  3.67s/it] 19%|█▊        | 1664/8917 [2:12:32<7:32:42,  3.74s/it] 19%|█▊        | 1665/8917 [2:12:36<7:35:13,  3.77s/it] 19%|█▊        | 1666/8917 [2:12:39<7:29:56,  3.72s/it] 19%|█▊        | 1667/8917 [2:12:43<7:22:35,  3.66s/it] 19%|█▊        | 1668/8917 [2:12:47<7:21:00,  3.65s/it] 19%|█▊        | 1669/8917 [2:12:51<7:37:44,  3.79s/it] 19%|█▊        | 1670/8917 [2:12:54<7:33:13,  3.75s/it] 19%|█▊        | 1671/8917 [2:12:58<7:35:34,  3.77s/it] 19%|█▉        | 1672/8917 [2:13:02<7:32:31,  3.75s/it] 19%|█▉        | 1673/8917 [2:13:05<7:22:36,  3.67s/it] 19%|█▉        | 1674/8917 [2:13:09<7:27:50,  3.71s/it] 19%|█▉        | 1675/8917 [2:13:13<7:25:56,  3.69s/it] 19%|█▉        | 1676/8917 [2:13:16<7:21:52,  3.66s/it] 19%|█▉        | 1677/8917 [2:13:20<7:12:58,  3.59s/it] 19%|█▉        | 1678/8917 [2:13:23<7:05:11,  3.52s/it] 19%|█▉        | 1679/8917 [2:13:27<7:12:23,  3.58s/it] 19%|█▉        | 1680/8917 [2:13:31<7:17:27,  3.63s/it] 19%|█▉        | 1681/8917 [2:13:34<7:08:33,  3.55s/it] 19%|█▉        | 1682/8917 [2:13:38<7:29:26,  3.73s/it] 19%|█▉        | 1683/8917 [2:13:42<7:28:55,  3.72s/it] 19%|█▉        | 1684/8917 [2:13:45<7:21:21,  3.66s/it] 19%|█▉        | 1685/8917 [2:13:49<7:24:08,  3.68s/it] 19%|█▉        | 1686/8917 [2:13:53<7:20:22,  3.65s/it] 19%|█▉        | 1687/8917 [2:13:56<7:20:25,  3.65s/it] 19%|█▉        | 1688/8917 [2:14:00<7:25:01,  3.69s/it] 19%|█▉        | 1689/8917 [2:14:04<7:27:38,  3.72s/it] 19%|█▉        | 1690/8917 [2:14:08<7:27:22,  3.71s/it] 19%|█▉        | 1691/8917 [2:14:11<7:19:52,  3.65s/it] 19%|█▉        | 1692/8917 [2:14:15<7:23:27,  3.68s/it] 19%|█▉        | 1693/8917 [2:14:19<7:27:00,  3.71s/it] 19%|█▉        | 1694/8917 [2:14:22<7:13:16,  3.60s/it] 19%|█▉        | 1695/8917 [2:14:26<7:21:02,  3.66s/it] 19%|█▉        | 1696/8917 [2:14:30<7:41:50,  3.84s/it] 19%|█▉        | 1697/8917 [2:14:33<7:27:17,  3.72s/it] 19%|█▉        | 1698/8917 [2:14:37<7:26:24,  3.71s/it] 19%|█▉        | 1699/8917 [2:14:41<7:26:45,  3.71s/it]09/19/2024 04:29:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023303115740418434, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3175421953201294, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3408453464508057}
 19%|█▉        | 1700/8917 [2:14:44<7:22:29,  3.68s/it] 19%|█▉        | 1701/8917 [2:14:48<7:28:45,  3.73s/it] 19%|█▉        | 1702/8917 [2:14:52<7:26:04,  3.71s/it] 19%|█▉        | 1703/8917 [2:14:56<7:35:54,  3.79s/it] 19%|█▉        | 1704/8917 [2:14:59<7:25:50,  3.71s/it] 19%|█▉        | 1705/8917 [2:15:03<7:24:55,  3.70s/it] 19%|█▉        | 1706/8917 [2:15:07<7:33:13,  3.77s/it] 19%|█▉        | 1707/8917 [2:15:11<7:33:20,  3.77s/it] 19%|█▉        | 1708/8917 [2:15:15<7:32:11,  3.76s/it] 19%|█▉        | 1709/8917 [2:15:18<7:24:33,  3.70s/it] 19%|█▉        | 1710/8917 [2:15:22<7:27:02,  3.72s/it] 19%|█▉        | 1711/8917 [2:15:26<7:38:00,  3.81s/it] 19%|█▉        | 1712/8917 [2:15:30<7:41:36,  3.84s/it] 19%|█▉        | 1713/8917 [2:15:34<7:41:43,  3.85s/it] 19%|█▉        | 1714/8917 [2:15:38<7:41:50,  3.85s/it] 19%|█▉        | 1715/8917 [2:15:41<7:34:54,  3.79s/it] 19%|█▉        | 1716/8917 [2:15:45<7:36:15,  3.80s/it] 19%|█▉        | 1717/8917 [2:15:49<7:30:36,  3.76s/it] 19%|█▉        | 1718/8917 [2:15:52<7:30:10,  3.75s/it] 19%|█▉        | 1719/8917 [2:15:57<7:45:37,  3.88s/it] 19%|█▉        | 1720/8917 [2:16:00<7:43:35,  3.86s/it] 19%|█▉        | 1721/8917 [2:16:04<7:46:47,  3.89s/it] 19%|█▉        | 1722/8917 [2:16:08<7:51:05,  3.93s/it] 19%|█▉        | 1723/8917 [2:16:12<7:43:05,  3.86s/it] 19%|█▉        | 1724/8917 [2:16:17<8:04:14,  4.04s/it] 19%|█▉        | 1725/8917 [2:16:20<7:40:49,  3.84s/it] 19%|█▉        | 1726/8917 [2:16:24<7:31:11,  3.76s/it] 19%|█▉        | 1727/8917 [2:16:27<7:33:17,  3.78s/it] 19%|█▉        | 1728/8917 [2:16:31<7:41:13,  3.85s/it] 19%|█▉        | 1729/8917 [2:16:35<7:24:25,  3.71s/it] 19%|█▉        | 1730/8917 [2:16:38<7:24:01,  3.71s/it] 19%|█▉        | 1731/8917 [2:16:42<7:33:50,  3.79s/it] 19%|█▉        | 1732/8917 [2:16:46<7:31:11,  3.77s/it] 19%|█▉        | 1733/8917 [2:16:50<7:38:57,  3.83s/it] 19%|█▉        | 1734/8917 [2:16:54<7:33:38,  3.79s/it] 19%|█▉        | 1735/8917 [2:16:57<7:19:09,  3.67s/it] 19%|█▉        | 1736/8917 [2:17:01<7:38:29,  3.83s/it] 19%|█▉        | 1737/8917 [2:17:05<7:35:18,  3.80s/it] 19%|█▉        | 1738/8917 [2:17:09<7:29:44,  3.76s/it] 20%|█▉        | 1739/8917 [2:17:13<7:39:07,  3.84s/it] 20%|█▉        | 1740/8917 [2:17:16<7:21:37,  3.69s/it] 20%|█▉        | 1741/8917 [2:17:20<7:26:26,  3.73s/it] 20%|█▉        | 1742/8917 [2:17:24<7:19:31,  3.68s/it] 20%|█▉        | 1743/8917 [2:17:28<7:27:59,  3.75s/it] 20%|█▉        | 1744/8917 [2:17:32<7:37:09,  3.82s/it] 20%|█▉        | 1745/8917 [2:17:35<7:25:28,  3.73s/it] 20%|█▉        | 1746/8917 [2:17:39<7:35:44,  3.81s/it] 20%|█▉        | 1747/8917 [2:17:43<7:31:23,  3.78s/it] 20%|█▉        | 1748/8917 [2:17:47<7:42:14,  3.87s/it] 20%|█▉        | 1749/8917 [2:17:50<7:33:12,  3.79s/it]09/19/2024 04:32:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03644128888845444, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6004914045333862, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.6369327306747437}
 20%|█▉        | 1750/8917 [2:17:54<7:39:18,  3.85s/it] 20%|█▉        | 1751/8917 [2:17:58<7:28:15,  3.75s/it] 20%|█▉        | 1752/8917 [2:18:02<7:29:20,  3.76s/it] 20%|█▉        | 1753/8917 [2:18:05<7:24:12,  3.72s/it] 20%|█▉        | 1754/8917 [2:18:09<7:27:10,  3.75s/it] 20%|█▉        | 1755/8917 [2:18:13<7:33:43,  3.80s/it] 20%|█▉        | 1756/8917 [2:18:17<7:22:29,  3.71s/it] 20%|█▉        | 1757/8917 [2:18:21<7:40:15,  3.86s/it] 20%|█▉        | 1758/8917 [2:18:24<7:21:25,  3.70s/it] 20%|█▉        | 1759/8917 [2:18:28<7:20:57,  3.70s/it] 20%|█▉        | 1760/8917 [2:18:31<7:20:03,  3.69s/it] 20%|█▉        | 1761/8917 [2:18:36<7:36:54,  3.83s/it] 20%|█▉        | 1762/8917 [2:18:40<7:46:06,  3.91s/it] 20%|█▉        | 1763/8917 [2:18:43<7:39:47,  3.86s/it] 20%|█▉        | 1764/8917 [2:18:47<7:27:53,  3.76s/it] 20%|█▉        | 1765/8917 [2:18:50<7:14:37,  3.65s/it] 20%|█▉        | 1766/8917 [2:18:54<7:16:58,  3.67s/it] 20%|█▉        | 1767/8917 [2:18:58<7:29:43,  3.77s/it] 20%|█▉        | 1768/8917 [2:19:02<7:23:01,  3.72s/it] 20%|█▉        | 1769/8917 [2:19:05<7:17:50,  3.68s/it] 20%|█▉        | 1770/8917 [2:19:09<7:19:00,  3.69s/it] 20%|█▉        | 1771/8917 [2:19:13<7:18:52,  3.68s/it] 20%|█▉        | 1772/8917 [2:19:17<7:26:07,  3.75s/it] 20%|█▉        | 1773/8917 [2:19:21<7:33:24,  3.81s/it] 20%|█▉        | 1774/8917 [2:19:24<7:38:47,  3.85s/it] 20%|█▉        | 1775/8917 [2:19:28<7:33:47,  3.81s/it] 20%|█▉        | 1776/8917 [2:19:32<7:29:50,  3.78s/it] 20%|█▉        | 1777/8917 [2:19:36<7:29:17,  3.78s/it] 20%|█▉        | 1778/8917 [2:19:39<7:24:36,  3.74s/it] 20%|█▉        | 1779/8917 [2:19:43<7:31:37,  3.80s/it] 20%|█▉        | 1780/8917 [2:19:47<7:39:27,  3.86s/it] 20%|█▉        | 1781/8917 [2:19:51<7:37:44,  3.85s/it] 20%|█▉        | 1782/8917 [2:19:54<7:20:07,  3.70s/it] 20%|█▉        | 1783/8917 [2:19:58<7:27:25,  3.76s/it] 20%|██        | 1784/8917 [2:20:02<7:24:00,  3.73s/it] 20%|██        | 1785/8917 [2:20:06<7:25:13,  3.75s/it] 20%|██        | 1786/8917 [2:20:10<7:29:28,  3.78s/it] 20%|██        | 1787/8917 [2:20:13<7:19:25,  3.70s/it] 20%|██        | 1788/8917 [2:20:17<7:32:55,  3.81s/it] 20%|██        | 1789/8917 [2:20:21<7:37:58,  3.85s/it] 20%|██        | 1790/8917 [2:20:25<7:39:58,  3.87s/it] 20%|██        | 1791/8917 [2:20:29<7:44:46,  3.91s/it] 20%|██        | 1792/8917 [2:20:33<7:35:41,  3.84s/it] 20%|██        | 1793/8917 [2:20:37<7:36:56,  3.85s/it] 20%|██        | 1794/8917 [2:20:40<7:34:22,  3.83s/it] 20%|██        | 1795/8917 [2:20:44<7:28:20,  3.78s/it] 20%|██        | 1796/8917 [2:20:48<7:26:16,  3.76s/it] 20%|██        | 1797/8917 [2:20:51<7:12:29,  3.64s/it] 20%|██        | 1798/8917 [2:20:55<7:22:05,  3.73s/it] 20%|██        | 1799/8917 [2:20:59<7:42:15,  3.90s/it]09/19/2024 04:35:37 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025036169216036797, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.193162202835083, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2181984186172485}
 20%|██        | 1800/8917 [2:21:03<7:23:52,  3.74s/it] 20%|██        | 1801/8917 [2:21:06<7:16:39,  3.68s/it] 20%|██        | 1802/8917 [2:21:10<7:20:02,  3.71s/it] 20%|██        | 1803/8917 [2:21:14<7:16:40,  3.68s/it] 20%|██        | 1804/8917 [2:21:17<7:16:03,  3.68s/it] 20%|██        | 1805/8917 [2:21:21<7:19:08,  3.70s/it] 20%|██        | 1806/8917 [2:21:25<7:11:08,  3.64s/it] 20%|██        | 1807/8917 [2:21:29<7:21:35,  3.73s/it] 20%|██        | 1808/8917 [2:21:32<7:24:30,  3.75s/it] 20%|██        | 1809/8917 [2:21:36<7:16:24,  3.68s/it] 20%|██        | 1810/8917 [2:21:40<7:24:26,  3.75s/it] 20%|██        | 1811/8917 [2:21:43<7:18:58,  3.71s/it] 20%|██        | 1812/8917 [2:21:47<7:12:05,  3.65s/it] 20%|██        | 1813/8917 [2:21:50<7:10:06,  3.63s/it] 20%|██        | 1814/8917 [2:21:54<7:17:09,  3.69s/it] 20%|██        | 1815/8917 [2:21:58<7:29:03,  3.79s/it] 20%|██        | 1816/8917 [2:22:02<7:31:57,  3.82s/it] 20%|██        | 1817/8917 [2:22:06<7:32:13,  3.82s/it] 20%|██        | 1818/8917 [2:22:10<7:24:48,  3.76s/it] 20%|██        | 1819/8917 [2:22:13<7:19:11,  3.71s/it] 20%|██        | 1820/8917 [2:22:17<7:24:19,  3.76s/it] 20%|██        | 1821/8917 [2:22:21<7:24:54,  3.76s/it] 20%|██        | 1822/8917 [2:22:25<7:21:07,  3.73s/it] 20%|██        | 1823/8917 [2:22:28<7:26:54,  3.78s/it] 20%|██        | 1824/8917 [2:22:32<7:18:35,  3.71s/it] 20%|██        | 1825/8917 [2:22:36<7:19:34,  3.72s/it] 20%|██        | 1826/8917 [2:22:40<7:35:08,  3.85s/it] 20%|██        | 1827/8917 [2:22:44<7:28:04,  3.79s/it] 21%|██        | 1828/8917 [2:22:47<7:32:13,  3.83s/it] 21%|██        | 1829/8917 [2:22:51<7:22:49,  3.75s/it] 21%|██        | 1830/8917 [2:22:55<7:17:00,  3.70s/it] 21%|██        | 1831/8917 [2:22:58<7:17:58,  3.71s/it] 21%|██        | 1832/8917 [2:23:02<7:25:07,  3.77s/it] 21%|██        | 1833/8917 [2:23:06<7:18:21,  3.71s/it] 21%|██        | 1834/8917 [2:23:10<7:23:50,  3.76s/it] 21%|██        | 1835/8917 [2:23:14<7:25:19,  3.77s/it] 21%|██        | 1836/8917 [2:23:17<7:28:40,  3.80s/it] 21%|██        | 1837/8917 [2:23:21<7:24:30,  3.77s/it] 21%|██        | 1838/8917 [2:23:25<7:23:55,  3.76s/it] 21%|██        | 1839/8917 [2:23:29<7:22:06,  3.75s/it] 21%|██        | 1840/8917 [2:23:32<7:16:02,  3.70s/it] 21%|██        | 1841/8917 [2:23:36<7:19:34,  3.73s/it] 21%|██        | 1842/8917 [2:23:40<7:15:59,  3.70s/it] 21%|██        | 1843/8917 [2:23:44<7:25:26,  3.78s/it] 21%|██        | 1844/8917 [2:23:47<7:27:14,  3.79s/it] 21%|██        | 1845/8917 [2:23:51<7:35:58,  3.87s/it] 21%|██        | 1846/8917 [2:23:55<7:25:14,  3.78s/it] 21%|██        | 1847/8917 [2:23:59<7:20:50,  3.74s/it] 21%|██        | 1848/8917 [2:24:02<7:22:57,  3.76s/it] 21%|██        | 1849/8917 [2:24:06<7:21:27,  3.75s/it]09/19/2024 04:38:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025126466527581215, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2327781915664673, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2579046487808228}
 21%|██        | 1850/8917 [2:24:10<7:21:53,  3.75s/it] 21%|██        | 1851/8917 [2:24:14<7:25:42,  3.78s/it] 21%|██        | 1852/8917 [2:24:17<7:21:04,  3.75s/it] 21%|██        | 1853/8917 [2:24:21<7:18:26,  3.72s/it] 21%|██        | 1854/8917 [2:24:25<7:22:46,  3.76s/it] 21%|██        | 1855/8917 [2:24:29<7:28:56,  3.81s/it] 21%|██        | 1856/8917 [2:24:32<7:21:40,  3.75s/it] 21%|██        | 1857/8917 [2:24:36<7:17:59,  3.72s/it] 21%|██        | 1858/8917 [2:24:40<7:17:51,  3.72s/it] 21%|██        | 1859/8917 [2:24:44<7:24:24,  3.78s/it] 21%|██        | 1860/8917 [2:24:48<7:26:45,  3.80s/it] 21%|██        | 1861/8917 [2:24:51<7:22:56,  3.77s/it] 21%|██        | 1862/8917 [2:24:55<7:19:42,  3.74s/it] 21%|██        | 1863/8917 [2:24:59<7:17:40,  3.72s/it] 21%|██        | 1864/8917 [2:25:03<7:26:34,  3.80s/it] 21%|██        | 1865/8917 [2:25:06<7:17:51,  3.73s/it] 21%|██        | 1866/8917 [2:25:10<7:13:29,  3.69s/it] 21%|██        | 1867/8917 [2:25:14<7:22:20,  3.76s/it] 21%|██        | 1868/8917 [2:25:17<7:15:03,  3.70s/it] 21%|██        | 1869/8917 [2:25:21<7:09:01,  3.65s/it] 21%|██        | 1870/8917 [2:25:25<7:17:19,  3.72s/it] 21%|██        | 1871/8917 [2:25:28<7:14:18,  3.70s/it] 21%|██        | 1872/8917 [2:25:32<7:17:28,  3.73s/it] 21%|██        | 1873/8917 [2:25:36<7:14:06,  3.70s/it] 21%|██        | 1874/8917 [2:25:40<7:20:27,  3.75s/it] 21%|██        | 1875/8917 [2:25:44<7:25:01,  3.79s/it] 21%|██        | 1876/8917 [2:25:47<7:06:23,  3.63s/it] 21%|██        | 1877/8917 [2:25:51<7:08:48,  3.65s/it] 21%|██        | 1878/8917 [2:25:55<7:20:53,  3.76s/it] 21%|██        | 1879/8917 [2:25:58<7:21:26,  3.76s/it] 21%|██        | 1880/8917 [2:26:02<7:19:11,  3.74s/it] 21%|██        | 1881/8917 [2:26:06<7:38:23,  3.91s/it] 21%|██        | 1882/8917 [2:26:10<7:27:33,  3.82s/it] 21%|██        | 1883/8917 [2:26:14<7:24:50,  3.79s/it] 21%|██        | 1884/8917 [2:26:18<7:27:52,  3.82s/it] 21%|██        | 1885/8917 [2:26:21<7:12:12,  3.69s/it] 21%|██        | 1886/8917 [2:26:25<7:13:55,  3.70s/it] 21%|██        | 1887/8917 [2:26:28<7:14:14,  3.71s/it] 21%|██        | 1888/8917 [2:26:32<7:16:49,  3.73s/it] 21%|██        | 1889/8917 [2:26:36<7:14:39,  3.71s/it] 21%|██        | 1890/8917 [2:26:40<7:27:46,  3.82s/it] 21%|██        | 1891/8917 [2:26:44<7:38:07,  3.91s/it] 21%|██        | 1892/8917 [2:26:47<7:23:02,  3.78s/it] 21%|██        | 1893/8917 [2:26:51<7:21:12,  3.77s/it] 21%|██        | 1894/8917 [2:26:55<7:10:59,  3.68s/it] 21%|██▏       | 1895/8917 [2:26:58<7:09:43,  3.67s/it] 21%|██▏       | 1896/8917 [2:27:02<7:07:00,  3.65s/it] 21%|██▏       | 1897/8917 [2:27:05<7:00:18,  3.59s/it] 21%|██▏       | 1898/8917 [2:27:09<7:10:24,  3.68s/it] 21%|██▏       | 1899/8917 [2:27:13<7:16:19,  3.73s/it]09/19/2024 04:41:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03511594980955124, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.5230180025100708, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5581339597702026}
 21%|██▏       | 1900/8917 [2:27:17<7:09:52,  3.68s/it] 21%|██▏       | 1901/8917 [2:27:21<7:16:58,  3.74s/it] 21%|██▏       | 1902/8917 [2:27:24<7:15:45,  3.73s/it] 21%|██▏       | 1903/8917 [2:27:28<7:17:34,  3.74s/it] 21%|██▏       | 1904/8917 [2:27:32<7:17:16,  3.74s/it] 21%|██▏       | 1905/8917 [2:27:35<7:10:31,  3.68s/it] 21%|██▏       | 1906/8917 [2:27:39<7:20:09,  3.77s/it] 21%|██▏       | 1907/8917 [2:27:43<7:10:15,  3.68s/it] 21%|██▏       | 1908/8917 [2:27:47<7:15:18,  3.73s/it] 21%|██▏       | 1909/8917 [2:27:50<7:15:31,  3.73s/it] 21%|██▏       | 1910/8917 [2:27:54<7:08:59,  3.67s/it] 21%|██▏       | 1911/8917 [2:27:58<7:12:04,  3.70s/it] 21%|██▏       | 1912/8917 [2:28:01<7:09:23,  3.68s/it] 21%|██▏       | 1913/8917 [2:28:05<7:12:06,  3.70s/it] 21%|██▏       | 1914/8917 [2:28:09<7:06:23,  3.65s/it] 21%|██▏       | 1915/8917 [2:28:12<7:13:51,  3.72s/it] 21%|██▏       | 1916/8917 [2:28:16<7:08:12,  3.67s/it] 21%|██▏       | 1917/8917 [2:28:20<7:12:03,  3.70s/it] 22%|██▏       | 1918/8917 [2:28:24<7:14:46,  3.73s/it] 22%|██▏       | 1919/8917 [2:28:27<7:06:48,  3.66s/it] 22%|██▏       | 1920/8917 [2:28:31<7:20:36,  3.78s/it] 22%|██▏       | 1921/8917 [2:28:35<7:33:04,  3.89s/it] 22%|██▏       | 1922/8917 [2:28:39<7:20:27,  3.78s/it] 22%|██▏       | 1923/8917 [2:28:43<7:28:34,  3.85s/it] 22%|██▏       | 1924/8917 [2:28:47<7:30:41,  3.87s/it] 22%|██▏       | 1925/8917 [2:28:50<7:14:28,  3.73s/it] 22%|██▏       | 1926/8917 [2:28:54<7:09:29,  3.69s/it] 22%|██▏       | 1927/8917 [2:28:58<7:21:33,  3.79s/it] 22%|██▏       | 1928/8917 [2:29:01<7:09:02,  3.68s/it] 22%|██▏       | 1929/8917 [2:29:05<6:58:21,  3.59s/it] 22%|██▏       | 1930/8917 [2:29:08<7:09:16,  3.69s/it] 22%|██▏       | 1931/8917 [2:29:12<7:16:39,  3.75s/it] 22%|██▏       | 1932/8917 [2:29:16<7:07:55,  3.68s/it] 22%|██▏       | 1933/8917 [2:29:20<7:09:35,  3.69s/it] 22%|██▏       | 1934/8917 [2:29:23<7:15:59,  3.75s/it] 22%|██▏       | 1935/8917 [2:29:27<7:21:27,  3.79s/it] 22%|██▏       | 1936/8917 [2:29:31<7:10:54,  3.70s/it] 22%|██▏       | 1937/8917 [2:29:35<7:15:13,  3.74s/it] 22%|██▏       | 1938/8917 [2:29:38<7:16:18,  3.75s/it] 22%|██▏       | 1939/8917 [2:29:42<7:20:12,  3.79s/it] 22%|██▏       | 1940/8917 [2:29:46<7:11:57,  3.71s/it] 22%|██▏       | 1941/8917 [2:29:50<7:22:10,  3.80s/it] 22%|██▏       | 1942/8917 [2:29:54<7:18:45,  3.77s/it] 22%|██▏       | 1943/8917 [2:29:57<7:12:18,  3.72s/it] 22%|██▏       | 1944/8917 [2:30:01<7:18:53,  3.78s/it] 22%|██▏       | 1945/8917 [2:30:05<7:22:27,  3.81s/it] 22%|██▏       | 1946/8917 [2:30:09<7:15:15,  3.75s/it] 22%|██▏       | 1947/8917 [2:30:12<7:15:18,  3.75s/it] 22%|██▏       | 1948/8917 [2:30:16<7:10:21,  3.71s/it] 22%|██▏       | 1949/8917 [2:30:20<7:19:40,  3.79s/it]09/19/2024 04:44:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02742486260831356, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.442129373550415, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4695541858673096}
 22%|██▏       | 1950/8917 [2:30:24<7:17:04,  3.76s/it] 22%|██▏       | 1951/8917 [2:30:27<7:06:43,  3.68s/it] 22%|██▏       | 1952/8917 [2:30:31<7:19:23,  3.79s/it] 22%|██▏       | 1953/8917 [2:30:35<7:07:48,  3.69s/it] 22%|██▏       | 1954/8917 [2:30:39<7:22:20,  3.81s/it] 22%|██▏       | 1955/8917 [2:30:43<7:30:28,  3.88s/it] 22%|██▏       | 1956/8917 [2:30:46<7:13:54,  3.74s/it] 22%|██▏       | 1957/8917 [2:30:49<7:00:07,  3.62s/it] 22%|██▏       | 1958/8917 [2:30:54<7:15:23,  3.75s/it] 22%|██▏       | 1959/8917 [2:30:57<7:17:14,  3.77s/it] 22%|██▏       | 1960/8917 [2:31:01<7:13:21,  3.74s/it] 22%|██▏       | 1961/8917 [2:31:05<7:08:18,  3.69s/it] 22%|██▏       | 1962/8917 [2:31:08<6:57:29,  3.60s/it] 22%|██▏       | 1963/8917 [2:31:11<6:49:08,  3.53s/it] 22%|██▏       | 1964/8917 [2:31:15<6:59:12,  3.62s/it] 22%|██▏       | 1965/8917 [2:31:19<7:00:41,  3.63s/it] 22%|██▏       | 1966/8917 [2:31:23<7:07:29,  3.69s/it] 22%|██▏       | 1967/8917 [2:31:27<7:24:11,  3.83s/it] 22%|██▏       | 1968/8917 [2:31:30<7:14:39,  3.75s/it] 22%|██▏       | 1969/8917 [2:31:34<7:13:04,  3.74s/it] 22%|██▏       | 1970/8917 [2:31:38<7:05:28,  3.67s/it] 22%|██▏       | 1971/8917 [2:31:42<7:15:45,  3.76s/it] 22%|██▏       | 1972/8917 [2:31:45<7:12:22,  3.74s/it] 22%|██▏       | 1973/8917 [2:31:49<7:11:18,  3.73s/it] 22%|██▏       | 1974/8917 [2:31:53<7:13:24,  3.75s/it] 22%|██▏       | 1975/8917 [2:31:56<7:06:09,  3.68s/it] 22%|██▏       | 1976/8917 [2:32:00<7:09:50,  3.72s/it] 22%|██▏       | 1977/8917 [2:32:04<7:17:11,  3.78s/it] 22%|██▏       | 1978/8917 [2:32:08<7:07:46,  3.70s/it] 22%|██▏       | 1979/8917 [2:32:12<7:19:43,  3.80s/it] 22%|██▏       | 1980/8917 [2:32:15<7:12:17,  3.74s/it] 22%|██▏       | 1981/8917 [2:32:19<7:09:10,  3.71s/it] 22%|██▏       | 1982/8917 [2:32:22<7:03:11,  3.66s/it] 22%|██▏       | 1983/8917 [2:32:26<7:02:11,  3.65s/it] 22%|██▏       | 1984/8917 [2:32:30<7:12:46,  3.75s/it] 22%|██▏       | 1985/8917 [2:32:34<7:12:08,  3.74s/it] 22%|██▏       | 1986/8917 [2:32:38<7:20:31,  3.81s/it] 22%|██▏       | 1987/8917 [2:32:41<7:09:50,  3.72s/it] 22%|██▏       | 1988/8917 [2:32:45<7:09:01,  3.71s/it] 22%|██▏       | 1989/8917 [2:32:49<7:09:38,  3.72s/it] 22%|██▏       | 1990/8917 [2:32:52<7:15:20,  3.77s/it] 22%|██▏       | 1991/8917 [2:32:56<7:17:06,  3.79s/it] 22%|██▏       | 1992/8917 [2:33:00<7:08:47,  3.72s/it] 22%|██▏       | 1993/8917 [2:33:04<7:11:46,  3.74s/it] 22%|██▏       | 1994/8917 [2:33:07<7:01:00,  3.65s/it] 22%|██▏       | 1995/8917 [2:33:11<7:06:52,  3.70s/it] 22%|██▏       | 1996/8917 [2:33:14<6:57:16,  3.62s/it] 22%|██▏       | 1997/8917 [2:33:18<7:13:17,  3.76s/it] 22%|██▏       | 1998/8917 [2:33:22<6:59:36,  3.64s/it] 22%|██▏       | 1999/8917 [2:33:26<7:10:46,  3.74s/it]09/19/2024 04:48:02 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 04:48:02 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:18,  2.79it/s][A
  1%|          | 2/221 [00:00<01:30,  2.43it/s][A
  1%|▏         | 3/221 [00:01<01:56,  1.88it/s][A
  2%|▏         | 4/221 [00:01<01:26,  2.52it/s][A
  2%|▏         | 5/221 [00:01<01:02,  3.44it/s][A
  3%|▎         | 6/221 [00:01<00:54,  3.98it/s][A
  4%|▎         | 8/221 [00:02<00:48,  4.40it/s][A
  4%|▍         | 9/221 [00:02<01:06,  3.21it/s][A
  5%|▍         | 10/221 [00:03<00:59,  3.57it/s][A
  5%|▌         | 12/221 [00:04<01:44,  2.00it/s][A
  6%|▌         | 13/221 [00:04<01:27,  2.38it/s][A
  7%|▋         | 15/221 [00:05<01:08,  3.00it/s][A
  7%|▋         | 16/221 [00:06<01:32,  2.21it/s][A
  8%|▊         | 17/221 [00:07<02:18,  1.48it/s][A
  8%|▊         | 18/221 [00:08<02:33,  1.32it/s][A
  9%|▊         | 19/221 [00:08<02:19,  1.44it/s][A
  9%|▉         | 20/221 [00:09<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:09<01:35,  2.10it/s][A
 10%|▉         | 22/221 [00:09<01:27,  2.28it/s][A
 11%|█         | 24/221 [00:10<01:01,  3.20it/s][A
 11%|█▏        | 25/221 [00:10<00:58,  3.35it/s][A
 12%|█▏        | 26/221 [00:10<00:55,  3.53it/s][A
 12%|█▏        | 27/221 [00:10<00:46,  4.22it/s][A
 13%|█▎        | 28/221 [00:11<01:06,  2.92it/s][A
 13%|█▎        | 29/221 [00:11<01:11,  2.68it/s][A
 14%|█▎        | 30/221 [00:12<01:21,  2.34it/s][A
 14%|█▍        | 31/221 [00:13<01:43,  1.83it/s][A
 15%|█▍        | 33/221 [00:13<01:16,  2.47it/s][A
 16%|█▌        | 35/221 [00:13<00:53,  3.49it/s][A
 16%|█▋        | 36/221 [00:14<00:52,  3.54it/s][A
 17%|█▋        | 37/221 [00:14<00:55,  3.29it/s][A
 17%|█▋        | 38/221 [00:15<01:48,  1.69it/s][A
 18%|█▊        | 40/221 [00:16<01:20,  2.26it/s][A
 19%|█▊        | 41/221 [00:16<01:10,  2.57it/s][A
 19%|█▉        | 42/221 [00:16<01:01,  2.91it/s][A
 19%|█▉        | 43/221 [00:16<00:51,  3.45it/s][A
 20%|█▉        | 44/221 [00:17<00:44,  3.95it/s][A
 20%|██        | 45/221 [00:22<04:39,  1.59s/it][A
 21%|██        | 46/221 [00:22<03:40,  1.26s/it][A
 21%|██▏       | 47/221 [00:23<03:09,  1.09s/it][A
 22%|██▏       | 48/221 [00:23<02:19,  1.24it/s][A
 22%|██▏       | 49/221 [00:23<01:47,  1.60it/s][A
 23%|██▎       | 50/221 [00:24<01:45,  1.62it/s][A
 23%|██▎       | 51/221 [00:24<01:21,  2.08it/s][A
 24%|██▎       | 52/221 [00:24<01:02,  2.71it/s][A
 24%|██▍       | 53/221 [00:24<00:48,  3.43it/s][A
 24%|██▍       | 54/221 [00:25<01:30,  1.85it/s][A
 25%|██▍       | 55/221 [00:28<02:59,  1.08s/it][A
 25%|██▌       | 56/221 [00:28<02:14,  1.23it/s][A
 26%|██▌       | 57/221 [00:28<01:45,  1.55it/s][A
 27%|██▋       | 59/221 [00:28<01:02,  2.60it/s][A
 27%|██▋       | 60/221 [00:29<01:29,  1.80it/s][A
 28%|██▊       | 61/221 [00:30<01:19,  2.02it/s][A
 28%|██▊       | 62/221 [00:30<01:08,  2.31it/s][A
 29%|██▊       | 63/221 [00:30<00:56,  2.79it/s][A
 29%|██▉       | 64/221 [00:31<01:19,  1.98it/s][A
 29%|██▉       | 65/221 [00:31<01:02,  2.51it/s][A
 30%|██▉       | 66/221 [00:31<01:04,  2.39it/s][A
 30%|███       | 67/221 [00:32<00:51,  2.98it/s][A
 31%|███       | 68/221 [00:32<00:42,  3.57it/s][A
 31%|███       | 69/221 [00:33<01:22,  1.83it/s][A
 32%|███▏      | 71/221 [00:33<00:54,  2.77it/s][A
 33%|███▎      | 72/221 [00:33<00:48,  3.10it/s][A
 33%|███▎      | 73/221 [00:34<01:08,  2.17it/s][A
 33%|███▎      | 74/221 [00:34<00:55,  2.63it/s][A
 34%|███▍      | 75/221 [00:35<01:03,  2.29it/s][A
 34%|███▍      | 76/221 [00:35<00:54,  2.65it/s][A
 35%|███▍      | 77/221 [00:37<01:45,  1.36it/s][A
 36%|███▌      | 79/221 [00:38<01:27,  1.62it/s][A
 37%|███▋      | 81/221 [00:39<01:20,  1.73it/s][A
 37%|███▋      | 82/221 [00:41<02:02,  1.14it/s][A
 38%|███▊      | 83/221 [00:41<01:54,  1.21it/s][A
 38%|███▊      | 84/221 [00:42<01:29,  1.53it/s][A
 39%|███▉      | 86/221 [00:42<01:09,  1.95it/s][A
 39%|███▉      | 87/221 [00:44<02:01,  1.10it/s][A
 40%|███▉      | 88/221 [00:45<01:39,  1.34it/s][A
 40%|████      | 89/221 [00:45<01:29,  1.48it/s][A
 41%|████      | 90/221 [00:45<01:14,  1.76it/s][A
 42%|████▏     | 92/221 [00:46<00:47,  2.72it/s][A
 42%|████▏     | 93/221 [00:46<00:46,  2.76it/s][A
 43%|████▎     | 94/221 [00:47<01:03,  1.99it/s][A
 43%|████▎     | 95/221 [00:47<00:57,  2.19it/s][A
 43%|████▎     | 96/221 [00:48<01:18,  1.58it/s][A
 44%|████▍     | 97/221 [00:48<01:01,  2.03it/s][A
 44%|████▍     | 98/221 [00:49<01:17,  1.58it/s][A
 45%|████▍     | 99/221 [00:50<01:01,  1.99it/s][A
 45%|████▌     | 100/221 [00:50<00:59,  2.04it/s][A
 46%|████▌     | 101/221 [00:50<00:46,  2.57it/s][A
 46%|████▌     | 102/221 [00:52<01:18,  1.51it/s][A
 47%|████▋     | 103/221 [00:52<00:59,  1.98it/s][A
 47%|████▋     | 104/221 [00:52<00:52,  2.21it/s][A
 48%|████▊     | 105/221 [00:53<00:56,  2.06it/s][A
 48%|████▊     | 106/221 [00:54<01:39,  1.15it/s][A
 48%|████▊     | 107/221 [00:55<01:20,  1.41it/s][A
 49%|████▉     | 108/221 [00:55<01:09,  1.62it/s][A
 49%|████▉     | 109/221 [00:56<01:02,  1.78it/s][A
 50%|█████     | 111/221 [00:56<00:45,  2.41it/s][A
 51%|█████     | 112/221 [00:56<00:44,  2.44it/s][A
 51%|█████     | 113/221 [00:57<00:38,  2.81it/s][A
 52%|█████▏    | 115/221 [00:57<00:28,  3.66it/s][A
 52%|█████▏    | 116/221 [00:57<00:31,  3.38it/s][A
 53%|█████▎    | 117/221 [00:58<00:40,  2.60it/s][A
 53%|█████▎    | 118/221 [00:58<00:40,  2.55it/s][A
 54%|█████▍    | 119/221 [00:59<00:53,  1.92it/s][A
 54%|█████▍    | 120/221 [01:00<00:46,  2.15it/s][A
 55%|█████▍    | 121/221 [01:00<00:45,  2.20it/s][A
 55%|█████▌    | 122/221 [01:00<00:40,  2.44it/s][A
 56%|█████▌    | 123/221 [01:01<01:03,  1.55it/s][A
 56%|█████▌    | 124/221 [01:02<00:48,  1.99it/s][A
 57%|█████▋    | 125/221 [01:03<01:09,  1.38it/s][A
 57%|█████▋    | 126/221 [01:09<03:31,  2.23s/it][A
 57%|█████▋    | 127/221 [01:09<02:49,  1.81s/it][A
 58%|█████▊    | 128/221 [01:10<02:14,  1.45s/it][A
 58%|█████▊    | 129/221 [01:10<01:40,  1.09s/it][A
 59%|█████▉    | 130/221 [01:11<01:24,  1.08it/s][A
 59%|█████▉    | 131/221 [01:12<01:16,  1.18it/s][A
 60%|█████▉    | 132/221 [01:13<01:28,  1.01it/s][A
 60%|██████    | 133/221 [01:13<01:07,  1.30it/s][A
 61%|██████    | 134/221 [01:15<01:38,  1.13s/it][A
 61%|██████    | 135/221 [01:16<01:25,  1.01it/s][A
 62%|██████▏   | 136/221 [01:16<01:11,  1.19it/s][A
 62%|██████▏   | 137/221 [01:17<00:58,  1.43it/s][A
 62%|██████▏   | 138/221 [01:18<01:11,  1.16it/s][A
 63%|██████▎   | 139/221 [01:18<00:54,  1.50it/s][A
 63%|██████▎   | 140/221 [01:19<00:53,  1.50it/s][A
 64%|██████▍   | 141/221 [01:19<00:53,  1.49it/s][A
 64%|██████▍   | 142/221 [01:20<00:45,  1.74it/s][A
 65%|██████▍   | 143/221 [01:20<00:42,  1.82it/s][A
 65%|██████▌   | 144/221 [01:21<00:39,  1.97it/s][A
 66%|██████▌   | 146/221 [01:21<00:22,  3.28it/s][A
 67%|██████▋   | 148/221 [01:21<00:23,  3.15it/s][A
 67%|██████▋   | 149/221 [01:22<00:29,  2.45it/s][A
 68%|██████▊   | 150/221 [01:23<00:28,  2.52it/s][A
 68%|██████▊   | 151/221 [01:23<00:27,  2.59it/s][A
 69%|██████▉   | 152/221 [01:24<00:35,  1.95it/s][A
 69%|██████▉   | 153/221 [01:24<00:27,  2.44it/s][A
 70%|██████▉   | 154/221 [01:24<00:26,  2.50it/s][A
 70%|███████   | 155/221 [01:25<00:27,  2.39it/s][A
 71%|███████   | 156/221 [01:25<00:23,  2.71it/s][A
 71%|███████   | 157/221 [01:28<01:21,  1.27s/it][A
 71%|███████▏  | 158/221 [01:29<01:04,  1.02s/it][A
 72%|███████▏  | 159/221 [01:29<00:51,  1.21it/s][A
 72%|███████▏  | 160/221 [01:29<00:38,  1.60it/s][A
 73%|███████▎  | 161/221 [01:29<00:28,  2.13it/s][A
 74%|███████▍  | 163/221 [01:30<00:20,  2.80it/s][A
 74%|███████▍  | 164/221 [01:30<00:21,  2.61it/s][A
 75%|███████▍  | 165/221 [01:31<00:24,  2.29it/s][A
 75%|███████▌  | 166/221 [01:32<00:25,  2.20it/s][A
 76%|███████▌  | 167/221 [01:32<00:20,  2.60it/s][A
 76%|███████▌  | 168/221 [01:39<02:03,  2.34s/it][A
 76%|███████▋  | 169/221 [01:39<01:31,  1.75s/it][A
 77%|███████▋  | 170/221 [01:40<01:09,  1.36s/it][A
 77%|███████▋  | 171/221 [01:40<00:54,  1.09s/it][A
 78%|███████▊  | 172/221 [01:40<00:40,  1.21it/s][A
 78%|███████▊  | 173/221 [01:41<00:38,  1.25it/s][A
 79%|███████▉  | 175/221 [01:41<00:23,  1.92it/s][A
 80%|███████▉  | 176/221 [01:42<00:19,  2.32it/s][A
 80%|████████  | 177/221 [01:42<00:16,  2.64it/s][A
 81%|████████  | 178/221 [01:42<00:18,  2.28it/s][A
 81%|████████  | 179/221 [01:43<00:18,  2.23it/s][A
 81%|████████▏ | 180/221 [01:43<00:14,  2.74it/s][A
 82%|████████▏ | 181/221 [01:43<00:11,  3.37it/s][A
 82%|████████▏ | 182/221 [01:44<00:14,  2.77it/s][A
 83%|████████▎ | 183/221 [01:45<00:19,  1.96it/s][A
 83%|████████▎ | 184/221 [01:45<00:19,  1.91it/s][A
 84%|████████▎ | 185/221 [01:45<00:14,  2.43it/s][A
 84%|████████▍ | 186/221 [01:46<00:13,  2.53it/s][A
 85%|████████▍ | 187/221 [01:46<00:12,  2.73it/s][A
 85%|████████▌ | 188/221 [01:46<00:10,  3.04it/s][A
 86%|████████▌ | 189/221 [01:47<00:12,  2.61it/s][A
 86%|████████▌ | 190/221 [01:47<00:14,  2.08it/s][A
 86%|████████▋ | 191/221 [01:48<00:12,  2.50it/s][A
 87%|████████▋ | 192/221 [01:48<00:12,  2.41it/s][A
 88%|████████▊ | 194/221 [01:50<00:17,  1.56it/s][A
 88%|████████▊ | 195/221 [01:50<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:50<00:11,  2.14it/s][A
 89%|████████▉ | 197/221 [01:51<00:10,  2.36it/s][A
 90%|████████▉ | 198/221 [01:51<00:08,  2.59it/s][A
 90%|█████████ | 199/221 [01:51<00:07,  3.09it/s][A
 90%|█████████ | 200/221 [01:52<00:07,  2.72it/s][A
 91%|█████████ | 201/221 [01:52<00:08,  2.34it/s][A
 91%|█████████▏| 202/221 [01:52<00:07,  2.63it/s][A
 92%|█████████▏| 203/221 [01:54<00:11,  1.53it/s][A
 92%|█████████▏| 204/221 [01:54<00:09,  1.81it/s][A
 93%|█████████▎| 205/221 [01:54<00:06,  2.35it/s][A
 93%|█████████▎| 206/221 [01:55<00:07,  1.98it/s][A
 94%|█████████▍| 208/221 [01:55<00:04,  2.72it/s][A
 95%|█████████▍| 209/221 [01:56<00:04,  2.87it/s][A
 95%|█████████▌| 211/221 [01:56<00:03,  2.69it/s][A
 96%|█████████▌| 212/221 [01:57<00:03,  2.73it/s][A
 96%|█████████▋| 213/221 [01:57<00:02,  3.13it/s][A
 97%|█████████▋| 214/221 [01:58<00:03,  2.19it/s][A
 98%|█████████▊| 216/221 [01:59<00:02,  2.32it/s][A
 98%|█████████▊| 217/221 [02:06<00:08,  2.13s/it][A
 99%|█████████▊| 218/221 [02:07<00:05,  1.74s/it][A
 99%|█████████▉| 219/221 [02:07<00:02,  1.42s/it][A
100%|█████████▉| 220/221 [02:10<00:01,  1.71s/it][A
100%|██████████| 221/221 [02:10<00:00,  1.27s/it][A100%|██████████| 221/221 [02:10<00:00,  1.69it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:32,  6.74it/s][A
  1%|          | 2/221 [00:00<01:42,  2.15it/s][A
  1%|▏         | 3/221 [00:01<01:27,  2.48it/s][A
  2%|▏         | 4/221 [00:01<01:24,  2.58it/s][A
  2%|▏         | 5/221 [00:02<01:50,  1.95it/s][A
  3%|▎         | 6/221 [00:02<01:23,  2.57it/s][A
  3%|▎         | 7/221 [00:02<01:10,  3.02it/s][A
  4%|▎         | 8/221 [00:03<01:26,  2.47it/s][A
  4%|▍         | 9/221 [00:04<02:23,  1.48it/s][A
  5%|▍         | 10/221 [00:04<02:05,  1.68it/s][A
  5%|▍         | 11/221 [00:05<02:16,  1.54it/s][A
  5%|▌         | 12/221 [00:06<01:59,  1.75it/s][A
  6%|▌         | 13/221 [00:06<02:02,  1.70it/s][A
  6%|▋         | 14/221 [00:07<02:04,  1.66it/s][A
  7%|▋         | 15/221 [00:07<01:42,  2.01it/s][A
  7%|▋         | 16/221 [00:08<01:53,  1.80it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:34,  2.15it/s][A
  9%|▊         | 19/221 [00:09<02:01,  1.67it/s][A
  9%|▉         | 20/221 [00:10<01:34,  2.12it/s][A
 10%|▉         | 21/221 [00:10<01:34,  2.12it/s][A
 10%|▉         | 22/221 [00:10<01:15,  2.65it/s][A
 10%|█         | 23/221 [00:10<01:06,  2.97it/s][A
 11%|█         | 24/221 [00:11<01:05,  3.02it/s][A
 11%|█▏        | 25/221 [00:12<01:30,  2.17it/s][A
 12%|█▏        | 26/221 [00:12<01:20,  2.41it/s][A
 12%|█▏        | 27/221 [00:14<02:47,  1.16it/s][A
 13%|█▎        | 28/221 [00:14<02:30,  1.29it/s][A
 13%|█▎        | 29/221 [00:16<03:20,  1.04s/it][A
 14%|█▎        | 30/221 [00:17<03:12,  1.01s/it][A
 14%|█▍        | 31/221 [00:18<02:48,  1.13it/s][A
 14%|█▍        | 32/221 [00:18<02:26,  1.29it/s][A
 15%|█▍        | 33/221 [00:19<02:31,  1.24it/s][A
 15%|█▌        | 34/221 [00:19<01:51,  1.68it/s][A
 16%|█▌        | 35/221 [00:19<01:23,  2.22it/s][A
 16%|█▋        | 36/221 [00:21<02:19,  1.33it/s][A
 17%|█▋        | 37/221 [00:21<02:03,  1.49it/s][A
 17%|█▋        | 38/221 [00:22<02:01,  1.51it/s][A
 18%|█▊        | 39/221 [00:22<01:39,  1.83it/s][A
 18%|█▊        | 40/221 [00:23<01:59,  1.52it/s][A
 19%|█▊        | 41/221 [00:23<01:46,  1.69it/s][A
 19%|█▉        | 42/221 [00:24<01:24,  2.13it/s][A
 19%|█▉        | 43/221 [00:24<01:16,  2.32it/s][A
 20%|█▉        | 44/221 [00:24<01:01,  2.87it/s][A
 20%|██        | 45/221 [00:24<01:02,  2.81it/s][A
 21%|██        | 46/221 [00:25<01:13,  2.38it/s][A
 21%|██▏       | 47/221 [00:26<01:19,  2.18it/s][A
 22%|██▏       | 48/221 [00:26<01:04,  2.68it/s][A
 22%|██▏       | 49/221 [00:26<01:00,  2.84it/s][A
 23%|██▎       | 50/221 [00:28<02:00,  1.41it/s][A
 23%|██▎       | 51/221 [00:28<01:46,  1.60it/s][A
 24%|██▎       | 52/221 [00:28<01:30,  1.86it/s][A
 24%|██▍       | 53/221 [00:29<01:18,  2.14it/s][A
 24%|██▍       | 54/221 [00:29<01:24,  1.99it/s][A
 25%|██▍       | 55/221 [00:30<01:13,  2.26it/s][A
 25%|██▌       | 56/221 [00:30<01:17,  2.13it/s][A
 26%|██▌       | 57/221 [00:30<01:06,  2.45it/s][A
 26%|██▌       | 58/221 [00:31<01:20,  2.02it/s][A
 27%|██▋       | 59/221 [00:31<01:02,  2.58it/s][A
 27%|██▋       | 60/221 [00:32<01:05,  2.45it/s][A
 28%|██▊       | 61/221 [00:32<01:20,  2.00it/s][A
 28%|██▊       | 62/221 [00:33<01:16,  2.09it/s][A
 29%|██▊       | 63/221 [00:33<01:06,  2.37it/s][A
 29%|██▉       | 64/221 [00:34<01:38,  1.60it/s][A
 29%|██▉       | 65/221 [00:35<02:03,  1.27it/s][A
 30%|██▉       | 66/221 [00:36<01:41,  1.52it/s][A
 30%|███       | 67/221 [00:37<01:58,  1.30it/s][A
 31%|███       | 68/221 [00:37<01:36,  1.58it/s][A
 31%|███       | 69/221 [00:38<01:36,  1.57it/s][A
 32%|███▏      | 70/221 [00:38<01:21,  1.85it/s][A
 32%|███▏      | 71/221 [00:38<01:10,  2.14it/s][A
 33%|███▎      | 72/221 [00:39<01:15,  1.96it/s][A
 33%|███▎      | 73/221 [00:40<01:23,  1.78it/s][A
 33%|███▎      | 74/221 [00:40<01:20,  1.84it/s][A
 34%|███▍      | 75/221 [00:40<01:11,  2.03it/s][A
 34%|███▍      | 76/221 [00:41<01:00,  2.38it/s][A
 35%|███▍      | 77/221 [00:42<01:17,  1.85it/s][A
 35%|███▌      | 78/221 [00:42<01:04,  2.23it/s][A
 36%|███▌      | 79/221 [00:43<01:17,  1.83it/s][A
 36%|███▌      | 80/221 [00:43<01:09,  2.02it/s][A
 37%|███▋      | 81/221 [00:43<01:02,  2.25it/s][A
 37%|███▋      | 82/221 [00:44<01:20,  1.72it/s][A
 38%|███▊      | 83/221 [00:45<01:35,  1.45it/s][A
 38%|███▊      | 84/221 [00:46<01:40,  1.37it/s][A
 38%|███▊      | 85/221 [00:47<02:06,  1.08it/s][A
 39%|███▉      | 86/221 [00:48<01:51,  1.21it/s][A
 39%|███▉      | 87/221 [00:49<01:51,  1.20it/s][A
 40%|███▉      | 88/221 [00:49<01:42,  1.29it/s][A
 40%|████      | 89/221 [00:50<01:28,  1.49it/s][A
 41%|████      | 90/221 [00:50<01:18,  1.67it/s][A
 41%|████      | 91/221 [00:50<01:03,  2.03it/s][A
 42%|████▏     | 92/221 [00:51<01:12,  1.79it/s][A
 43%|████▎     | 94/221 [00:52<00:48,  2.61it/s][A
 43%|████▎     | 95/221 [00:52<00:51,  2.44it/s][A
 43%|████▎     | 96/221 [00:53<00:59,  2.10it/s][A
 44%|████▍     | 97/221 [00:53<00:51,  2.39it/s][A
 44%|████▍     | 98/221 [00:54<01:20,  1.53it/s][A
 45%|████▍     | 99/221 [00:55<01:15,  1.63it/s][A
 45%|████▌     | 100/221 [00:55<01:16,  1.58it/s][A
 46%|████▌     | 101/221 [00:56<01:27,  1.38it/s][A
 46%|████▌     | 102/221 [00:57<01:16,  1.56it/s][A
 47%|████▋     | 104/221 [00:58<01:02,  1.87it/s][A
 48%|████▊     | 105/221 [00:58<00:55,  2.09it/s][A
 48%|████▊     | 106/221 [00:58<00:47,  2.42it/s][A
 48%|████▊     | 107/221 [00:59<00:48,  2.37it/s][A
 49%|████▉     | 108/221 [00:59<00:55,  2.02it/s][A
 49%|████▉     | 109/221 [01:00<01:04,  1.75it/s][A
 50%|████▉     | 110/221 [01:02<01:44,  1.07it/s][A
 50%|█████     | 111/221 [01:02<01:24,  1.30it/s][A
 51%|█████     | 112/221 [01:02<01:06,  1.64it/s][A
 51%|█████     | 113/221 [01:03<00:56,  1.91it/s][A
 52%|█████▏    | 114/221 [01:03<00:42,  2.50it/s][A
 52%|█████▏    | 115/221 [01:04<01:10,  1.51it/s][A
 52%|█████▏    | 116/221 [01:04<00:53,  1.97it/s][A
 53%|█████▎    | 117/221 [01:05<01:07,  1.55it/s][A
 53%|█████▎    | 118/221 [01:06<01:06,  1.54it/s][A
 54%|█████▍    | 119/221 [01:06<00:52,  1.94it/s][A
 54%|█████▍    | 120/221 [01:07<00:59,  1.70it/s][A
 55%|█████▍    | 121/221 [01:07<00:52,  1.92it/s][A
 55%|█████▌    | 122/221 [01:07<00:43,  2.29it/s][A
 56%|█████▌    | 123/221 [01:08<00:44,  2.19it/s][A
 56%|█████▌    | 124/221 [01:08<00:37,  2.61it/s][A
 57%|█████▋    | 125/221 [01:09<00:55,  1.73it/s][A
 57%|█████▋    | 126/221 [01:09<00:45,  2.11it/s][A
 57%|█████▋    | 127/221 [01:10<00:53,  1.76it/s][A
 58%|█████▊    | 128/221 [01:11<00:55,  1.68it/s][A
 58%|█████▊    | 129/221 [01:11<00:43,  2.11it/s][A
 59%|█████▉    | 130/221 [01:12<00:48,  1.87it/s][A
 59%|█████▉    | 131/221 [01:12<00:39,  2.30it/s][A
 60%|█████▉    | 132/221 [01:12<00:33,  2.63it/s][A
 60%|██████    | 133/221 [01:13<00:38,  2.29it/s][A
 61%|██████    | 134/221 [01:13<00:37,  2.29it/s][A
 61%|██████    | 135/221 [01:13<00:28,  2.97it/s][A
 62%|██████▏   | 136/221 [01:14<00:30,  2.79it/s][A
 62%|██████▏   | 137/221 [01:14<00:32,  2.56it/s][A
 62%|██████▏   | 138/221 [01:15<00:39,  2.09it/s][A
 63%|██████▎   | 139/221 [01:15<00:41,  1.96it/s][A
 63%|██████▎   | 140/221 [01:16<00:41,  1.94it/s][A
 64%|██████▍   | 141/221 [01:17<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:17<00:39,  2.01it/s][A
 65%|██████▍   | 143/221 [01:17<00:35,  2.18it/s][A
 65%|██████▌   | 144/221 [01:18<00:33,  2.28it/s][A
 66%|██████▌   | 145/221 [01:18<00:27,  2.74it/s][A
 66%|██████▌   | 146/221 [01:18<00:22,  3.40it/s][A
 67%|██████▋   | 147/221 [01:18<00:21,  3.49it/s][A
 67%|██████▋   | 148/221 [01:19<00:21,  3.34it/s][A
 67%|██████▋   | 149/221 [01:19<00:24,  2.96it/s][A
 68%|██████▊   | 150/221 [01:20<00:26,  2.70it/s][A
 69%|██████▉   | 152/221 [01:21<00:29,  2.37it/s][A
 69%|██████▉   | 153/221 [01:21<00:26,  2.55it/s][A
 70%|██████▉   | 154/221 [01:21<00:31,  2.14it/s][A
 70%|███████   | 155/221 [01:22<00:36,  1.82it/s][A
 71%|███████   | 156/221 [01:23<00:36,  1.78it/s][A
 71%|███████   | 157/221 [01:23<00:36,  1.75it/s][A
 71%|███████▏  | 158/221 [01:24<00:28,  2.17it/s][A
 72%|███████▏  | 159/221 [01:24<00:34,  1.81it/s][A
 72%|███████▏  | 160/221 [01:25<00:31,  1.96it/s][A
 73%|███████▎  | 161/221 [01:25<00:25,  2.34it/s][A
 74%|███████▍  | 163/221 [01:26<00:22,  2.53it/s][A
 74%|███████▍  | 164/221 [01:26<00:20,  2.77it/s][A
 75%|███████▍  | 165/221 [01:26<00:19,  2.88it/s][A
 75%|███████▌  | 166/221 [01:27<00:25,  2.17it/s][A
 76%|███████▌  | 167/221 [01:27<00:22,  2.38it/s][A
 76%|███████▌  | 168/221 [01:28<00:21,  2.41it/s][A
 76%|███████▋  | 169/221 [01:28<00:17,  2.92it/s][A
 77%|███████▋  | 170/221 [01:28<00:19,  2.64it/s][A
 77%|███████▋  | 171/221 [01:29<00:22,  2.22it/s][A
 78%|███████▊  | 172/221 [01:30<00:22,  2.13it/s][A
 78%|███████▊  | 173/221 [01:30<00:26,  1.80it/s][A
 79%|███████▊  | 174/221 [01:31<00:28,  1.64it/s][A
 79%|███████▉  | 175/221 [01:31<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.90it/s][A
 80%|████████  | 177/221 [01:32<00:22,  1.99it/s][A
 81%|████████  | 178/221 [01:33<00:22,  1.88it/s][A
 81%|████████  | 179/221 [01:33<00:19,  2.10it/s][A
 81%|████████▏ | 180/221 [01:34<00:21,  1.95it/s][A
 82%|████████▏ | 181/221 [01:35<00:30,  1.31it/s][A
 82%|████████▏ | 182/221 [01:36<00:28,  1.37it/s][A
 83%|████████▎ | 183/221 [01:36<00:24,  1.55it/s][A
 83%|████████▎ | 184/221 [01:37<00:23,  1.56it/s][A
 84%|████████▎ | 185/221 [01:39<00:36,  1.02s/it][A
 84%|████████▍ | 186/221 [01:39<00:26,  1.32it/s][A
 85%|████████▍ | 187/221 [01:40<00:28,  1.19it/s][A
 85%|████████▌ | 188/221 [01:41<00:31,  1.06it/s][A
 86%|████████▌ | 189/221 [01:42<00:25,  1.25it/s][A
 86%|████████▌ | 190/221 [01:42<00:23,  1.31it/s][A
 86%|████████▋ | 191/221 [01:43<00:21,  1.38it/s][A
 87%|████████▋ | 192/221 [01:43<00:17,  1.70it/s][A
 87%|████████▋ | 193/221 [01:43<00:12,  2.18it/s][A
 88%|████████▊ | 194/221 [01:44<00:12,  2.10it/s][A
 88%|████████▊ | 195/221 [01:45<00:15,  1.72it/s][A
 89%|████████▊ | 196/221 [01:45<00:15,  1.64it/s][A
 89%|████████▉ | 197/221 [01:46<00:13,  1.82it/s][A
 90%|████████▉ | 198/221 [01:46<00:09,  2.34it/s][A
 90%|█████████ | 199/221 [01:46<00:08,  2.72it/s][A
 90%|█████████ | 200/221 [01:47<00:10,  2.07it/s][A
 91%|█████████ | 201/221 [01:47<00:09,  2.10it/s][A
 91%|█████████▏| 202/221 [01:48<00:07,  2.42it/s][A
 92%|█████████▏| 203/221 [01:48<00:07,  2.46it/s][A
 92%|█████████▏| 204/221 [01:49<00:07,  2.17it/s][A
 93%|█████████▎| 205/221 [01:49<00:06,  2.55it/s][A
 93%|█████████▎| 206/221 [01:49<00:06,  2.47it/s][A
 94%|█████████▎| 207/221 [01:50<00:06,  2.16it/s][A
 94%|█████████▍| 208/221 [01:51<00:07,  1.70it/s][A
 95%|█████████▍| 209/221 [01:52<00:07,  1.51it/s][A
 95%|█████████▌| 211/221 [01:52<00:04,  2.05it/s][A
 96%|█████████▌| 212/221 [01:53<00:04,  2.20it/s][A
 96%|█████████▋| 213/221 [01:54<00:04,  1.66it/s][A
 97%|█████████▋| 214/221 [01:54<00:03,  1.86it/s][A
 97%|█████████▋| 215/221 [01:54<00:02,  2.08it/s][A
 98%|█████████▊| 216/221 [01:55<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:56<00:02,  1.85it/s][A
 99%|█████████▊| 218/221 [01:56<00:01,  2.14it/s][A
 99%|█████████▉| 219/221 [01:56<00:00,  2.08it/s][A
100%|█████████▉| 220/221 [01:57<00:00,  2.46it/s][A
100%|██████████| 221/221 [01:58<00:00,  1.57it/s][A100%|██████████| 221/221 [01:58<00:00,  1.87it/s]
09/19/2024 04:56:55 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 1999--===========

09/19/2024 04:56:55 - INFO - __main__ -   {'area_r1': 43.9, 'area_recall': '43.9/71.2/81.1', 'area_ravg': 65.4}
09/19/2024 04:56:55 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 1999--===========

09/19/2024 04:56:55 - INFO - __main__ -   {'forward_r1': 50.3, 'forward_recall': '50.3/77.5/86.3', 'forward_ravg': 71.4}
09/19/2024 04:56:55 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 1999--===========

09/19/2024 04:56:55 - INFO - __main__ -   {'area_video_r1': 49.1, 'area_video_recall': '49.1/77.3/86.8', 'area_video_ravg': 71.0}
09/19/2024 04:56:55 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 1499=======

09/19/2024 04:56:55 - INFO - __main__ -   {'area_video_r1': 49.4, 'area_video_recall': '49.4/78.4/87.0', 'area_video_ravg': 71.6}
09/19/2024 04:56:55 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 1999--===========

09/19/2024 04:56:55 - INFO - __main__ -   {'area_video_r1': 61.0, 'area_video_recall': '61.0/80.7/87.7', 'area_video_ravg': 76.4, 'area_video_back_r1': 59.6, 'area_video_back_recall': '59.6/83.6/89.9', 'area_video_back_ravg': 77.7}
09/19/2024 04:56:55 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 1999=======

09/19/2024 04:56:55 - INFO - __main__ -   {'area_video_r1': 61.0, 'area_video_recall': '61.0/80.7/87.7', 'area_video_ravg': 76.4, 'area_video_back_r1': 59.6, 'area_video_back_recall': '59.6/83.6/89.9', 'area_video_back_ravg': 77.7}
09/19/2024 04:56:55 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 1999--===========

09/19/2024 04:56:55 - INFO - __main__ -   {'video_r1': 34.5, 'video_recall': '34.5/59.5/69.8', 'video_ravg': 54.6}
09/19/2024 04:56:55 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 04:56:55 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 04:56:55 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 1999--===========

09/19/2024 04:56:55 - INFO - __main__ -   {'video_r1': 59.4, 'video_recall': '59.4/79.4/85.7', 'video_ravg': 74.8}
09/19/2024 04:56:55 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 1499=======

09/19/2024 04:56:55 - INFO - __main__ -   {'video_r1': 60.0, 'video_recall': '60.0/79.1/84.8', 'video_ravg': 74.6}
09/19/2024 04:57:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032104335725307465, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6688458919525146, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.700950264930725}
 22%|██▏       | 2000/8917 [2:42:49<329:32:10, 171.51s/it] 22%|██▏       | 2001/8917 [2:42:52<232:22:32, 120.96s/it] 22%|██▏       | 2002/8917 [2:42:55<164:45:08, 85.77s/it]  22%|██▏       | 2003/8917 [2:42:59<117:33:41, 61.21s/it] 22%|██▏       | 2004/8917 [2:43:03<84:12:09, 43.85s/it]  22%|██▏       | 2005/8917 [2:43:06<61:07:14, 31.83s/it] 22%|██▏       | 2006/8917 [2:43:10<44:55:36, 23.40s/it] 23%|██▎       | 2007/8917 [2:43:14<33:29:44, 17.45s/it] 23%|██▎       | 2008/8917 [2:43:18<25:36:23, 13.34s/it] 23%|██▎       | 2009/8917 [2:43:21<19:58:56, 10.41s/it] 23%|██▎       | 2010/8917 [2:43:25<16:06:44,  8.40s/it] 23%|██▎       | 2011/8917 [2:43:28<13:11:09,  6.87s/it] 23%|██▎       | 2012/8917 [2:43:32<11:12:24,  5.84s/it] 23%|██▎       | 2013/8917 [2:43:35<9:59:02,  5.21s/it]  23%|██▎       | 2014/8917 [2:43:39<9:15:26,  4.83s/it] 23%|██▎       | 2015/8917 [2:43:43<8:40:11,  4.52s/it] 23%|██▎       | 2016/8917 [2:43:46<8:02:12,  4.19s/it] 23%|██▎       | 2017/8917 [2:43:50<7:43:59,  4.03s/it] 23%|██▎       | 2018/8917 [2:43:54<7:29:17,  3.91s/it] 23%|██▎       | 2019/8917 [2:43:58<7:33:00,  3.94s/it] 23%|██▎       | 2020/8917 [2:44:01<7:23:25,  3.86s/it] 23%|██▎       | 2021/8917 [2:44:05<7:11:43,  3.76s/it] 23%|██▎       | 2022/8917 [2:44:09<7:17:03,  3.80s/it] 23%|██▎       | 2023/8917 [2:44:13<7:23:21,  3.86s/it] 23%|██▎       | 2024/8917 [2:44:16<7:16:14,  3.80s/it] 23%|██▎       | 2025/8917 [2:44:20<7:19:06,  3.82s/it] 23%|██▎       | 2026/8917 [2:44:24<7:18:42,  3.82s/it] 23%|██▎       | 2027/8917 [2:44:27<7:02:05,  3.68s/it] 23%|██▎       | 2028/8917 [2:44:31<7:05:25,  3.71s/it] 23%|██▎       | 2029/8917 [2:44:35<7:16:07,  3.80s/it] 23%|██▎       | 2030/8917 [2:44:39<7:07:14,  3.72s/it] 23%|██▎       | 2031/8917 [2:44:42<7:02:29,  3.68s/it] 23%|██▎       | 2032/8917 [2:44:46<7:14:31,  3.79s/it] 23%|██▎       | 2033/8917 [2:44:50<7:16:10,  3.80s/it] 23%|██▎       | 2034/8917 [2:44:54<7:07:40,  3.73s/it] 23%|██▎       | 2035/8917 [2:44:58<7:06:27,  3.72s/it] 23%|██▎       | 2036/8917 [2:45:01<7:12:16,  3.77s/it] 23%|██▎       | 2037/8917 [2:45:05<7:16:55,  3.81s/it] 23%|██▎       | 2038/8917 [2:45:09<7:08:46,  3.74s/it] 23%|██▎       | 2039/8917 [2:45:12<6:58:06,  3.65s/it] 23%|██▎       | 2040/8917 [2:45:16<7:10:09,  3.75s/it] 23%|██▎       | 2041/8917 [2:45:20<7:07:56,  3.73s/it] 23%|██▎       | 2042/8917 [2:45:24<7:03:10,  3.69s/it] 23%|██▎       | 2043/8917 [2:45:27<6:51:36,  3.59s/it] 23%|██▎       | 2044/8917 [2:45:30<6:47:34,  3.56s/it] 23%|██▎       | 2045/8917 [2:45:34<7:01:58,  3.68s/it] 23%|██▎       | 2046/8917 [2:45:38<6:58:43,  3.66s/it] 23%|██▎       | 2047/8917 [2:45:42<7:00:46,  3.67s/it] 23%|██▎       | 2048/8917 [2:45:45<7:00:05,  3.67s/it] 23%|██▎       | 2049/8917 [2:45:49<6:52:46,  3.61s/it]09/19/2024 05:00:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030989086255431175, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4013363122940063, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4323253631591797}
 23%|██▎       | 2050/8917 [2:45:53<7:01:02,  3.68s/it] 23%|██▎       | 2051/8917 [2:45:56<6:59:36,  3.67s/it] 23%|██▎       | 2052/8917 [2:46:00<6:57:35,  3.65s/it] 23%|██▎       | 2053/8917 [2:46:04<6:59:48,  3.67s/it] 23%|██▎       | 2054/8917 [2:46:07<6:59:27,  3.67s/it] 23%|██▎       | 2055/8917 [2:46:11<7:06:44,  3.73s/it] 23%|██▎       | 2056/8917 [2:46:15<7:09:24,  3.76s/it] 23%|██▎       | 2057/8917 [2:46:19<6:59:34,  3.67s/it] 23%|██▎       | 2058/8917 [2:46:22<7:07:49,  3.74s/it] 23%|██▎       | 2059/8917 [2:46:26<7:00:46,  3.68s/it] 23%|██▎       | 2060/8917 [2:46:30<7:06:23,  3.73s/it] 23%|██▎       | 2061/8917 [2:46:34<7:12:24,  3.78s/it] 23%|██▎       | 2062/8917 [2:46:37<7:05:17,  3.72s/it] 23%|██▎       | 2063/8917 [2:46:41<6:59:38,  3.67s/it] 23%|██▎       | 2064/8917 [2:46:45<7:07:33,  3.74s/it] 23%|██▎       | 2065/8917 [2:46:48<7:01:20,  3.69s/it] 23%|██▎       | 2066/8917 [2:46:52<7:05:12,  3.72s/it] 23%|██▎       | 2067/8917 [2:46:56<7:09:58,  3.77s/it] 23%|██▎       | 2068/8917 [2:47:00<7:06:59,  3.74s/it] 23%|██▎       | 2069/8917 [2:47:03<7:05:15,  3.73s/it] 23%|██▎       | 2070/8917 [2:47:07<7:02:24,  3.70s/it] 23%|██▎       | 2071/8917 [2:47:11<7:02:35,  3.70s/it] 23%|██▎       | 2072/8917 [2:47:15<7:07:15,  3.75s/it] 23%|██▎       | 2073/8917 [2:47:18<7:01:54,  3.70s/it] 23%|██▎       | 2074/8917 [2:47:22<7:08:06,  3.75s/it] 23%|██▎       | 2075/8917 [2:47:26<7:08:01,  3.75s/it] 23%|██▎       | 2076/8917 [2:47:30<7:12:24,  3.79s/it] 23%|██▎       | 2077/8917 [2:47:33<7:11:19,  3.78s/it] 23%|██▎       | 2078/8917 [2:47:37<7:02:17,  3.70s/it] 23%|██▎       | 2079/8917 [2:47:41<7:07:04,  3.75s/it] 23%|██▎       | 2080/8917 [2:47:45<7:12:43,  3.80s/it] 23%|██▎       | 2081/8917 [2:47:49<7:13:09,  3.80s/it] 23%|██▎       | 2082/8917 [2:47:52<7:07:32,  3.75s/it] 23%|██▎       | 2083/8917 [2:47:56<7:09:44,  3.77s/it] 23%|██▎       | 2084/8917 [2:48:00<7:08:58,  3.77s/it] 23%|██▎       | 2085/8917 [2:48:03<7:07:32,  3.75s/it] 23%|██▎       | 2086/8917 [2:48:07<7:00:01,  3.69s/it] 23%|██▎       | 2087/8917 [2:48:11<7:00:07,  3.69s/it] 23%|██▎       | 2088/8917 [2:48:14<7:04:03,  3.73s/it] 23%|██▎       | 2089/8917 [2:48:19<7:15:23,  3.83s/it] 23%|██▎       | 2090/8917 [2:48:22<7:13:30,  3.81s/it] 23%|██▎       | 2091/8917 [2:48:26<7:05:11,  3.74s/it] 23%|██▎       | 2092/8917 [2:48:29<6:52:03,  3.62s/it] 23%|██▎       | 2093/8917 [2:48:33<6:50:36,  3.61s/it] 23%|██▎       | 2094/8917 [2:48:37<7:00:34,  3.70s/it] 23%|██▎       | 2095/8917 [2:48:41<7:05:53,  3.75s/it] 24%|██▎       | 2096/8917 [2:48:44<7:03:03,  3.72s/it] 24%|██▎       | 2097/8917 [2:48:48<7:04:06,  3.73s/it] 24%|██▎       | 2098/8917 [2:48:52<7:03:03,  3.72s/it] 24%|██▎       | 2099/8917 [2:48:56<7:11:19,  3.80s/it]09/19/2024 05:03:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02220827341079712, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.009026288986206, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0312345027923584}
 24%|██▎       | 2100/8917 [2:48:59<6:56:23,  3.66s/it] 24%|██▎       | 2101/8917 [2:49:03<7:08:19,  3.77s/it] 24%|██▎       | 2102/8917 [2:49:07<7:15:06,  3.83s/it] 24%|██▎       | 2103/8917 [2:49:11<7:12:45,  3.81s/it] 24%|██▎       | 2104/8917 [2:49:15<7:14:57,  3.83s/it] 24%|██▎       | 2105/8917 [2:49:18<7:10:10,  3.79s/it] 24%|██▎       | 2106/8917 [2:49:22<7:10:12,  3.79s/it] 24%|██▎       | 2107/8917 [2:49:26<7:04:19,  3.74s/it] 24%|██▎       | 2108/8917 [2:49:29<6:59:15,  3.69s/it] 24%|██▎       | 2109/8917 [2:49:33<7:01:48,  3.72s/it] 24%|██▎       | 2110/8917 [2:49:37<7:15:13,  3.84s/it] 24%|██▎       | 2111/8917 [2:49:41<7:10:57,  3.80s/it] 24%|██▎       | 2112/8917 [2:49:45<7:04:46,  3.75s/it] 24%|██▎       | 2113/8917 [2:49:49<7:16:31,  3.85s/it] 24%|██▎       | 2114/8917 [2:49:52<7:06:25,  3.76s/it] 24%|██▎       | 2115/8917 [2:49:56<6:56:05,  3.67s/it] 24%|██▎       | 2116/8917 [2:49:59<7:00:38,  3.71s/it] 24%|██▎       | 2117/8917 [2:50:03<6:58:53,  3.70s/it] 24%|██▍       | 2118/8917 [2:50:07<6:55:50,  3.67s/it] 24%|██▍       | 2119/8917 [2:50:11<7:11:57,  3.81s/it] 24%|██▍       | 2120/8917 [2:50:15<7:09:11,  3.79s/it] 24%|██▍       | 2121/8917 [2:50:18<7:01:39,  3.72s/it] 24%|██▍       | 2122/8917 [2:50:22<7:03:35,  3.74s/it] 24%|██▍       | 2123/8917 [2:50:26<6:56:50,  3.68s/it] 24%|██▍       | 2124/8917 [2:50:30<7:08:05,  3.78s/it] 24%|██▍       | 2125/8917 [2:50:33<7:08:13,  3.78s/it] 24%|██▍       | 2126/8917 [2:50:37<7:03:06,  3.74s/it] 24%|██▍       | 2127/8917 [2:50:41<7:07:33,  3.78s/it] 24%|██▍       | 2128/8917 [2:50:44<7:02:07,  3.73s/it] 24%|██▍       | 2129/8917 [2:50:48<6:57:33,  3.69s/it] 24%|██▍       | 2130/8917 [2:50:52<6:54:36,  3.67s/it] 24%|██▍       | 2131/8917 [2:50:56<7:11:22,  3.81s/it] 24%|██▍       | 2132/8917 [2:51:00<7:19:11,  3.88s/it] 24%|██▍       | 2133/8917 [2:51:03<7:07:55,  3.78s/it] 24%|██▍       | 2134/8917 [2:51:07<7:02:21,  3.74s/it] 24%|██▍       | 2135/8917 [2:51:11<6:57:31,  3.69s/it] 24%|██▍       | 2136/8917 [2:51:14<6:58:43,  3.70s/it] 24%|██▍       | 2137/8917 [2:51:18<7:03:03,  3.74s/it] 24%|██▍       | 2138/8917 [2:51:22<6:58:57,  3.71s/it] 24%|██▍       | 2139/8917 [2:51:25<6:57:07,  3.69s/it] 24%|██▍       | 2140/8917 [2:51:29<6:51:43,  3.65s/it] 24%|██▍       | 2141/8917 [2:51:33<6:50:22,  3.63s/it] 24%|██▍       | 2142/8917 [2:51:36<6:54:39,  3.67s/it] 24%|██▍       | 2143/8917 [2:51:40<6:51:17,  3.64s/it] 24%|██▍       | 2144/8917 [2:51:44<7:01:12,  3.73s/it] 24%|██▍       | 2145/8917 [2:51:47<6:50:00,  3.63s/it] 24%|██▍       | 2146/8917 [2:51:51<6:51:23,  3.65s/it] 24%|██▍       | 2147/8917 [2:51:55<6:57:44,  3.70s/it] 24%|██▍       | 2148/8917 [2:51:59<7:06:04,  3.78s/it] 24%|██▍       | 2149/8917 [2:52:03<7:11:44,  3.83s/it]09/19/2024 05:06:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.031194159761071205, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.451599359512329, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4827935695648193}
 24%|██▍       | 2150/8917 [2:52:07<7:14:13,  3.85s/it] 24%|██▍       | 2151/8917 [2:52:10<7:02:41,  3.75s/it] 24%|██▍       | 2152/8917 [2:52:14<7:08:52,  3.80s/it] 24%|██▍       | 2153/8917 [2:52:18<6:59:13,  3.72s/it] 24%|██▍       | 2154/8917 [2:52:21<7:05:51,  3.78s/it] 24%|██▍       | 2155/8917 [2:52:25<6:58:28,  3.71s/it] 24%|██▍       | 2156/8917 [2:52:29<6:53:30,  3.67s/it] 24%|██▍       | 2157/8917 [2:52:32<6:52:29,  3.66s/it] 24%|██▍       | 2158/8917 [2:52:36<6:50:40,  3.65s/it] 24%|██▍       | 2159/8917 [2:52:39<6:38:03,  3.53s/it] 24%|██▍       | 2160/8917 [2:52:43<6:49:54,  3.64s/it] 24%|██▍       | 2161/8917 [2:52:47<6:55:01,  3.69s/it] 24%|██▍       | 2162/8917 [2:52:51<6:58:32,  3.72s/it] 24%|██▍       | 2163/8917 [2:52:54<6:54:39,  3.68s/it] 24%|██▍       | 2164/8917 [2:52:58<6:48:33,  3.63s/it] 24%|██▍       | 2165/8917 [2:53:02<6:53:38,  3.68s/it] 24%|██▍       | 2166/8917 [2:53:05<6:47:32,  3.62s/it] 24%|██▍       | 2167/8917 [2:53:08<6:42:58,  3.58s/it] 24%|██▍       | 2168/8917 [2:53:12<6:56:02,  3.70s/it] 24%|██▍       | 2169/8917 [2:53:16<6:56:10,  3.70s/it] 24%|██▍       | 2170/8917 [2:53:20<6:55:42,  3.70s/it] 24%|██▍       | 2171/8917 [2:53:23<6:51:50,  3.66s/it] 24%|██▍       | 2172/8917 [2:53:27<6:56:47,  3.71s/it] 24%|██▍       | 2173/8917 [2:53:31<6:52:27,  3.67s/it] 24%|██▍       | 2174/8917 [2:53:35<6:55:56,  3.70s/it] 24%|██▍       | 2175/8917 [2:53:38<6:51:44,  3.66s/it] 24%|██▍       | 2176/8917 [2:53:42<6:53:03,  3.68s/it] 24%|██▍       | 2177/8917 [2:53:46<6:58:49,  3.73s/it] 24%|██▍       | 2178/8917 [2:53:49<6:51:05,  3.66s/it] 24%|██▍       | 2179/8917 [2:53:53<6:56:42,  3.71s/it] 24%|██▍       | 2180/8917 [2:53:57<7:02:58,  3.77s/it] 24%|██▍       | 2181/8917 [2:54:00<6:54:01,  3.69s/it] 24%|██▍       | 2182/8917 [2:54:04<6:49:51,  3.65s/it] 24%|██▍       | 2183/8917 [2:54:08<6:52:01,  3.67s/it] 24%|██▍       | 2184/8917 [2:54:12<7:02:54,  3.77s/it] 25%|██▍       | 2185/8917 [2:54:16<7:08:45,  3.82s/it] 25%|██▍       | 2186/8917 [2:54:19<7:06:30,  3.80s/it] 25%|██▍       | 2187/8917 [2:54:23<7:02:20,  3.77s/it] 25%|██▍       | 2188/8917 [2:54:27<7:02:02,  3.76s/it] 25%|██▍       | 2189/8917 [2:54:31<7:00:26,  3.75s/it] 25%|██▍       | 2190/8917 [2:54:34<6:59:59,  3.75s/it] 25%|██▍       | 2191/8917 [2:54:38<6:56:15,  3.71s/it] 25%|██▍       | 2192/8917 [2:54:42<6:51:38,  3.67s/it] 25%|██▍       | 2193/8917 [2:54:46<7:01:35,  3.76s/it] 25%|██▍       | 2194/8917 [2:54:50<7:12:18,  3.86s/it] 25%|██▍       | 2195/8917 [2:54:53<7:07:58,  3.82s/it] 25%|██▍       | 2196/8917 [2:54:57<7:02:14,  3.77s/it] 25%|██▍       | 2197/8917 [2:55:01<7:00:29,  3.75s/it] 25%|██▍       | 2198/8917 [2:55:05<7:01:58,  3.77s/it] 25%|██▍       | 2199/8917 [2:55:08<6:51:30,  3.68s/it]09/19/2024 05:09:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023773586377501488, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.354152798652649, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3779263496398926}
 25%|██▍       | 2200/8917 [2:55:12<6:48:50,  3.65s/it] 25%|██▍       | 2201/8917 [2:55:15<6:51:24,  3.68s/it] 25%|██▍       | 2202/8917 [2:55:19<6:56:59,  3.73s/it] 25%|██▍       | 2203/8917 [2:55:23<7:01:40,  3.77s/it] 25%|██▍       | 2204/8917 [2:55:27<6:58:22,  3.74s/it] 25%|██▍       | 2205/8917 [2:55:30<6:55:03,  3.71s/it] 25%|██▍       | 2206/8917 [2:55:34<6:49:41,  3.66s/it] 25%|██▍       | 2207/8917 [2:55:38<6:51:14,  3.68s/it] 25%|██▍       | 2208/8917 [2:55:42<7:00:25,  3.76s/it] 25%|██▍       | 2209/8917 [2:55:45<7:02:21,  3.78s/it] 25%|██▍       | 2210/8917 [2:55:49<7:09:18,  3.84s/it] 25%|██▍       | 2211/8917 [2:55:53<7:04:44,  3.80s/it] 25%|██▍       | 2212/8917 [2:55:57<7:17:27,  3.91s/it] 25%|██▍       | 2213/8917 [2:56:01<7:01:07,  3.77s/it] 25%|██▍       | 2214/8917 [2:56:04<6:51:44,  3.69s/it] 25%|██▍       | 2215/8917 [2:56:08<6:54:16,  3.71s/it] 25%|██▍       | 2216/8917 [2:56:12<6:56:55,  3.73s/it] 25%|██▍       | 2217/8917 [2:56:15<6:55:14,  3.72s/it] 25%|██▍       | 2218/8917 [2:56:19<6:52:10,  3.69s/it] 25%|██▍       | 2219/8917 [2:56:23<6:55:13,  3.72s/it] 25%|██▍       | 2220/8917 [2:56:26<6:52:39,  3.70s/it] 25%|██▍       | 2221/8917 [2:56:30<6:58:02,  3.75s/it] 25%|██▍       | 2222/8917 [2:56:34<7:03:16,  3.79s/it] 25%|██▍       | 2223/8917 [2:56:38<7:06:20,  3.82s/it] 25%|██▍       | 2224/8917 [2:56:42<7:03:49,  3.80s/it] 25%|██▍       | 2225/8917 [2:56:46<7:06:55,  3.83s/it] 25%|██▍       | 2226/8917 [2:56:49<6:57:57,  3.75s/it] 25%|██▍       | 2227/8917 [2:56:53<6:43:52,  3.62s/it] 25%|██▍       | 2228/8917 [2:56:56<6:42:53,  3.61s/it] 25%|██▍       | 2229/8917 [2:57:00<6:47:04,  3.65s/it] 25%|██▌       | 2230/8917 [2:57:04<7:00:44,  3.78s/it] 25%|██▌       | 2231/8917 [2:57:08<7:00:18,  3.77s/it] 25%|██▌       | 2232/8917 [2:57:11<6:53:26,  3.71s/it] 25%|██▌       | 2233/8917 [2:57:15<6:47:47,  3.66s/it] 25%|██▌       | 2234/8917 [2:57:19<6:55:05,  3.73s/it] 25%|██▌       | 2235/8917 [2:57:23<7:01:40,  3.79s/it] 25%|██▌       | 2236/8917 [2:57:26<6:51:31,  3.70s/it] 25%|██▌       | 2237/8917 [2:57:30<6:55:34,  3.73s/it] 25%|██▌       | 2238/8917 [2:57:33<6:46:40,  3.65s/it] 25%|██▌       | 2239/8917 [2:57:37<6:41:36,  3.61s/it] 25%|██▌       | 2240/8917 [2:57:41<6:44:47,  3.64s/it] 25%|██▌       | 2241/8917 [2:57:45<6:51:14,  3.70s/it] 25%|██▌       | 2242/8917 [2:57:48<6:54:12,  3.72s/it] 25%|██▌       | 2243/8917 [2:57:52<6:47:39,  3.66s/it] 25%|██▌       | 2244/8917 [2:57:56<6:56:49,  3.75s/it] 25%|██▌       | 2245/8917 [2:58:00<6:55:35,  3.74s/it] 25%|██▌       | 2246/8917 [2:58:03<6:52:06,  3.71s/it] 25%|██▌       | 2247/8917 [2:58:07<6:49:55,  3.69s/it] 25%|██▌       | 2248/8917 [2:58:10<6:50:07,  3.69s/it] 25%|██▌       | 2249/8917 [2:58:14<6:56:42,  3.75s/it]09/19/2024 05:12:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024502282962203026, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.171149730682373, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1956520080566406}
 25%|██▌       | 2250/8917 [2:58:18<6:56:35,  3.75s/it] 25%|██▌       | 2251/8917 [2:58:22<6:53:23,  3.72s/it] 25%|██▌       | 2252/8917 [2:58:25<6:51:32,  3.70s/it] 25%|██▌       | 2253/8917 [2:58:29<7:02:22,  3.80s/it] 25%|██▌       | 2254/8917 [2:58:33<6:55:03,  3.74s/it] 25%|██▌       | 2255/8917 [2:58:37<7:09:08,  3.87s/it] 25%|██▌       | 2256/8917 [2:58:41<6:55:58,  3.75s/it] 25%|██▌       | 2257/8917 [2:58:44<6:53:01,  3.72s/it] 25%|██▌       | 2258/8917 [2:58:48<6:58:17,  3.77s/it] 25%|██▌       | 2259/8917 [2:58:52<6:54:38,  3.74s/it] 25%|██▌       | 2260/8917 [2:58:55<6:46:25,  3.66s/it] 25%|██▌       | 2261/8917 [2:58:59<6:49:44,  3.69s/it] 25%|██▌       | 2262/8917 [2:59:03<6:54:19,  3.74s/it] 25%|██▌       | 2263/8917 [2:59:07<6:55:12,  3.74s/it] 25%|██▌       | 2264/8917 [2:59:11<7:01:47,  3.80s/it] 25%|██▌       | 2265/8917 [2:59:15<7:03:23,  3.82s/it] 25%|██▌       | 2266/8917 [2:59:18<6:47:48,  3.68s/it] 25%|██▌       | 2267/8917 [2:59:21<6:33:12,  3.55s/it] 25%|██▌       | 2268/8917 [2:59:25<6:35:42,  3.57s/it] 25%|██▌       | 2269/8917 [2:59:28<6:41:12,  3.62s/it] 25%|██▌       | 2270/8917 [2:59:33<6:55:52,  3.75s/it] 25%|██▌       | 2271/8917 [2:59:36<6:57:08,  3.77s/it] 25%|██▌       | 2272/8917 [2:59:40<6:49:11,  3.69s/it] 25%|██▌       | 2273/8917 [2:59:44<6:49:53,  3.70s/it] 26%|██▌       | 2274/8917 [2:59:47<6:44:27,  3.65s/it] 26%|██▌       | 2275/8917 [2:59:51<6:47:28,  3.68s/it] 26%|██▌       | 2276/8917 [2:59:54<6:42:45,  3.64s/it] 26%|██▌       | 2277/8917 [2:59:58<6:40:11,  3.62s/it] 26%|██▌       | 2278/8917 [3:00:02<6:54:48,  3.75s/it] 26%|██▌       | 2279/8917 [3:00:06<6:55:11,  3.75s/it] 26%|██▌       | 2280/8917 [3:00:10<6:59:30,  3.79s/it] 26%|██▌       | 2281/8917 [3:00:13<6:50:08,  3.71s/it] 26%|██▌       | 2282/8917 [3:00:16<6:35:47,  3.58s/it] 26%|██▌       | 2283/8917 [3:00:20<6:43:07,  3.65s/it] 26%|██▌       | 2284/8917 [3:00:24<6:35:30,  3.58s/it] 26%|██▌       | 2285/8917 [3:00:27<6:36:04,  3.58s/it] 26%|██▌       | 2286/8917 [3:00:31<6:39:48,  3.62s/it] 26%|██▌       | 2287/8917 [3:00:35<6:55:47,  3.76s/it] 26%|██▌       | 2288/8917 [3:00:39<6:45:35,  3.67s/it] 26%|██▌       | 2289/8917 [3:00:42<6:39:17,  3.61s/it] 26%|██▌       | 2290/8917 [3:00:46<7:06:17,  3.86s/it] 26%|██▌       | 2291/8917 [3:00:50<7:02:26,  3.83s/it] 26%|██▌       | 2292/8917 [3:00:54<6:57:23,  3.78s/it] 26%|██▌       | 2293/8917 [3:00:58<7:04:52,  3.85s/it] 26%|██▌       | 2294/8917 [3:01:02<7:01:22,  3.82s/it] 26%|██▌       | 2295/8917 [3:01:06<7:08:28,  3.88s/it] 26%|██▌       | 2296/8917 [3:01:09<7:02:47,  3.83s/it] 26%|██▌       | 2297/8917 [3:01:13<6:54:02,  3.75s/it] 26%|██▌       | 2298/8917 [3:01:17<6:50:58,  3.73s/it] 26%|██▌       | 2299/8917 [3:01:20<6:44:14,  3.66s/it]09/19/2024 05:15:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024315156042575836, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.432328462600708, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4566435813903809}
 26%|██▌       | 2300/8917 [3:01:24<6:48:09,  3.70s/it] 26%|██▌       | 2301/8917 [3:01:27<6:35:36,  3.59s/it] 26%|██▌       | 2302/8917 [3:01:31<6:38:21,  3.61s/it] 26%|██▌       | 2303/8917 [3:01:35<6:53:57,  3.76s/it] 26%|██▌       | 2304/8917 [3:01:38<6:41:26,  3.64s/it] 26%|██▌       | 2305/8917 [3:01:42<6:44:36,  3.67s/it] 26%|██▌       | 2306/8917 [3:01:46<6:52:57,  3.75s/it] 26%|██▌       | 2307/8917 [3:01:50<6:55:22,  3.77s/it] 26%|██▌       | 2308/8917 [3:01:54<7:00:19,  3.82s/it] 26%|██▌       | 2309/8917 [3:01:57<6:51:11,  3.73s/it] 26%|██▌       | 2310/8917 [3:02:01<6:55:03,  3.77s/it] 26%|██▌       | 2311/8917 [3:02:05<6:52:13,  3.74s/it] 26%|██▌       | 2312/8917 [3:02:08<6:45:57,  3.69s/it] 26%|██▌       | 2313/8917 [3:02:12<6:45:56,  3.69s/it] 26%|██▌       | 2314/8917 [3:02:16<6:35:22,  3.59s/it] 26%|██▌       | 2315/8917 [3:02:19<6:44:28,  3.68s/it] 26%|██▌       | 2316/8917 [3:02:23<6:55:35,  3.78s/it] 26%|██▌       | 2317/8917 [3:02:27<7:04:54,  3.86s/it] 26%|██▌       | 2318/8917 [3:02:31<6:51:35,  3.74s/it] 26%|██▌       | 2319/8917 [3:02:34<6:43:10,  3.67s/it] 26%|██▌       | 2320/8917 [3:02:38<6:51:57,  3.75s/it] 26%|██▌       | 2321/8917 [3:02:42<7:03:34,  3.85s/it] 26%|██▌       | 2322/8917 [3:02:46<6:51:25,  3.74s/it] 26%|██▌       | 2323/8917 [3:02:49<6:41:33,  3.65s/it] 26%|██▌       | 2324/8917 [3:02:53<6:50:25,  3.74s/it] 26%|██▌       | 2325/8917 [3:02:57<6:53:44,  3.77s/it] 26%|██▌       | 2326/8917 [3:03:01<6:48:39,  3.72s/it] 26%|██▌       | 2327/8917 [3:03:04<6:47:41,  3.71s/it] 26%|██▌       | 2328/8917 [3:03:08<6:43:53,  3.68s/it] 26%|██▌       | 2329/8917 [3:03:12<6:51:13,  3.75s/it] 26%|██▌       | 2330/8917 [3:03:16<7:00:41,  3.83s/it] 26%|██▌       | 2331/8917 [3:03:20<6:56:26,  3.79s/it] 26%|██▌       | 2332/8917 [3:03:23<6:46:11,  3.70s/it] 26%|██▌       | 2333/8917 [3:03:27<6:52:00,  3.75s/it] 26%|██▌       | 2334/8917 [3:03:31<6:47:44,  3.72s/it] 26%|██▌       | 2335/8917 [3:03:35<6:55:06,  3.78s/it] 26%|██▌       | 2336/8917 [3:03:39<6:58:55,  3.82s/it] 26%|██▌       | 2337/8917 [3:03:42<6:58:40,  3.82s/it] 26%|██▌       | 2338/8917 [3:03:46<6:47:41,  3.72s/it] 26%|██▌       | 2339/8917 [3:03:50<6:52:22,  3.76s/it] 26%|██▌       | 2340/8917 [3:03:53<6:43:09,  3.68s/it] 26%|██▋       | 2341/8917 [3:03:57<6:46:38,  3.71s/it] 26%|██▋       | 2342/8917 [3:04:01<6:44:59,  3.70s/it] 26%|██▋       | 2343/8917 [3:04:04<6:40:20,  3.65s/it] 26%|██▋       | 2344/8917 [3:04:08<6:44:50,  3.70s/it] 26%|██▋       | 2345/8917 [3:04:12<6:45:40,  3.70s/it] 26%|██▋       | 2346/8917 [3:04:15<6:45:54,  3.71s/it] 26%|██▋       | 2347/8917 [3:04:19<6:48:21,  3.73s/it] 26%|██▋       | 2348/8917 [3:04:23<6:55:02,  3.79s/it] 26%|██▋       | 2349/8917 [3:04:27<6:50:00,  3.75s/it]09/19/2024 05:19:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04001138359308243, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2973490953445435, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3373605012893677}
 26%|██▋       | 2350/8917 [3:04:30<6:38:36,  3.64s/it] 26%|██▋       | 2351/8917 [3:04:34<6:52:34,  3.77s/it] 26%|██▋       | 2352/8917 [3:04:38<6:49:46,  3.75s/it] 26%|██▋       | 2353/8917 [3:04:41<6:39:06,  3.65s/it] 26%|██▋       | 2354/8917 [3:04:45<6:34:38,  3.61s/it] 26%|██▋       | 2355/8917 [3:04:49<7:02:52,  3.87s/it] 26%|██▋       | 2356/8917 [3:04:53<6:54:40,  3.79s/it] 26%|██▋       | 2357/8917 [3:04:57<6:49:14,  3.74s/it] 26%|██▋       | 2358/8917 [3:05:00<6:49:54,  3.75s/it] 26%|██▋       | 2359/8917 [3:05:04<6:49:15,  3.74s/it] 26%|██▋       | 2360/8917 [3:05:08<6:50:56,  3.76s/it] 26%|██▋       | 2361/8917 [3:05:12<7:08:05,  3.92s/it] 26%|██▋       | 2362/8917 [3:05:16<7:00:42,  3.85s/it] 26%|██▋       | 2363/8917 [3:05:19<6:46:54,  3.73s/it] 27%|██▋       | 2364/8917 [3:05:23<6:33:40,  3.60s/it] 27%|██▋       | 2365/8917 [3:05:26<6:32:46,  3.60s/it] 27%|██▋       | 2366/8917 [3:05:30<6:42:22,  3.69s/it] 27%|██▋       | 2367/8917 [3:05:34<6:37:50,  3.64s/it] 27%|██▋       | 2368/8917 [3:05:37<6:36:55,  3.64s/it] 27%|██▋       | 2369/8917 [3:05:41<6:40:11,  3.67s/it] 27%|██▋       | 2370/8917 [3:05:45<6:42:58,  3.69s/it] 27%|██▋       | 2371/8917 [3:05:49<7:03:55,  3.89s/it] 27%|██▋       | 2372/8917 [3:05:52<6:46:31,  3.73s/it] 27%|██▋       | 2373/8917 [3:05:56<6:35:44,  3.63s/it] 27%|██▋       | 2374/8917 [3:06:00<6:40:34,  3.67s/it] 27%|██▋       | 2375/8917 [3:06:03<6:42:57,  3.70s/it] 27%|██▋       | 2376/8917 [3:06:07<6:36:27,  3.64s/it] 27%|██▋       | 2377/8917 [3:06:11<6:44:12,  3.71s/it] 27%|██▋       | 2378/8917 [3:06:15<6:50:52,  3.77s/it] 27%|██▋       | 2379/8917 [3:06:18<6:48:52,  3.75s/it] 27%|██▋       | 2380/8917 [3:06:22<7:00:44,  3.86s/it] 27%|██▋       | 2381/8917 [3:06:26<6:46:36,  3.73s/it] 27%|██▋       | 2382/8917 [3:06:29<6:41:21,  3.69s/it] 27%|██▋       | 2383/8917 [3:06:33<6:35:28,  3.63s/it] 27%|██▋       | 2384/8917 [3:06:37<6:42:30,  3.70s/it] 27%|██▋       | 2385/8917 [3:06:40<6:37:23,  3.65s/it] 27%|██▋       | 2386/8917 [3:06:44<6:48:24,  3.75s/it] 27%|██▋       | 2387/8917 [3:06:48<6:55:02,  3.81s/it] 27%|██▋       | 2388/8917 [3:06:52<6:46:45,  3.74s/it] 27%|██▋       | 2389/8917 [3:06:56<6:45:17,  3.73s/it] 27%|██▋       | 2390/8917 [3:06:59<6:41:29,  3.69s/it] 27%|██▋       | 2391/8917 [3:07:03<6:35:35,  3.64s/it] 27%|██▋       | 2392/8917 [3:07:06<6:30:45,  3.59s/it] 27%|██▋       | 2393/8917 [3:07:10<6:35:58,  3.64s/it] 27%|██▋       | 2394/8917 [3:07:14<6:36:35,  3.65s/it] 27%|██▋       | 2395/8917 [3:07:17<6:39:48,  3.68s/it] 27%|██▋       | 2396/8917 [3:07:21<6:48:51,  3.76s/it] 27%|██▋       | 2397/8917 [3:07:25<6:51:48,  3.79s/it] 27%|██▋       | 2398/8917 [3:07:29<6:50:23,  3.78s/it] 27%|██▋       | 2399/8917 [3:07:33<6:49:46,  3.77s/it]09/19/2024 05:22:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028128717094659805, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.418914556503296, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4470432996749878}
 27%|██▋       | 2400/8917 [3:07:36<6:44:28,  3.72s/it] 27%|██▋       | 2401/8917 [3:07:40<6:43:14,  3.71s/it] 27%|██▋       | 2402/8917 [3:07:44<6:42:23,  3.71s/it] 27%|██▋       | 2403/8917 [3:07:47<6:44:43,  3.73s/it] 27%|██▋       | 2404/8917 [3:07:51<6:40:51,  3.69s/it] 27%|██▋       | 2405/8917 [3:07:55<6:48:31,  3.76s/it] 27%|██▋       | 2406/8917 [3:07:59<6:52:38,  3.80s/it] 27%|██▋       | 2407/8917 [3:08:03<6:55:12,  3.83s/it] 27%|██▋       | 2408/8917 [3:08:06<6:48:29,  3.77s/it] 27%|██▋       | 2409/8917 [3:08:10<6:46:58,  3.75s/it] 27%|██▋       | 2410/8917 [3:08:14<6:48:30,  3.77s/it] 27%|██▋       | 2411/8917 [3:08:17<6:36:16,  3.65s/it] 27%|██▋       | 2412/8917 [3:08:21<6:38:53,  3.68s/it] 27%|██▋       | 2413/8917 [3:08:25<6:48:40,  3.77s/it] 27%|██▋       | 2414/8917 [3:08:29<6:45:29,  3.74s/it] 27%|██▋       | 2415/8917 [3:08:32<6:48:25,  3.77s/it] 27%|██▋       | 2416/8917 [3:08:36<6:45:20,  3.74s/it] 27%|██▋       | 2417/8917 [3:08:40<6:46:30,  3.75s/it] 27%|██▋       | 2418/8917 [3:08:44<6:46:59,  3.76s/it] 27%|██▋       | 2419/8917 [3:08:47<6:38:35,  3.68s/it] 27%|██▋       | 2420/8917 [3:08:51<6:36:45,  3.66s/it] 27%|██▋       | 2421/8917 [3:08:55<6:44:56,  3.74s/it] 27%|██▋       | 2422/8917 [3:08:59<6:56:16,  3.85s/it] 27%|██▋       | 2423/8917 [3:09:02<6:49:15,  3.78s/it] 27%|██▋       | 2424/8917 [3:09:06<6:36:24,  3.66s/it] 27%|██▋       | 2425/8917 [3:09:10<6:36:41,  3.67s/it] 27%|██▋       | 2426/8917 [3:09:13<6:45:18,  3.75s/it] 27%|██▋       | 2427/8917 [3:09:17<6:41:01,  3.71s/it] 27%|██▋       | 2428/8917 [3:09:21<6:41:49,  3.72s/it] 27%|██▋       | 2429/8917 [3:09:24<6:36:20,  3.67s/it] 27%|██▋       | 2430/8917 [3:09:28<6:35:34,  3.66s/it] 27%|██▋       | 2431/8917 [3:09:32<6:47:21,  3.77s/it] 27%|██▋       | 2432/8917 [3:09:36<6:42:24,  3.72s/it] 27%|██▋       | 2433/8917 [3:09:39<6:44:26,  3.74s/it] 27%|██▋       | 2434/8917 [3:09:43<6:46:34,  3.76s/it] 27%|██▋       | 2435/8917 [3:09:47<6:52:19,  3.82s/it] 27%|██▋       | 2436/8917 [3:09:51<6:41:46,  3.72s/it] 27%|██▋       | 2437/8917 [3:09:54<6:39:53,  3.70s/it] 27%|██▋       | 2438/8917 [3:09:58<6:45:09,  3.75s/it] 27%|██▋       | 2439/8917 [3:10:02<6:41:40,  3.72s/it] 27%|██▋       | 2440/8917 [3:10:06<6:46:11,  3.76s/it] 27%|██▋       | 2441/8917 [3:10:09<6:43:41,  3.74s/it] 27%|██▋       | 2442/8917 [3:10:13<6:47:36,  3.78s/it] 27%|██▋       | 2443/8917 [3:10:17<6:40:05,  3.71s/it] 27%|██▋       | 2444/8917 [3:10:21<6:43:34,  3.74s/it] 27%|██▋       | 2445/8917 [3:10:24<6:42:34,  3.73s/it] 27%|██▋       | 2446/8917 [3:10:28<6:37:12,  3.68s/it] 27%|██▋       | 2447/8917 [3:10:31<6:32:39,  3.64s/it] 27%|██▋       | 2448/8917 [3:10:35<6:35:27,  3.67s/it] 27%|██▋       | 2449/8917 [3:10:39<6:38:27,  3.70s/it]09/19/2024 05:25:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026885127648711205, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4447575807571411, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4716427326202393}
 27%|██▋       | 2450/8917 [3:10:42<6:26:54,  3.59s/it] 27%|██▋       | 2451/8917 [3:10:46<6:32:35,  3.64s/it] 27%|██▋       | 2452/8917 [3:10:50<6:38:16,  3.70s/it] 28%|██▊       | 2453/8917 [3:10:54<6:38:55,  3.70s/it] 28%|██▊       | 2454/8917 [3:10:57<6:28:38,  3.61s/it] 28%|██▊       | 2455/8917 [3:11:01<6:36:35,  3.68s/it] 28%|██▊       | 2456/8917 [3:11:04<6:28:29,  3.61s/it] 28%|██▊       | 2457/8917 [3:11:08<6:27:59,  3.60s/it] 28%|██▊       | 2458/8917 [3:11:12<6:28:46,  3.61s/it] 28%|██▊       | 2459/8917 [3:11:15<6:26:39,  3.59s/it] 28%|██▊       | 2460/8917 [3:11:19<6:33:03,  3.65s/it] 28%|██▊       | 2461/8917 [3:11:22<6:22:25,  3.55s/it] 28%|██▊       | 2462/8917 [3:11:25<5:59:34,  3.34s/it] 28%|██▊       | 2463/8917 [3:11:28<5:43:40,  3.20s/it] 28%|██▊       | 2464/8917 [3:11:31<5:32:54,  3.10s/it] 28%|██▊       | 2465/8917 [3:11:34<5:25:05,  3.02s/it] 28%|██▊       | 2466/8917 [3:11:36<5:20:07,  2.98s/it] 28%|██▊       | 2467/8917 [3:11:39<5:16:40,  2.95s/it] 28%|██▊       | 2468/8917 [3:11:42<5:14:10,  2.92s/it] 28%|██▊       | 2469/8917 [3:11:45<5:11:56,  2.90s/it] 28%|██▊       | 2470/8917 [3:11:48<5:12:37,  2.91s/it] 28%|██▊       | 2471/8917 [3:11:51<5:11:13,  2.90s/it] 28%|██▊       | 2472/8917 [3:11:54<5:10:15,  2.89s/it] 28%|██▊       | 2473/8917 [3:11:57<5:09:35,  2.88s/it] 28%|██▊       | 2474/8917 [3:11:59<5:08:53,  2.88s/it] 28%|██▊       | 2475/8917 [3:12:02<5:08:29,  2.87s/it] 28%|██▊       | 2476/8917 [3:12:05<5:08:32,  2.87s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
 28%|██▊       | 2477/8917 [3:12:53<29:14:34, 16.35s/it] 28%|██▊       | 2478/8917 [3:13:00<24:12:05, 13.53s/it] 28%|██▊       | 2479/8917 [3:13:06<20:14:42, 11.32s/it] 28%|██▊       | 2480/8917 [3:13:11<16:51:51,  9.43s/it] 28%|██▊       | 2481/8917 [3:13:17<15:05:32,  8.44s/it] 28%|██▊       | 2482/8917 [3:13:22<13:15:55,  7.42s/it] 28%|██▊       | 2483/8917 [3:13:26<11:11:03,  6.26s/it] 28%|██▊       | 2484/8917 [3:13:30<9:47:13,  5.48s/it]  28%|██▊       | 2485/8917 [3:13:33<8:50:52,  4.95s/it] 28%|██▊       | 2486/8917 [3:13:37<8:15:48,  4.63s/it] 28%|██▊       | 2487/8917 [3:13:41<7:58:38,  4.47s/it] 28%|██▊       | 2488/8917 [3:13:45<7:35:20,  4.25s/it] 28%|██▊       | 2489/8917 [3:13:49<7:18:39,  4.09s/it] 28%|██▊       | 2490/8917 [3:13:53<7:12:45,  4.04s/it] 28%|██▊       | 2491/8917 [3:13:56<6:49:05,  3.82s/it] 28%|██▊       | 2492/8917 [3:14:00<6:42:48,  3.76s/it] 28%|██▊       | 2493/8917 [3:14:03<6:45:44,  3.79s/it] 28%|██▊       | 2494/8917 [3:14:07<6:41:45,  3.75s/it] 28%|██▊       | 2495/8917 [3:14:11<6:48:53,  3.82s/it] 28%|██▊       | 2496/8917 [3:14:14<6:37:22,  3.71s/it] 28%|██▊       | 2497/8917 [3:14:18<6:31:54,  3.66s/it] 28%|██▊       | 2498/8917 [3:14:22<6:37:40,  3.72s/it] 28%|██▊       | 2499/8917 [3:14:25<6:31:36,  3.66s/it]09/19/2024 05:29:02 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 05:29:02 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<05:30,  1.50s/it][A
  1%|          | 2/221 [00:01<03:03,  1.19it/s][A
  1%|▏         | 3/221 [00:02<03:02,  1.19it/s][A
  2%|▏         | 4/221 [00:02<02:08,  1.69it/s][A
  3%|▎         | 6/221 [00:03<01:08,  3.12it/s][A
  3%|▎         | 7/221 [00:03<01:04,  3.30it/s][A
  4%|▎         | 8/221 [00:03<01:03,  3.34it/s][A
  4%|▍         | 9/221 [00:04<01:15,  2.82it/s][A
  5%|▍         | 10/221 [00:04<00:59,  3.55it/s][A
  5%|▌         | 12/221 [00:05<01:52,  1.86it/s][A
  6%|▋         | 14/221 [00:06<01:14,  2.78it/s][A
  7%|▋         | 15/221 [00:06<01:13,  2.80it/s][A
  7%|▋         | 16/221 [00:07<01:33,  2.19it/s][A
  8%|▊         | 17/221 [00:07<01:43,  1.97it/s][A
  8%|▊         | 18/221 [00:08<01:33,  2.17it/s][A
  9%|▊         | 19/221 [00:08<01:45,  1.92it/s][A
 10%|▉         | 21/221 [00:09<01:13,  2.71it/s][A
 10%|▉         | 22/221 [00:10<01:33,  2.14it/s][A
 11%|█         | 24/221 [00:10<01:08,  2.90it/s][A
 11%|█▏        | 25/221 [00:10<01:06,  2.96it/s][A
 12%|█▏        | 26/221 [00:11<01:06,  2.92it/s][A
 13%|█▎        | 28/221 [00:11<00:54,  3.54it/s][A
 13%|█▎        | 29/221 [00:11<00:52,  3.66it/s][A
 14%|█▎        | 30/221 [00:12<01:06,  2.89it/s][A
 14%|█▍        | 31/221 [00:12<01:09,  2.75it/s][A
 15%|█▍        | 33/221 [00:13<00:55,  3.38it/s][A
 16%|█▌        | 35/221 [00:13<00:44,  4.22it/s][A
 16%|█▋        | 36/221 [00:13<00:42,  4.39it/s][A
 17%|█▋        | 37/221 [00:13<00:47,  3.85it/s][A
 17%|█▋        | 38/221 [00:15<01:40,  1.82it/s][A
 18%|█▊        | 39/221 [00:15<01:21,  2.23it/s][A
 18%|█▊        | 40/221 [00:15<01:18,  2.31it/s][A
 19%|█▊        | 41/221 [00:15<01:03,  2.86it/s][A
 19%|█▉        | 42/221 [00:16<00:57,  3.11it/s][A
 20%|█▉        | 44/221 [00:16<00:38,  4.56it/s][A
 20%|██        | 45/221 [00:22<04:50,  1.65s/it][A
 21%|██        | 46/221 [00:22<03:50,  1.32s/it][A
 21%|██▏       | 47/221 [00:23<03:18,  1.14s/it][A
 22%|██▏       | 48/221 [00:23<02:27,  1.17it/s][A
 22%|██▏       | 49/221 [00:23<01:59,  1.45it/s][A
 23%|██▎       | 50/221 [00:24<01:48,  1.58it/s][A
 23%|██▎       | 51/221 [00:24<01:26,  1.96it/s][A
 24%|██▍       | 53/221 [00:24<00:56,  2.96it/s][A
 24%|██▍       | 54/221 [00:25<01:12,  2.30it/s][A
 25%|██▍       | 55/221 [00:28<02:53,  1.05s/it][A
 25%|██▌       | 56/221 [00:28<02:15,  1.22it/s][A
 26%|██▌       | 57/221 [00:28<01:46,  1.54it/s][A
 27%|██▋       | 59/221 [00:28<01:03,  2.54it/s][A
 27%|██▋       | 60/221 [00:29<01:04,  2.49it/s][A
 28%|██▊       | 61/221 [00:29<00:58,  2.72it/s][A
 28%|██▊       | 62/221 [00:29<00:54,  2.94it/s][A
 29%|██▊       | 63/221 [00:30<00:50,  3.15it/s][A
 29%|██▉       | 64/221 [00:31<01:14,  2.10it/s][A
 30%|██▉       | 66/221 [00:31<00:55,  2.80it/s][A
 30%|███       | 67/221 [00:31<00:47,  3.21it/s][A
 31%|███       | 68/221 [00:31<00:40,  3.74it/s][A
 31%|███       | 69/221 [00:32<01:05,  2.31it/s][A
 32%|███▏      | 71/221 [00:33<00:46,  3.23it/s][A
 33%|███▎      | 72/221 [00:33<00:45,  3.28it/s][A
 33%|███▎      | 73/221 [00:33<00:59,  2.50it/s][A
 33%|███▎      | 74/221 [00:34<00:49,  2.99it/s][A
 34%|███▍      | 75/221 [00:34<00:52,  2.80it/s][A
 34%|███▍      | 76/221 [00:34<00:50,  2.89it/s][A
 35%|███▍      | 77/221 [00:36<01:50,  1.30it/s][A
 36%|███▌      | 79/221 [00:38<02:04,  1.14it/s][A
 36%|███▌      | 80/221 [00:38<01:37,  1.44it/s][A
 37%|███▋      | 81/221 [00:39<01:39,  1.40it/s][A
 37%|███▋      | 82/221 [00:42<03:11,  1.38s/it][A
 38%|███▊      | 83/221 [00:43<02:40,  1.16s/it][A
 38%|███▊      | 84/221 [00:43<02:00,  1.14it/s][A
 39%|███▉      | 86/221 [00:44<01:24,  1.60it/s][A
 39%|███▉      | 87/221 [00:45<01:42,  1.30it/s][A
 40%|███▉      | 88/221 [00:45<01:30,  1.47it/s][A
 40%|████      | 89/221 [00:46<01:21,  1.62it/s][A
 41%|████      | 90/221 [00:46<01:11,  1.84it/s][A
 42%|████▏     | 92/221 [00:46<00:45,  2.86it/s][A
 42%|████▏     | 93/221 [00:47<00:44,  2.89it/s][A
 43%|████▎     | 94/221 [00:48<01:07,  1.87it/s][A
 43%|████▎     | 95/221 [00:48<00:58,  2.17it/s][A
 43%|████▎     | 96/221 [00:49<01:23,  1.50it/s][A
 44%|████▍     | 97/221 [00:49<01:03,  1.95it/s][A
 44%|████▍     | 98/221 [00:50<01:12,  1.69it/s][A
 45%|████▍     | 99/221 [00:50<00:56,  2.17it/s][A
 45%|████▌     | 100/221 [00:51<01:00,  2.02it/s][A
 46%|████▌     | 101/221 [00:51<00:46,  2.56it/s][A
 46%|████▌     | 102/221 [00:52<01:13,  1.62it/s][A
 47%|████▋     | 104/221 [00:53<00:49,  2.37it/s][A
 48%|████▊     | 105/221 [00:53<00:58,  1.97it/s][A
 48%|████▊     | 106/221 [00:57<02:22,  1.24s/it][A
 48%|████▊     | 107/221 [00:57<01:52,  1.02it/s][A
 49%|████▉     | 108/221 [00:57<01:29,  1.27it/s][A
 49%|████▉     | 109/221 [00:58<01:14,  1.51it/s][A
 50%|█████     | 111/221 [00:58<00:50,  2.16it/s][A
 51%|█████     | 112/221 [00:58<00:48,  2.25it/s][A
 51%|█████     | 113/221 [00:59<00:40,  2.64it/s][A
 52%|█████▏    | 115/221 [00:59<00:31,  3.38it/s][A
 52%|█████▏    | 116/221 [00:59<00:35,  2.96it/s][A
 53%|█████▎    | 117/221 [01:00<00:41,  2.51it/s][A
 53%|█████▎    | 118/221 [01:00<00:41,  2.48it/s][A
 54%|█████▍    | 119/221 [01:01<00:39,  2.60it/s][A
 54%|█████▍    | 120/221 [01:01<00:34,  2.91it/s][A
 55%|█████▍    | 121/221 [01:01<00:37,  2.70it/s][A
 55%|█████▌    | 122/221 [01:02<00:35,  2.82it/s][A
 56%|█████▌    | 123/221 [01:04<01:27,  1.12it/s][A
 56%|█████▌    | 124/221 [01:04<01:05,  1.49it/s][A
 57%|█████▋    | 125/221 [01:05<01:14,  1.29it/s][A
 57%|█████▋    | 126/221 [01:07<01:58,  1.24s/it][A
 57%|█████▋    | 127/221 [01:09<01:54,  1.21s/it][A
 58%|█████▊    | 128/221 [01:09<01:32,  1.01it/s][A
 58%|█████▊    | 129/221 [01:09<01:12,  1.27it/s][A
 59%|█████▉    | 130/221 [01:10<01:01,  1.48it/s][A
 59%|█████▉    | 131/221 [01:10<00:55,  1.61it/s][A
 60%|█████▉    | 132/221 [01:12<01:25,  1.05it/s][A
 60%|██████    | 133/221 [01:12<01:05,  1.35it/s][A
 61%|██████    | 134/221 [01:14<01:29,  1.03s/it][A
 61%|██████    | 135/221 [01:15<01:40,  1.17s/it][A
 62%|██████▏   | 136/221 [01:16<01:17,  1.10it/s][A
 62%|██████▏   | 137/221 [01:16<01:01,  1.36it/s][A
 62%|██████▏   | 138/221 [01:17<00:59,  1.41it/s][A
 63%|██████▎   | 139/221 [01:17<00:44,  1.85it/s][A
 63%|██████▎   | 140/221 [01:18<00:47,  1.70it/s][A
 64%|██████▍   | 141/221 [01:18<00:45,  1.77it/s][A
 64%|██████▍   | 142/221 [01:18<00:38,  2.06it/s][A
 65%|██████▍   | 143/221 [01:19<00:38,  2.04it/s][A
 65%|██████▌   | 144/221 [01:19<00:31,  2.48it/s][A
 66%|██████▌   | 146/221 [01:19<00:19,  3.89it/s][A
 67%|██████▋   | 148/221 [01:20<00:21,  3.38it/s][A
 67%|██████▋   | 149/221 [01:21<00:26,  2.71it/s][A
 68%|██████▊   | 150/221 [01:21<00:24,  2.92it/s][A
 68%|██████▊   | 151/221 [01:21<00:23,  3.04it/s][A
 69%|██████▉   | 152/221 [01:21<00:20,  3.30it/s][A
 69%|██████▉   | 153/221 [01:21<00:16,  4.04it/s][A
 70%|██████▉   | 154/221 [01:22<00:16,  4.08it/s][A
 70%|███████   | 155/221 [01:22<00:14,  4.66it/s][A
 71%|███████   | 156/221 [01:22<00:13,  4.73it/s][A
 71%|███████   | 157/221 [01:29<02:29,  2.34s/it][A
 71%|███████▏  | 158/221 [01:30<01:56,  1.86s/it][A
 72%|███████▏  | 159/221 [01:30<01:25,  1.38s/it][A
 72%|███████▏  | 160/221 [01:31<01:03,  1.04s/it][A
 73%|███████▎  | 162/221 [01:31<00:34,  1.70it/s][A
 74%|███████▍  | 163/221 [01:31<00:29,  1.97it/s][A
 74%|███████▍  | 164/221 [01:31<00:24,  2.30it/s][A
 75%|███████▍  | 165/221 [01:32<00:31,  1.77it/s][A
 75%|███████▌  | 166/221 [01:33<00:29,  1.86it/s][A
 76%|███████▌  | 167/221 [01:33<00:23,  2.26it/s][A
 76%|███████▌  | 168/221 [01:40<02:01,  2.29s/it][A
 76%|███████▋  | 169/221 [01:40<01:35,  1.84s/it][A
 77%|███████▋  | 170/221 [01:41<01:12,  1.42s/it][A
 77%|███████▋  | 171/221 [01:41<00:55,  1.11s/it][A
 78%|███████▊  | 172/221 [01:41<00:41,  1.19it/s][A
 78%|███████▊  | 173/221 [01:42<00:37,  1.27it/s][A
 79%|███████▊  | 174/221 [01:42<00:27,  1.69it/s][A
 79%|███████▉  | 175/221 [01:43<00:23,  1.99it/s][A
 80%|███████▉  | 176/221 [01:43<00:18,  2.48it/s][A
 80%|████████  | 177/221 [01:43<00:15,  2.80it/s][A
 81%|████████  | 178/221 [01:43<00:17,  2.45it/s][A
 81%|████████  | 179/221 [01:44<00:17,  2.46it/s][A
 81%|████████▏ | 180/221 [01:44<00:13,  3.02it/s][A
 82%|████████▏ | 182/221 [01:44<00:11,  3.50it/s][A
 83%|████████▎ | 183/221 [01:45<00:15,  2.49it/s][A
 83%|████████▎ | 184/221 [01:46<00:16,  2.23it/s][A
 84%|████████▎ | 185/221 [01:46<00:13,  2.69it/s][A
 84%|████████▍ | 186/221 [01:46<00:12,  2.82it/s][A
 85%|████████▍ | 187/221 [01:47<00:10,  3.11it/s][A
 85%|████████▌ | 188/221 [01:47<00:09,  3.59it/s][A
 86%|████████▌ | 189/221 [01:47<00:10,  2.98it/s][A
 86%|████████▌ | 190/221 [01:48<00:11,  2.66it/s][A
 86%|████████▋ | 191/221 [01:48<00:09,  3.16it/s][A
 87%|████████▋ | 192/221 [01:48<00:09,  3.20it/s][A
 87%|████████▋ | 193/221 [01:48<00:07,  3.77it/s][A
 88%|████████▊ | 194/221 [01:50<00:20,  1.31it/s][A
 88%|████████▊ | 195/221 [01:50<00:15,  1.70it/s][A
 89%|████████▊ | 196/221 [01:51<00:13,  1.91it/s][A
 89%|████████▉ | 197/221 [01:51<00:10,  2.30it/s][A
 90%|████████▉ | 198/221 [01:51<00:08,  2.63it/s][A
 90%|█████████ | 199/221 [01:51<00:06,  3.28it/s][A
 90%|█████████ | 200/221 [01:52<00:06,  3.08it/s][A
 91%|█████████ | 201/221 [01:52<00:08,  2.37it/s][A
 91%|█████████▏| 202/221 [01:53<00:06,  2.87it/s][A
 92%|█████████▏| 203/221 [01:54<00:10,  1.76it/s][A
 92%|█████████▏| 204/221 [01:54<00:07,  2.25it/s][A
 93%|█████████▎| 206/221 [01:54<00:05,  2.76it/s][A
 94%|█████████▎| 207/221 [01:54<00:04,  3.36it/s][A
 94%|█████████▍| 208/221 [01:55<00:04,  3.24it/s][A
 95%|█████████▍| 209/221 [01:55<00:03,  3.93it/s][A
 95%|█████████▌| 211/221 [01:56<00:03,  3.22it/s][A
 96%|█████████▌| 212/221 [01:56<00:02,  3.18it/s][A
 96%|█████████▋| 213/221 [01:56<00:02,  3.07it/s][A
 97%|█████████▋| 214/221 [01:57<00:02,  3.44it/s][A
 97%|█████████▋| 215/221 [01:57<00:01,  3.27it/s][A
 98%|█████████▊| 216/221 [01:57<00:01,  2.79it/s][A
 98%|█████████▊| 217/221 [02:05<00:09,  2.45s/it][A
 99%|█████████▊| 218/221 [02:05<00:05,  1.86s/it][A
 99%|█████████▉| 219/221 [02:06<00:02,  1.45s/it][A
100%|█████████▉| 220/221 [02:08<00:01,  1.66s/it][A
100%|██████████| 221/221 [02:08<00:00,  1.21s/it][A100%|██████████| 221/221 [02:08<00:00,  1.72it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:23,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:55,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:30,  7.17it/s][A
  1%|          | 2/221 [00:00<01:19,  2.76it/s][A
  1%|▏         | 3/221 [00:00<01:07,  3.22it/s][A
  2%|▏         | 4/221 [00:01<01:34,  2.30it/s][A
  2%|▏         | 5/221 [00:01<01:36,  2.24it/s][A
  3%|▎         | 6/221 [00:02<01:16,  2.80it/s][A
  3%|▎         | 7/221 [00:02<01:12,  2.95it/s][A
  4%|▎         | 8/221 [00:02<01:14,  2.86it/s][A
  4%|▍         | 9/221 [00:03<01:58,  1.79it/s][A
  5%|▍         | 10/221 [00:04<01:40,  2.09it/s][A
  5%|▍         | 11/221 [00:04<01:53,  1.85it/s][A
  5%|▌         | 12/221 [00:05<01:40,  2.08it/s][A
  6%|▌         | 13/221 [00:06<02:28,  1.40it/s][A
  6%|▋         | 14/221 [00:06<01:53,  1.82it/s][A
  7%|▋         | 15/221 [00:06<01:28,  2.32it/s][A
  7%|▋         | 16/221 [00:07<01:53,  1.81it/s][A
  8%|▊         | 17/221 [00:08<01:52,  1.81it/s][A
  8%|▊         | 18/221 [00:08<01:38,  2.06it/s][A
  9%|▊         | 19/221 [00:09<01:53,  1.79it/s][A
  9%|▉         | 20/221 [00:09<01:28,  2.26it/s][A
 10%|▉         | 21/221 [00:09<01:31,  2.19it/s][A
 10%|▉         | 22/221 [00:10<01:15,  2.64it/s][A
 10%|█         | 23/221 [00:10<01:42,  1.93it/s][A
 11%|█         | 24/221 [00:11<01:29,  2.21it/s][A
 11%|█▏        | 25/221 [00:12<01:56,  1.69it/s][A
 12%|█▏        | 26/221 [00:12<02:00,  1.62it/s][A
 12%|█▏        | 27/221 [00:13<01:59,  1.63it/s][A
 13%|█▎        | 28/221 [00:13<01:44,  1.84it/s][A
 13%|█▎        | 29/221 [00:16<03:43,  1.16s/it][A
 14%|█▎        | 30/221 [00:17<03:25,  1.07s/it][A
 14%|█▍        | 31/221 [00:17<02:47,  1.14it/s][A
 14%|█▍        | 32/221 [00:18<02:33,  1.23it/s][A
 15%|█▍        | 33/221 [00:18<02:17,  1.36it/s][A
 15%|█▌        | 34/221 [00:19<01:55,  1.62it/s][A
 16%|█▌        | 35/221 [00:19<01:28,  2.11it/s][A
 16%|█▋        | 36/221 [00:20<01:53,  1.63it/s][A
 17%|█▋        | 37/221 [00:20<01:39,  1.85it/s][A
 17%|█▋        | 38/221 [00:21<01:48,  1.68it/s][A
 18%|█▊        | 39/221 [00:22<02:20,  1.29it/s][A
 18%|█▊        | 40/221 [00:23<02:31,  1.19it/s][A
 19%|█▊        | 41/221 [00:24<02:08,  1.40it/s][A
 19%|█▉        | 42/221 [00:24<01:42,  1.74it/s][A
 19%|█▉        | 43/221 [00:24<01:48,  1.64it/s][A
 20%|█▉        | 44/221 [00:25<01:24,  2.11it/s][A
 20%|██        | 45/221 [00:25<01:25,  2.05it/s][A
 21%|██        | 46/221 [00:26<01:34,  1.84it/s][A
 21%|██▏       | 47/221 [00:26<01:28,  1.97it/s][A
 22%|██▏       | 48/221 [00:26<01:14,  2.32it/s][A
 22%|██▏       | 49/221 [00:27<01:27,  1.97it/s][A
 23%|██▎       | 50/221 [00:28<01:45,  1.62it/s][A
 23%|██▎       | 51/221 [00:29<01:40,  1.70it/s][A
 24%|██▎       | 52/221 [00:29<01:25,  1.98it/s][A
 24%|██▍       | 53/221 [00:29<01:11,  2.36it/s][A
 24%|██▍       | 54/221 [00:30<01:22,  2.02it/s][A
 25%|██▍       | 55/221 [00:30<01:18,  2.12it/s][A
 25%|██▌       | 56/221 [00:31<01:44,  1.58it/s][A
 26%|██▌       | 57/221 [00:32<01:27,  1.87it/s][A
 26%|██▌       | 58/221 [00:33<01:56,  1.40it/s][A
 27%|██▋       | 59/221 [00:33<01:30,  1.79it/s][A
 27%|██▋       | 60/221 [00:33<01:18,  2.06it/s][A
 28%|██▊       | 61/221 [00:34<01:40,  1.60it/s][A
 28%|██▊       | 62/221 [00:35<01:31,  1.74it/s][A
 29%|██▊       | 63/221 [00:35<01:15,  2.11it/s][A
 29%|██▉       | 64/221 [00:36<01:38,  1.60it/s][A
 29%|██▉       | 65/221 [00:37<01:53,  1.38it/s][A
 30%|██▉       | 66/221 [00:37<01:41,  1.53it/s][A
 30%|███       | 67/221 [00:38<01:27,  1.75it/s][A
 31%|███       | 68/221 [00:38<01:09,  2.21it/s][A
 31%|███       | 69/221 [00:38<01:10,  2.16it/s][A
 32%|███▏      | 70/221 [00:39<01:06,  2.26it/s][A
 32%|███▏      | 71/221 [00:39<01:14,  2.02it/s][A
 33%|███▎      | 72/221 [00:40<01:16,  1.94it/s][A
 33%|███▎      | 73/221 [00:41<01:23,  1.76it/s][A
 33%|███▎      | 74/221 [00:41<01:34,  1.55it/s][A
 34%|███▍      | 75/221 [00:42<01:22,  1.78it/s][A
 34%|███▍      | 76/221 [00:42<01:09,  2.08it/s][A
 35%|███▍      | 77/221 [00:43<01:21,  1.77it/s][A
 35%|███▌      | 78/221 [00:43<01:09,  2.06it/s][A
 36%|███▌      | 79/221 [00:44<01:29,  1.59it/s][A
 36%|███▌      | 80/221 [00:44<01:18,  1.79it/s][A
 37%|███▋      | 81/221 [00:45<01:03,  2.20it/s][A
 37%|███▋      | 82/221 [00:46<01:32,  1.50it/s][A
 38%|███▊      | 83/221 [00:46<01:29,  1.54it/s][A
 38%|███▊      | 84/221 [00:47<01:45,  1.30it/s][A
 38%|███▊      | 85/221 [00:48<01:30,  1.51it/s][A
 39%|███▉      | 86/221 [00:49<01:33,  1.44it/s][A
 39%|███▉      | 87/221 [00:49<01:23,  1.61it/s][A
 40%|███▉      | 88/221 [00:50<01:24,  1.58it/s][A
 40%|████      | 89/221 [00:50<01:20,  1.63it/s][A
 41%|████      | 90/221 [00:51<01:26,  1.51it/s][A
 41%|████      | 91/221 [00:51<01:06,  1.95it/s][A
 42%|████▏     | 92/221 [00:52<01:25,  1.50it/s][A
 43%|████▎     | 94/221 [00:53<00:52,  2.41it/s][A
 43%|████▎     | 95/221 [00:53<00:56,  2.23it/s][A
 43%|████▎     | 96/221 [00:54<01:04,  1.95it/s][A
 44%|████▍     | 97/221 [00:54<00:57,  2.17it/s][A
 44%|████▍     | 98/221 [00:55<01:19,  1.55it/s][A
 45%|████▍     | 99/221 [00:56<01:13,  1.65it/s][A
 45%|████▌     | 100/221 [00:56<01:13,  1.65it/s][A
 46%|████▌     | 101/221 [00:57<01:09,  1.71it/s][A
 46%|████▌     | 102/221 [00:58<01:17,  1.54it/s][A
 47%|████▋     | 103/221 [00:58<00:57,  2.05it/s][A
 47%|████▋     | 104/221 [00:59<01:21,  1.44it/s][A
 48%|████▊     | 105/221 [00:59<01:07,  1.73it/s][A
 48%|████▊     | 106/221 [01:00<00:57,  2.01it/s][A
 48%|████▊     | 107/221 [01:00<00:57,  1.98it/s][A
 49%|████▉     | 108/221 [01:00<00:53,  2.10it/s][A
 49%|████▉     | 109/221 [01:01<00:47,  2.35it/s][A
 50%|████▉     | 110/221 [01:02<00:57,  1.93it/s][A
 50%|█████     | 111/221 [01:02<00:51,  2.14it/s][A
 51%|█████     | 112/221 [01:02<00:43,  2.52it/s][A
 51%|█████     | 113/221 [01:02<00:38,  2.84it/s][A
 52%|█████▏    | 114/221 [01:03<00:39,  2.69it/s][A
 52%|█████▏    | 115/221 [01:04<00:53,  1.99it/s][A
 52%|█████▏    | 116/221 [01:04<00:43,  2.42it/s][A
 53%|█████▎    | 117/221 [01:05<00:53,  1.96it/s][A
 53%|█████▎    | 118/221 [01:05<00:56,  1.83it/s][A
 54%|█████▍    | 119/221 [01:05<00:47,  2.13it/s][A
 54%|█████▍    | 120/221 [01:06<00:50,  2.00it/s][A
 55%|█████▍    | 121/221 [01:07<00:50,  1.98it/s][A
 55%|█████▌    | 122/221 [01:07<00:40,  2.44it/s][A
 56%|█████▌    | 123/221 [01:07<00:42,  2.33it/s][A
 56%|█████▌    | 124/221 [01:07<00:33,  2.88it/s][A
 57%|█████▋    | 125/221 [01:08<00:45,  2.11it/s][A
 57%|█████▋    | 126/221 [01:08<00:39,  2.43it/s][A
 57%|█████▋    | 127/221 [01:09<00:48,  1.94it/s][A
 58%|█████▊    | 128/221 [01:10<00:48,  1.90it/s][A
 58%|█████▊    | 129/221 [01:10<00:39,  2.33it/s][A
 59%|█████▉    | 130/221 [01:11<00:49,  1.83it/s][A
 59%|█████▉    | 131/221 [01:11<00:37,  2.40it/s][A
 60%|█████▉    | 132/221 [01:11<00:35,  2.51it/s][A
 60%|██████    | 133/221 [01:12<00:45,  1.91it/s][A
 61%|██████    | 134/221 [01:12<00:40,  2.17it/s][A
 61%|██████    | 135/221 [01:13<00:35,  2.39it/s][A
 62%|██████▏   | 136/221 [01:13<00:35,  2.38it/s][A
 62%|██████▏   | 137/221 [01:13<00:35,  2.38it/s][A
 62%|██████▏   | 138/221 [01:14<00:34,  2.39it/s][A
 63%|██████▎   | 139/221 [01:14<00:34,  2.40it/s][A
 63%|██████▎   | 140/221 [01:15<00:38,  2.09it/s][A
 64%|██████▍   | 141/221 [01:15<00:37,  2.13it/s][A
 64%|██████▍   | 142/221 [01:16<00:39,  1.99it/s][A
 65%|██████▍   | 143/221 [01:16<00:31,  2.49it/s][A
 65%|██████▌   | 144/221 [01:16<00:26,  2.90it/s][A
 66%|██████▌   | 145/221 [01:17<00:35,  2.11it/s][A
 66%|██████▌   | 146/221 [01:17<00:28,  2.65it/s][A
 67%|██████▋   | 147/221 [01:17<00:23,  3.18it/s][A
 67%|██████▋   | 148/221 [01:18<00:22,  3.19it/s][A
 67%|██████▋   | 149/221 [01:18<00:22,  3.27it/s][A
 68%|██████▊   | 150/221 [01:18<00:23,  3.07it/s][A
 69%|██████▉   | 152/221 [01:19<00:24,  2.85it/s][A
 69%|██████▉   | 153/221 [01:20<00:25,  2.71it/s][A
 70%|██████▉   | 154/221 [01:20<00:27,  2.43it/s][A
 70%|███████   | 155/221 [01:21<00:28,  2.30it/s][A
 71%|███████   | 156/221 [01:22<00:38,  1.70it/s][A
 71%|███████   | 157/221 [01:22<00:36,  1.76it/s][A
 71%|███████▏  | 158/221 [01:22<00:31,  1.98it/s][A
 72%|███████▏  | 159/221 [01:23<00:27,  2.25it/s][A
 72%|███████▏  | 160/221 [01:24<00:33,  1.80it/s][A
 73%|███████▎  | 161/221 [01:24<00:26,  2.28it/s][A
 73%|███████▎  | 162/221 [01:24<00:25,  2.36it/s][A
 74%|███████▍  | 163/221 [01:25<00:25,  2.32it/s][A
 74%|███████▍  | 164/221 [01:25<00:20,  2.73it/s][A
 75%|███████▍  | 165/221 [01:25<00:18,  3.00it/s][A
 75%|███████▌  | 166/221 [01:25<00:17,  3.12it/s][A
 76%|███████▌  | 167/221 [01:26<00:17,  3.10it/s][A
 76%|███████▌  | 168/221 [01:26<00:19,  2.76it/s][A
 76%|███████▋  | 169/221 [01:27<00:21,  2.47it/s][A
 77%|███████▋  | 170/221 [01:28<00:28,  1.78it/s][A
 77%|███████▋  | 171/221 [01:28<00:33,  1.50it/s][A
 78%|███████▊  | 172/221 [01:29<00:30,  1.62it/s][A
 78%|███████▊  | 173/221 [01:30<00:30,  1.57it/s][A
 79%|███████▊  | 174/221 [01:31<00:33,  1.42it/s][A
 79%|███████▉  | 175/221 [01:31<00:33,  1.38it/s][A
 80%|███████▉  | 176/221 [01:32<00:30,  1.48it/s][A
 80%|████████  | 177/221 [01:32<00:27,  1.62it/s][A
 81%|████████  | 178/221 [01:33<00:27,  1.56it/s][A
 81%|████████  | 179/221 [01:33<00:24,  1.74it/s][A
 81%|████████▏ | 180/221 [01:34<00:24,  1.68it/s][A
 82%|████████▏ | 181/221 [01:35<00:22,  1.77it/s][A
 82%|████████▏ | 182/221 [01:35<00:20,  1.94it/s][A
 83%|████████▎ | 183/221 [01:35<00:17,  2.17it/s][A
 83%|████████▎ | 184/221 [01:36<00:17,  2.08it/s][A
 84%|████████▎ | 185/221 [01:37<00:23,  1.52it/s][A
 84%|████████▍ | 186/221 [01:37<00:17,  2.02it/s][A
 85%|████████▍ | 187/221 [01:38<00:21,  1.60it/s][A
 85%|████████▌ | 188/221 [01:38<00:18,  1.83it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.90it/s][A
 86%|████████▌ | 190/221 [01:39<00:16,  1.90it/s][A
 86%|████████▋ | 191/221 [01:40<00:14,  2.01it/s][A
 87%|████████▋ | 192/221 [01:40<00:12,  2.35it/s][A
 87%|████████▋ | 193/221 [01:40<00:10,  2.60it/s][A
 88%|████████▊ | 194/221 [01:41<00:11,  2.27it/s][A
 88%|████████▊ | 195/221 [01:42<00:14,  1.81it/s][A
 89%|████████▊ | 196/221 [01:43<00:17,  1.43it/s][A
 89%|████████▉ | 197/221 [01:43<00:14,  1.68it/s][A
 90%|████████▉ | 198/221 [01:43<00:11,  2.05it/s][A
 90%|█████████ | 199/221 [01:44<00:09,  2.37it/s][A
 90%|█████████ | 200/221 [01:44<00:10,  2.00it/s][A
 91%|█████████ | 201/221 [01:45<00:09,  2.08it/s][A
 91%|█████████▏| 202/221 [01:45<00:07,  2.45it/s][A
 92%|█████████▏| 203/221 [01:45<00:07,  2.31it/s][A
 92%|█████████▏| 204/221 [01:46<00:08,  2.03it/s][A
 93%|█████████▎| 205/221 [01:46<00:06,  2.52it/s][A
 93%|█████████▎| 206/221 [01:46<00:05,  2.86it/s][A
 94%|█████████▎| 207/221 [01:47<00:06,  2.05it/s][A
 94%|█████████▍| 208/221 [01:48<00:07,  1.84it/s][A
 95%|█████████▍| 209/221 [01:49<00:07,  1.70it/s][A
 95%|█████████▌| 210/221 [01:49<00:05,  1.97it/s][A
 95%|█████████▌| 211/221 [01:49<00:04,  2.10it/s][A
 96%|█████████▌| 212/221 [01:50<00:03,  2.35it/s][A
 96%|█████████▋| 213/221 [01:51<00:05,  1.53it/s][A
 97%|█████████▋| 214/221 [01:51<00:03,  1.87it/s][A
 97%|█████████▋| 215/221 [01:51<00:02,  2.40it/s][A
 98%|█████████▊| 216/221 [01:52<00:02,  2.22it/s][A
 98%|█████████▊| 217/221 [01:52<00:01,  2.19it/s][A
 99%|█████████▊| 218/221 [01:53<00:01,  2.46it/s][A
 99%|█████████▉| 219/221 [01:53<00:01,  1.99it/s][A
100%|█████████▉| 220/221 [01:54<00:00,  2.25it/s][A
100%|██████████| 221/221 [01:54<00:00,  1.90it/s][A100%|██████████| 221/221 [01:54<00:00,  1.92it/s]
09/19/2024 05:37:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 2499--===========

09/19/2024 05:37:57 - INFO - __main__ -   {'area_r1': 44.3, 'area_recall': '44.3/71.0/81.4', 'area_ravg': 65.6}
09/19/2024 05:37:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 2499--===========

09/19/2024 05:37:57 - INFO - __main__ -   {'forward_r1': 50.8, 'forward_recall': '50.8/80.2/87.2', 'forward_ravg': 72.7}
09/19/2024 05:37:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 2499--===========

09/19/2024 05:37:57 - INFO - __main__ -   {'area_video_r1': 49.9, 'area_video_recall': '49.9/79.1/87.2', 'area_video_ravg': 72.1}
09/19/2024 05:37:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 2499=======

09/19/2024 05:37:57 - INFO - __main__ -   {'area_video_r1': 49.9, 'area_video_recall': '49.9/79.1/87.2', 'area_video_ravg': 72.1}
09/19/2024 05:37:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 2499--===========

09/19/2024 05:37:57 - INFO - __main__ -   {'area_video_r1': 61.2, 'area_video_recall': '61.2/82.1/87.9', 'area_video_ravg': 77.1, 'area_video_back_r1': 61.5, 'area_video_back_recall': '61.5/84.3/90.8', 'area_video_back_ravg': 78.9}
09/19/2024 05:37:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 2499=======

09/19/2024 05:37:57 - INFO - __main__ -   {'area_video_r1': 61.2, 'area_video_recall': '61.2/82.1/87.9', 'area_video_ravg': 77.1, 'area_video_back_r1': 61.5, 'area_video_back_recall': '61.5/84.3/90.8', 'area_video_back_ravg': 78.9}
09/19/2024 05:37:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 2499--===========

09/19/2024 05:37:57 - INFO - __main__ -   {'video_r1': 33.4, 'video_recall': '33.4/58.4/69.6', 'video_ravg': 53.8}
09/19/2024 05:37:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 05:37:57 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 05:37:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 2499--===========

09/19/2024 05:37:57 - INFO - __main__ -   {'video_r1': 58.8, 'video_recall': '58.8/79.8/85.0', 'video_ravg': 74.5}
09/19/2024 05:37:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 1499=======

09/19/2024 05:37:57 - INFO - __main__ -   {'video_r1': 60.0, 'video_recall': '60.0/79.1/84.8', 'video_ravg': 74.6}
09/19/2024 05:38:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03974323347210884, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2013087272644043, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2410519123077393}
 28%|██▊       | 2500/8917 [3:23:54<308:31:16, 173.08s/it] 28%|██▊       | 2501/8917 [3:23:57<217:36:58, 122.10s/it] 28%|██▊       | 2502/8917 [3:24:01<154:13:48, 86.55s/it]  28%|██▊       | 2503/8917 [3:24:04<109:50:46, 61.65s/it] 28%|██▊       | 2504/8917 [3:24:08<78:47:48, 44.23s/it]  28%|██▊       | 2505/8917 [3:24:11<57:09:33, 32.09s/it] 28%|██▊       | 2506/8917 [3:24:16<42:11:18, 23.69s/it] 28%|██▊       | 2507/8917 [3:24:19<31:28:20, 17.68s/it] 28%|██▊       | 2508/8917 [3:24:23<24:02:28, 13.50s/it] 28%|██▊       | 2509/8917 [3:24:27<18:49:35, 10.58s/it] 28%|██▊       | 2510/8917 [3:24:31<15:15:27,  8.57s/it] 28%|██▊       | 2511/8917 [3:24:34<12:41:45,  7.13s/it] 28%|██▊       | 2512/8917 [3:24:38<10:55:25,  6.14s/it] 28%|██▊       | 2513/8917 [3:24:42<9:33:12,  5.37s/it]  28%|██▊       | 2514/8917 [3:24:45<8:34:37,  4.82s/it] 28%|██▊       | 2515/8917 [3:24:49<8:04:27,  4.54s/it] 28%|██▊       | 2516/8917 [3:24:53<7:35:04,  4.27s/it] 28%|██▊       | 2517/8917 [3:24:57<7:25:23,  4.18s/it] 28%|██▊       | 2518/8917 [3:25:00<7:09:48,  4.03s/it] 28%|██▊       | 2519/8917 [3:25:04<6:43:34,  3.78s/it] 28%|██▊       | 2520/8917 [3:25:07<6:39:53,  3.75s/it] 28%|██▊       | 2521/8917 [3:25:11<6:38:46,  3.74s/it] 28%|██▊       | 2522/8917 [3:25:15<6:44:51,  3.80s/it] 28%|██▊       | 2523/8917 [3:25:18<6:30:37,  3.67s/it] 28%|██▊       | 2524/8917 [3:25:22<6:32:20,  3.68s/it] 28%|██▊       | 2525/8917 [3:25:26<6:31:33,  3.68s/it] 28%|██▊       | 2526/8917 [3:25:29<6:30:54,  3.67s/it] 28%|██▊       | 2527/8917 [3:25:33<6:22:15,  3.59s/it] 28%|██▊       | 2528/8917 [3:25:37<6:26:28,  3.63s/it] 28%|██▊       | 2529/8917 [3:25:40<6:31:42,  3.68s/it] 28%|██▊       | 2530/8917 [3:25:44<6:29:07,  3.66s/it] 28%|██▊       | 2531/8917 [3:25:48<6:30:26,  3.67s/it] 28%|██▊       | 2532/8917 [3:25:51<6:28:03,  3.65s/it] 28%|██▊       | 2533/8917 [3:25:55<6:43:48,  3.80s/it] 28%|██▊       | 2534/8917 [3:25:59<6:39:21,  3.75s/it] 28%|██▊       | 2535/8917 [3:26:03<6:31:14,  3.68s/it] 28%|██▊       | 2536/8917 [3:26:07<6:42:06,  3.78s/it] 28%|██▊       | 2537/8917 [3:26:10<6:42:06,  3.78s/it] 28%|██▊       | 2538/8917 [3:26:14<6:37:31,  3.74s/it] 28%|██▊       | 2539/8917 [3:26:18<6:39:32,  3.76s/it] 28%|██▊       | 2540/8917 [3:26:21<6:38:34,  3.75s/it] 28%|██▊       | 2541/8917 [3:26:25<6:41:12,  3.78s/it] 29%|██▊       | 2542/8917 [3:26:29<6:30:52,  3.68s/it] 29%|██▊       | 2543/8917 [3:26:33<6:36:50,  3.74s/it] 29%|██▊       | 2544/8917 [3:26:36<6:37:39,  3.74s/it] 29%|██▊       | 2545/8917 [3:26:40<6:29:13,  3.67s/it] 29%|██▊       | 2546/8917 [3:26:44<6:39:48,  3.77s/it] 29%|██▊       | 2547/8917 [3:26:48<6:41:02,  3.78s/it] 29%|██▊       | 2548/8917 [3:26:52<6:43:05,  3.80s/it] 29%|██▊       | 2549/8917 [3:26:55<6:33:11,  3.70s/it]09/19/2024 05:41:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04018066078424454, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4721705913543701, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5123512744903564}
 29%|██▊       | 2550/8917 [3:26:59<6:44:58,  3.82s/it] 29%|██▊       | 2551/8917 [3:27:03<6:43:01,  3.80s/it] 29%|██▊       | 2552/8917 [3:27:07<6:43:49,  3.81s/it] 29%|██▊       | 2553/8917 [3:27:10<6:38:06,  3.75s/it] 29%|██▊       | 2554/8917 [3:27:14<6:41:12,  3.78s/it] 29%|██▊       | 2555/8917 [3:27:18<6:38:53,  3.76s/it] 29%|██▊       | 2556/8917 [3:27:22<6:39:43,  3.77s/it] 29%|██▊       | 2557/8917 [3:27:25<6:30:28,  3.68s/it] 29%|██▊       | 2558/8917 [3:27:29<6:30:29,  3.68s/it] 29%|██▊       | 2559/8917 [3:27:32<6:27:36,  3.66s/it] 29%|██▊       | 2560/8917 [3:27:36<6:32:00,  3.70s/it] 29%|██▊       | 2561/8917 [3:27:40<6:28:38,  3.67s/it] 29%|██▊       | 2562/8917 [3:27:43<6:27:02,  3.65s/it] 29%|██▊       | 2563/8917 [3:27:47<6:32:51,  3.71s/it] 29%|██▉       | 2564/8917 [3:27:51<6:33:28,  3.72s/it] 29%|██▉       | 2565/8917 [3:27:55<6:31:02,  3.69s/it] 29%|██▉       | 2566/8917 [3:27:58<6:30:51,  3.69s/it] 29%|██▉       | 2567/8917 [3:28:02<6:30:24,  3.69s/it] 29%|██▉       | 2568/8917 [3:28:06<6:37:51,  3.76s/it] 29%|██▉       | 2569/8917 [3:28:10<6:49:35,  3.87s/it] 29%|██▉       | 2570/8917 [3:28:14<6:38:18,  3.77s/it] 29%|██▉       | 2571/8917 [3:28:17<6:37:25,  3.76s/it] 29%|██▉       | 2572/8917 [3:28:21<6:40:04,  3.78s/it] 29%|██▉       | 2573/8917 [3:28:25<6:49:50,  3.88s/it] 29%|██▉       | 2574/8917 [3:28:29<6:31:53,  3.71s/it] 29%|██▉       | 2575/8917 [3:28:32<6:32:28,  3.71s/it] 29%|██▉       | 2576/8917 [3:28:36<6:34:16,  3.73s/it] 29%|██▉       | 2577/8917 [3:28:40<6:29:40,  3.69s/it] 29%|██▉       | 2578/8917 [3:28:43<6:27:33,  3.67s/it] 29%|██▉       | 2579/8917 [3:28:47<6:34:24,  3.73s/it] 29%|██▉       | 2580/8917 [3:28:51<6:44:44,  3.83s/it] 29%|██▉       | 2581/8917 [3:28:55<6:41:43,  3.80s/it] 29%|██▉       | 2582/8917 [3:28:59<6:34:49,  3.74s/it] 29%|██▉       | 2583/8917 [3:29:02<6:35:13,  3.74s/it] 29%|██▉       | 2584/8917 [3:29:06<6:38:05,  3.77s/it] 29%|██▉       | 2585/8917 [3:29:10<6:27:37,  3.67s/it] 29%|██▉       | 2586/8917 [3:29:14<6:35:28,  3.75s/it] 29%|██▉       | 2587/8917 [3:29:17<6:37:21,  3.77s/it] 29%|██▉       | 2588/8917 [3:29:21<6:44:31,  3.84s/it] 29%|██▉       | 2589/8917 [3:29:25<6:35:38,  3.75s/it] 29%|██▉       | 2590/8917 [3:29:29<6:36:38,  3.76s/it] 29%|██▉       | 2591/8917 [3:29:32<6:37:07,  3.77s/it] 29%|██▉       | 2592/8917 [3:29:36<6:40:48,  3.80s/it] 29%|██▉       | 2593/8917 [3:29:40<6:38:29,  3.78s/it] 29%|██▉       | 2594/8917 [3:29:44<6:32:38,  3.73s/it] 29%|██▉       | 2595/8917 [3:29:48<6:36:46,  3.77s/it] 29%|██▉       | 2596/8917 [3:29:51<6:39:48,  3.80s/it] 29%|██▉       | 2597/8917 [3:29:55<6:32:08,  3.72s/it] 29%|██▉       | 2598/8917 [3:29:59<6:33:53,  3.74s/it] 29%|██▉       | 2599/8917 [3:30:02<6:31:45,  3.72s/it]09/19/2024 05:44:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03589843958616257, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4964056015014648, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.532304048538208}
 29%|██▉       | 2600/8917 [3:30:06<6:35:57,  3.76s/it] 29%|██▉       | 2601/8917 [3:30:10<6:30:09,  3.71s/it] 29%|██▉       | 2602/8917 [3:30:14<6:33:28,  3.74s/it] 29%|██▉       | 2603/8917 [3:30:17<6:30:58,  3.72s/it] 29%|██▉       | 2604/8917 [3:30:21<6:39:25,  3.80s/it] 29%|██▉       | 2605/8917 [3:30:25<6:32:01,  3.73s/it] 29%|██▉       | 2606/8917 [3:30:28<6:25:52,  3.67s/it] 29%|██▉       | 2607/8917 [3:30:32<6:34:24,  3.75s/it] 29%|██▉       | 2608/8917 [3:30:36<6:29:08,  3.70s/it] 29%|██▉       | 2609/8917 [3:30:40<6:42:34,  3.83s/it] 29%|██▉       | 2610/8917 [3:30:44<6:37:29,  3.78s/it] 29%|██▉       | 2611/8917 [3:30:47<6:21:57,  3.63s/it] 29%|██▉       | 2612/8917 [3:30:51<6:32:26,  3.73s/it] 29%|██▉       | 2613/8917 [3:30:55<6:40:42,  3.81s/it] 29%|██▉       | 2614/8917 [3:30:59<6:38:58,  3.80s/it] 29%|██▉       | 2615/8917 [3:31:02<6:29:57,  3.71s/it] 29%|██▉       | 2616/8917 [3:31:06<6:24:04,  3.66s/it] 29%|██▉       | 2617/8917 [3:31:10<6:27:43,  3.69s/it] 29%|██▉       | 2618/8917 [3:31:13<6:30:24,  3.72s/it] 29%|██▉       | 2619/8917 [3:31:17<6:34:13,  3.76s/it] 29%|██▉       | 2620/8917 [3:31:21<6:26:59,  3.69s/it] 29%|██▉       | 2621/8917 [3:31:24<6:23:52,  3.66s/it] 29%|██▉       | 2622/8917 [3:31:28<6:34:07,  3.76s/it] 29%|██▉       | 2623/8917 [3:31:32<6:31:02,  3.73s/it] 29%|██▉       | 2624/8917 [3:31:35<6:23:58,  3.66s/it] 29%|██▉       | 2625/8917 [3:31:39<6:30:51,  3.73s/it] 29%|██▉       | 2626/8917 [3:31:43<6:25:09,  3.67s/it] 29%|██▉       | 2627/8917 [3:31:47<6:41:18,  3.83s/it] 29%|██▉       | 2628/8917 [3:31:51<6:35:22,  3.77s/it] 29%|██▉       | 2629/8917 [3:31:54<6:33:11,  3.75s/it] 29%|██▉       | 2630/8917 [3:31:58<6:26:18,  3.69s/it] 30%|██▉       | 2631/8917 [3:32:02<6:32:15,  3.74s/it] 30%|██▉       | 2632/8917 [3:32:06<6:41:46,  3.84s/it] 30%|██▉       | 2633/8917 [3:32:09<6:24:09,  3.67s/it] 30%|██▉       | 2634/8917 [3:32:13<6:15:53,  3.59s/it] 30%|██▉       | 2635/8917 [3:32:16<6:26:04,  3.69s/it] 30%|██▉       | 2636/8917 [3:32:20<6:33:41,  3.76s/it] 30%|██▉       | 2637/8917 [3:32:24<6:32:08,  3.75s/it] 30%|██▉       | 2638/8917 [3:32:28<6:32:11,  3.75s/it] 30%|██▉       | 2639/8917 [3:32:32<6:31:35,  3.74s/it] 30%|██▉       | 2640/8917 [3:32:35<6:27:50,  3.71s/it] 30%|██▉       | 2641/8917 [3:32:39<6:32:02,  3.75s/it] 30%|██▉       | 2642/8917 [3:32:43<6:28:13,  3.71s/it] 30%|██▉       | 2643/8917 [3:32:46<6:30:51,  3.74s/it] 30%|██▉       | 2644/8917 [3:32:51<6:39:59,  3.83s/it] 30%|██▉       | 2645/8917 [3:32:54<6:29:22,  3.72s/it] 30%|██▉       | 2646/8917 [3:32:58<6:34:51,  3.78s/it] 30%|██▉       | 2647/8917 [3:33:02<6:32:45,  3.76s/it] 30%|██▉       | 2648/8917 [3:33:05<6:31:58,  3.75s/it] 30%|██▉       | 2649/8917 [3:33:09<6:32:45,  3.76s/it]09/19/2024 05:47:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0564630888402462, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3580304384231567, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4144935607910156}
 30%|██▉       | 2650/8917 [3:33:13<6:50:59,  3.93s/it] 30%|██▉       | 2651/8917 [3:33:17<6:46:32,  3.89s/it] 30%|██▉       | 2652/8917 [3:33:21<6:30:27,  3.74s/it] 30%|██▉       | 2653/8917 [3:33:25<6:39:45,  3.83s/it] 30%|██▉       | 2654/8917 [3:33:28<6:32:32,  3.76s/it] 30%|██▉       | 2655/8917 [3:33:32<6:29:06,  3.73s/it] 30%|██▉       | 2656/8917 [3:33:35<6:21:12,  3.65s/it] 30%|██▉       | 2657/8917 [3:33:39<6:19:39,  3.64s/it] 30%|██▉       | 2658/8917 [3:33:42<6:07:06,  3.52s/it] 30%|██▉       | 2659/8917 [3:33:46<6:24:01,  3.68s/it] 30%|██▉       | 2660/8917 [3:33:50<6:34:59,  3.79s/it] 30%|██▉       | 2661/8917 [3:33:54<6:33:16,  3.77s/it] 30%|██▉       | 2662/8917 [3:33:58<6:37:17,  3.81s/it] 30%|██▉       | 2663/8917 [3:34:02<6:27:42,  3.72s/it] 30%|██▉       | 2664/8917 [3:34:05<6:33:52,  3.78s/it] 30%|██▉       | 2665/8917 [3:34:09<6:33:52,  3.78s/it] 30%|██▉       | 2666/8917 [3:34:13<6:23:21,  3.68s/it] 30%|██▉       | 2667/8917 [3:34:16<6:25:24,  3.70s/it] 30%|██▉       | 2668/8917 [3:34:20<6:31:06,  3.76s/it] 30%|██▉       | 2669/8917 [3:34:24<6:36:28,  3.81s/it] 30%|██▉       | 2670/8917 [3:34:28<6:25:56,  3.71s/it] 30%|██▉       | 2671/8917 [3:34:32<6:34:02,  3.79s/it] 30%|██▉       | 2672/8917 [3:34:36<6:38:34,  3.83s/it] 30%|██▉       | 2673/8917 [3:34:39<6:21:03,  3.66s/it] 30%|██▉       | 2674/8917 [3:34:43<6:20:32,  3.66s/it] 30%|██▉       | 2675/8917 [3:34:46<6:19:28,  3.65s/it] 30%|███       | 2676/8917 [3:34:50<6:14:59,  3.61s/it] 30%|███       | 2677/8917 [3:34:53<6:15:57,  3.62s/it] 30%|███       | 2678/8917 [3:34:57<6:17:17,  3.63s/it] 30%|███       | 2679/8917 [3:35:01<6:19:15,  3.65s/it] 30%|███       | 2680/8917 [3:35:04<6:23:54,  3.69s/it] 30%|███       | 2681/8917 [3:35:08<6:16:19,  3.62s/it] 30%|███       | 2682/8917 [3:35:12<6:21:00,  3.67s/it] 30%|███       | 2683/8917 [3:35:16<6:33:07,  3.78s/it] 30%|███       | 2684/8917 [3:35:19<6:22:25,  3.68s/it] 30%|███       | 2685/8917 [3:35:23<6:21:10,  3.67s/it] 30%|███       | 2686/8917 [3:35:27<6:29:37,  3.75s/it] 30%|███       | 2687/8917 [3:35:30<6:28:41,  3.74s/it] 30%|███       | 2688/8917 [3:35:34<6:19:56,  3.66s/it] 30%|███       | 2689/8917 [3:35:38<6:30:52,  3.77s/it] 30%|███       | 2690/8917 [3:35:42<6:30:15,  3.76s/it] 30%|███       | 2691/8917 [3:35:45<6:19:08,  3.65s/it] 30%|███       | 2692/8917 [3:35:49<6:17:28,  3.64s/it] 30%|███       | 2693/8917 [3:35:53<6:24:14,  3.70s/it] 30%|███       | 2694/8917 [3:35:56<6:19:58,  3.66s/it] 30%|███       | 2695/8917 [3:35:59<6:08:29,  3.55s/it] 30%|███       | 2696/8917 [3:36:03<6:10:14,  3.57s/it] 30%|███       | 2697/8917 [3:36:07<6:35:28,  3.81s/it] 30%|███       | 2698/8917 [3:36:11<6:31:26,  3.78s/it] 30%|███       | 2699/8917 [3:36:15<6:23:47,  3.70s/it]09/19/2024 05:50:52 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04567885398864746, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4359909296035767, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4816697835922241}
 30%|███       | 2700/8917 [3:36:18<6:24:12,  3.71s/it] 30%|███       | 2701/8917 [3:36:22<6:25:53,  3.72s/it] 30%|███       | 2702/8917 [3:36:26<6:27:37,  3.74s/it] 30%|███       | 2703/8917 [3:36:29<6:22:15,  3.69s/it] 30%|███       | 2704/8917 [3:36:33<6:27:12,  3.74s/it] 30%|███       | 2705/8917 [3:36:37<6:25:44,  3.73s/it] 30%|███       | 2706/8917 [3:36:41<6:21:01,  3.68s/it] 30%|███       | 2707/8917 [3:36:44<6:23:04,  3.70s/it] 30%|███       | 2708/8917 [3:36:48<6:27:31,  3.74s/it] 30%|███       | 2709/8917 [3:36:52<6:30:58,  3.78s/it] 30%|███       | 2710/8917 [3:36:55<6:18:53,  3.66s/it] 30%|███       | 2711/8917 [3:36:59<6:15:25,  3.63s/it] 30%|███       | 2712/8917 [3:37:02<6:05:30,  3.53s/it] 30%|███       | 2713/8917 [3:37:06<6:09:51,  3.58s/it] 30%|███       | 2714/8917 [3:37:10<6:18:23,  3.66s/it] 30%|███       | 2715/8917 [3:37:14<6:23:16,  3.71s/it] 30%|███       | 2716/8917 [3:37:17<6:21:53,  3.70s/it] 30%|███       | 2717/8917 [3:37:21<6:22:45,  3.70s/it] 30%|███       | 2718/8917 [3:37:25<6:25:12,  3.73s/it] 30%|███       | 2719/8917 [3:37:29<6:36:42,  3.84s/it] 31%|███       | 2720/8917 [3:37:33<6:34:01,  3.82s/it] 31%|███       | 2721/8917 [3:37:36<6:31:11,  3.79s/it] 31%|███       | 2722/8917 [3:37:40<6:24:25,  3.72s/it] 31%|███       | 2723/8917 [3:37:44<6:23:37,  3.72s/it] 31%|███       | 2724/8917 [3:37:48<6:27:21,  3.75s/it] 31%|███       | 2725/8917 [3:37:51<6:22:45,  3.71s/it] 31%|███       | 2726/8917 [3:37:55<6:28:26,  3.76s/it] 31%|███       | 2727/8917 [3:37:59<6:36:28,  3.84s/it] 31%|███       | 2728/8917 [3:38:03<6:37:09,  3.85s/it] 31%|███       | 2729/8917 [3:38:07<6:32:03,  3.80s/it] 31%|███       | 2730/8917 [3:38:10<6:28:29,  3.77s/it] 31%|███       | 2731/8917 [3:38:14<6:18:46,  3.67s/it] 31%|███       | 2732/8917 [3:38:18<6:22:49,  3.71s/it] 31%|███       | 2733/8917 [3:38:21<6:19:39,  3.68s/it] 31%|███       | 2734/8917 [3:38:25<6:15:04,  3.64s/it] 31%|███       | 2735/8917 [3:38:29<6:26:17,  3.75s/it] 31%|███       | 2736/8917 [3:38:33<6:38:18,  3.87s/it] 31%|███       | 2737/8917 [3:38:37<6:39:31,  3.88s/it] 31%|███       | 2738/8917 [3:38:40<6:26:09,  3.75s/it] 31%|███       | 2739/8917 [3:38:44<6:27:29,  3.76s/it] 31%|███       | 2740/8917 [3:38:48<6:24:32,  3.74s/it] 31%|███       | 2741/8917 [3:38:51<6:23:49,  3.73s/it] 31%|███       | 2742/8917 [3:38:55<6:18:38,  3.68s/it] 31%|███       | 2743/8917 [3:38:59<6:29:54,  3.79s/it] 31%|███       | 2744/8917 [3:39:02<6:16:29,  3.66s/it] 31%|███       | 2745/8917 [3:39:06<6:16:11,  3.66s/it] 31%|███       | 2746/8917 [3:39:10<6:16:37,  3.66s/it] 31%|███       | 2747/8917 [3:39:14<6:34:11,  3.83s/it] 31%|███       | 2748/8917 [3:39:18<6:28:10,  3.78s/it] 31%|███       | 2749/8917 [3:39:21<6:31:19,  3.81s/it]09/19/2024 05:53:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02900725044310093, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.149106502532959, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1781136989593506}
 31%|███       | 2750/8917 [3:39:25<6:21:20,  3.71s/it] 31%|███       | 2751/8917 [3:39:29<6:29:47,  3.79s/it] 31%|███       | 2752/8917 [3:39:32<6:17:52,  3.68s/it] 31%|███       | 2753/8917 [3:39:36<6:21:46,  3.72s/it] 31%|███       | 2754/8917 [3:39:40<6:11:55,  3.62s/it] 31%|███       | 2755/8917 [3:39:43<6:15:22,  3.66s/it] 31%|███       | 2756/8917 [3:39:47<6:25:01,  3.75s/it] 31%|███       | 2757/8917 [3:39:51<6:27:40,  3.78s/it] 31%|███       | 2758/8917 [3:39:55<6:38:08,  3.88s/it] 31%|███       | 2759/8917 [3:39:59<6:24:06,  3.74s/it] 31%|███       | 2760/8917 [3:40:02<6:25:46,  3.76s/it] 31%|███       | 2761/8917 [3:40:06<6:28:51,  3.79s/it] 31%|███       | 2762/8917 [3:40:10<6:21:52,  3.72s/it] 31%|███       | 2763/8917 [3:40:14<6:24:14,  3.75s/it] 31%|███       | 2764/8917 [3:40:17<6:20:07,  3.71s/it] 31%|███       | 2765/8917 [3:40:21<6:18:01,  3.69s/it] 31%|███       | 2766/8917 [3:40:25<6:19:22,  3.70s/it] 31%|███       | 2767/8917 [3:40:28<6:23:08,  3.74s/it] 31%|███       | 2768/8917 [3:40:32<6:20:34,  3.71s/it] 31%|███       | 2769/8917 [3:40:36<6:20:37,  3.71s/it] 31%|███       | 2770/8917 [3:40:40<6:21:37,  3.73s/it] 31%|███       | 2771/8917 [3:40:43<6:22:17,  3.73s/it] 31%|███       | 2772/8917 [3:40:47<6:20:21,  3.71s/it] 31%|███       | 2773/8917 [3:40:51<6:15:59,  3.67s/it] 31%|███       | 2774/8917 [3:40:55<6:25:54,  3.77s/it] 31%|███       | 2775/8917 [3:40:58<6:26:45,  3.78s/it] 31%|███       | 2776/8917 [3:41:02<6:24:09,  3.75s/it] 31%|███       | 2777/8917 [3:41:06<6:21:01,  3.72s/it] 31%|███       | 2778/8917 [3:41:09<6:22:15,  3.74s/it] 31%|███       | 2779/8917 [3:41:13<6:08:49,  3.61s/it] 31%|███       | 2780/8917 [3:41:17<6:24:24,  3.76s/it] 31%|███       | 2781/8917 [3:41:21<6:24:57,  3.76s/it] 31%|███       | 2782/8917 [3:41:24<6:24:00,  3.76s/it] 31%|███       | 2783/8917 [3:41:28<6:21:14,  3.73s/it] 31%|███       | 2784/8917 [3:41:32<6:23:01,  3.75s/it] 31%|███       | 2785/8917 [3:41:36<6:22:42,  3.74s/it] 31%|███       | 2786/8917 [3:41:39<6:21:48,  3.74s/it] 31%|███▏      | 2787/8917 [3:41:43<6:28:10,  3.80s/it] 31%|███▏      | 2788/8917 [3:41:47<6:23:08,  3.75s/it] 31%|███▏      | 2789/8917 [3:41:51<6:24:11,  3.76s/it] 31%|███▏      | 2790/8917 [3:41:54<6:17:07,  3.69s/it] 31%|███▏      | 2791/8917 [3:41:58<6:24:05,  3.76s/it] 31%|███▏      | 2792/8917 [3:42:02<6:13:47,  3.66s/it] 31%|███▏      | 2793/8917 [3:42:06<6:27:38,  3.80s/it] 31%|███▏      | 2794/8917 [3:42:09<6:25:03,  3.77s/it] 31%|███▏      | 2795/8917 [3:42:13<6:16:49,  3.69s/it] 31%|███▏      | 2796/8917 [3:42:17<6:17:22,  3.70s/it] 31%|███▏      | 2797/8917 [3:42:20<6:13:37,  3.66s/it] 31%|███▏      | 2798/8917 [3:42:24<6:14:24,  3.67s/it] 31%|███▏      | 2799/8917 [3:42:28<6:12:31,  3.65s/it]09/19/2024 05:57:05 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01924852654337883, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3999354839324951, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4191839694976807}
 31%|███▏      | 2800/8917 [3:42:32<6:23:25,  3.76s/it] 31%|███▏      | 2801/8917 [3:42:35<6:17:39,  3.70s/it] 31%|███▏      | 2802/8917 [3:42:39<6:21:43,  3.75s/it] 31%|███▏      | 2803/8917 [3:42:43<6:25:06,  3.78s/it] 31%|███▏      | 2804/8917 [3:42:46<6:17:50,  3.71s/it] 31%|███▏      | 2805/8917 [3:42:50<6:08:33,  3.62s/it] 31%|███▏      | 2806/8917 [3:42:53<6:10:04,  3.63s/it] 31%|███▏      | 2807/8917 [3:42:58<6:26:07,  3.79s/it] 31%|███▏      | 2808/8917 [3:43:01<6:25:08,  3.78s/it] 32%|███▏      | 2809/8917 [3:43:05<6:23:29,  3.77s/it] 32%|███▏      | 2810/8917 [3:43:09<6:29:30,  3.83s/it] 32%|███▏      | 2811/8917 [3:43:13<6:22:00,  3.75s/it] 32%|███▏      | 2812/8917 [3:43:16<6:20:38,  3.74s/it] 32%|███▏      | 2813/8917 [3:43:20<6:11:51,  3.66s/it] 32%|███▏      | 2814/8917 [3:43:23<6:11:42,  3.65s/it] 32%|███▏      | 2815/8917 [3:43:27<6:18:00,  3.72s/it] 32%|███▏      | 2816/8917 [3:43:31<6:21:33,  3.75s/it] 32%|███▏      | 2817/8917 [3:43:35<6:14:20,  3.68s/it] 32%|███▏      | 2818/8917 [3:43:38<6:15:26,  3.69s/it] 32%|███▏      | 2819/8917 [3:43:42<6:14:43,  3.69s/it] 32%|███▏      | 2820/8917 [3:43:46<6:16:28,  3.70s/it] 32%|███▏      | 2821/8917 [3:43:50<6:21:38,  3.76s/it] 32%|███▏      | 2822/8917 [3:43:53<6:22:49,  3.77s/it] 32%|███▏      | 2823/8917 [3:43:57<6:29:37,  3.84s/it] 32%|███▏      | 2824/8917 [3:44:01<6:22:37,  3.77s/it] 32%|███▏      | 2825/8917 [3:44:04<6:05:13,  3.60s/it] 32%|███▏      | 2826/8917 [3:44:08<6:12:02,  3.66s/it] 32%|███▏      | 2827/8917 [3:44:12<6:11:00,  3.66s/it] 32%|███▏      | 2828/8917 [3:44:16<6:18:46,  3.73s/it] 32%|███▏      | 2829/8917 [3:44:19<6:13:21,  3.68s/it] 32%|███▏      | 2830/8917 [3:44:23<6:09:27,  3.64s/it] 32%|███▏      | 2831/8917 [3:44:27<6:15:49,  3.71s/it] 32%|███▏      | 2832/8917 [3:44:30<6:21:43,  3.76s/it] 32%|███▏      | 2833/8917 [3:44:34<6:10:01,  3.65s/it] 32%|███▏      | 2834/8917 [3:44:38<6:10:06,  3.65s/it] 32%|███▏      | 2835/8917 [3:44:41<6:08:17,  3.63s/it] 32%|███▏      | 2836/8917 [3:44:45<6:09:43,  3.65s/it] 32%|███▏      | 2837/8917 [3:44:49<6:16:47,  3.72s/it] 32%|███▏      | 2838/8917 [3:44:52<6:18:23,  3.73s/it] 32%|███▏      | 2839/8917 [3:44:56<6:11:52,  3.67s/it] 32%|███▏      | 2840/8917 [3:45:00<6:09:58,  3.65s/it] 32%|███▏      | 2841/8917 [3:45:03<6:14:11,  3.70s/it] 32%|███▏      | 2842/8917 [3:45:07<6:13:24,  3.69s/it] 32%|███▏      | 2843/8917 [3:45:11<6:09:36,  3.65s/it] 32%|███▏      | 2844/8917 [3:45:14<6:12:08,  3.68s/it] 32%|███▏      | 2845/8917 [3:45:18<6:18:51,  3.74s/it] 32%|███▏      | 2846/8917 [3:45:22<6:18:17,  3.74s/it] 32%|███▏      | 2847/8917 [3:45:26<6:12:15,  3.68s/it] 32%|███▏      | 2848/8917 [3:45:29<6:08:30,  3.64s/it] 32%|███▏      | 2849/8917 [3:45:33<6:25:07,  3.81s/it]09/19/2024 06:00:11 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024013465270400047, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.5038243532180786, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.527837872505188}
 32%|███▏      | 2850/8917 [3:45:37<6:20:31,  3.76s/it] 32%|███▏      | 2851/8917 [3:45:40<6:03:54,  3.60s/it] 32%|███▏      | 2852/8917 [3:45:44<6:14:01,  3.70s/it] 32%|███▏      | 2853/8917 [3:45:48<6:21:59,  3.78s/it] 32%|███▏      | 2854/8917 [3:45:52<6:27:10,  3.83s/it] 32%|███▏      | 2855/8917 [3:45:56<6:25:06,  3.81s/it] 32%|███▏      | 2856/8917 [3:46:00<6:27:02,  3.83s/it] 32%|███▏      | 2857/8917 [3:46:03<6:15:56,  3.72s/it] 32%|███▏      | 2858/8917 [3:46:07<6:16:45,  3.73s/it] 32%|███▏      | 2859/8917 [3:46:10<6:09:23,  3.66s/it] 32%|███▏      | 2860/8917 [3:46:14<6:16:56,  3.73s/it] 32%|███▏      | 2861/8917 [3:46:18<6:14:15,  3.71s/it] 32%|███▏      | 2862/8917 [3:46:22<6:22:28,  3.79s/it] 32%|███▏      | 2863/8917 [3:46:25<6:14:18,  3.71s/it] 32%|███▏      | 2864/8917 [3:46:29<6:12:25,  3.69s/it] 32%|███▏      | 2865/8917 [3:46:33<6:10:40,  3.67s/it] 32%|███▏      | 2866/8917 [3:46:36<6:12:10,  3.69s/it] 32%|███▏      | 2867/8917 [3:46:40<6:21:51,  3.79s/it] 32%|███▏      | 2868/8917 [3:46:44<6:21:42,  3.79s/it] 32%|███▏      | 2869/8917 [3:46:48<6:21:10,  3.78s/it] 32%|███▏      | 2870/8917 [3:46:52<6:19:26,  3.76s/it] 32%|███▏      | 2871/8917 [3:46:56<6:21:20,  3.78s/it] 32%|███▏      | 2872/8917 [3:46:59<6:13:59,  3.71s/it] 32%|███▏      | 2873/8917 [3:47:03<6:19:13,  3.76s/it] 32%|███▏      | 2874/8917 [3:47:07<6:26:07,  3.83s/it] 32%|███▏      | 2875/8917 [3:47:11<6:20:19,  3.78s/it] 32%|███▏      | 2876/8917 [3:47:15<6:24:09,  3.82s/it] 32%|███▏      | 2877/8917 [3:47:18<6:22:00,  3.79s/it] 32%|███▏      | 2878/8917 [3:47:22<6:16:25,  3.74s/it] 32%|███▏      | 2879/8917 [3:47:26<6:17:01,  3.75s/it] 32%|███▏      | 2880/8917 [3:47:30<6:23:57,  3.82s/it] 32%|███▏      | 2881/8917 [3:47:33<6:14:27,  3.72s/it] 32%|███▏      | 2882/8917 [3:47:37<6:05:19,  3.63s/it] 32%|███▏      | 2883/8917 [3:47:41<6:15:55,  3.74s/it] 32%|███▏      | 2884/8917 [3:47:44<6:14:16,  3.72s/it] 32%|███▏      | 2885/8917 [3:47:48<6:19:05,  3.77s/it] 32%|███▏      | 2886/8917 [3:47:52<6:15:44,  3.74s/it] 32%|███▏      | 2887/8917 [3:47:56<6:19:21,  3.77s/it] 32%|███▏      | 2888/8917 [3:47:59<6:12:33,  3.71s/it] 32%|███▏      | 2889/8917 [3:48:03<6:07:05,  3.65s/it] 32%|███▏      | 2890/8917 [3:48:06<6:08:52,  3.67s/it] 32%|███▏      | 2891/8917 [3:48:10<6:04:42,  3.63s/it] 32%|███▏      | 2892/8917 [3:48:14<6:06:58,  3.65s/it] 32%|███▏      | 2893/8917 [3:48:18<6:16:54,  3.75s/it] 32%|███▏      | 2894/8917 [3:48:21<6:13:12,  3.72s/it] 32%|███▏      | 2895/8917 [3:48:25<6:20:20,  3.79s/it] 32%|███▏      | 2896/8917 [3:48:29<6:14:22,  3.73s/it] 32%|███▏      | 2897/8917 [3:48:33<6:21:43,  3.80s/it] 32%|███▏      | 2898/8917 [3:48:37<6:27:23,  3.86s/it] 33%|███▎      | 2899/8917 [3:48:40<6:21:33,  3.80s/it]09/19/2024 06:03:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028775988146662712, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2249833345413208, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2537592649459839}
 33%|███▎      | 2900/8917 [3:48:44<6:07:14,  3.66s/it] 33%|███▎      | 2901/8917 [3:48:48<6:12:13,  3.71s/it] 33%|███▎      | 2902/8917 [3:48:51<6:13:00,  3.72s/it] 33%|███▎      | 2903/8917 [3:48:55<6:11:06,  3.70s/it] 33%|███▎      | 2904/8917 [3:48:59<6:14:37,  3.74s/it] 33%|███▎      | 2905/8917 [3:49:02<6:10:56,  3.70s/it] 33%|███▎      | 2906/8917 [3:49:06<6:19:56,  3.79s/it] 33%|███▎      | 2907/8917 [3:49:10<6:22:43,  3.82s/it] 33%|███▎      | 2908/8917 [3:49:14<6:19:09,  3.79s/it] 33%|███▎      | 2909/8917 [3:49:18<6:18:56,  3.78s/it] 33%|███▎      | 2910/8917 [3:49:21<6:11:26,  3.71s/it] 33%|███▎      | 2911/8917 [3:49:25<6:10:52,  3.71s/it] 33%|███▎      | 2912/8917 [3:49:29<6:08:45,  3.68s/it] 33%|███▎      | 2913/8917 [3:49:33<6:15:15,  3.75s/it] 33%|███▎      | 2914/8917 [3:49:36<6:12:56,  3.73s/it] 33%|███▎      | 2915/8917 [3:49:40<6:05:12,  3.65s/it] 33%|███▎      | 2916/8917 [3:49:43<6:04:30,  3.64s/it] 33%|███▎      | 2917/8917 [3:49:47<6:00:57,  3.61s/it] 33%|███▎      | 2918/8917 [3:49:51<6:09:23,  3.69s/it] 33%|███▎      | 2919/8917 [3:49:55<6:17:09,  3.77s/it] 33%|███▎      | 2920/8917 [3:49:59<6:28:00,  3.88s/it] 33%|███▎      | 2921/8917 [3:50:02<6:16:18,  3.77s/it] 33%|███▎      | 2922/8917 [3:50:06<6:16:15,  3.77s/it] 33%|███▎      | 2923/8917 [3:50:10<6:10:09,  3.71s/it] 33%|███▎      | 2924/8917 [3:50:13<6:05:30,  3.66s/it] 33%|███▎      | 2925/8917 [3:50:17<6:13:14,  3.74s/it] 33%|███▎      | 2926/8917 [3:50:21<6:17:56,  3.79s/it] 33%|███▎      | 2927/8917 [3:50:25<6:10:08,  3.71s/it] 33%|███▎      | 2928/8917 [3:50:28<6:04:37,  3.65s/it] 33%|███▎      | 2929/8917 [3:50:32<6:08:09,  3.69s/it] 33%|███▎      | 2930/8917 [3:50:36<6:12:38,  3.73s/it] 33%|███▎      | 2931/8917 [3:50:39<6:11:02,  3.72s/it] 33%|███▎      | 2932/8917 [3:50:43<6:10:19,  3.71s/it] 33%|███▎      | 2933/8917 [3:50:47<6:06:16,  3.67s/it] 33%|███▎      | 2934/8917 [3:50:51<6:10:51,  3.72s/it] 33%|███▎      | 2935/8917 [3:50:54<6:02:50,  3.64s/it] 33%|███▎      | 2936/8917 [3:50:58<6:14:07,  3.75s/it] 33%|███▎      | 2937/8917 [3:51:02<6:17:13,  3.78s/it] 33%|███▎      | 2938/8917 [3:51:06<6:20:17,  3.82s/it] 33%|███▎      | 2939/8917 [3:51:09<6:09:03,  3.70s/it] 33%|███▎      | 2940/8917 [3:51:13<6:13:41,  3.75s/it] 33%|███▎      | 2941/8917 [3:51:17<6:05:23,  3.67s/it] 33%|███▎      | 2942/8917 [3:51:21<6:18:20,  3.80s/it] 33%|███▎      | 2943/8917 [3:51:25<6:20:57,  3.83s/it] 33%|███▎      | 2944/8917 [3:51:28<6:14:57,  3.77s/it] 33%|███▎      | 2945/8917 [3:51:32<6:10:26,  3.72s/it] 33%|███▎      | 2946/8917 [3:51:35<6:08:04,  3.70s/it] 33%|███▎      | 2947/8917 [3:51:39<6:16:36,  3.78s/it] 33%|███▎      | 2948/8917 [3:51:43<6:15:28,  3.77s/it] 33%|███▎      | 2949/8917 [3:51:47<6:15:01,  3.77s/it]09/19/2024 06:06:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03440996631979942, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2105412483215332, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2449512481689453}
 33%|███▎      | 2950/8917 [3:51:51<6:09:26,  3.71s/it] 33%|███▎      | 2951/8917 [3:51:54<6:08:14,  3.70s/it] 33%|███▎      | 2952/8917 [3:51:58<5:58:41,  3.61s/it] 33%|███▎      | 2953/8917 [3:52:01<6:00:58,  3.63s/it] 33%|███▎      | 2954/8917 [3:52:05<5:58:08,  3.60s/it] 33%|███▎      | 2955/8917 [3:52:09<6:09:53,  3.72s/it] 33%|███▎      | 2956/8917 [3:52:12<6:05:51,  3.68s/it] 33%|███▎      | 2957/8917 [3:52:16<6:07:06,  3.70s/it] 33%|███▎      | 2958/8917 [3:52:20<6:04:06,  3.67s/it] 33%|███▎      | 2959/8917 [3:52:23<6:05:54,  3.68s/it] 33%|███▎      | 2960/8917 [3:52:27<6:09:27,  3.72s/it] 33%|███▎      | 2961/8917 [3:52:31<6:16:17,  3.79s/it] 33%|███▎      | 2962/8917 [3:52:35<6:07:42,  3.70s/it] 33%|███▎      | 2963/8917 [3:52:38<6:04:26,  3.67s/it] 33%|███▎      | 2964/8917 [3:52:42<6:06:00,  3.69s/it] 33%|███▎      | 2965/8917 [3:52:46<6:08:23,  3.71s/it] 33%|███▎      | 2966/8917 [3:52:50<6:14:01,  3.77s/it] 33%|███▎      | 2967/8917 [3:52:53<6:08:25,  3.72s/it] 33%|███▎      | 2968/8917 [3:52:57<6:12:03,  3.75s/it] 33%|███▎      | 2969/8917 [3:53:01<6:09:51,  3.73s/it] 33%|███▎      | 2970/8917 [3:53:04<6:07:32,  3.71s/it] 33%|███▎      | 2971/8917 [3:53:08<6:05:00,  3.68s/it] 33%|███▎      | 2972/8917 [3:53:12<6:03:08,  3.67s/it] 33%|███▎      | 2973/8917 [3:53:16<6:06:32,  3.70s/it] 33%|███▎      | 2974/8917 [3:53:19<6:10:36,  3.74s/it] 33%|███▎      | 2975/8917 [3:53:23<6:17:37,  3.81s/it] 33%|███▎      | 2976/8917 [3:53:27<6:03:26,  3.67s/it] 33%|███▎      | 2977/8917 [3:53:30<6:05:43,  3.69s/it] 33%|███▎      | 2978/8917 [3:53:34<6:04:33,  3.68s/it] 33%|███▎      | 2979/8917 [3:53:38<6:01:22,  3.65s/it] 33%|███▎      | 2980/8917 [3:53:41<6:03:18,  3.67s/it] 33%|███▎      | 2981/8917 [3:53:46<6:17:13,  3.81s/it] 33%|███▎      | 2982/8917 [3:53:49<6:03:24,  3.67s/it] 33%|███▎      | 2983/8917 [3:53:52<5:58:14,  3.62s/it] 33%|███▎      | 2984/8917 [3:53:56<6:11:02,  3.75s/it] 33%|███▎      | 2985/8917 [3:54:00<6:11:29,  3.76s/it] 33%|███▎      | 2986/8917 [3:54:04<6:11:43,  3.76s/it] 33%|███▎      | 2987/8917 [3:54:08<6:13:53,  3.78s/it] 34%|███▎      | 2988/8917 [3:54:11<6:03:25,  3.68s/it] 34%|███▎      | 2989/8917 [3:54:15<5:54:24,  3.59s/it] 34%|███▎      | 2990/8917 [3:54:18<6:03:39,  3.68s/it] 34%|███▎      | 2991/8917 [3:54:22<6:02:12,  3.67s/it] 34%|███▎      | 2992/8917 [3:54:26<6:05:41,  3.70s/it] 34%|███▎      | 2993/8917 [3:54:30<6:03:33,  3.68s/it] 34%|███▎      | 2994/8917 [3:54:33<6:07:31,  3.72s/it] 34%|███▎      | 2995/8917 [3:54:37<5:56:47,  3.61s/it] 34%|███▎      | 2996/8917 [3:54:41<6:03:57,  3.69s/it] 34%|███▎      | 2997/8917 [3:54:44<6:05:05,  3.70s/it] 34%|███▎      | 2998/8917 [3:54:48<6:01:30,  3.66s/it] 34%|███▎      | 2999/8917 [3:54:52<6:08:40,  3.74s/it]09/19/2024 06:09:28 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 06:09:28 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<04:31,  1.23s/it][A
  1%|          | 2/221 [00:01<02:33,  1.43it/s][A
  1%|▏         | 3/221 [00:02<02:32,  1.43it/s][A
  2%|▏         | 4/221 [00:02<01:52,  1.93it/s][A
  3%|▎         | 6/221 [00:02<01:00,  3.53it/s][A
  3%|▎         | 7/221 [00:02<00:55,  3.86it/s][A
  4%|▎         | 8/221 [00:03<01:02,  3.43it/s][A
  4%|▍         | 9/221 [00:03<01:05,  3.23it/s][A
  5%|▍         | 10/221 [00:03<01:08,  3.07it/s][A
  5%|▌         | 12/221 [00:05<01:56,  1.79it/s][A
  6%|▌         | 13/221 [00:05<01:35,  2.18it/s][A
  7%|▋         | 15/221 [00:06<01:13,  2.79it/s][A
  7%|▋         | 16/221 [00:06<01:27,  2.34it/s][A
  8%|▊         | 17/221 [00:07<01:41,  2.01it/s][A
  8%|▊         | 18/221 [00:08<01:46,  1.91it/s][A
  9%|▊         | 19/221 [00:08<02:01,  1.66it/s][A
 10%|▉         | 21/221 [00:09<01:27,  2.27it/s][A
 10%|▉         | 22/221 [00:09<01:30,  2.19it/s][A
 11%|█         | 24/221 [00:10<01:09,  2.83it/s][A
 11%|█▏        | 25/221 [00:10<01:13,  2.67it/s][A
 12%|█▏        | 26/221 [00:11<01:10,  2.78it/s][A
 13%|█▎        | 28/221 [00:12<01:17,  2.49it/s][A
 13%|█▎        | 29/221 [00:12<01:10,  2.71it/s][A
 14%|█▎        | 30/221 [00:12<01:22,  2.33it/s][A
 14%|█▍        | 31/221 [00:13<01:32,  2.05it/s][A
 15%|█▍        | 33/221 [00:14<01:11,  2.62it/s][A
 16%|█▌        | 35/221 [00:14<00:53,  3.45it/s][A
 16%|█▋        | 36/221 [00:14<00:52,  3.51it/s][A
 17%|█▋        | 37/221 [00:15<00:57,  3.18it/s][A
 17%|█▋        | 38/221 [00:15<01:14,  2.45it/s][A
 18%|█▊        | 39/221 [00:15<01:03,  2.84it/s][A
 18%|█▊        | 40/221 [00:16<01:07,  2.69it/s][A
 19%|█▊        | 41/221 [00:16<00:55,  3.26it/s][A
 19%|█▉        | 42/221 [00:16<00:49,  3.64it/s][A
 20%|█▉        | 44/221 [00:16<00:34,  5.11it/s][A
 20%|██        | 45/221 [00:23<04:58,  1.70s/it][A
 21%|██        | 46/221 [00:23<04:01,  1.38s/it][A
 21%|██▏       | 47/221 [00:24<03:24,  1.17s/it][A
 22%|██▏       | 48/221 [00:24<02:31,  1.14it/s][A
 22%|██▏       | 49/221 [00:24<01:57,  1.46it/s][A
 23%|██▎       | 50/221 [00:24<01:42,  1.67it/s][A
 23%|██▎       | 51/221 [00:25<01:21,  2.08it/s][A
 24%|██▎       | 52/221 [00:25<01:03,  2.65it/s][A
 24%|██▍       | 53/221 [00:25<00:55,  3.04it/s][A
 24%|██▍       | 54/221 [00:26<01:14,  2.24it/s][A
 25%|██▍       | 55/221 [00:27<02:06,  1.31it/s][A
 25%|██▌       | 56/221 [00:27<01:37,  1.69it/s][A
 26%|██▌       | 57/221 [00:28<01:17,  2.11it/s][A
 27%|██▋       | 59/221 [00:28<00:47,  3.44it/s][A
 27%|██▋       | 60/221 [00:28<01:00,  2.66it/s][A
 28%|██▊       | 61/221 [00:29<00:52,  3.06it/s][A
 28%|██▊       | 62/221 [00:29<00:51,  3.08it/s][A
 29%|██▊       | 63/221 [00:29<00:53,  2.98it/s][A
 29%|██▉       | 64/221 [00:30<01:16,  2.04it/s][A
 30%|██▉       | 66/221 [00:31<00:58,  2.65it/s][A
 30%|███       | 67/221 [00:31<00:51,  3.01it/s][A
 31%|███       | 68/221 [00:31<00:43,  3.54it/s][A
 31%|███       | 69/221 [00:32<01:11,  2.14it/s][A
 32%|███▏      | 70/221 [00:32<00:55,  2.73it/s][A
 32%|███▏      | 71/221 [00:32<00:50,  2.98it/s][A
 33%|███▎      | 72/221 [00:33<00:50,  2.95it/s][A
 33%|███▎      | 73/221 [00:33<01:02,  2.37it/s][A
 33%|███▎      | 74/221 [00:33<00:49,  2.97it/s][A
 34%|███▍      | 75/221 [00:34<00:49,  2.97it/s][A
 34%|███▍      | 76/221 [00:34<00:48,  2.99it/s][A
 35%|███▍      | 77/221 [00:36<01:58,  1.21it/s][A
 35%|███▌      | 78/221 [00:36<01:28,  1.61it/s][A
 36%|███▌      | 79/221 [00:37<01:39,  1.43it/s][A
 37%|███▋      | 81/221 [00:38<01:30,  1.55it/s][A
 37%|███▋      | 82/221 [00:43<03:52,  1.67s/it][A
 38%|███▊      | 83/221 [00:44<03:11,  1.39s/it][A
 38%|███▊      | 84/221 [00:44<02:24,  1.05s/it][A
 39%|███▉      | 86/221 [00:44<01:34,  1.42it/s][A
 39%|███▉      | 87/221 [00:45<01:37,  1.37it/s][A
 40%|███▉      | 88/221 [00:45<01:20,  1.65it/s][A
 40%|████      | 89/221 [00:46<01:14,  1.77it/s][A
 41%|████      | 90/221 [00:46<01:02,  2.09it/s][A
 42%|████▏     | 92/221 [00:46<00:41,  3.08it/s][A
 42%|████▏     | 93/221 [00:47<00:43,  2.98it/s][A
 43%|████▎     | 94/221 [00:48<01:03,  2.00it/s][A
 43%|████▎     | 95/221 [00:48<00:56,  2.23it/s][A
 43%|████▎     | 96/221 [00:49<01:19,  1.58it/s][A
 44%|████▍     | 97/221 [00:49<01:01,  2.01it/s][A
 44%|████▍     | 98/221 [00:50<01:13,  1.66it/s][A
 45%|████▍     | 99/221 [00:50<00:59,  2.06it/s][A
 45%|████▌     | 100/221 [00:51<01:09,  1.74it/s][A
 46%|████▌     | 101/221 [00:51<00:54,  2.20it/s][A
 46%|████▌     | 102/221 [00:53<01:25,  1.39it/s][A
 47%|████▋     | 103/221 [00:53<01:03,  1.86it/s][A
 47%|████▋     | 104/221 [00:53<00:57,  2.05it/s][A
 48%|████▊     | 105/221 [00:54<00:59,  1.95it/s][A
 48%|████▊     | 106/221 [00:55<01:42,  1.13it/s][A
 48%|████▊     | 107/221 [00:56<01:25,  1.33it/s][A
 49%|████▉     | 108/221 [00:56<01:07,  1.68it/s][A
 49%|████▉     | 109/221 [00:56<00:56,  1.98it/s][A
 50%|█████     | 111/221 [00:57<00:43,  2.53it/s][A
 51%|█████     | 112/221 [00:57<00:40,  2.68it/s][A
 51%|█████     | 113/221 [00:57<00:36,  2.95it/s][A
 52%|█████▏    | 115/221 [00:58<00:35,  2.96it/s][A
 52%|█████▏    | 116/221 [00:59<00:37,  2.80it/s][A
 53%|█████▎    | 117/221 [00:59<00:39,  2.62it/s][A
 53%|█████▎    | 118/221 [00:59<00:40,  2.54it/s][A
 54%|█████▍    | 119/221 [01:00<00:40,  2.51it/s][A
 54%|█████▍    | 120/221 [01:00<00:33,  2.99it/s][A
 55%|█████▍    | 121/221 [01:00<00:35,  2.81it/s][A
 55%|█████▌    | 122/221 [01:01<00:34,  2.87it/s][A
 56%|█████▌    | 123/221 [01:02<01:13,  1.34it/s][A
 56%|█████▌    | 124/221 [01:03<00:56,  1.71it/s][A
 57%|█████▋    | 125/221 [01:04<01:04,  1.48it/s][A
 57%|█████▋    | 126/221 [01:06<01:51,  1.18s/it][A
 57%|█████▋    | 127/221 [01:07<01:39,  1.06s/it][A
 58%|█████▊    | 128/221 [01:07<01:21,  1.14it/s][A
 58%|█████▊    | 129/221 [01:08<01:09,  1.31it/s][A
 59%|█████▉    | 130/221 [01:08<00:58,  1.56it/s][A
 59%|█████▉    | 131/221 [01:09<01:01,  1.46it/s][A
 60%|█████▉    | 132/221 [01:10<01:18,  1.14it/s][A
 60%|██████    | 133/221 [01:10<01:03,  1.39it/s][A
 61%|██████    | 134/221 [01:13<01:45,  1.21s/it][A
 61%|██████    | 135/221 [01:14<01:43,  1.21s/it][A
 62%|██████▏   | 136/221 [01:14<01:21,  1.04it/s][A
 62%|██████▏   | 137/221 [01:15<01:04,  1.31it/s][A
 62%|██████▏   | 138/221 [01:15<00:58,  1.43it/s][A
 63%|██████▎   | 139/221 [01:15<00:43,  1.88it/s][A
 63%|██████▎   | 140/221 [01:16<00:46,  1.75it/s][A
 64%|██████▍   | 141/221 [01:16<00:39,  2.01it/s][A
 64%|██████▍   | 142/221 [01:17<00:36,  2.19it/s][A
 65%|██████▍   | 143/221 [01:17<00:35,  2.19it/s][A
 65%|██████▌   | 144/221 [01:17<00:28,  2.71it/s][A
 66%|██████▌   | 145/221 [01:17<00:21,  3.46it/s][A
 66%|██████▌   | 146/221 [01:18<00:17,  4.28it/s][A
 67%|██████▋   | 148/221 [01:18<00:20,  3.55it/s][A
 67%|██████▋   | 149/221 [01:19<00:27,  2.60it/s][A
 68%|██████▊   | 150/221 [01:20<00:32,  2.19it/s][A
 68%|██████▊   | 151/221 [01:20<00:29,  2.39it/s][A
 69%|██████▉   | 152/221 [01:20<00:28,  2.40it/s][A
 69%|██████▉   | 153/221 [01:20<00:22,  2.97it/s][A
 70%|██████▉   | 154/221 [01:21<00:23,  2.79it/s][A
 70%|███████   | 155/221 [01:21<00:23,  2.84it/s][A
 71%|███████   | 156/221 [01:21<00:20,  3.14it/s][A
 71%|███████   | 157/221 [01:26<01:41,  1.59s/it][A
 71%|███████▏  | 158/221 [01:27<01:23,  1.32s/it][A
 72%|███████▏  | 159/221 [01:27<01:00,  1.03it/s][A
 72%|███████▏  | 160/221 [01:27<00:44,  1.36it/s][A
 73%|███████▎  | 162/221 [01:27<00:24,  2.36it/s][A
 74%|███████▍  | 163/221 [01:28<00:23,  2.52it/s][A
 74%|███████▍  | 164/221 [01:28<00:21,  2.64it/s][A
 75%|███████▍  | 165/221 [01:29<00:28,  2.00it/s][A
 75%|███████▌  | 166/221 [01:29<00:27,  1.98it/s][A
 76%|███████▌  | 167/221 [01:29<00:21,  2.56it/s][A
 76%|███████▌  | 168/221 [01:35<01:47,  2.03s/it][A
 76%|███████▋  | 169/221 [01:36<01:21,  1.57s/it][A
 77%|███████▋  | 170/221 [01:36<01:02,  1.22s/it][A
 77%|███████▋  | 171/221 [01:37<00:49,  1.01it/s][A
 78%|███████▊  | 172/221 [01:37<00:38,  1.27it/s][A
 78%|███████▊  | 173/221 [01:37<00:32,  1.46it/s][A
 79%|███████▊  | 174/221 [01:38<00:24,  1.93it/s][A
 79%|███████▉  | 175/221 [01:38<00:19,  2.30it/s][A
 80%|███████▉  | 176/221 [01:38<00:17,  2.52it/s][A
 80%|████████  | 177/221 [01:38<00:16,  2.74it/s][A
 81%|████████  | 178/221 [01:39<00:16,  2.61it/s][A
 81%|████████  | 179/221 [01:39<00:16,  2.59it/s][A
 81%|████████▏ | 180/221 [01:39<00:13,  3.13it/s][A
 82%|████████▏ | 181/221 [01:39<00:10,  3.89it/s][A
 82%|████████▏ | 182/221 [01:40<00:12,  3.03it/s][A
 83%|████████▎ | 183/221 [01:41<00:18,  2.01it/s][A
 83%|████████▎ | 184/221 [01:41<00:17,  2.06it/s][A
 84%|████████▎ | 185/221 [01:41<00:14,  2.57it/s][A
 84%|████████▍ | 186/221 [01:42<00:13,  2.62it/s][A
 85%|████████▍ | 187/221 [01:42<00:11,  2.90it/s][A
 85%|████████▌ | 188/221 [01:42<00:09,  3.47it/s][A
 86%|████████▌ | 189/221 [01:43<00:10,  2.93it/s][A
 86%|████████▌ | 190/221 [01:43<00:11,  2.73it/s][A
 86%|████████▋ | 191/221 [01:43<00:09,  3.17it/s][A
 87%|████████▋ | 192/221 [01:44<00:10,  2.89it/s][A
 87%|████████▋ | 193/221 [01:44<00:07,  3.63it/s][A
 88%|████████▊ | 194/221 [01:46<00:19,  1.36it/s][A
 88%|████████▊ | 195/221 [01:46<00:15,  1.69it/s][A
 89%|████████▊ | 196/221 [01:46<00:13,  1.90it/s][A
 89%|████████▉ | 197/221 [01:47<00:10,  2.32it/s][A
 90%|████████▉ | 198/221 [01:47<00:08,  2.65it/s][A
 90%|█████████ | 199/221 [01:47<00:07,  3.08it/s][A
 90%|█████████ | 200/221 [01:47<00:07,  2.80it/s][A
 91%|█████████ | 201/221 [01:48<00:09,  2.14it/s][A
 91%|█████████▏| 202/221 [01:48<00:06,  2.79it/s][A
 92%|█████████▏| 203/221 [01:49<00:10,  1.72it/s][A
 92%|█████████▏| 204/221 [01:50<00:08,  1.94it/s][A
 93%|█████████▎| 206/221 [01:50<00:05,  2.91it/s][A
 94%|█████████▎| 207/221 [01:50<00:04,  3.13it/s][A
 94%|█████████▍| 208/221 [01:50<00:03,  3.35it/s][A
 95%|█████████▍| 209/221 [01:51<00:02,  4.03it/s][A
 95%|█████████▌| 211/221 [01:51<00:02,  3.49it/s][A
 96%|█████████▌| 212/221 [01:52<00:02,  3.17it/s][A
 97%|█████████▋| 214/221 [01:52<00:01,  3.63it/s][A
 97%|█████████▋| 215/221 [01:52<00:01,  3.59it/s][A
 98%|█████████▊| 216/221 [01:53<00:01,  3.01it/s][A
 98%|█████████▊| 217/221 [01:57<00:05,  1.43s/it][A
 99%|█████████▊| 218/221 [01:58<00:03,  1.14s/it][A
 99%|█████████▉| 219/221 [01:58<00:01,  1.06it/s][A
100%|█████████▉| 220/221 [01:59<00:00,  1.21it/s][A
100%|██████████| 221/221 [01:59<00:00,  1.58it/s][A100%|██████████| 221/221 [01:59<00:00,  1.85it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:51,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:55,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:28,  7.71it/s][A
  1%|          | 2/221 [00:00<00:58,  3.73it/s][A
  1%|▏         | 3/221 [00:00<01:12,  3.02it/s][A
  2%|▏         | 4/221 [00:01<01:40,  2.15it/s][A
  2%|▏         | 5/221 [00:02<01:48,  2.00it/s][A
  3%|▎         | 6/221 [00:02<01:23,  2.59it/s][A
  3%|▎         | 7/221 [00:02<01:14,  2.88it/s][A
  4%|▎         | 8/221 [00:03<01:31,  2.34it/s][A
  4%|▍         | 9/221 [00:03<01:46,  1.99it/s][A
  5%|▍         | 10/221 [00:04<01:43,  2.05it/s][A
  5%|▍         | 11/221 [00:05<02:33,  1.37it/s][A
  5%|▌         | 12/221 [00:05<02:05,  1.66it/s][A
  6%|▌         | 13/221 [00:06<02:33,  1.36it/s][A
  6%|▋         | 14/221 [00:07<01:56,  1.78it/s][A
  7%|▋         | 15/221 [00:07<01:35,  2.16it/s][A
  7%|▋         | 16/221 [00:08<01:56,  1.76it/s][A
  8%|▊         | 17/221 [00:09<02:28,  1.38it/s][A
  8%|▊         | 18/221 [00:09<02:03,  1.64it/s][A
  9%|▊         | 19/221 [00:10<02:27,  1.37it/s][A
  9%|▉         | 20/221 [00:10<01:57,  1.71it/s][A
 10%|▉         | 21/221 [00:11<01:46,  1.87it/s][A
 10%|▉         | 22/221 [00:11<01:23,  2.39it/s][A
 10%|█         | 23/221 [00:11<01:28,  2.24it/s][A
 11%|█         | 24/221 [00:12<01:11,  2.77it/s][A
 11%|█▏        | 25/221 [00:13<01:50,  1.78it/s][A
 12%|█▏        | 26/221 [00:14<02:18,  1.41it/s][A
 12%|█▏        | 27/221 [00:14<02:00,  1.61it/s][A
 13%|█▎        | 28/221 [00:15<02:05,  1.54it/s][A
 13%|█▎        | 29/221 [00:17<03:08,  1.02it/s][A
 14%|█▎        | 30/221 [00:17<02:52,  1.11it/s][A
 14%|█▍        | 31/221 [00:18<02:35,  1.22it/s][A
 14%|█▍        | 32/221 [00:18<02:16,  1.38it/s][A
 15%|█▍        | 33/221 [00:19<02:00,  1.56it/s][A
 15%|█▌        | 34/221 [00:19<01:56,  1.60it/s][A
 16%|█▌        | 35/221 [00:20<01:26,  2.14it/s][A
 16%|█▋        | 36/221 [00:20<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:21<01:29,  2.05it/s][A
 17%|█▋        | 38/221 [00:21<01:38,  1.86it/s][A
 18%|█▊        | 39/221 [00:22<01:41,  1.80it/s][A
 18%|█▊        | 40/221 [00:23<01:51,  1.62it/s][A
 19%|█▊        | 41/221 [00:23<01:36,  1.87it/s][A
 19%|█▉        | 42/221 [00:23<01:20,  2.22it/s][A
 19%|█▉        | 43/221 [00:24<01:14,  2.39it/s][A
 20%|█▉        | 44/221 [00:24<01:00,  2.92it/s][A
 20%|██        | 45/221 [00:24<01:02,  2.80it/s][A
 21%|██        | 46/221 [00:25<01:23,  2.10it/s][A
 21%|██▏       | 47/221 [00:25<01:19,  2.19it/s][A
 22%|██▏       | 48/221 [00:25<01:06,  2.61it/s][A
 22%|██▏       | 49/221 [00:26<01:31,  1.88it/s][A
 23%|██▎       | 50/221 [00:27<01:45,  1.62it/s][A
 23%|██▎       | 51/221 [00:28<01:34,  1.79it/s][A
 24%|██▎       | 52/221 [00:28<01:24,  2.00it/s][A
 24%|██▍       | 53/221 [00:28<01:14,  2.24it/s][A
 24%|██▍       | 54/221 [00:29<01:20,  2.08it/s][A
 25%|██▍       | 55/221 [00:29<01:09,  2.40it/s][A
 25%|██▌       | 56/221 [00:30<01:33,  1.76it/s][A
 26%|██▌       | 57/221 [00:30<01:17,  2.12it/s][A
 26%|██▌       | 58/221 [00:31<01:52,  1.45it/s][A
 27%|██▋       | 59/221 [00:32<01:31,  1.78it/s][A
 27%|██▋       | 60/221 [00:32<01:30,  1.77it/s][A
 28%|██▊       | 61/221 [00:33<01:47,  1.49it/s][A
 28%|██▊       | 62/221 [00:34<01:38,  1.62it/s][A
 29%|██▊       | 63/221 [00:34<01:17,  2.03it/s][A
 29%|██▉       | 64/221 [00:35<01:39,  1.58it/s][A
 29%|██▉       | 65/221 [00:35<01:38,  1.58it/s][A
 30%|██▉       | 66/221 [00:36<01:29,  1.73it/s][A
 30%|███       | 67/221 [00:36<01:17,  1.98it/s][A
 31%|███       | 68/221 [00:36<01:00,  2.52it/s][A
 31%|███       | 69/221 [00:37<01:03,  2.38it/s][A
 32%|███▏      | 70/221 [00:37<01:05,  2.32it/s][A
 32%|███▏      | 71/221 [00:38<01:16,  1.97it/s][A
 33%|███▎      | 72/221 [00:38<01:13,  2.03it/s][A
 33%|███▎      | 73/221 [00:39<01:21,  1.82it/s][A
 33%|███▎      | 74/221 [00:40<01:25,  1.73it/s][A
 34%|███▍      | 75/221 [00:40<01:10,  2.06it/s][A
 34%|███▍      | 76/221 [00:40<01:00,  2.38it/s][A
 35%|███▍      | 77/221 [00:41<01:07,  2.15it/s][A
 35%|███▌      | 78/221 [00:42<01:17,  1.86it/s][A
 36%|███▌      | 79/221 [00:43<01:33,  1.52it/s][A
 36%|███▌      | 80/221 [00:43<01:22,  1.72it/s][A
 37%|███▋      | 81/221 [00:43<01:04,  2.19it/s][A
 37%|███▋      | 82/221 [00:45<01:48,  1.28it/s][A
 38%|███▊      | 83/221 [00:46<01:49,  1.26it/s][A
 38%|███▊      | 84/221 [00:46<01:45,  1.30it/s][A
 38%|███▊      | 85/221 [00:47<01:31,  1.49it/s][A
 39%|███▉      | 86/221 [00:47<01:30,  1.49it/s][A
 39%|███▉      | 87/221 [00:48<01:19,  1.69it/s][A
 40%|███▉      | 88/221 [00:48<01:10,  1.90it/s][A
 40%|████      | 89/221 [00:49<01:19,  1.67it/s][A
 41%|████      | 90/221 [00:50<01:19,  1.65it/s][A
 41%|████      | 91/221 [00:50<01:07,  1.93it/s][A
 42%|████▏     | 92/221 [00:51<01:19,  1.63it/s][A
 43%|████▎     | 94/221 [00:51<00:48,  2.62it/s][A
 43%|████▎     | 95/221 [00:52<00:58,  2.16it/s][A
 43%|████▎     | 96/221 [00:52<01:04,  1.95it/s][A
 44%|████▍     | 97/221 [00:53<00:57,  2.16it/s][A
 44%|████▍     | 98/221 [00:53<01:13,  1.68it/s][A
 45%|████▍     | 99/221 [00:54<01:14,  1.64it/s][A
 45%|████▌     | 100/221 [00:55<01:14,  1.62it/s][A
 46%|████▌     | 101/221 [00:55<01:10,  1.69it/s][A
 46%|████▌     | 102/221 [00:56<01:13,  1.62it/s][A
 47%|████▋     | 103/221 [00:56<00:54,  2.15it/s][A
 47%|████▋     | 104/221 [00:57<01:16,  1.52it/s][A
 48%|████▊     | 105/221 [00:57<01:00,  1.91it/s][A
 48%|████▊     | 106/221 [00:58<00:50,  2.28it/s][A
 48%|████▊     | 107/221 [00:58<00:54,  2.08it/s][A
 49%|████▉     | 108/221 [00:59<00:53,  2.12it/s][A
 49%|████▉     | 109/221 [00:59<00:48,  2.29it/s][A
 50%|████▉     | 110/221 [01:00<01:04,  1.72it/s][A
 50%|█████     | 111/221 [01:01<01:05,  1.67it/s][A
 51%|█████     | 112/221 [01:01<00:53,  2.05it/s][A
 51%|█████     | 113/221 [01:01<00:45,  2.37it/s][A
 52%|█████▏    | 114/221 [01:02<00:50,  2.13it/s][A
 52%|█████▏    | 115/221 [01:03<01:02,  1.71it/s][A
 52%|█████▏    | 116/221 [01:03<00:49,  2.11it/s][A
 53%|█████▎    | 117/221 [01:03<00:49,  2.11it/s][A
 53%|█████▎    | 118/221 [01:04<01:01,  1.69it/s][A
 54%|█████▍    | 119/221 [01:04<00:48,  2.12it/s][A
 54%|█████▍    | 120/221 [01:05<00:48,  2.07it/s][A
 55%|█████▍    | 121/221 [01:05<00:53,  1.86it/s][A
 55%|█████▌    | 122/221 [01:06<00:47,  2.07it/s][A
 56%|█████▌    | 123/221 [01:06<00:46,  2.10it/s][A
 56%|█████▌    | 124/221 [01:07<00:41,  2.36it/s][A
 57%|█████▋    | 125/221 [01:07<00:53,  1.79it/s][A
 57%|█████▋    | 126/221 [01:08<00:45,  2.07it/s][A
 57%|█████▋    | 127/221 [01:09<00:53,  1.75it/s][A
 58%|█████▊    | 128/221 [01:09<00:47,  1.97it/s][A
 58%|█████▊    | 129/221 [01:09<00:41,  2.22it/s][A
 59%|█████▉    | 130/221 [01:10<00:42,  2.16it/s][A
 59%|█████▉    | 131/221 [01:10<00:32,  2.75it/s][A
 60%|█████▉    | 132/221 [01:10<00:34,  2.56it/s][A
 60%|██████    | 133/221 [01:11<00:53,  1.65it/s][A
 61%|██████    | 134/221 [01:12<00:46,  1.89it/s][A
 61%|██████    | 135/221 [01:12<00:42,  2.03it/s][A
 62%|██████▏   | 136/221 [01:13<00:41,  2.06it/s][A
 62%|██████▏   | 137/221 [01:13<00:36,  2.28it/s][A
 62%|██████▏   | 138/221 [01:13<00:39,  2.12it/s][A
 63%|██████▎   | 139/221 [01:14<00:43,  1.90it/s][A
 63%|██████▎   | 140/221 [01:15<00:42,  1.91it/s][A
 64%|██████▍   | 141/221 [01:15<00:35,  2.24it/s][A
 64%|██████▍   | 142/221 [01:15<00:36,  2.15it/s][A
 65%|██████▍   | 143/221 [01:16<00:29,  2.66it/s][A
 66%|██████▌   | 145/221 [01:16<00:28,  2.71it/s][A
 66%|██████▌   | 146/221 [01:17<00:25,  2.97it/s][A
 67%|██████▋   | 147/221 [01:17<00:21,  3.48it/s][A
 67%|██████▋   | 148/221 [01:17<00:22,  3.22it/s][A
 67%|██████▋   | 149/221 [01:17<00:21,  3.28it/s][A
 68%|██████▊   | 150/221 [01:18<00:22,  3.13it/s][A
 69%|██████▉   | 152/221 [01:19<00:24,  2.83it/s][A
 69%|██████▉   | 153/221 [01:19<00:25,  2.69it/s][A
 70%|██████▉   | 154/221 [01:20<00:29,  2.28it/s][A
 70%|███████   | 155/221 [01:20<00:33,  1.95it/s][A
 71%|███████   | 156/221 [01:21<00:34,  1.87it/s][A
 71%|███████   | 157/221 [01:21<00:34,  1.83it/s][A
 71%|███████▏  | 158/221 [01:22<00:30,  2.06it/s][A
 72%|███████▏  | 159/221 [01:22<00:25,  2.40it/s][A
 72%|███████▏  | 160/221 [01:22<00:25,  2.35it/s][A
 73%|███████▎  | 161/221 [01:23<00:19,  3.00it/s][A
 73%|███████▎  | 162/221 [01:23<00:22,  2.67it/s][A
 74%|███████▍  | 163/221 [01:24<00:23,  2.43it/s][A
 74%|███████▍  | 164/221 [01:24<00:24,  2.32it/s][A
 75%|███████▍  | 165/221 [01:24<00:21,  2.62it/s][A
 75%|███████▌  | 166/221 [01:25<00:21,  2.53it/s][A
 76%|███████▌  | 167/221 [01:25<00:18,  2.87it/s][A
 76%|███████▌  | 168/221 [01:25<00:20,  2.63it/s][A
 76%|███████▋  | 169/221 [01:26<00:25,  2.05it/s][A
 77%|███████▋  | 170/221 [01:27<00:34,  1.47it/s][A
 77%|███████▋  | 171/221 [01:28<00:37,  1.35it/s][A
 78%|███████▊  | 172/221 [01:29<00:32,  1.52it/s][A
 78%|███████▊  | 173/221 [01:29<00:27,  1.76it/s][A
 79%|███████▊  | 174/221 [01:30<00:36,  1.28it/s][A
 79%|███████▉  | 175/221 [01:31<00:38,  1.20it/s][A
 80%|███████▉  | 176/221 [01:32<00:34,  1.29it/s][A
 80%|████████  | 177/221 [01:32<00:29,  1.47it/s][A
 81%|████████  | 178/221 [01:33<00:29,  1.48it/s][A
 81%|████████  | 179/221 [01:33<00:24,  1.73it/s][A
 81%|████████▏ | 180/221 [01:34<00:26,  1.54it/s][A
 82%|████████▏ | 181/221 [01:35<00:22,  1.80it/s][A
 82%|████████▏ | 182/221 [01:35<00:22,  1.71it/s][A
 83%|████████▎ | 183/221 [01:35<00:18,  2.02it/s][A
 83%|████████▎ | 184/221 [01:36<00:20,  1.78it/s][A
 84%|████████▎ | 185/221 [01:37<00:26,  1.38it/s][A
 84%|████████▍ | 186/221 [01:37<00:18,  1.84it/s][A
 85%|████████▍ | 187/221 [01:38<00:20,  1.68it/s][A
 85%|████████▌ | 188/221 [01:38<00:16,  2.01it/s][A
 86%|████████▌ | 189/221 [01:39<00:17,  1.82it/s][A
 86%|████████▌ | 190/221 [01:39<00:15,  1.96it/s][A
 86%|████████▋ | 191/221 [01:40<00:14,  2.06it/s][A
 87%|████████▋ | 192/221 [01:40<00:12,  2.35it/s][A
 87%|████████▋ | 193/221 [01:40<00:10,  2.68it/s][A
 88%|████████▊ | 194/221 [01:41<00:11,  2.28it/s][A
 88%|████████▊ | 195/221 [01:42<00:18,  1.39it/s][A
 89%|████████▊ | 196/221 [01:44<00:22,  1.11it/s][A
 89%|████████▉ | 197/221 [01:44<00:18,  1.32it/s][A
 90%|████████▉ | 198/221 [01:44<00:14,  1.61it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.98it/s][A
 90%|█████████ | 200/221 [01:46<00:13,  1.56it/s][A
 91%|█████████ | 201/221 [01:46<00:11,  1.79it/s][A
 91%|█████████▏| 202/221 [01:46<00:08,  2.24it/s][A
 92%|█████████▏| 203/221 [01:47<00:07,  2.38it/s][A
 92%|█████████▏| 204/221 [01:48<00:14,  1.16it/s][A
 93%|█████████▎| 205/221 [01:49<00:10,  1.53it/s][A
 93%|█████████▎| 206/221 [01:49<00:09,  1.61it/s][A
 94%|█████████▎| 207/221 [01:50<00:08,  1.65it/s][A
 94%|█████████▍| 208/221 [01:50<00:08,  1.54it/s][A
 95%|█████████▍| 209/221 [01:51<00:08,  1.42it/s][A
 95%|█████████▌| 210/221 [01:51<00:05,  1.84it/s][A
 95%|█████████▌| 211/221 [01:52<00:04,  2.01it/s][A
 96%|█████████▌| 212/221 [01:52<00:03,  2.44it/s][A
 96%|█████████▋| 213/221 [01:53<00:04,  1.92it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  2.04it/s][A
 97%|█████████▋| 215/221 [01:53<00:02,  2.58it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  2.23it/s][A
 98%|█████████▊| 217/221 [01:55<00:02,  1.99it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  2.38it/s][A
 99%|█████████▉| 219/221 [01:55<00:00,  2.33it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  2.46it/s][A
100%|██████████| 221/221 [01:56<00:00,  2.18it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]
09/19/2024 06:18:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 2999--===========

09/19/2024 06:18:14 - INFO - __main__ -   {'area_r1': 44.5, 'area_recall': '44.5/71.7/81.8', 'area_ravg': 66.0}
09/19/2024 06:18:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 2999--===========

09/19/2024 06:18:14 - INFO - __main__ -   {'forward_r1': 50.0, 'forward_recall': '50.0/78.2/87.6', 'forward_ravg': 71.9}
09/19/2024 06:18:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 2999--===========

09/19/2024 06:18:14 - INFO - __main__ -   {'area_video_r1': 49.3, 'area_video_recall': '49.3/78.2/87.3', 'area_video_ravg': 71.6}
09/19/2024 06:18:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 2499=======

09/19/2024 06:18:14 - INFO - __main__ -   {'area_video_r1': 49.9, 'area_video_recall': '49.9/79.1/87.2', 'area_video_ravg': 72.1}
09/19/2024 06:18:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 2999--===========

09/19/2024 06:18:14 - INFO - __main__ -   {'area_video_r1': 62.0, 'area_video_recall': '62.0/82.0/87.8', 'area_video_ravg': 77.3, 'area_video_back_r1': 61.2, 'area_video_back_recall': '61.2/84.0/90.7', 'area_video_back_ravg': 78.7}
09/19/2024 06:18:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 2999=======

09/19/2024 06:18:14 - INFO - __main__ -   {'area_video_r1': 62.0, 'area_video_recall': '62.0/82.0/87.8', 'area_video_ravg': 77.3, 'area_video_back_r1': 61.2, 'area_video_back_recall': '61.2/84.0/90.7', 'area_video_back_ravg': 78.7}
09/19/2024 06:18:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 2999--===========

09/19/2024 06:18:14 - INFO - __main__ -   {'video_r1': 32.6, 'video_recall': '32.6/57.9/69.9', 'video_ravg': 53.5}
09/19/2024 06:18:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 06:18:14 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 06:18:14 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 2999--===========

09/19/2024 06:18:14 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/79.5/84.7', 'video_ravg': 74.8}
09/19/2024 06:18:14 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 2999=======

09/19/2024 06:18:14 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/79.5/84.7', 'video_ravg': 74.8}
09/19/2024 06:18:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.017828186973929405, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1417834758758545, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.159611701965332}
 34%|███▎      | 3000/8917 [4:04:12<280:16:39, 170.53s/it] 34%|███▎      | 3001/8917 [4:04:15<197:38:29, 120.27s/it] 34%|███▎      | 3002/8917 [4:04:18<140:11:49, 85.33s/it]  34%|███▎      | 3003/8917 [4:04:22<99:55:16, 60.82s/it]  34%|███▎      | 3004/8917 [4:04:26<71:50:07, 43.74s/it] 34%|███▎      | 3005/8917 [4:04:29<52:00:46, 31.67s/it] 34%|███▎      | 3006/8917 [4:04:33<38:13:47, 23.28s/it] 34%|███▎      | 3007/8917 [4:04:37<28:32:10, 17.38s/it] 34%|███▎      | 3008/8917 [4:04:40<21:46:18, 13.26s/it] 34%|███▎      | 3009/8917 [4:04:44<16:58:33, 10.34s/it] 34%|███▍      | 3010/8917 [4:04:48<13:53:30,  8.47s/it] 34%|███▍      | 3011/8917 [4:04:52<11:29:16,  7.00s/it] 34%|███▍      | 3012/8917 [4:04:55<9:51:01,  6.01s/it]  34%|███▍      | 3013/8917 [4:04:59<8:51:55,  5.41s/it] 34%|███▍      | 3014/8917 [4:05:03<7:58:22,  4.86s/it] 34%|███▍      | 3015/8917 [4:05:06<7:14:20,  4.42s/it] 34%|███▍      | 3016/8917 [4:05:10<7:00:34,  4.28s/it] 34%|███▍      | 3017/8917 [4:05:14<6:51:38,  4.19s/it] 34%|███▍      | 3018/8917 [4:05:18<6:50:04,  4.17s/it] 34%|███▍      | 3019/8917 [4:05:22<6:38:03,  4.05s/it] 34%|███▍      | 3020/8917 [4:05:25<6:20:42,  3.87s/it] 34%|███▍      | 3021/8917 [4:05:29<6:14:15,  3.81s/it] 34%|███▍      | 3022/8917 [4:05:32<6:00:47,  3.67s/it] 34%|███▍      | 3023/8917 [4:05:36<5:59:49,  3.66s/it] 34%|███▍      | 3024/8917 [4:05:40<6:04:08,  3.71s/it] 34%|███▍      | 3025/8917 [4:05:44<6:01:39,  3.68s/it] 34%|███▍      | 3026/8917 [4:05:48<6:12:28,  3.79s/it] 34%|███▍      | 3027/8917 [4:05:52<6:19:27,  3.87s/it] 34%|███▍      | 3028/8917 [4:05:55<6:05:23,  3.72s/it] 34%|███▍      | 3029/8917 [4:05:59<6:16:56,  3.84s/it] 34%|███▍      | 3030/8917 [4:06:03<6:07:22,  3.74s/it] 34%|███▍      | 3031/8917 [4:06:07<6:12:22,  3.80s/it] 34%|███▍      | 3032/8917 [4:06:10<6:05:12,  3.72s/it] 34%|███▍      | 3033/8917 [4:06:14<6:10:18,  3.78s/it] 34%|███▍      | 3034/8917 [4:06:18<6:10:24,  3.78s/it] 34%|███▍      | 3035/8917 [4:06:21<5:58:32,  3.66s/it] 34%|███▍      | 3036/8917 [4:06:25<5:59:46,  3.67s/it] 34%|███▍      | 3037/8917 [4:06:29<6:12:51,  3.80s/it] 34%|███▍      | 3038/8917 [4:06:32<5:56:43,  3.64s/it] 34%|███▍      | 3039/8917 [4:06:36<6:00:41,  3.68s/it] 34%|███▍      | 3040/8917 [4:06:40<5:56:34,  3.64s/it] 34%|███▍      | 3041/8917 [4:06:43<5:58:05,  3.66s/it] 34%|███▍      | 3042/8917 [4:06:47<6:01:12,  3.69s/it] 34%|███▍      | 3043/8917 [4:06:51<6:10:05,  3.78s/it] 34%|███▍      | 3044/8917 [4:06:55<6:14:13,  3.82s/it] 34%|███▍      | 3045/8917 [4:06:59<6:08:47,  3.77s/it] 34%|███▍      | 3046/8917 [4:07:02<6:05:12,  3.73s/it] 34%|███▍      | 3047/8917 [4:07:06<5:54:30,  3.62s/it] 34%|███▍      | 3048/8917 [4:07:09<5:56:43,  3.65s/it] 34%|███▍      | 3049/8917 [4:07:13<5:54:01,  3.62s/it]09/19/2024 06:21:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03040296770632267, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4639453887939453, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.494348406791687}
 34%|███▍      | 3050/8917 [4:07:17<6:06:21,  3.75s/it] 34%|███▍      | 3051/8917 [4:07:21<6:02:32,  3.71s/it] 34%|███▍      | 3052/8917 [4:07:24<6:07:16,  3.76s/it] 34%|███▍      | 3053/8917 [4:07:28<6:09:08,  3.78s/it] 34%|███▍      | 3054/8917 [4:07:32<6:07:22,  3.76s/it] 34%|███▍      | 3055/8917 [4:07:36<6:12:00,  3.81s/it] 34%|███▍      | 3056/8917 [4:07:39<6:05:41,  3.74s/it] 34%|███▍      | 3057/8917 [4:07:43<6:02:33,  3.71s/it] 34%|███▍      | 3058/8917 [4:07:47<6:07:33,  3.76s/it] 34%|███▍      | 3059/8917 [4:07:51<6:00:04,  3.69s/it] 34%|███▍      | 3060/8917 [4:07:55<6:09:09,  3.78s/it] 34%|███▍      | 3061/8917 [4:07:58<5:59:51,  3.69s/it] 34%|███▍      | 3062/8917 [4:08:01<5:47:59,  3.57s/it] 34%|███▍      | 3063/8917 [4:08:05<5:56:08,  3.65s/it] 34%|███▍      | 3064/8917 [4:08:09<6:12:58,  3.82s/it] 34%|███▍      | 3065/8917 [4:08:13<6:07:02,  3.76s/it] 34%|███▍      | 3066/8917 [4:08:17<6:01:22,  3.71s/it] 34%|███▍      | 3067/8917 [4:08:20<5:57:10,  3.66s/it] 34%|███▍      | 3068/8917 [4:08:24<6:03:55,  3.73s/it] 34%|███▍      | 3069/8917 [4:08:28<5:59:49,  3.69s/it] 34%|███▍      | 3070/8917 [4:08:31<6:04:59,  3.75s/it] 34%|███▍      | 3071/8917 [4:08:35<6:01:50,  3.71s/it] 34%|███▍      | 3072/8917 [4:08:39<6:02:02,  3.72s/it] 34%|███▍      | 3073/8917 [4:08:43<6:07:12,  3.77s/it] 34%|███▍      | 3074/8917 [4:08:46<5:56:31,  3.66s/it] 34%|███▍      | 3075/8917 [4:08:50<5:51:34,  3.61s/it] 34%|███▍      | 3076/8917 [4:08:53<5:50:10,  3.60s/it] 35%|███▍      | 3077/8917 [4:08:57<5:50:13,  3.60s/it] 35%|███▍      | 3078/8917 [4:09:01<5:59:11,  3.69s/it] 35%|███▍      | 3079/8917 [4:09:05<6:06:16,  3.76s/it] 35%|███▍      | 3080/8917 [4:09:08<5:56:58,  3.67s/it] 35%|███▍      | 3081/8917 [4:09:12<5:56:47,  3.67s/it] 35%|███▍      | 3082/8917 [4:09:16<5:59:36,  3.70s/it] 35%|███▍      | 3083/8917 [4:09:20<6:14:49,  3.85s/it] 35%|███▍      | 3084/8917 [4:09:23<6:10:06,  3.81s/it] 35%|███▍      | 3085/8917 [4:09:27<5:58:38,  3.69s/it] 35%|███▍      | 3086/8917 [4:09:30<5:57:17,  3.68s/it] 35%|███▍      | 3087/8917 [4:09:34<6:00:53,  3.71s/it] 35%|███▍      | 3088/8917 [4:09:38<5:59:47,  3.70s/it] 35%|███▍      | 3089/8917 [4:09:42<6:06:46,  3.78s/it] 35%|███▍      | 3090/8917 [4:09:46<6:02:49,  3.74s/it] 35%|███▍      | 3091/8917 [4:09:49<6:05:16,  3.76s/it] 35%|███▍      | 3092/8917 [4:09:53<6:04:33,  3.76s/it] 35%|███▍      | 3093/8917 [4:09:56<5:52:32,  3.63s/it] 35%|███▍      | 3094/8917 [4:10:00<5:50:38,  3.61s/it] 35%|███▍      | 3095/8917 [4:10:04<5:52:23,  3.63s/it] 35%|███▍      | 3096/8917 [4:10:07<5:52:43,  3.64s/it] 35%|███▍      | 3097/8917 [4:10:11<5:52:06,  3.63s/it] 35%|███▍      | 3098/8917 [4:10:15<5:55:30,  3.67s/it] 35%|███▍      | 3099/8917 [4:10:19<6:00:17,  3.72s/it]09/19/2024 06:24:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02595561556518078, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2015142440795898, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.227469801902771}
 35%|███▍      | 3100/8917 [4:10:22<5:56:01,  3.67s/it] 35%|███▍      | 3101/8917 [4:10:26<6:05:19,  3.77s/it] 35%|███▍      | 3102/8917 [4:10:30<6:02:32,  3.74s/it] 35%|███▍      | 3103/8917 [4:10:33<5:55:51,  3.67s/it] 35%|███▍      | 3104/8917 [4:10:37<5:58:16,  3.70s/it] 35%|███▍      | 3105/8917 [4:10:41<5:58:57,  3.71s/it] 35%|███▍      | 3106/8917 [4:10:44<5:56:12,  3.68s/it] 35%|███▍      | 3107/8917 [4:10:48<5:50:37,  3.62s/it] 35%|███▍      | 3108/8917 [4:10:52<5:56:36,  3.68s/it] 35%|███▍      | 3109/8917 [4:10:56<6:05:46,  3.78s/it] 35%|███▍      | 3110/8917 [4:11:00<6:06:49,  3.79s/it] 35%|███▍      | 3111/8917 [4:11:03<5:58:30,  3.70s/it] 35%|███▍      | 3112/8917 [4:11:07<5:57:45,  3.70s/it] 35%|███▍      | 3113/8917 [4:11:10<5:51:41,  3.64s/it] 35%|███▍      | 3114/8917 [4:11:14<5:56:29,  3.69s/it] 35%|███▍      | 3115/8917 [4:11:18<5:59:53,  3.72s/it] 35%|███▍      | 3116/8917 [4:11:22<6:02:07,  3.75s/it] 35%|███▍      | 3117/8917 [4:11:25<5:57:24,  3.70s/it] 35%|███▍      | 3118/8917 [4:11:29<5:58:49,  3.71s/it] 35%|███▍      | 3119/8917 [4:11:33<6:09:15,  3.82s/it] 35%|███▍      | 3120/8917 [4:11:37<6:02:17,  3.75s/it] 35%|███▌      | 3121/8917 [4:11:40<6:05:08,  3.78s/it] 35%|███▌      | 3122/8917 [4:11:44<5:59:10,  3.72s/it] 35%|███▌      | 3123/8917 [4:11:48<6:04:34,  3.78s/it] 35%|███▌      | 3124/8917 [4:11:51<5:52:30,  3.65s/it] 35%|███▌      | 3125/8917 [4:11:55<5:58:12,  3.71s/it] 35%|███▌      | 3126/8917 [4:11:59<6:02:25,  3.75s/it] 35%|███▌      | 3127/8917 [4:12:03<6:02:05,  3.75s/it] 35%|███▌      | 3128/8917 [4:12:07<6:08:32,  3.82s/it] 35%|███▌      | 3129/8917 [4:12:11<6:07:06,  3.81s/it] 35%|███▌      | 3130/8917 [4:12:14<6:10:50,  3.84s/it] 35%|███▌      | 3131/8917 [4:12:18<6:04:26,  3.78s/it] 35%|███▌      | 3132/8917 [4:12:22<6:01:11,  3.75s/it] 35%|███▌      | 3133/8917 [4:12:25<5:57:43,  3.71s/it] 35%|███▌      | 3134/8917 [4:12:29<6:07:40,  3.81s/it] 35%|███▌      | 3135/8917 [4:12:34<6:19:13,  3.94s/it] 35%|███▌      | 3136/8917 [4:12:37<6:05:17,  3.79s/it] 35%|███▌      | 3137/8917 [4:12:41<6:04:33,  3.78s/it] 35%|███▌      | 3138/8917 [4:12:45<6:02:38,  3.77s/it] 35%|███▌      | 3139/8917 [4:12:48<6:02:40,  3.77s/it] 35%|███▌      | 3140/8917 [4:12:52<5:55:04,  3.69s/it] 35%|███▌      | 3141/8917 [4:12:55<5:47:51,  3.61s/it] 35%|███▌      | 3142/8917 [4:12:59<5:43:03,  3.56s/it] 35%|███▌      | 3143/8917 [4:13:03<5:48:52,  3.63s/it] 35%|███▌      | 3144/8917 [4:13:06<5:55:11,  3.69s/it] 35%|███▌      | 3145/8917 [4:13:10<5:51:23,  3.65s/it] 35%|███▌      | 3146/8917 [4:13:14<5:58:09,  3.72s/it] 35%|███▌      | 3147/8917 [4:13:18<6:03:33,  3.78s/it] 35%|███▌      | 3148/8917 [4:13:21<5:54:45,  3.69s/it] 35%|███▌      | 3149/8917 [4:13:25<6:04:29,  3.79s/it]09/19/2024 06:28:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.038266729563474655, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.376739501953125, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4150062799453735}
 35%|███▌      | 3150/8917 [4:13:29<6:06:39,  3.81s/it] 35%|███▌      | 3151/8917 [4:13:33<5:58:26,  3.73s/it] 35%|███▌      | 3152/8917 [4:13:37<6:05:35,  3.81s/it] 35%|███▌      | 3153/8917 [4:13:40<5:59:53,  3.75s/it] 35%|███▌      | 3154/8917 [4:13:44<6:01:44,  3.77s/it] 35%|███▌      | 3155/8917 [4:13:48<6:00:23,  3.75s/it] 35%|███▌      | 3156/8917 [4:13:52<6:01:22,  3.76s/it] 35%|███▌      | 3157/8917 [4:13:55<6:00:23,  3.75s/it] 35%|███▌      | 3158/8917 [4:13:59<6:09:27,  3.85s/it] 35%|███▌      | 3159/8917 [4:14:03<5:54:03,  3.69s/it] 35%|███▌      | 3160/8917 [4:14:06<5:52:52,  3.68s/it] 35%|███▌      | 3161/8917 [4:14:10<5:53:19,  3.68s/it] 35%|███▌      | 3162/8917 [4:14:14<5:52:41,  3.68s/it] 35%|███▌      | 3163/8917 [4:14:17<5:53:32,  3.69s/it] 35%|███▌      | 3164/8917 [4:14:21<6:00:53,  3.76s/it] 35%|███▌      | 3165/8917 [4:14:25<6:02:27,  3.78s/it] 36%|███▌      | 3166/8917 [4:14:29<5:56:21,  3.72s/it] 36%|███▌      | 3167/8917 [4:14:32<5:50:12,  3.65s/it] 36%|███▌      | 3168/8917 [4:14:36<5:51:24,  3.67s/it] 36%|███▌      | 3169/8917 [4:14:40<5:52:50,  3.68s/it] 36%|███▌      | 3170/8917 [4:14:43<5:55:29,  3.71s/it] 36%|███▌      | 3171/8917 [4:14:47<5:51:06,  3.67s/it] 36%|███▌      | 3172/8917 [4:14:51<6:01:32,  3.78s/it] 36%|███▌      | 3173/8917 [4:14:55<5:56:54,  3.73s/it] 36%|███▌      | 3174/8917 [4:14:58<5:55:24,  3.71s/it] 36%|███▌      | 3175/8917 [4:15:02<5:54:44,  3.71s/it] 36%|███▌      | 3176/8917 [4:15:06<5:54:16,  3.70s/it] 36%|███▌      | 3177/8917 [4:15:09<5:56:47,  3.73s/it] 36%|███▌      | 3178/8917 [4:15:13<6:03:11,  3.80s/it] 36%|███▌      | 3179/8917 [4:15:17<5:59:30,  3.76s/it] 36%|███▌      | 3180/8917 [4:15:21<6:00:48,  3.77s/it] 36%|███▌      | 3181/8917 [4:15:25<6:07:55,  3.85s/it] 36%|███▌      | 3182/8917 [4:15:28<5:49:42,  3.66s/it] 36%|███▌      | 3183/8917 [4:15:32<5:58:33,  3.75s/it] 36%|███▌      | 3184/8917 [4:15:36<6:00:31,  3.77s/it] 36%|███▌      | 3185/8917 [4:15:40<5:55:13,  3.72s/it] 36%|███▌      | 3186/8917 [4:15:44<6:04:59,  3.82s/it] 36%|███▌      | 3187/8917 [4:15:47<6:03:06,  3.80s/it] 36%|███▌      | 3188/8917 [4:15:51<5:59:26,  3.76s/it] 36%|███▌      | 3189/8917 [4:15:55<5:52:05,  3.69s/it] 36%|███▌      | 3190/8917 [4:15:58<5:46:55,  3.63s/it] 36%|███▌      | 3191/8917 [4:16:02<6:06:10,  3.84s/it] 36%|███▌      | 3192/8917 [4:16:06<6:00:55,  3.78s/it] 36%|███▌      | 3193/8917 [4:16:10<5:57:17,  3.75s/it] 36%|███▌      | 3194/8917 [4:16:14<6:02:50,  3.80s/it] 36%|███▌      | 3195/8917 [4:16:17<5:54:04,  3.71s/it] 36%|███▌      | 3196/8917 [4:16:21<5:50:43,  3.68s/it] 36%|███▌      | 3197/8917 [4:16:25<5:58:06,  3.76s/it] 36%|███▌      | 3198/8917 [4:16:28<6:00:38,  3.78s/it] 36%|███▌      | 3199/8917 [4:16:32<6:03:59,  3.82s/it]09/19/2024 06:31:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04140698164701462, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.596705436706543, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.6381124258041382}
 36%|███▌      | 3200/8917 [4:16:36<5:56:50,  3.75s/it] 36%|███▌      | 3201/8917 [4:16:40<5:58:45,  3.77s/it] 36%|███▌      | 3202/8917 [4:16:43<5:56:39,  3.74s/it] 36%|███▌      | 3203/8917 [4:16:47<5:47:40,  3.65s/it] 36%|███▌      | 3204/8917 [4:16:51<5:53:42,  3.71s/it] 36%|███▌      | 3205/8917 [4:16:55<5:56:15,  3.74s/it] 36%|███▌      | 3206/8917 [4:16:58<5:57:58,  3.76s/it] 36%|███▌      | 3207/8917 [4:17:02<5:55:19,  3.73s/it] 36%|███▌      | 3208/8917 [4:17:06<5:52:13,  3.70s/it] 36%|███▌      | 3209/8917 [4:17:09<5:49:05,  3.67s/it] 36%|███▌      | 3210/8917 [4:17:13<5:46:00,  3.64s/it] 36%|███▌      | 3211/8917 [4:17:16<5:45:18,  3.63s/it] 36%|███▌      | 3212/8917 [4:17:20<5:44:56,  3.63s/it] 36%|███▌      | 3213/8917 [4:17:24<5:52:17,  3.71s/it] 36%|███▌      | 3214/8917 [4:17:28<5:55:17,  3.74s/it] 36%|███▌      | 3215/8917 [4:17:31<5:51:07,  3.69s/it] 36%|███▌      | 3216/8917 [4:17:35<5:51:12,  3.70s/it] 36%|███▌      | 3217/8917 [4:17:39<6:01:03,  3.80s/it] 36%|███▌      | 3218/8917 [4:17:43<6:07:15,  3.87s/it] 36%|███▌      | 3219/8917 [4:17:47<5:59:27,  3.79s/it] 36%|███▌      | 3220/8917 [4:17:50<5:54:17,  3.73s/it] 36%|███▌      | 3221/8917 [4:17:54<5:49:54,  3.69s/it] 36%|███▌      | 3222/8917 [4:17:58<6:00:15,  3.80s/it] 36%|███▌      | 3223/8917 [4:18:02<5:56:46,  3.76s/it] 36%|███▌      | 3224/8917 [4:18:05<5:49:22,  3.68s/it] 36%|███▌      | 3225/8917 [4:18:09<5:50:31,  3.69s/it] 36%|███▌      | 3226/8917 [4:18:12<5:41:41,  3.60s/it] 36%|███▌      | 3227/8917 [4:18:16<5:51:50,  3.71s/it] 36%|███▌      | 3228/8917 [4:18:20<5:53:57,  3.73s/it] 36%|███▌      | 3229/8917 [4:18:24<5:57:10,  3.77s/it] 36%|███▌      | 3230/8917 [4:18:28<6:03:59,  3.84s/it] 36%|███▌      | 3231/8917 [4:18:32<6:10:54,  3.91s/it] 36%|███▌      | 3232/8917 [4:18:36<6:07:25,  3.88s/it] 36%|███▋      | 3233/8917 [4:18:39<5:58:17,  3.78s/it] 36%|███▋      | 3234/8917 [4:18:43<5:48:16,  3.68s/it] 36%|███▋      | 3235/8917 [4:18:46<5:46:03,  3.65s/it] 36%|███▋      | 3236/8917 [4:18:50<5:44:59,  3.64s/it] 36%|███▋      | 3237/8917 [4:18:54<5:45:44,  3.65s/it] 36%|███▋      | 3238/8917 [4:18:58<5:53:57,  3.74s/it] 36%|███▋      | 3239/8917 [4:19:01<5:47:23,  3.67s/it] 36%|███▋      | 3240/8917 [4:19:05<5:48:51,  3.69s/it] 36%|███▋      | 3241/8917 [4:19:08<5:48:53,  3.69s/it] 36%|███▋      | 3242/8917 [4:19:12<5:46:40,  3.67s/it] 36%|███▋      | 3243/8917 [4:19:16<5:52:16,  3.73s/it] 36%|███▋      | 3244/8917 [4:19:20<5:59:23,  3.80s/it] 36%|███▋      | 3245/8917 [4:19:24<6:05:03,  3.86s/it] 36%|███▋      | 3246/8917 [4:19:27<5:53:43,  3.74s/it] 36%|███▋      | 3247/8917 [4:19:31<5:57:50,  3.79s/it] 36%|███▋      | 3248/8917 [4:19:35<5:45:12,  3.65s/it] 36%|███▋      | 3249/8917 [4:19:38<5:44:14,  3.64s/it]09/19/2024 06:34:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.033687200397253036, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4152562618255615, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4489434957504272}
 36%|███▋      | 3250/8917 [4:19:42<5:38:39,  3.59s/it] 36%|███▋      | 3251/8917 [4:19:46<6:11:06,  3.93s/it] 36%|███▋      | 3252/8917 [4:19:50<6:01:58,  3.83s/it] 36%|███▋      | 3253/8917 [4:19:54<5:59:40,  3.81s/it] 36%|███▋      | 3254/8917 [4:19:58<6:08:40,  3.91s/it] 37%|███▋      | 3255/8917 [4:20:02<5:59:40,  3.81s/it] 37%|███▋      | 3256/8917 [4:20:05<5:49:10,  3.70s/it] 37%|███▋      | 3257/8917 [4:20:09<5:44:11,  3.65s/it] 37%|███▋      | 3258/8917 [4:20:12<5:53:11,  3.74s/it] 37%|███▋      | 3259/8917 [4:20:16<5:54:32,  3.76s/it] 37%|███▋      | 3260/8917 [4:20:20<5:50:36,  3.72s/it] 37%|███▋      | 3261/8917 [4:20:24<5:58:44,  3.81s/it] 37%|███▋      | 3262/8917 [4:20:28<6:04:47,  3.87s/it] 37%|███▋      | 3263/8917 [4:20:31<5:50:15,  3.72s/it] 37%|███▋      | 3264/8917 [4:20:35<5:51:27,  3.73s/it] 37%|███▋      | 3265/8917 [4:20:39<5:47:37,  3.69s/it] 37%|███▋      | 3266/8917 [4:20:42<5:49:58,  3.72s/it] 37%|███▋      | 3267/8917 [4:20:46<5:55:35,  3.78s/it] 37%|███▋      | 3268/8917 [4:20:50<6:00:13,  3.83s/it] 37%|███▋      | 3269/8917 [4:20:54<6:01:59,  3.85s/it] 37%|███▋      | 3270/8917 [4:20:58<5:55:21,  3.78s/it] 37%|███▋      | 3271/8917 [4:21:01<5:52:57,  3.75s/it] 37%|███▋      | 3272/8917 [4:21:05<5:49:41,  3.72s/it] 37%|███▋      | 3273/8917 [4:21:09<5:57:34,  3.80s/it] 37%|███▋      | 3274/8917 [4:21:12<5:42:41,  3.64s/it] 37%|███▋      | 3275/8917 [4:21:16<5:49:15,  3.71s/it] 37%|███▋      | 3276/8917 [4:21:20<5:55:44,  3.78s/it] 37%|███▋      | 3277/8917 [4:21:24<5:54:09,  3.77s/it] 37%|███▋      | 3278/8917 [4:21:28<5:51:43,  3.74s/it] 37%|███▋      | 3279/8917 [4:21:32<5:59:16,  3.82s/it] 37%|███▋      | 3280/8917 [4:21:35<5:50:32,  3.73s/it] 37%|███▋      | 3281/8917 [4:21:39<5:51:37,  3.74s/it] 37%|███▋      | 3282/8917 [4:21:43<5:48:41,  3.71s/it] 37%|███▋      | 3283/8917 [4:21:47<5:57:30,  3.81s/it] 37%|███▋      | 3284/8917 [4:21:50<5:50:37,  3.73s/it] 37%|███▋      | 3285/8917 [4:21:54<5:53:04,  3.76s/it] 37%|███▋      | 3286/8917 [4:21:58<5:53:34,  3.77s/it] 37%|███▋      | 3287/8917 [4:22:02<5:54:56,  3.78s/it] 37%|███▋      | 3288/8917 [4:22:06<6:02:47,  3.87s/it] 37%|███▋      | 3289/8917 [4:22:09<5:56:11,  3.80s/it] 37%|███▋      | 3290/8917 [4:22:13<5:57:16,  3.81s/it] 37%|███▋      | 3291/8917 [4:22:17<5:51:01,  3.74s/it] 37%|███▋      | 3292/8917 [4:22:20<5:47:53,  3.71s/it] 37%|███▋      | 3293/8917 [4:22:24<5:48:59,  3.72s/it] 37%|███▋      | 3294/8917 [4:22:28<5:43:26,  3.66s/it] 37%|███▋      | 3295/8917 [4:22:31<5:46:37,  3.70s/it] 37%|███▋      | 3296/8917 [4:22:35<5:48:54,  3.72s/it] 37%|███▋      | 3297/8917 [4:22:39<5:48:09,  3.72s/it] 37%|███▋      | 3298/8917 [4:22:43<5:52:43,  3.77s/it] 37%|███▋      | 3299/8917 [4:22:47<5:57:31,  3.82s/it]09/19/2024 06:37:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03825737163424492, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6840879917144775, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7223453521728516}
 37%|███▋      | 3300/8917 [4:22:50<5:50:24,  3.74s/it] 37%|███▋      | 3301/8917 [4:22:54<5:48:08,  3.72s/it] 37%|███▋      | 3302/8917 [4:22:58<5:44:40,  3.68s/it] 37%|███▋      | 3303/8917 [4:23:01<5:39:21,  3.63s/it] 37%|███▋      | 3304/8917 [4:23:05<5:42:30,  3.66s/it] 37%|███▋      | 3305/8917 [4:23:09<5:48:28,  3.73s/it] 37%|███▋      | 3306/8917 [4:23:12<5:49:34,  3.74s/it] 37%|███▋      | 3307/8917 [4:23:16<5:54:27,  3.79s/it] 37%|███▋      | 3308/8917 [4:23:20<5:55:17,  3.80s/it] 37%|███▋      | 3309/8917 [4:23:24<6:03:43,  3.89s/it] 37%|███▋      | 3310/8917 [4:23:28<5:57:27,  3.83s/it] 37%|███▋      | 3311/8917 [4:23:31<5:48:29,  3.73s/it] 37%|███▋      | 3312/8917 [4:23:35<5:50:19,  3.75s/it] 37%|███▋      | 3313/8917 [4:23:39<5:47:37,  3.72s/it] 37%|███▋      | 3314/8917 [4:23:43<5:45:11,  3.70s/it] 37%|███▋      | 3315/8917 [4:23:46<5:52:42,  3.78s/it] 37%|███▋      | 3316/8917 [4:23:50<5:49:17,  3.74s/it] 37%|███▋      | 3317/8917 [4:23:54<5:53:01,  3.78s/it] 37%|███▋      | 3318/8917 [4:23:58<5:45:10,  3.70s/it] 37%|███▋      | 3319/8917 [4:24:01<5:49:49,  3.75s/it] 37%|███▋      | 3320/8917 [4:24:05<5:45:36,  3.70s/it] 37%|███▋      | 3321/8917 [4:24:09<5:46:17,  3.71s/it] 37%|███▋      | 3322/8917 [4:24:12<5:39:05,  3.64s/it] 37%|███▋      | 3323/8917 [4:24:16<5:41:35,  3.66s/it] 37%|███▋      | 3324/8917 [4:24:19<5:39:03,  3.64s/it] 37%|███▋      | 3325/8917 [4:24:23<5:41:29,  3.66s/it] 37%|███▋      | 3326/8917 [4:24:27<5:47:07,  3.73s/it] 37%|███▋      | 3327/8917 [4:24:30<5:36:30,  3.61s/it] 37%|███▋      | 3328/8917 [4:24:34<5:43:36,  3.69s/it] 37%|███▋      | 3329/8917 [4:24:38<5:54:35,  3.81s/it] 37%|███▋      | 3330/8917 [4:24:42<5:47:17,  3.73s/it] 37%|███▋      | 3331/8917 [4:24:46<5:53:33,  3.80s/it] 37%|███▋      | 3332/8917 [4:24:49<5:41:37,  3.67s/it] 37%|███▋      | 3333/8917 [4:24:53<5:38:42,  3.64s/it] 37%|███▋      | 3334/8917 [4:24:57<5:39:49,  3.65s/it] 37%|███▋      | 3335/8917 [4:25:00<5:47:13,  3.73s/it] 37%|███▋      | 3336/8917 [4:25:04<5:45:39,  3.72s/it] 37%|███▋      | 3337/8917 [4:25:08<5:40:38,  3.66s/it] 37%|███▋      | 3338/8917 [4:25:12<5:48:35,  3.75s/it] 37%|███▋      | 3339/8917 [4:25:15<5:51:56,  3.79s/it] 37%|███▋      | 3340/8917 [4:25:19<5:47:37,  3.74s/it] 37%|███▋      | 3341/8917 [4:25:23<5:58:34,  3.86s/it] 37%|███▋      | 3342/8917 [4:25:27<6:01:33,  3.89s/it] 37%|███▋      | 3343/8917 [4:25:31<5:56:04,  3.83s/it] 38%|███▊      | 3344/8917 [4:25:35<5:50:55,  3.78s/it] 38%|███▊      | 3345/8917 [4:25:38<5:39:50,  3.66s/it] 38%|███▊      | 3346/8917 [4:25:42<5:43:44,  3.70s/it] 38%|███▊      | 3347/8917 [4:25:45<5:38:46,  3.65s/it] 38%|███▊      | 3348/8917 [4:25:49<5:33:19,  3.59s/it] 38%|███▊      | 3349/8917 [4:25:53<5:46:07,  3.73s/it]09/19/2024 06:40:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024255137890577316, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.131507396697998, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1557625532150269}
 38%|███▊      | 3350/8917 [4:25:57<5:46:42,  3.74s/it] 38%|███▊      | 3351/8917 [4:26:00<5:48:36,  3.76s/it] 38%|███▊      | 3352/8917 [4:26:04<5:41:17,  3.68s/it] 38%|███▊      | 3353/8917 [4:26:08<5:41:50,  3.69s/it] 38%|███▊      | 3354/8917 [4:26:11<5:46:23,  3.74s/it] 38%|███▊      | 3355/8917 [4:26:15<5:41:49,  3.69s/it] 38%|███▊      | 3356/8917 [4:26:19<5:45:24,  3.73s/it] 38%|███▊      | 3357/8917 [4:26:23<5:45:43,  3.73s/it] 38%|███▊      | 3358/8917 [4:26:26<5:50:49,  3.79s/it] 38%|███▊      | 3359/8917 [4:26:30<5:54:54,  3.83s/it] 38%|███▊      | 3360/8917 [4:26:34<5:44:44,  3.72s/it] 38%|███▊      | 3361/8917 [4:26:38<5:48:06,  3.76s/it] 38%|███▊      | 3362/8917 [4:26:41<5:47:56,  3.76s/it] 38%|███▊      | 3363/8917 [4:26:45<5:49:22,  3.77s/it] 38%|███▊      | 3364/8917 [4:26:49<6:02:03,  3.91s/it] 38%|███▊      | 3365/8917 [4:26:53<5:56:15,  3.85s/it] 38%|███▊      | 3366/8917 [4:26:57<5:47:27,  3.76s/it] 38%|███▊      | 3367/8917 [4:27:00<5:41:40,  3.69s/it] 38%|███▊      | 3368/8917 [4:27:04<5:39:45,  3.67s/it] 38%|███▊      | 3369/8917 [4:27:08<5:40:47,  3.69s/it] 38%|███▊      | 3370/8917 [4:27:11<5:41:24,  3.69s/it] 38%|███▊      | 3371/8917 [4:27:15<5:38:05,  3.66s/it] 38%|███▊      | 3372/8917 [4:27:19<5:41:22,  3.69s/it] 38%|███▊      | 3373/8917 [4:27:22<5:36:07,  3.64s/it] 38%|███▊      | 3374/8917 [4:27:26<5:42:13,  3.70s/it] 38%|███▊      | 3375/8917 [4:27:30<5:35:48,  3.64s/it] 38%|███▊      | 3376/8917 [4:27:34<5:56:15,  3.86s/it] 38%|███▊      | 3377/8917 [4:27:38<5:58:35,  3.88s/it] 38%|███▊      | 3378/8917 [4:27:42<5:57:10,  3.87s/it] 38%|███▊      | 3379/8917 [4:27:46<5:55:51,  3.86s/it] 38%|███▊      | 3380/8917 [4:27:49<5:58:44,  3.89s/it] 38%|███▊      | 3381/8917 [4:27:53<5:47:51,  3.77s/it] 38%|███▊      | 3382/8917 [4:27:57<5:45:29,  3.75s/it] 38%|███▊      | 3383/8917 [4:28:00<5:47:24,  3.77s/it] 38%|███▊      | 3384/8917 [4:28:04<5:48:45,  3.78s/it] 38%|███▊      | 3385/8917 [4:28:08<5:46:44,  3.76s/it] 38%|███▊      | 3386/8917 [4:28:11<5:39:13,  3.68s/it] 38%|███▊      | 3387/8917 [4:28:16<6:01:13,  3.92s/it] 38%|███▊      | 3388/8917 [4:28:20<6:00:40,  3.91s/it] 38%|███▊      | 3389/8917 [4:28:23<5:44:02,  3.73s/it] 38%|███▊      | 3390/8917 [4:28:27<5:36:48,  3.66s/it] 38%|███▊      | 3391/8917 [4:28:31<5:54:16,  3.85s/it] 38%|███▊      | 3392/8917 [4:28:35<5:51:02,  3.81s/it] 38%|███▊      | 3393/8917 [4:28:38<5:45:00,  3.75s/it] 38%|███▊      | 3394/8917 [4:28:42<5:42:54,  3.73s/it] 38%|███▊      | 3395/8917 [4:28:46<5:38:10,  3.67s/it] 38%|███▊      | 3396/8917 [4:28:49<5:44:13,  3.74s/it] 38%|███▊      | 3397/8917 [4:28:53<5:37:36,  3.67s/it] 38%|███▊      | 3398/8917 [4:28:57<5:36:23,  3.66s/it] 38%|███▊      | 3399/8917 [4:29:00<5:38:19,  3.68s/it]09/19/2024 06:43:38 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.033079855144023895, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3780922889709473, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4111721515655518}
 38%|███▊      | 3400/8917 [4:29:04<5:38:27,  3.68s/it] 38%|███▊      | 3401/8917 [4:29:07<5:33:53,  3.63s/it] 38%|███▊      | 3402/8917 [4:29:11<5:43:33,  3.74s/it] 38%|███▊      | 3403/8917 [4:29:15<5:36:47,  3.66s/it] 38%|███▊      | 3404/8917 [4:29:19<5:34:15,  3.64s/it] 38%|███▊      | 3405/8917 [4:29:22<5:34:25,  3.64s/it] 38%|███▊      | 3406/8917 [4:29:26<5:41:35,  3.72s/it] 38%|███▊      | 3407/8917 [4:29:30<5:42:22,  3.73s/it] 38%|███▊      | 3408/8917 [4:29:34<5:43:43,  3.74s/it] 38%|███▊      | 3409/8917 [4:29:38<5:49:01,  3.80s/it] 38%|███▊      | 3410/8917 [4:29:41<5:47:23,  3.78s/it] 38%|███▊      | 3411/8917 [4:29:45<5:42:31,  3.73s/it] 38%|███▊      | 3412/8917 [4:29:49<5:51:06,  3.83s/it] 38%|███▊      | 3413/8917 [4:29:53<5:51:32,  3.83s/it] 38%|███▊      | 3414/8917 [4:29:56<5:42:09,  3.73s/it] 38%|███▊      | 3415/8917 [4:30:01<5:58:47,  3.91s/it] 38%|███▊      | 3416/8917 [4:30:04<5:51:33,  3.83s/it] 38%|███▊      | 3417/8917 [4:30:08<5:41:46,  3.73s/it] 38%|███▊      | 3418/8917 [4:30:11<5:37:32,  3.68s/it] 38%|███▊      | 3419/8917 [4:30:15<5:29:42,  3.60s/it] 38%|███▊      | 3420/8917 [4:30:19<5:42:10,  3.73s/it] 38%|███▊      | 3421/8917 [4:30:23<5:46:28,  3.78s/it] 38%|███▊      | 3422/8917 [4:30:26<5:43:49,  3.75s/it] 38%|███▊      | 3423/8917 [4:30:30<5:36:02,  3.67s/it] 38%|███▊      | 3424/8917 [4:30:34<5:40:31,  3.72s/it] 38%|███▊      | 3425/8917 [4:30:37<5:39:16,  3.71s/it] 38%|███▊      | 3426/8917 [4:30:41<5:35:08,  3.66s/it] 38%|███▊      | 3427/8917 [4:30:45<5:42:28,  3.74s/it] 38%|███▊      | 3428/8917 [4:30:49<5:50:28,  3.83s/it] 38%|███▊      | 3429/8917 [4:30:53<5:45:48,  3.78s/it] 38%|███▊      | 3430/8917 [4:30:56<5:45:23,  3.78s/it] 38%|███▊      | 3431/8917 [4:31:00<5:35:00,  3.66s/it] 38%|███▊      | 3432/8917 [4:31:03<5:37:26,  3.69s/it] 38%|███▊      | 3433/8917 [4:31:07<5:35:37,  3.67s/it] 39%|███▊      | 3434/8917 [4:31:11<5:38:18,  3.70s/it] 39%|███▊      | 3435/8917 [4:31:15<5:44:49,  3.77s/it] 39%|███▊      | 3436/8917 [4:31:19<5:45:23,  3.78s/it] 39%|███▊      | 3437/8917 [4:31:23<5:48:49,  3.82s/it] 39%|███▊      | 3438/8917 [4:31:26<5:37:46,  3.70s/it] 39%|███▊      | 3439/8917 [4:31:29<5:32:18,  3.64s/it] 39%|███▊      | 3440/8917 [4:31:33<5:29:17,  3.61s/it] 39%|███▊      | 3441/8917 [4:31:37<5:47:26,  3.81s/it] 39%|███▊      | 3442/8917 [4:31:41<5:52:37,  3.86s/it] 39%|███▊      | 3443/8917 [4:31:45<5:42:13,  3.75s/it] 39%|███▊      | 3444/8917 [4:31:48<5:42:26,  3.75s/it] 39%|███▊      | 3445/8917 [4:31:52<5:38:16,  3.71s/it] 39%|███▊      | 3446/8917 [4:31:56<5:41:37,  3.75s/it] 39%|███▊      | 3447/8917 [4:32:00<5:41:41,  3.75s/it] 39%|███▊      | 3448/8917 [4:32:03<5:43:39,  3.77s/it] 39%|███▊      | 3449/8917 [4:32:07<5:35:30,  3.68s/it]09/19/2024 06:46:45 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018775101751089096, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.383538842201233, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4023139476776123}
 39%|███▊      | 3450/8917 [4:32:11<5:39:50,  3.73s/it] 39%|███▊      | 3451/8917 [4:32:15<5:40:01,  3.73s/it] 39%|███▊      | 3452/8917 [4:32:19<5:46:20,  3.80s/it] 39%|███▊      | 3453/8917 [4:32:22<5:37:39,  3.71s/it] 39%|███▊      | 3454/8917 [4:32:25<5:31:19,  3.64s/it] 39%|███▊      | 3455/8917 [4:32:29<5:38:48,  3.72s/it] 39%|███▉      | 3456/8917 [4:32:33<5:45:45,  3.80s/it] 39%|███▉      | 3457/8917 [4:32:37<5:38:36,  3.72s/it] 39%|███▉      | 3458/8917 [4:32:40<5:24:31,  3.57s/it] 39%|███▉      | 3459/8917 [4:32:44<5:40:35,  3.74s/it] 39%|███▉      | 3460/8917 [4:32:48<5:42:56,  3.77s/it] 39%|███▉      | 3461/8917 [4:32:52<5:38:52,  3.73s/it] 39%|███▉      | 3462/8917 [4:32:55<5:36:13,  3.70s/it] 39%|███▉      | 3463/8917 [4:32:59<5:40:28,  3.75s/it] 39%|███▉      | 3464/8917 [4:33:03<5:42:07,  3.76s/it] 39%|███▉      | 3465/8917 [4:33:07<5:41:33,  3.76s/it] 39%|███▉      | 3466/8917 [4:33:10<5:35:21,  3.69s/it] 39%|███▉      | 3467/8917 [4:33:14<5:35:02,  3.69s/it] 39%|███▉      | 3468/8917 [4:33:17<5:29:55,  3.63s/it] 39%|███▉      | 3469/8917 [4:33:21<5:34:53,  3.69s/it] 39%|███▉      | 3470/8917 [4:33:25<5:31:31,  3.65s/it] 39%|███▉      | 3471/8917 [4:33:29<5:36:45,  3.71s/it] 39%|███▉      | 3472/8917 [4:33:32<5:34:03,  3.68s/it] 39%|███▉      | 3473/8917 [4:33:36<5:27:54,  3.61s/it] 39%|███▉      | 3474/8917 [4:33:40<5:35:41,  3.70s/it] 39%|███▉      | 3475/8917 [4:33:43<5:28:06,  3.62s/it] 39%|███▉      | 3476/8917 [4:33:47<5:30:31,  3.64s/it] 39%|███▉      | 3477/8917 [4:33:51<5:43:05,  3.78s/it] 39%|███▉      | 3478/8917 [4:33:55<5:37:17,  3.72s/it] 39%|███▉      | 3479/8917 [4:33:58<5:34:11,  3.69s/it] 39%|███▉      | 3480/8917 [4:34:02<5:30:49,  3.65s/it] 39%|███▉      | 3481/8917 [4:34:06<5:41:33,  3.77s/it] 39%|███▉      | 3482/8917 [4:34:10<5:42:09,  3.78s/it] 39%|███▉      | 3483/8917 [4:34:13<5:39:13,  3.75s/it] 39%|███▉      | 3484/8917 [4:34:17<5:53:02,  3.90s/it] 39%|███▉      | 3485/8917 [4:34:21<5:47:04,  3.83s/it] 39%|███▉      | 3486/8917 [4:34:25<5:43:21,  3.79s/it] 39%|███▉      | 3487/8917 [4:34:28<5:29:39,  3.64s/it] 39%|███▉      | 3488/8917 [4:34:32<5:33:47,  3.69s/it] 39%|███▉      | 3489/8917 [4:34:36<5:30:44,  3.66s/it] 39%|███▉      | 3490/8917 [4:34:39<5:31:31,  3.67s/it] 39%|███▉      | 3491/8917 [4:34:43<5:30:27,  3.65s/it] 39%|███▉      | 3492/8917 [4:34:46<5:26:52,  3.62s/it] 39%|███▉      | 3493/8917 [4:34:50<5:35:48,  3.71s/it] 39%|███▉      | 3494/8917 [4:34:54<5:33:55,  3.69s/it] 39%|███▉      | 3495/8917 [4:34:57<5:22:58,  3.57s/it] 39%|███▉      | 3496/8917 [4:35:01<5:30:57,  3.66s/it] 39%|███▉      | 3497/8917 [4:35:05<5:34:01,  3.70s/it] 39%|███▉      | 3498/8917 [4:35:08<5:27:29,  3.63s/it] 39%|███▉      | 3499/8917 [4:35:12<5:33:29,  3.69s/it]09/19/2024 06:49:49 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 06:49:49 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<02:47,  1.31it/s][A
  1%|          | 2/221 [00:01<02:37,  1.39it/s][A
  1%|▏         | 3/221 [00:02<02:40,  1.35it/s][A
  2%|▏         | 4/221 [00:02<02:05,  1.72it/s][A
  3%|▎         | 6/221 [00:02<01:08,  3.12it/s][A
  3%|▎         | 7/221 [00:02<01:05,  3.27it/s][A
  4%|▎         | 8/221 [00:03<01:07,  3.15it/s][A
  4%|▍         | 9/221 [00:03<01:13,  2.87it/s][A
  5%|▍         | 10/221 [00:04<01:08,  3.08it/s][A
  5%|▌         | 12/221 [00:11<06:42,  1.93s/it][A
  6%|▌         | 13/221 [00:11<05:10,  1.49s/it][A
  7%|▋         | 15/221 [00:12<03:17,  1.04it/s][A
  7%|▋         | 16/221 [00:12<03:02,  1.12it/s][A
  8%|▊         | 17/221 [00:13<03:08,  1.08it/s][A
  8%|▊         | 18/221 [00:14<02:36,  1.29it/s][A
  9%|▊         | 19/221 [00:15<02:44,  1.23it/s][A
  9%|▉         | 20/221 [00:15<02:03,  1.63it/s][A
 10%|▉         | 21/221 [00:15<01:38,  2.03it/s][A
 10%|▉         | 22/221 [00:16<02:08,  1.55it/s][A
 11%|█         | 24/221 [00:16<01:22,  2.40it/s][A
 11%|█▏        | 25/221 [00:17<01:20,  2.42it/s][A
 12%|█▏        | 26/221 [00:17<01:09,  2.79it/s][A
 13%|█▎        | 28/221 [00:18<01:10,  2.76it/s][A
 13%|█▎        | 29/221 [00:18<01:07,  2.84it/s][A
 14%|█▎        | 30/221 [00:19<01:26,  2.21it/s][A
 14%|█▍        | 31/221 [00:19<01:26,  2.19it/s][A
 15%|█▍        | 33/221 [00:20<01:07,  2.79it/s][A
 16%|█▌        | 35/221 [00:20<00:55,  3.35it/s][A
 16%|█▋        | 36/221 [00:20<00:52,  3.52it/s][A
 17%|█▋        | 37/221 [00:21<00:54,  3.35it/s][A
 17%|█▋        | 38/221 [00:21<01:05,  2.81it/s][A
 18%|█▊        | 39/221 [00:21<01:02,  2.91it/s][A
 18%|█▊        | 40/221 [00:22<01:08,  2.65it/s][A
 19%|█▊        | 41/221 [00:22<00:55,  3.22it/s][A
 19%|█▉        | 42/221 [00:22<00:50,  3.54it/s][A
 19%|█▉        | 43/221 [00:22<00:43,  4.11it/s][A
 20%|█▉        | 44/221 [00:23<00:37,  4.70it/s][A
 20%|██        | 45/221 [00:28<05:02,  1.72s/it][A
 21%|██        | 46/221 [00:28<03:46,  1.29s/it][A
 21%|██▏       | 47/221 [00:29<03:10,  1.10s/it][A
 22%|██▏       | 48/221 [00:29<02:18,  1.25it/s][A
 22%|██▏       | 49/221 [00:29<01:47,  1.60it/s][A
 23%|██▎       | 50/221 [00:30<01:34,  1.80it/s][A
 23%|██▎       | 51/221 [00:30<01:19,  2.15it/s][A
 24%|██▍       | 53/221 [00:30<00:57,  2.95it/s][A
 24%|██▍       | 54/221 [00:31<01:07,  2.48it/s][A
 25%|██▍       | 55/221 [00:32<01:43,  1.60it/s][A
 25%|██▌       | 56/221 [00:32<01:22,  2.00it/s][A
 26%|██▌       | 57/221 [00:32<01:12,  2.25it/s][A
 27%|██▋       | 59/221 [00:33<00:46,  3.52it/s][A
 27%|██▋       | 60/221 [00:33<00:51,  3.14it/s][A
 28%|██▊       | 61/221 [00:33<00:46,  3.43it/s][A
 28%|██▊       | 62/221 [00:34<00:46,  3.44it/s][A
 29%|██▊       | 63/221 [00:34<00:53,  2.95it/s][A
 29%|██▉       | 64/221 [00:35<01:14,  2.12it/s][A
 29%|██▉       | 65/221 [00:35<00:57,  2.71it/s][A
 30%|██▉       | 66/221 [00:35<00:58,  2.63it/s][A
 30%|███       | 67/221 [00:36<00:47,  3.23it/s][A
 31%|███       | 68/221 [00:36<00:41,  3.71it/s][A
 31%|███       | 69/221 [00:37<01:41,  1.49it/s][A
 32%|███▏      | 70/221 [00:37<01:16,  1.97it/s][A
 32%|███▏      | 71/221 [00:38<01:11,  2.10it/s][A
 33%|███▎      | 72/221 [00:38<01:01,  2.43it/s][A
 33%|███▎      | 73/221 [00:39<01:03,  2.33it/s][A
 33%|███▎      | 74/221 [00:39<00:49,  2.99it/s][A
 34%|███▍      | 75/221 [00:39<00:47,  3.08it/s][A
 34%|███▍      | 76/221 [00:39<00:49,  2.95it/s][A
 35%|███▍      | 77/221 [00:41<01:35,  1.51it/s][A
 35%|███▌      | 78/221 [00:41<01:13,  1.95it/s][A
 36%|███▌      | 79/221 [00:42<01:39,  1.42it/s][A
 36%|███▌      | 80/221 [00:42<01:13,  1.91it/s][A
 37%|███▋      | 81/221 [00:43<01:23,  1.68it/s][A
 37%|███▋      | 82/221 [00:46<03:03,  1.32s/it][A
 38%|███▊      | 83/221 [00:47<02:31,  1.10s/it][A
 38%|███▊      | 84/221 [00:47<01:52,  1.22it/s][A
 39%|███▉      | 86/221 [00:47<01:16,  1.76it/s][A
 39%|███▉      | 87/221 [00:48<01:21,  1.64it/s][A
 40%|███▉      | 88/221 [00:48<01:09,  1.90it/s][A
 40%|████      | 89/221 [00:49<01:00,  2.17it/s][A
 41%|████      | 90/221 [00:49<00:55,  2.34it/s][A
 42%|████▏     | 92/221 [00:49<00:39,  3.28it/s][A
 42%|████▏     | 93/221 [00:50<00:41,  3.07it/s][A
 43%|████▎     | 94/221 [00:51<01:14,  1.70it/s][A
 43%|████▎     | 95/221 [00:51<01:02,  2.01it/s][A
 43%|████▎     | 96/221 [00:52<01:18,  1.59it/s][A
 44%|████▍     | 97/221 [00:52<01:00,  2.04it/s][A
 44%|████▍     | 98/221 [00:53<01:08,  1.78it/s][A
 45%|████▍     | 99/221 [00:53<00:59,  2.06it/s][A
 45%|████▌     | 100/221 [00:54<01:06,  1.81it/s][A
 46%|████▌     | 101/221 [00:54<00:51,  2.34it/s][A
 46%|████▌     | 102/221 [00:55<01:19,  1.49it/s][A
 47%|████▋     | 103/221 [00:56<00:59,  1.99it/s][A
 47%|████▋     | 104/221 [00:56<00:53,  2.21it/s][A
 48%|████▊     | 105/221 [00:56<00:56,  2.06it/s][A
 48%|████▊     | 106/221 [01:00<02:46,  1.45s/it][A
 48%|████▊     | 107/221 [01:01<02:09,  1.13s/it][A
 49%|████▉     | 108/221 [01:01<01:45,  1.07it/s][A
 49%|████▉     | 109/221 [01:01<01:19,  1.41it/s][A
 50%|█████     | 111/221 [01:02<00:53,  2.07it/s][A
 51%|█████     | 112/221 [01:02<00:49,  2.21it/s][A
 51%|█████     | 113/221 [01:02<00:40,  2.70it/s][A
 52%|█████▏    | 115/221 [01:03<00:37,  2.81it/s][A
 52%|█████▏    | 116/221 [01:03<00:36,  2.89it/s][A
 53%|█████▎    | 117/221 [01:04<00:42,  2.47it/s][A
 53%|█████▎    | 118/221 [01:04<00:41,  2.47it/s][A
 54%|█████▍    | 119/221 [01:05<00:45,  2.25it/s][A
 54%|█████▍    | 120/221 [01:05<00:37,  2.67it/s][A
 55%|█████▍    | 121/221 [01:05<00:41,  2.42it/s][A
 55%|█████▌    | 122/221 [01:06<00:39,  2.52it/s][A
 56%|█████▌    | 123/221 [01:07<01:07,  1.46it/s][A
 56%|█████▌    | 124/221 [01:07<00:51,  1.90it/s][A
 57%|█████▋    | 125/221 [01:08<01:09,  1.38it/s][A
 57%|█████▋    | 126/221 [01:14<03:26,  2.17s/it][A
 57%|█████▋    | 127/221 [01:15<02:44,  1.75s/it][A
 58%|█████▊    | 128/221 [01:15<02:06,  1.36s/it][A
 58%|█████▊    | 129/221 [01:16<01:45,  1.15s/it][A
 59%|█████▉    | 130/221 [01:16<01:19,  1.14it/s][A
 59%|█████▉    | 131/221 [01:17<01:25,  1.05it/s][A
 60%|█████▉    | 132/221 [01:19<01:35,  1.07s/it][A
 60%|██████    | 133/221 [01:19<01:21,  1.08it/s][A
 61%|██████    | 134/221 [01:20<01:29,  1.03s/it][A
 61%|██████    | 135/221 [01:22<01:30,  1.05s/it][A
 62%|██████▏   | 136/221 [01:22<01:11,  1.19it/s][A
 62%|██████▏   | 137/221 [01:22<00:57,  1.46it/s][A
 62%|██████▏   | 138/221 [01:23<00:51,  1.60it/s][A
 63%|██████▎   | 139/221 [01:23<00:42,  1.95it/s][A
 63%|██████▎   | 140/221 [01:24<00:44,  1.82it/s][A
 64%|██████▍   | 141/221 [01:24<00:38,  2.09it/s][A
 64%|██████▍   | 142/221 [01:24<00:32,  2.43it/s][A
 65%|██████▍   | 143/221 [01:25<00:32,  2.40it/s][A
 65%|██████▌   | 144/221 [01:25<00:29,  2.60it/s][A
 66%|██████▌   | 146/221 [01:25<00:18,  3.99it/s][A
 67%|██████▋   | 148/221 [01:26<00:21,  3.32it/s][A
 67%|██████▋   | 149/221 [01:26<00:25,  2.85it/s][A
 68%|██████▊   | 150/221 [01:27<00:28,  2.46it/s][A
 68%|██████▊   | 151/221 [01:27<00:30,  2.32it/s][A
 69%|██████▉   | 152/221 [01:28<00:30,  2.23it/s][A
 69%|██████▉   | 153/221 [01:28<00:24,  2.81it/s][A
 70%|██████▉   | 154/221 [01:28<00:23,  2.87it/s][A
 70%|███████   | 155/221 [01:29<00:22,  2.88it/s][A
 71%|███████   | 156/221 [01:29<00:20,  3.14it/s][A
 71%|███████   | 157/221 [01:33<01:23,  1.31s/it][A
 71%|███████▏  | 158/221 [01:33<01:11,  1.14s/it][A
 72%|███████▏  | 160/221 [01:34<00:42,  1.44it/s][A
 73%|███████▎  | 161/221 [01:34<00:32,  1.82it/s][A
 74%|███████▍  | 163/221 [01:34<00:23,  2.46it/s][A
 74%|███████▍  | 164/221 [01:35<00:20,  2.72it/s][A
 75%|███████▍  | 165/221 [01:35<00:24,  2.31it/s][A
 75%|███████▌  | 166/221 [01:36<00:26,  2.07it/s][A
 76%|███████▌  | 167/221 [01:36<00:22,  2.35it/s][A
 76%|███████▌  | 168/221 [01:42<01:37,  1.85s/it][A
 76%|███████▋  | 169/221 [01:42<01:16,  1.47s/it][A
 77%|███████▋  | 170/221 [01:43<00:59,  1.17s/it][A
 77%|███████▋  | 171/221 [01:43<00:46,  1.09it/s][A
 78%|███████▊  | 172/221 [01:43<00:36,  1.34it/s][A
 78%|███████▊  | 173/221 [01:44<00:29,  1.60it/s][A
 79%|███████▊  | 174/221 [01:44<00:23,  1.97it/s][A
 79%|███████▉  | 175/221 [01:44<00:19,  2.35it/s][A
 80%|███████▉  | 176/221 [01:44<00:18,  2.41it/s][A
 80%|████████  | 177/221 [01:45<00:16,  2.72it/s][A
 81%|████████  | 178/221 [01:45<00:18,  2.33it/s][A
 81%|████████  | 179/221 [01:46<00:16,  2.51it/s][A
 81%|████████▏ | 180/221 [01:46<00:14,  2.91it/s][A
 82%|████████▏ | 181/221 [01:46<00:11,  3.58it/s][A
 82%|████████▏ | 182/221 [01:46<00:12,  3.12it/s][A
 83%|████████▎ | 183/221 [01:47<00:15,  2.43it/s][A
 83%|████████▎ | 184/221 [01:47<00:15,  2.36it/s][A
 84%|████████▎ | 185/221 [01:48<00:13,  2.59it/s][A
 84%|████████▍ | 186/221 [01:48<00:13,  2.69it/s][A
 85%|████████▍ | 187/221 [01:48<00:11,  2.98it/s][A
 85%|████████▌ | 188/221 [01:48<00:08,  3.76it/s][A
 86%|████████▌ | 189/221 [01:49<00:08,  3.80it/s][A
 86%|████████▌ | 190/221 [01:49<00:10,  3.06it/s][A
 86%|████████▋ | 191/221 [01:49<00:08,  3.56it/s][A
 87%|████████▋ | 192/221 [01:50<00:09,  3.11it/s][A
 87%|████████▋ | 193/221 [01:50<00:07,  3.87it/s][A
 88%|████████▊ | 194/221 [01:50<00:10,  2.58it/s][A
 88%|████████▊ | 195/221 [01:51<00:09,  2.73it/s][A
 89%|████████▊ | 196/221 [01:51<00:09,  2.77it/s][A
 89%|████████▉ | 197/221 [01:51<00:07,  3.19it/s][A
 90%|████████▉ | 198/221 [01:52<00:07,  3.27it/s][A
 90%|█████████ | 199/221 [01:52<00:06,  3.60it/s][A
 90%|█████████ | 200/221 [01:52<00:06,  3.04it/s][A
 91%|█████████ | 201/221 [01:53<00:08,  2.23it/s][A
 91%|█████████▏| 202/221 [01:53<00:07,  2.58it/s][A
 92%|█████████▏| 203/221 [01:54<00:09,  1.93it/s][A
 92%|█████████▏| 204/221 [01:54<00:07,  2.19it/s][A
 93%|█████████▎| 206/221 [01:55<00:04,  3.29it/s][A
 94%|█████████▎| 207/221 [01:55<00:03,  3.91it/s][A
 94%|█████████▍| 208/221 [01:55<00:03,  3.58it/s][A
 95%|█████████▌| 210/221 [01:55<00:02,  5.25it/s][A
 95%|█████████▌| 211/221 [01:56<00:02,  3.45it/s][A
 96%|█████████▌| 212/221 [01:56<00:02,  3.52it/s][A
 96%|█████████▋| 213/221 [01:56<00:02,  3.77it/s][A
 97%|█████████▋| 214/221 [01:57<00:01,  3.76it/s][A
 97%|█████████▋| 215/221 [01:57<00:01,  3.59it/s][A
 98%|█████████▊| 216/221 [01:57<00:01,  3.39it/s][A
 98%|█████████▊| 217/221 [01:59<00:03,  1.18it/s][A
 99%|█████████▊| 218/221 [02:00<00:02,  1.44it/s][A
 99%|█████████▉| 219/221 [02:00<00:01,  1.69it/s][A
100%|█████████▉| 220/221 [02:01<00:00,  1.85it/s][A
100%|██████████| 221/221 [02:01<00:00,  2.22it/s][A100%|██████████| 221/221 [02:01<00:00,  1.82it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:30,  7.17it/s][A
  1%|          | 2/221 [00:00<01:29,  2.44it/s][A
  1%|▏         | 3/221 [00:01<01:18,  2.77it/s][A
  2%|▏         | 4/221 [00:01<01:22,  2.63it/s][A
  2%|▏         | 5/221 [00:02<01:37,  2.21it/s][A
  3%|▎         | 6/221 [00:02<01:19,  2.70it/s][A
  3%|▎         | 7/221 [00:02<01:13,  2.91it/s][A
  4%|▎         | 8/221 [00:03<01:30,  2.35it/s][A
  4%|▍         | 9/221 [00:03<01:55,  1.83it/s][A
  5%|▍         | 10/221 [00:04<02:03,  1.71it/s][A
  5%|▍         | 11/221 [00:05<02:13,  1.57it/s][A
  5%|▌         | 12/221 [00:05<01:53,  1.83it/s][A
  6%|▌         | 13/221 [00:06<02:18,  1.50it/s][A
  6%|▋         | 14/221 [00:06<01:55,  1.79it/s][A
  7%|▋         | 15/221 [00:07<01:36,  2.13it/s][A
  7%|▋         | 16/221 [00:08<01:59,  1.71it/s][A
  8%|▊         | 17/221 [00:08<02:17,  1.48it/s][A
  8%|▊         | 18/221 [00:09<01:52,  1.81it/s][A
  9%|▊         | 19/221 [00:09<02:01,  1.67it/s][A
  9%|▉         | 20/221 [00:10<01:39,  2.01it/s][A
 10%|▉         | 21/221 [00:10<01:30,  2.21it/s][A
 10%|▉         | 22/221 [00:10<01:17,  2.58it/s][A
 10%|█         | 23/221 [00:11<01:24,  2.34it/s][A
 11%|█         | 24/221 [00:11<01:09,  2.85it/s][A
 11%|█▏        | 25/221 [00:12<01:49,  1.79it/s][A
 12%|█▏        | 26/221 [00:13<02:22,  1.37it/s][A
 12%|█▏        | 27/221 [00:13<01:49,  1.77it/s][A
 13%|█▎        | 28/221 [00:14<02:10,  1.48it/s][A
 13%|█▎        | 29/221 [00:15<02:06,  1.52it/s][A
 14%|█▎        | 30/221 [00:15<02:01,  1.57it/s][A
 14%|█▍        | 31/221 [00:16<01:50,  1.71it/s][A
 14%|█▍        | 32/221 [00:16<01:40,  1.88it/s][A
 15%|█▍        | 33/221 [00:17<02:04,  1.51it/s][A
 15%|█▌        | 34/221 [00:18<01:50,  1.69it/s][A
 16%|█▌        | 35/221 [00:18<01:24,  2.19it/s][A
 16%|█▋        | 36/221 [00:18<01:24,  2.18it/s][A
 17%|█▋        | 37/221 [00:19<01:20,  2.28it/s][A
 17%|█▋        | 38/221 [00:19<01:30,  2.02it/s][A
 18%|█▊        | 39/221 [00:20<01:30,  2.01it/s][A
 18%|█▊        | 40/221 [00:21<01:48,  1.67it/s][A
 19%|█▊        | 41/221 [00:21<01:32,  1.96it/s][A
 19%|█▉        | 42/221 [00:21<01:14,  2.42it/s][A
 19%|█▉        | 43/221 [00:22<01:33,  1.90it/s][A
 20%|█▉        | 44/221 [00:22<01:20,  2.20it/s][A
 20%|██        | 45/221 [00:23<01:12,  2.44it/s][A
 21%|██        | 46/221 [00:23<01:21,  2.16it/s][A
 21%|██▏       | 47/221 [00:24<01:20,  2.16it/s][A
 22%|██▏       | 48/221 [00:24<01:04,  2.69it/s][A
 22%|██▏       | 49/221 [00:25<01:32,  1.86it/s][A
 23%|██▎       | 50/221 [00:26<02:07,  1.34it/s][A
 23%|██▎       | 51/221 [00:26<01:50,  1.54it/s][A
 24%|██▎       | 52/221 [00:27<01:26,  1.95it/s][A
 24%|██▍       | 53/221 [00:27<01:20,  2.09it/s][A
 24%|██▍       | 54/221 [00:27<01:15,  2.21it/s][A
 25%|██▍       | 55/221 [00:28<01:05,  2.52it/s][A
 25%|██▌       | 56/221 [00:29<01:40,  1.64it/s][A
 26%|██▌       | 57/221 [00:29<01:23,  1.97it/s][A
 26%|██▌       | 58/221 [00:30<01:38,  1.65it/s][A
 27%|██▋       | 59/221 [00:30<01:13,  2.20it/s][A
 27%|██▋       | 60/221 [00:30<01:10,  2.30it/s][A
 28%|██▊       | 61/221 [00:31<01:40,  1.59it/s][A
 28%|██▊       | 62/221 [00:32<01:29,  1.77it/s][A
 29%|██▊       | 63/221 [00:32<01:13,  2.15it/s][A
 29%|██▉       | 64/221 [00:33<01:49,  1.44it/s][A
 29%|██▉       | 65/221 [00:34<01:45,  1.48it/s][A
 30%|██▉       | 66/221 [00:34<01:37,  1.58it/s][A
 30%|███       | 67/221 [00:35<01:27,  1.75it/s][A
 31%|███       | 68/221 [00:35<01:10,  2.17it/s][A
 31%|███       | 69/221 [00:35<01:07,  2.25it/s][A
 32%|███▏      | 70/221 [00:36<01:12,  2.07it/s][A
 32%|███▏      | 71/221 [00:37<01:16,  1.96it/s][A
 33%|███▎      | 72/221 [00:37<01:14,  2.00it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.88it/s][A
 33%|███▎      | 74/221 [00:38<01:16,  1.92it/s][A
 34%|███▍      | 75/221 [00:39<01:07,  2.15it/s][A
 34%|███▍      | 76/221 [00:39<01:02,  2.33it/s][A
 35%|███▍      | 77/221 [00:39<00:59,  2.43it/s][A
 35%|███▌      | 78/221 [00:40<01:14,  1.92it/s][A
 36%|███▌      | 79/221 [00:41<01:43,  1.38it/s][A
 36%|███▌      | 80/221 [00:42<01:26,  1.64it/s][A
 37%|███▋      | 81/221 [00:42<01:10,  1.99it/s][A
 37%|███▋      | 82/221 [00:43<01:31,  1.52it/s][A
 38%|███▊      | 83/221 [00:43<01:28,  1.56it/s][A
 38%|███▊      | 84/221 [00:44<01:33,  1.46it/s][A
 38%|███▊      | 85/221 [00:45<01:28,  1.53it/s][A
 39%|███▉      | 86/221 [00:45<01:27,  1.55it/s][A
 39%|███▉      | 87/221 [00:46<01:16,  1.75it/s][A
 40%|███▉      | 88/221 [00:46<01:08,  1.93it/s][A
 40%|████      | 89/221 [00:47<01:15,  1.76it/s][A
 41%|████      | 90/221 [00:48<01:16,  1.71it/s][A
 41%|████      | 91/221 [00:48<01:09,  1.88it/s][A
 42%|████▏     | 92/221 [00:49<01:11,  1.80it/s][A
 43%|████▎     | 94/221 [00:49<00:45,  2.76it/s][A
 43%|████▎     | 95/221 [00:50<01:02,  2.03it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:02,  1.99it/s][A
 44%|████▍     | 98/221 [00:52<01:10,  1.73it/s][A
 45%|████▍     | 99/221 [00:52<01:13,  1.66it/s][A
 45%|████▌     | 100/221 [00:53<01:14,  1.62it/s][A
 46%|████▌     | 101/221 [00:53<01:10,  1.71it/s][A
 46%|████▌     | 102/221 [00:54<01:27,  1.37it/s][A
 47%|████▋     | 104/221 [00:56<01:15,  1.55it/s][A
 48%|████▊     | 105/221 [00:56<01:03,  1.82it/s][A
 48%|████▊     | 106/221 [00:56<00:53,  2.17it/s][A
 48%|████▊     | 107/221 [00:56<00:50,  2.24it/s][A
 49%|████▉     | 108/221 [00:57<00:53,  2.10it/s][A
 49%|████▉     | 109/221 [00:57<00:45,  2.45it/s][A
 50%|████▉     | 110/221 [00:58<01:00,  1.83it/s][A
 50%|█████     | 111/221 [00:59<01:02,  1.77it/s][A
 51%|█████     | 112/221 [00:59<00:58,  1.88it/s][A
 51%|█████     | 113/221 [00:59<00:47,  2.28it/s][A
 52%|█████▏    | 114/221 [01:00<00:46,  2.30it/s][A
 52%|█████▏    | 115/221 [01:00<00:53,  1.97it/s][A
 52%|█████▏    | 116/221 [01:01<00:44,  2.35it/s][A
 53%|█████▎    | 117/221 [01:01<00:43,  2.37it/s][A
 53%|█████▎    | 118/221 [01:02<00:56,  1.83it/s][A
 54%|█████▍    | 119/221 [01:02<00:45,  2.26it/s][A
 54%|█████▍    | 120/221 [01:03<00:48,  2.07it/s][A
 55%|█████▍    | 121/221 [01:04<00:58,  1.72it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.88it/s][A
 56%|█████▌    | 123/221 [01:04<00:48,  2.04it/s][A
 56%|█████▌    | 124/221 [01:05<00:42,  2.28it/s][A
 57%|█████▋    | 125/221 [01:06<01:00,  1.58it/s][A
 57%|█████▋    | 126/221 [01:06<00:47,  2.00it/s][A
 57%|█████▋    | 127/221 [01:07<01:04,  1.45it/s][A
 58%|█████▊    | 128/221 [01:07<00:56,  1.64it/s][A
 58%|█████▊    | 129/221 [01:08<00:47,  1.94it/s][A
 59%|█████▉    | 130/221 [01:08<00:41,  2.19it/s][A
 59%|█████▉    | 131/221 [01:08<00:36,  2.50it/s][A
 60%|█████▉    | 132/221 [01:09<00:39,  2.23it/s][A
 60%|██████    | 133/221 [01:10<01:04,  1.37it/s][A
 61%|██████    | 134/221 [01:11<00:49,  1.76it/s][A
 61%|██████    | 135/221 [01:11<00:48,  1.78it/s][A
 62%|██████▏   | 136/221 [01:11<00:42,  2.00it/s][A
 62%|██████▏   | 137/221 [01:12<00:38,  2.20it/s][A
 62%|██████▏   | 138/221 [01:12<00:38,  2.17it/s][A
 63%|██████▎   | 139/221 [01:13<00:53,  1.54it/s][A
 63%|██████▎   | 140/221 [01:14<00:47,  1.71it/s][A
 64%|██████▍   | 141/221 [01:14<00:37,  2.12it/s][A
 64%|██████▍   | 142/221 [01:14<00:37,  2.11it/s][A
 65%|██████▍   | 143/221 [01:15<00:32,  2.43it/s][A
 65%|██████▌   | 144/221 [01:15<00:25,  3.06it/s][A
 66%|██████▌   | 145/221 [01:16<00:39,  1.95it/s][A
 66%|██████▌   | 146/221 [01:16<00:30,  2.44it/s][A
 67%|██████▋   | 147/221 [01:16<00:24,  3.07it/s][A
 67%|██████▋   | 148/221 [01:16<00:24,  2.99it/s][A
 67%|██████▋   | 149/221 [01:17<00:19,  3.62it/s][A
 68%|██████▊   | 150/221 [01:17<00:20,  3.53it/s][A
 69%|██████▉   | 152/221 [01:18<00:21,  3.16it/s][A
 69%|██████▉   | 153/221 [01:18<00:22,  3.09it/s][A
 70%|██████▉   | 154/221 [01:18<00:24,  2.69it/s][A
 70%|███████   | 155/221 [01:19<00:30,  2.14it/s][A
 71%|███████   | 156/221 [01:20<00:28,  2.30it/s][A
 71%|███████   | 157/221 [01:20<00:28,  2.22it/s][A
 71%|███████▏  | 158/221 [01:20<00:25,  2.45it/s][A
 72%|███████▏  | 159/221 [01:21<00:21,  2.92it/s][A
 72%|███████▏  | 160/221 [01:21<00:24,  2.47it/s][A
 73%|███████▎  | 161/221 [01:21<00:18,  3.17it/s][A
 73%|███████▎  | 162/221 [01:22<00:19,  3.08it/s][A
 74%|███████▍  | 163/221 [01:22<00:19,  2.97it/s][A
 74%|███████▍  | 164/221 [01:22<00:17,  3.20it/s][A
 75%|███████▍  | 165/221 [01:22<00:15,  3.53it/s][A
 75%|███████▌  | 166/221 [01:23<00:26,  2.09it/s][A
 76%|███████▌  | 167/221 [01:24<00:22,  2.44it/s][A
 76%|███████▌  | 168/221 [01:24<00:21,  2.42it/s][A
 76%|███████▋  | 169/221 [01:25<00:25,  2.07it/s][A
 77%|███████▋  | 170/221 [01:26<00:40,  1.26it/s][A
 77%|███████▋  | 171/221 [01:27<00:38,  1.31it/s][A
 78%|███████▊  | 172/221 [01:27<00:30,  1.61it/s][A
 78%|███████▊  | 173/221 [01:27<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:29<00:34,  1.36it/s][A
 79%|███████▉  | 175/221 [01:30<00:36,  1.26it/s][A
 80%|███████▉  | 176/221 [01:30<00:33,  1.36it/s][A
 80%|████████  | 177/221 [01:31<00:28,  1.54it/s][A
 81%|████████  | 178/221 [01:31<00:28,  1.49it/s][A
 81%|████████  | 179/221 [01:32<00:24,  1.75it/s][A
 81%|████████▏ | 180/221 [01:32<00:25,  1.61it/s][A
 82%|████████▏ | 181/221 [01:33<00:21,  1.90it/s][A
 82%|████████▏ | 182/221 [01:33<00:21,  1.82it/s][A
 83%|████████▎ | 183/221 [01:34<00:17,  2.16it/s][A
 83%|████████▎ | 184/221 [01:35<00:23,  1.61it/s][A
 84%|████████▎ | 185/221 [01:35<00:24,  1.47it/s][A
 85%|████████▍ | 187/221 [01:36<00:17,  1.94it/s][A
 85%|████████▌ | 188/221 [01:36<00:16,  2.04it/s][A
 86%|████████▌ | 189/221 [01:37<00:16,  1.90it/s][A
 86%|████████▌ | 190/221 [01:38<00:17,  1.79it/s][A
 86%|████████▋ | 191/221 [01:38<00:16,  1.87it/s][A
 87%|████████▋ | 192/221 [01:39<00:13,  2.12it/s][A
 87%|████████▋ | 193/221 [01:39<00:11,  2.42it/s][A
 88%|████████▊ | 194/221 [01:39<00:10,  2.46it/s][A
 88%|████████▊ | 195/221 [01:41<00:19,  1.31it/s][A
 89%|████████▊ | 196/221 [01:42<00:24,  1.04it/s][A
 89%|████████▉ | 197/221 [01:43<00:19,  1.22it/s][A
 90%|████████▉ | 198/221 [01:43<00:14,  1.57it/s][A
 90%|█████████ | 199/221 [01:43<00:11,  1.90it/s][A
 90%|█████████ | 200/221 [01:44<00:13,  1.51it/s][A
 91%|█████████ | 201/221 [01:45<00:11,  1.75it/s][A
 91%|█████████▏| 202/221 [01:45<00:09,  2.00it/s][A
 92%|█████████▏| 203/221 [01:45<00:08,  2.01it/s][A
 92%|█████████▏| 204/221 [01:48<00:17,  1.04s/it][A
 93%|█████████▎| 205/221 [01:48<00:12,  1.25it/s][A
 93%|█████████▎| 206/221 [01:48<00:09,  1.53it/s][A
 94%|█████████▎| 207/221 [01:49<00:08,  1.72it/s][A
 94%|█████████▍| 208/221 [01:49<00:08,  1.49it/s][A
 95%|█████████▍| 209/221 [01:50<00:08,  1.35it/s][A
 95%|█████████▌| 210/221 [01:51<00:06,  1.77it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.97it/s][A
 96%|█████████▌| 212/221 [01:51<00:03,  2.43it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.94it/s][A
 97%|█████████▋| 214/221 [01:52<00:03,  2.01it/s][A
 97%|█████████▋| 215/221 [01:52<00:02,  2.61it/s][A
 98%|█████████▊| 216/221 [01:53<00:02,  2.47it/s][A
 98%|█████████▊| 217/221 [01:54<00:01,  2.08it/s][A
 99%|█████████▊| 218/221 [01:54<00:01,  2.51it/s][A
 99%|█████████▉| 219/221 [01:54<00:00,  2.46it/s][A
100%|█████████▉| 220/221 [01:55<00:00,  2.37it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.81it/s][A100%|██████████| 221/221 [01:56<00:00,  1.91it/s]
09/19/2024 06:58:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 3499--===========

09/19/2024 06:58:34 - INFO - __main__ -   {'area_r1': 45.6, 'area_recall': '45.6/73.5/83.1', 'area_ravg': 67.4}
09/19/2024 06:58:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 3499--===========

09/19/2024 06:58:34 - INFO - __main__ -   {'forward_r1': 49.9, 'forward_recall': '49.9/78.1/87.2', 'forward_ravg': 71.7}
09/19/2024 06:58:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 3499--===========

09/19/2024 06:58:34 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 06:58:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 06:58:34 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 06:58:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 3499--===========

09/19/2024 06:58:34 - INFO - __main__ -   {'area_video_r1': 62.6, 'area_video_recall': '62.6/82.9/88.5', 'area_video_ravg': 78.0, 'area_video_back_r1': 62.0, 'area_video_back_recall': '62.0/85.4/91.2', 'area_video_back_ravg': 79.5}
09/19/2024 06:58:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 3499=======

09/19/2024 06:58:34 - INFO - __main__ -   {'area_video_r1': 62.6, 'area_video_recall': '62.6/82.9/88.5', 'area_video_ravg': 78.0, 'area_video_back_r1': 62.0, 'area_video_back_recall': '62.0/85.4/91.2', 'area_video_back_ravg': 79.5}
09/19/2024 06:58:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 3499--===========

09/19/2024 06:58:34 - INFO - __main__ -   {'video_r1': 31.7, 'video_recall': '31.7/56.1/68.7', 'video_ravg': 52.1}
09/19/2024 06:58:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 06:58:34 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 06:58:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 3499--===========

09/19/2024 06:58:34 - INFO - __main__ -   {'video_r1': 60.1, 'video_recall': '60.1/79.1/83.4', 'video_ravg': 74.2}
09/19/2024 06:58:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 2999=======

09/19/2024 06:58:34 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/79.5/84.7', 'video_ravg': 74.8}
09/19/2024 06:59:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018704211339354515, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1149982213974, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1337023973464966}
 39%|███▉      | 3500/8917 [4:44:33<257:06:30, 170.87s/it] 39%|███▉      | 3501/8917 [4:44:36<181:19:56, 120.53s/it] 39%|███▉      | 3502/8917 [4:44:40<128:31:05, 85.44s/it]  39%|███▉      | 3503/8917 [4:44:44<91:45:55, 61.02s/it]  39%|███▉      | 3504/8917 [4:44:48<65:57:26, 43.87s/it] 39%|███▉      | 3505/8917 [4:44:52<47:54:27, 31.87s/it] 39%|███▉      | 3506/8917 [4:44:55<35:07:48, 23.37s/it] 39%|███▉      | 3507/8917 [4:44:59<26:28:24, 17.62s/it] 39%|███▉      | 3508/8917 [4:45:03<20:21:36, 13.55s/it] 39%|███▉      | 3509/8917 [4:45:07<15:53:51, 10.58s/it] 39%|███▉      | 3510/8917 [4:45:10<12:42:16,  8.46s/it] 39%|███▉      | 3511/8917 [4:45:14<10:37:27,  7.07s/it] 39%|███▉      | 3512/8917 [4:45:18<9:03:40,  6.04s/it]  39%|███▉      | 3513/8917 [4:45:22<8:02:37,  5.36s/it] 39%|███▉      | 3514/8917 [4:45:25<7:18:34,  4.87s/it] 39%|███▉      | 3515/8917 [4:45:29<6:52:25,  4.58s/it] 39%|███▉      | 3516/8917 [4:45:33<6:39:57,  4.44s/it] 39%|███▉      | 3517/8917 [4:45:37<6:16:34,  4.18s/it] 39%|███▉      | 3518/8917 [4:45:41<6:05:29,  4.06s/it] 39%|███▉      | 3519/8917 [4:45:45<6:00:57,  4.01s/it] 39%|███▉      | 3520/8917 [4:45:49<6:01:06,  4.01s/it] 39%|███▉      | 3521/8917 [4:45:52<5:53:52,  3.93s/it] 39%|███▉      | 3522/8917 [4:45:56<5:44:32,  3.83s/it] 40%|███▉      | 3523/8917 [4:46:00<5:37:07,  3.75s/it] 40%|███▉      | 3524/8917 [4:46:03<5:33:39,  3.71s/it] 40%|███▉      | 3525/8917 [4:46:07<5:31:00,  3.68s/it] 40%|███▉      | 3526/8917 [4:46:11<5:30:46,  3.68s/it] 40%|███▉      | 3527/8917 [4:46:14<5:34:02,  3.72s/it] 40%|███▉      | 3528/8917 [4:46:18<5:34:06,  3.72s/it] 40%|███▉      | 3529/8917 [4:46:21<5:25:04,  3.62s/it] 40%|███▉      | 3530/8917 [4:46:25<5:29:44,  3.67s/it] 40%|███▉      | 3531/8917 [4:46:29<5:30:45,  3.68s/it] 40%|███▉      | 3532/8917 [4:46:33<5:33:11,  3.71s/it] 40%|███▉      | 3533/8917 [4:46:36<5:26:56,  3.64s/it] 40%|███▉      | 3534/8917 [4:46:40<5:24:26,  3.62s/it] 40%|███▉      | 3535/8917 [4:46:43<5:22:28,  3.60s/it] 40%|███▉      | 3536/8917 [4:46:47<5:28:27,  3.66s/it] 40%|███▉      | 3537/8917 [4:46:51<5:39:49,  3.79s/it] 40%|███▉      | 3538/8917 [4:46:55<5:36:22,  3.75s/it] 40%|███▉      | 3539/8917 [4:46:58<5:31:19,  3.70s/it] 40%|███▉      | 3540/8917 [4:47:02<5:34:13,  3.73s/it] 40%|███▉      | 3541/8917 [4:47:06<5:33:18,  3.72s/it] 40%|███▉      | 3542/8917 [4:47:10<5:27:53,  3.66s/it] 40%|███▉      | 3543/8917 [4:47:14<5:39:09,  3.79s/it] 40%|███▉      | 3544/8917 [4:47:17<5:31:54,  3.71s/it] 40%|███▉      | 3545/8917 [4:47:21<5:31:27,  3.70s/it] 40%|███▉      | 3546/8917 [4:47:24<5:29:08,  3.68s/it] 40%|███▉      | 3547/8917 [4:47:28<5:37:35,  3.77s/it] 40%|███▉      | 3548/8917 [4:47:32<5:34:36,  3.74s/it] 40%|███▉      | 3549/8917 [4:47:36<5:40:04,  3.80s/it]09/19/2024 07:02:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.019148262217640877, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1052542924880981, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1244025230407715}
 40%|███▉      | 3550/8917 [4:47:40<5:39:44,  3.80s/it] 40%|███▉      | 3551/8917 [4:47:43<5:34:24,  3.74s/it] 40%|███▉      | 3552/8917 [4:47:47<5:38:11,  3.78s/it] 40%|███▉      | 3553/8917 [4:47:51<5:36:42,  3.77s/it] 40%|███▉      | 3554/8917 [4:47:55<5:33:42,  3.73s/it] 40%|███▉      | 3555/8917 [4:47:58<5:31:51,  3.71s/it] 40%|███▉      | 3556/8917 [4:48:02<5:36:08,  3.76s/it] 40%|███▉      | 3557/8917 [4:48:06<5:30:35,  3.70s/it] 40%|███▉      | 3558/8917 [4:48:09<5:26:19,  3.65s/it] 40%|███▉      | 3559/8917 [4:48:13<5:30:26,  3.70s/it] 40%|███▉      | 3560/8917 [4:48:17<5:31:39,  3.71s/it] 40%|███▉      | 3561/8917 [4:48:21<5:30:39,  3.70s/it] 40%|███▉      | 3562/8917 [4:48:24<5:24:51,  3.64s/it] 40%|███▉      | 3563/8917 [4:48:28<5:21:24,  3.60s/it] 40%|███▉      | 3564/8917 [4:48:31<5:18:52,  3.57s/it] 40%|███▉      | 3565/8917 [4:48:35<5:23:27,  3.63s/it] 40%|███▉      | 3566/8917 [4:48:39<5:43:16,  3.85s/it] 40%|████      | 3567/8917 [4:48:43<5:31:43,  3.72s/it] 40%|████      | 3568/8917 [4:48:47<5:39:24,  3.81s/it] 40%|████      | 3569/8917 [4:48:50<5:33:19,  3.74s/it] 40%|████      | 3570/8917 [4:48:54<5:47:39,  3.90s/it] 40%|████      | 3571/8917 [4:48:58<5:41:16,  3.83s/it] 40%|████      | 3572/8917 [4:49:02<5:34:31,  3.76s/it] 40%|████      | 3573/8917 [4:49:05<5:31:34,  3.72s/it] 40%|████      | 3574/8917 [4:49:09<5:30:15,  3.71s/it] 40%|████      | 3575/8917 [4:49:13<5:40:05,  3.82s/it] 40%|████      | 3576/8917 [4:49:17<5:29:57,  3.71s/it] 40%|████      | 3577/8917 [4:49:20<5:35:31,  3.77s/it] 40%|████      | 3578/8917 [4:49:24<5:31:04,  3.72s/it] 40%|████      | 3579/8917 [4:49:28<5:32:58,  3.74s/it] 40%|████      | 3580/8917 [4:49:32<5:34:48,  3.76s/it] 40%|████      | 3581/8917 [4:49:36<5:37:44,  3.80s/it] 40%|████      | 3582/8917 [4:49:39<5:35:22,  3.77s/it] 40%|████      | 3583/8917 [4:49:43<5:29:40,  3.71s/it] 40%|████      | 3584/8917 [4:49:46<5:24:03,  3.65s/it] 40%|████      | 3585/8917 [4:49:50<5:32:59,  3.75s/it] 40%|████      | 3586/8917 [4:49:54<5:41:23,  3.84s/it] 40%|████      | 3587/8917 [4:49:58<5:34:42,  3.77s/it] 40%|████      | 3588/8917 [4:50:02<5:37:01,  3.79s/it] 40%|████      | 3589/8917 [4:50:05<5:30:54,  3.73s/it] 40%|████      | 3590/8917 [4:50:09<5:38:51,  3.82s/it] 40%|████      | 3591/8917 [4:50:13<5:37:06,  3.80s/it] 40%|████      | 3592/8917 [4:50:17<5:35:58,  3.79s/it] 40%|████      | 3593/8917 [4:50:21<5:32:50,  3.75s/it] 40%|████      | 3594/8917 [4:50:24<5:32:08,  3.74s/it] 40%|████      | 3595/8917 [4:50:28<5:20:39,  3.62s/it] 40%|████      | 3596/8917 [4:50:31<5:23:55,  3.65s/it] 40%|████      | 3597/8917 [4:50:35<5:24:02,  3.65s/it] 40%|████      | 3598/8917 [4:50:39<5:18:29,  3.59s/it] 40%|████      | 3599/8917 [4:50:42<5:27:44,  3.70s/it]09/19/2024 07:05:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02993205562233925, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3788666725158691, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4087986946105957}
 40%|████      | 3600/8917 [4:50:46<5:26:45,  3.69s/it] 40%|████      | 3601/8917 [4:50:50<5:38:20,  3.82s/it] 40%|████      | 3602/8917 [4:50:54<5:44:23,  3.89s/it] 40%|████      | 3603/8917 [4:50:58<5:39:01,  3.83s/it] 40%|████      | 3604/8917 [4:51:02<5:40:19,  3.84s/it] 40%|████      | 3605/8917 [4:51:05<5:34:02,  3.77s/it] 40%|████      | 3606/8917 [4:51:09<5:30:39,  3.74s/it] 40%|████      | 3607/8917 [4:51:13<5:39:49,  3.84s/it] 40%|████      | 3608/8917 [4:51:17<5:35:42,  3.79s/it] 40%|████      | 3609/8917 [4:51:20<5:25:16,  3.68s/it] 40%|████      | 3610/8917 [4:51:24<5:26:24,  3.69s/it] 40%|████      | 3611/8917 [4:51:28<5:31:40,  3.75s/it] 41%|████      | 3612/8917 [4:51:31<5:24:50,  3.67s/it] 41%|████      | 3613/8917 [4:51:35<5:30:23,  3.74s/it] 41%|████      | 3614/8917 [4:51:39<5:24:30,  3.67s/it] 41%|████      | 3615/8917 [4:51:43<5:27:20,  3.70s/it] 41%|████      | 3616/8917 [4:51:46<5:25:32,  3.68s/it] 41%|████      | 3617/8917 [4:51:50<5:38:34,  3.83s/it] 41%|████      | 3618/8917 [4:51:54<5:39:17,  3.84s/it] 41%|████      | 3619/8917 [4:51:58<5:36:44,  3.81s/it] 41%|████      | 3620/8917 [4:52:02<5:38:20,  3.83s/it] 41%|████      | 3621/8917 [4:52:06<5:33:35,  3.78s/it] 41%|████      | 3622/8917 [4:52:09<5:34:04,  3.79s/it] 41%|████      | 3623/8917 [4:52:13<5:39:44,  3.85s/it] 41%|████      | 3624/8917 [4:52:17<5:28:48,  3.73s/it] 41%|████      | 3625/8917 [4:52:21<5:34:23,  3.79s/it] 41%|████      | 3626/8917 [4:52:24<5:24:31,  3.68s/it] 41%|████      | 3627/8917 [4:52:28<5:28:17,  3.72s/it] 41%|████      | 3628/8917 [4:52:31<5:22:03,  3.65s/it] 41%|████      | 3629/8917 [4:52:35<5:20:01,  3.63s/it] 41%|████      | 3630/8917 [4:52:39<5:26:48,  3.71s/it] 41%|████      | 3631/8917 [4:52:43<5:36:58,  3.82s/it] 41%|████      | 3632/8917 [4:52:47<5:32:34,  3.78s/it] 41%|████      | 3633/8917 [4:52:50<5:19:10,  3.62s/it] 41%|████      | 3634/8917 [4:52:54<5:21:42,  3.65s/it] 41%|████      | 3635/8917 [4:52:57<5:25:10,  3.69s/it] 41%|████      | 3636/8917 [4:53:01<5:24:45,  3.69s/it] 41%|████      | 3637/8917 [4:53:05<5:23:13,  3.67s/it] 41%|████      | 3638/8917 [4:53:09<5:27:06,  3.72s/it] 41%|████      | 3639/8917 [4:53:12<5:28:48,  3.74s/it] 41%|████      | 3640/8917 [4:53:16<5:23:49,  3.68s/it] 41%|████      | 3641/8917 [4:53:20<5:20:40,  3.65s/it] 41%|████      | 3642/8917 [4:53:23<5:23:30,  3.68s/it] 41%|████      | 3643/8917 [4:53:27<5:24:09,  3.69s/it] 41%|████      | 3644/8917 [4:53:30<5:15:26,  3.59s/it] 41%|████      | 3645/8917 [4:53:34<5:18:53,  3.63s/it] 41%|████      | 3646/8917 [4:53:38<5:14:28,  3.58s/it] 41%|████      | 3647/8917 [4:53:41<5:10:47,  3.54s/it] 41%|████      | 3648/8917 [4:53:45<5:28:48,  3.74s/it] 41%|████      | 3649/8917 [4:53:49<5:23:13,  3.68s/it]09/19/2024 07:08:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024299664422869682, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0620536804199219, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0863533020019531}
 41%|████      | 3650/8917 [4:53:53<5:29:44,  3.76s/it] 41%|████      | 3651/8917 [4:53:57<5:38:13,  3.85s/it] 41%|████      | 3652/8917 [4:54:00<5:30:15,  3.76s/it] 41%|████      | 3653/8917 [4:54:04<5:32:35,  3.79s/it] 41%|████      | 3654/8917 [4:54:08<5:28:42,  3.75s/it] 41%|████      | 3655/8917 [4:54:11<5:25:50,  3.72s/it] 41%|████      | 3656/8917 [4:54:15<5:34:47,  3.82s/it] 41%|████      | 3657/8917 [4:54:19<5:33:42,  3.81s/it] 41%|████      | 3658/8917 [4:54:23<5:28:13,  3.74s/it] 41%|████      | 3659/8917 [4:54:27<5:36:40,  3.84s/it] 41%|████      | 3660/8917 [4:54:30<5:20:31,  3.66s/it] 41%|████      | 3661/8917 [4:54:34<5:21:13,  3.67s/it] 41%|████      | 3662/8917 [4:54:38<5:35:51,  3.83s/it] 41%|████      | 3663/8917 [4:54:42<5:33:07,  3.80s/it] 41%|████      | 3664/8917 [4:54:45<5:28:42,  3.75s/it] 41%|████      | 3665/8917 [4:54:49<5:17:59,  3.63s/it] 41%|████      | 3666/8917 [4:54:53<5:35:55,  3.84s/it] 41%|████      | 3667/8917 [4:54:57<5:37:27,  3.86s/it] 41%|████      | 3668/8917 [4:55:01<5:29:52,  3.77s/it] 41%|████      | 3669/8917 [4:55:05<5:37:15,  3.86s/it] 41%|████      | 3670/8917 [4:55:09<5:37:28,  3.86s/it] 41%|████      | 3671/8917 [4:55:12<5:31:11,  3.79s/it] 41%|████      | 3672/8917 [4:55:16<5:28:45,  3.76s/it] 41%|████      | 3673/8917 [4:55:20<5:41:22,  3.91s/it] 41%|████      | 3674/8917 [4:55:24<5:36:07,  3.85s/it] 41%|████      | 3675/8917 [4:55:27<5:29:13,  3.77s/it] 41%|████      | 3676/8917 [4:55:31<5:22:23,  3.69s/it] 41%|████      | 3677/8917 [4:55:35<5:27:02,  3.74s/it] 41%|████      | 3678/8917 [4:55:39<5:30:21,  3.78s/it] 41%|████▏     | 3679/8917 [4:55:42<5:27:21,  3.75s/it] 41%|████▏     | 3680/8917 [4:55:46<5:25:22,  3.73s/it] 41%|████▏     | 3681/8917 [4:55:49<5:15:58,  3.62s/it] 41%|████▏     | 3682/8917 [4:55:53<5:27:47,  3.76s/it] 41%|████▏     | 3683/8917 [4:55:57<5:25:23,  3.73s/it] 41%|████▏     | 3684/8917 [4:56:01<5:25:03,  3.73s/it] 41%|████▏     | 3685/8917 [4:56:04<5:21:32,  3.69s/it] 41%|████▏     | 3686/8917 [4:56:08<5:18:58,  3.66s/it] 41%|████▏     | 3687/8917 [4:56:12<5:30:22,  3.79s/it] 41%|████▏     | 3688/8917 [4:56:16<5:25:34,  3.74s/it] 41%|████▏     | 3689/8917 [4:56:19<5:25:28,  3.74s/it] 41%|████▏     | 3690/8917 [4:56:23<5:24:20,  3.72s/it] 41%|████▏     | 3691/8917 [4:56:27<5:20:36,  3.68s/it] 41%|████▏     | 3692/8917 [4:56:30<5:16:04,  3.63s/it] 41%|████▏     | 3693/8917 [4:56:34<5:17:37,  3.65s/it] 41%|████▏     | 3694/8917 [4:56:38<5:18:47,  3.66s/it] 41%|████▏     | 3695/8917 [4:56:41<5:22:22,  3.70s/it] 41%|████▏     | 3696/8917 [4:56:45<5:22:54,  3.71s/it] 41%|████▏     | 3697/8917 [4:56:49<5:17:03,  3.64s/it] 41%|████▏     | 3698/8917 [4:56:53<5:23:25,  3.72s/it] 41%|████▏     | 3699/8917 [4:56:56<5:30:16,  3.80s/it]09/19/2024 07:11:34 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02373441308736801, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.124565839767456, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.148300290107727}
 41%|████▏     | 3700/8917 [4:57:00<5:27:59,  3.77s/it] 42%|████▏     | 3701/8917 [4:57:04<5:21:43,  3.70s/it] 42%|████▏     | 3702/8917 [4:57:07<5:18:43,  3.67s/it] 42%|████▏     | 3703/8917 [4:57:11<5:16:27,  3.64s/it] 42%|████▏     | 3704/8917 [4:57:15<5:20:43,  3.69s/it] 42%|████▏     | 3705/8917 [4:57:19<5:23:16,  3.72s/it] 42%|████▏     | 3706/8917 [4:57:22<5:20:11,  3.69s/it] 42%|████▏     | 3707/8917 [4:57:26<5:15:15,  3.63s/it] 42%|████▏     | 3708/8917 [4:57:29<5:19:43,  3.68s/it] 42%|████▏     | 3709/8917 [4:57:33<5:16:07,  3.64s/it] 42%|████▏     | 3710/8917 [4:57:37<5:21:44,  3.71s/it] 42%|████▏     | 3711/8917 [4:57:40<5:19:37,  3.68s/it] 42%|████▏     | 3712/8917 [4:57:44<5:24:58,  3.75s/it] 42%|████▏     | 3713/8917 [4:57:48<5:23:54,  3.73s/it] 42%|████▏     | 3714/8917 [4:57:52<5:23:21,  3.73s/it] 42%|████▏     | 3715/8917 [4:57:55<5:14:59,  3.63s/it] 42%|████▏     | 3716/8917 [4:57:59<5:28:51,  3.79s/it] 42%|████▏     | 3717/8917 [4:58:03<5:33:49,  3.85s/it] 42%|████▏     | 3718/8917 [4:58:07<5:24:43,  3.75s/it] 42%|████▏     | 3719/8917 [4:58:11<5:24:54,  3.75s/it] 42%|████▏     | 3720/8917 [4:58:14<5:15:38,  3.64s/it] 42%|████▏     | 3721/8917 [4:58:17<5:08:00,  3.56s/it] 42%|████▏     | 3722/8917 [4:58:21<5:21:57,  3.72s/it] 42%|████▏     | 3723/8917 [4:58:25<5:23:25,  3.74s/it] 42%|████▏     | 3724/8917 [4:58:29<5:17:43,  3.67s/it] 42%|████▏     | 3725/8917 [4:58:32<5:15:18,  3.64s/it] 42%|████▏     | 3726/8917 [4:58:36<5:11:20,  3.60s/it] 42%|████▏     | 3727/8917 [4:58:39<5:08:46,  3.57s/it] 42%|████▏     | 3728/8917 [4:58:43<5:17:37,  3.67s/it] 42%|████▏     | 3729/8917 [4:58:47<5:20:52,  3.71s/it] 42%|████▏     | 3730/8917 [4:58:51<5:18:49,  3.69s/it] 42%|████▏     | 3731/8917 [4:58:54<5:12:16,  3.61s/it] 42%|████▏     | 3732/8917 [4:58:57<5:05:21,  3.53s/it] 42%|████▏     | 3733/8917 [4:59:01<5:09:41,  3.58s/it] 42%|████▏     | 3734/8917 [4:59:05<5:16:30,  3.66s/it] 42%|████▏     | 3735/8917 [4:59:09<5:19:55,  3.70s/it] 42%|████▏     | 3736/8917 [4:59:12<5:16:19,  3.66s/it] 42%|████▏     | 3737/8917 [4:59:16<5:15:03,  3.65s/it] 42%|████▏     | 3738/8917 [4:59:20<5:20:12,  3.71s/it] 42%|████▏     | 3739/8917 [4:59:23<5:16:52,  3.67s/it] 42%|████▏     | 3740/8917 [4:59:27<5:19:15,  3.70s/it] 42%|████▏     | 3741/8917 [4:59:31<5:14:32,  3.65s/it] 42%|████▏     | 3742/8917 [4:59:34<5:18:15,  3.69s/it] 42%|████▏     | 3743/8917 [4:59:38<5:22:58,  3.75s/it] 42%|████▏     | 3744/8917 [4:59:42<5:16:22,  3.67s/it] 42%|████▏     | 3745/8917 [4:59:46<5:20:58,  3.72s/it] 42%|████▏     | 3746/8917 [4:59:49<5:12:50,  3.63s/it] 42%|████▏     | 3747/8917 [4:59:53<5:16:53,  3.68s/it] 42%|████▏     | 3748/8917 [4:59:57<5:27:03,  3.80s/it] 42%|████▏     | 3749/8917 [5:00:01<5:27:17,  3.80s/it]09/19/2024 07:14:38 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032445263117551804, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7693092823028564, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.8017545938491821}
 42%|████▏     | 3750/8917 [5:00:05<5:29:43,  3.83s/it] 42%|████▏     | 3751/8917 [5:00:09<5:31:17,  3.85s/it] 42%|████▏     | 3752/8917 [5:00:13<5:34:15,  3.88s/it] 42%|████▏     | 3753/8917 [5:00:16<5:30:29,  3.84s/it] 42%|████▏     | 3754/8917 [5:00:20<5:22:56,  3.75s/it] 42%|████▏     | 3755/8917 [5:00:24<5:25:20,  3.78s/it] 42%|████▏     | 3756/8917 [5:00:28<5:30:03,  3.84s/it] 42%|████▏     | 3757/8917 [5:00:31<5:28:15,  3.82s/it] 42%|████▏     | 3758/8917 [5:00:35<5:23:27,  3.76s/it] 42%|████▏     | 3759/8917 [5:00:39<5:26:42,  3.80s/it] 42%|████▏     | 3760/8917 [5:00:42<5:18:59,  3.71s/it] 42%|████▏     | 3761/8917 [5:00:46<5:15:23,  3.67s/it] 42%|████▏     | 3762/8917 [5:00:50<5:13:55,  3.65s/it] 42%|████▏     | 3763/8917 [5:00:54<5:29:44,  3.84s/it] 42%|████▏     | 3764/8917 [5:00:58<5:28:31,  3.83s/it] 42%|████▏     | 3765/8917 [5:01:01<5:26:05,  3.80s/it] 42%|████▏     | 3766/8917 [5:01:05<5:20:53,  3.74s/it] 42%|████▏     | 3767/8917 [5:01:09<5:18:20,  3.71s/it] 42%|████▏     | 3768/8917 [5:01:13<5:24:02,  3.78s/it] 42%|████▏     | 3769/8917 [5:01:16<5:25:56,  3.80s/it] 42%|████▏     | 3770/8917 [5:01:20<5:27:58,  3.82s/it] 42%|████▏     | 3771/8917 [5:01:24<5:29:20,  3.84s/it] 42%|████▏     | 3772/8917 [5:01:28<5:28:08,  3.83s/it] 42%|████▏     | 3773/8917 [5:01:32<5:26:48,  3.81s/it] 42%|████▏     | 3774/8917 [5:01:36<5:31:17,  3.87s/it] 42%|████▏     | 3775/8917 [5:01:39<5:18:19,  3.71s/it] 42%|████▏     | 3776/8917 [5:01:43<5:12:04,  3.64s/it] 42%|████▏     | 3777/8917 [5:01:46<5:13:57,  3.66s/it] 42%|████▏     | 3778/8917 [5:01:50<5:23:34,  3.78s/it] 42%|████▏     | 3779/8917 [5:01:54<5:27:31,  3.82s/it] 42%|████▏     | 3780/8917 [5:01:58<5:25:50,  3.81s/it] 42%|████▏     | 3781/8917 [5:02:01<5:15:26,  3.69s/it] 42%|████▏     | 3782/8917 [5:02:05<5:14:42,  3.68s/it] 42%|████▏     | 3783/8917 [5:02:09<5:15:14,  3.68s/it] 42%|████▏     | 3784/8917 [5:02:13<5:20:48,  3.75s/it] 42%|████▏     | 3785/8917 [5:02:17<5:21:41,  3.76s/it] 42%|████▏     | 3786/8917 [5:02:20<5:13:26,  3.67s/it] 42%|████▏     | 3787/8917 [5:02:24<5:15:56,  3.70s/it] 42%|████▏     | 3788/8917 [5:02:27<5:17:03,  3.71s/it] 42%|████▏     | 3789/8917 [5:02:31<5:24:07,  3.79s/it] 43%|████▎     | 3790/8917 [5:02:35<5:16:07,  3.70s/it] 43%|████▎     | 3791/8917 [5:02:38<5:03:57,  3.56s/it] 43%|████▎     | 3792/8917 [5:02:42<5:10:58,  3.64s/it] 43%|████▎     | 3793/8917 [5:02:46<5:21:42,  3.77s/it] 43%|████▎     | 3794/8917 [5:02:50<5:30:09,  3.87s/it] 43%|████▎     | 3795/8917 [5:02:54<5:21:43,  3.77s/it] 43%|████▎     | 3796/8917 [5:02:57<5:18:28,  3.73s/it] 43%|████▎     | 3797/8917 [5:03:01<5:13:48,  3.68s/it] 43%|████▎     | 3798/8917 [5:03:04<5:10:08,  3.64s/it] 43%|████▎     | 3799/8917 [5:03:08<5:14:43,  3.69s/it]09/19/2024 07:17:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018104998394846916, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.413649082183838, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4317541122436523}
 43%|████▎     | 3800/8917 [5:03:12<5:12:52,  3.67s/it] 43%|████▎     | 3801/8917 [5:03:16<5:14:49,  3.69s/it] 43%|████▎     | 3802/8917 [5:03:19<5:14:54,  3.69s/it] 43%|████▎     | 3803/8917 [5:03:23<5:23:26,  3.79s/it] 43%|████▎     | 3804/8917 [5:03:27<5:18:16,  3.73s/it] 43%|████▎     | 3805/8917 [5:03:31<5:16:59,  3.72s/it] 43%|████▎     | 3806/8917 [5:03:34<5:12:23,  3.67s/it] 43%|████▎     | 3807/8917 [5:03:38<5:06:12,  3.60s/it] 43%|████▎     | 3808/8917 [5:03:41<5:08:28,  3.62s/it] 43%|████▎     | 3809/8917 [5:03:45<5:10:58,  3.65s/it] 43%|████▎     | 3810/8917 [5:03:49<5:22:23,  3.79s/it] 43%|████▎     | 3811/8917 [5:03:53<5:15:59,  3.71s/it] 43%|████▎     | 3812/8917 [5:03:56<5:12:44,  3.68s/it] 43%|████▎     | 3813/8917 [5:04:00<5:20:58,  3.77s/it] 43%|████▎     | 3814/8917 [5:04:04<5:19:36,  3.76s/it] 43%|████▎     | 3815/8917 [5:04:07<5:13:56,  3.69s/it] 43%|████▎     | 3816/8917 [5:04:11<5:15:09,  3.71s/it] 43%|████▎     | 3817/8917 [5:04:15<5:12:24,  3.68s/it] 43%|████▎     | 3818/8917 [5:04:19<5:13:40,  3.69s/it] 43%|████▎     | 3819/8917 [5:04:22<5:15:38,  3.71s/it] 43%|████▎     | 3820/8917 [5:04:26<5:19:34,  3.76s/it] 43%|████▎     | 3821/8917 [5:04:30<5:26:00,  3.84s/it] 43%|████▎     | 3822/8917 [5:04:34<5:24:56,  3.83s/it] 43%|████▎     | 3823/8917 [5:04:37<5:11:53,  3.67s/it] 43%|████▎     | 3824/8917 [5:04:41<5:08:45,  3.64s/it] 43%|████▎     | 3825/8917 [5:04:44<5:04:42,  3.59s/it] 43%|████▎     | 3826/8917 [5:04:48<5:12:51,  3.69s/it] 43%|████▎     | 3827/8917 [5:04:52<5:13:02,  3.69s/it] 43%|████▎     | 3828/8917 [5:04:56<5:13:18,  3.69s/it] 43%|████▎     | 3829/8917 [5:04:59<5:13:06,  3.69s/it] 43%|████▎     | 3830/8917 [5:05:03<5:15:40,  3.72s/it] 43%|████▎     | 3831/8917 [5:05:07<5:12:53,  3.69s/it] 43%|████▎     | 3832/8917 [5:05:11<5:19:50,  3.77s/it] 43%|████▎     | 3833/8917 [5:05:14<5:16:39,  3.74s/it] 43%|████▎     | 3834/8917 [5:05:18<5:15:35,  3.73s/it] 43%|████▎     | 3835/8917 [5:05:22<5:14:04,  3.71s/it] 43%|████▎     | 3836/8917 [5:05:25<5:11:40,  3.68s/it] 43%|████▎     | 3837/8917 [5:05:29<5:17:43,  3.75s/it] 43%|████▎     | 3838/8917 [5:05:33<5:19:00,  3.77s/it] 43%|████▎     | 3839/8917 [5:05:37<5:22:29,  3.81s/it] 43%|████▎     | 3840/8917 [5:05:40<5:07:59,  3.64s/it] 43%|████▎     | 3841/8917 [5:05:44<5:13:26,  3.71s/it] 43%|████▎     | 3842/8917 [5:05:48<5:14:24,  3.72s/it] 43%|████▎     | 3843/8917 [5:05:52<5:15:31,  3.73s/it] 43%|████▎     | 3844/8917 [5:05:55<5:15:34,  3.73s/it] 43%|████▎     | 3845/8917 [5:05:59<5:15:54,  3.74s/it] 43%|████▎     | 3846/8917 [5:06:03<5:14:07,  3.72s/it] 43%|████▎     | 3847/8917 [5:06:07<5:16:36,  3.75s/it] 43%|████▎     | 3848/8917 [5:06:10<5:15:01,  3.73s/it] 43%|████▎     | 3849/8917 [5:06:14<5:06:42,  3.63s/it]09/19/2024 07:20:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02362716943025589, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.6252267360687256, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.6488538980484009}
 43%|████▎     | 3850/8917 [5:06:17<5:04:50,  3.61s/it] 43%|████▎     | 3851/8917 [5:06:21<5:14:35,  3.73s/it] 43%|████▎     | 3852/8917 [5:06:25<5:13:12,  3.71s/it] 43%|████▎     | 3853/8917 [5:06:29<5:12:43,  3.71s/it] 43%|████▎     | 3854/8917 [5:06:32<5:11:54,  3.70s/it] 43%|████▎     | 3855/8917 [5:06:36<5:05:18,  3.62s/it] 43%|████▎     | 3856/8917 [5:06:39<5:03:11,  3.59s/it] 43%|████▎     | 3857/8917 [5:06:43<5:12:25,  3.70s/it] 43%|████▎     | 3858/8917 [5:06:47<5:08:35,  3.66s/it] 43%|████▎     | 3859/8917 [5:06:51<5:11:11,  3.69s/it] 43%|████▎     | 3860/8917 [5:06:54<5:17:14,  3.76s/it] 43%|████▎     | 3861/8917 [5:06:58<5:15:43,  3.75s/it] 43%|████▎     | 3862/8917 [5:07:02<5:12:14,  3.71s/it] 43%|████▎     | 3863/8917 [5:07:06<5:15:19,  3.74s/it] 43%|████▎     | 3864/8917 [5:07:09<5:09:03,  3.67s/it] 43%|████▎     | 3865/8917 [5:07:12<5:00:53,  3.57s/it] 43%|████▎     | 3866/8917 [5:07:17<5:13:03,  3.72s/it] 43%|████▎     | 3867/8917 [5:07:20<5:13:42,  3.73s/it] 43%|████▎     | 3868/8917 [5:07:24<5:23:06,  3.84s/it] 43%|████▎     | 3869/8917 [5:07:28<5:15:35,  3.75s/it] 43%|████▎     | 3870/8917 [5:07:32<5:13:19,  3.72s/it] 43%|████▎     | 3871/8917 [5:07:35<5:17:25,  3.77s/it] 43%|████▎     | 3872/8917 [5:07:39<5:20:41,  3.81s/it] 43%|████▎     | 3873/8917 [5:07:43<5:18:25,  3.79s/it] 43%|████▎     | 3874/8917 [5:07:47<5:14:24,  3.74s/it] 43%|████▎     | 3875/8917 [5:07:50<5:11:48,  3.71s/it] 43%|████▎     | 3876/8917 [5:07:54<5:16:39,  3.77s/it] 43%|████▎     | 3877/8917 [5:07:58<5:12:49,  3.72s/it] 43%|████▎     | 3878/8917 [5:08:02<5:15:05,  3.75s/it] 44%|████▎     | 3879/8917 [5:08:05<5:11:22,  3.71s/it] 44%|████▎     | 3880/8917 [5:08:09<5:15:13,  3.76s/it] 44%|████▎     | 3881/8917 [5:08:13<5:11:27,  3.71s/it] 44%|████▎     | 3882/8917 [5:08:17<5:12:35,  3.73s/it] 44%|████▎     | 3883/8917 [5:08:21<5:18:43,  3.80s/it] 44%|████▎     | 3884/8917 [5:08:24<5:16:09,  3.77s/it] 44%|████▎     | 3885/8917 [5:08:28<5:18:45,  3.80s/it] 44%|████▎     | 3886/8917 [5:08:32<5:19:11,  3.81s/it] 44%|████▎     | 3887/8917 [5:08:35<5:12:47,  3.73s/it] 44%|████▎     | 3888/8917 [5:08:39<5:09:32,  3.69s/it] 44%|████▎     | 3889/8917 [5:08:43<5:16:56,  3.78s/it] 44%|████▎     | 3890/8917 [5:08:47<5:24:35,  3.87s/it] 44%|████▎     | 3891/8917 [5:08:51<5:19:14,  3.81s/it] 44%|████▎     | 3892/8917 [5:08:54<5:14:33,  3.76s/it] 44%|████▎     | 3893/8917 [5:08:58<5:13:38,  3.75s/it] 44%|████▎     | 3894/8917 [5:09:02<5:16:19,  3.78s/it] 44%|████▎     | 3895/8917 [5:09:06<5:14:11,  3.75s/it] 44%|████▎     | 3896/8917 [5:09:10<5:16:29,  3.78s/it] 44%|████▎     | 3897/8917 [5:09:13<5:10:26,  3.71s/it] 44%|████▎     | 3898/8917 [5:09:17<5:09:31,  3.70s/it] 44%|████▎     | 3899/8917 [5:09:20<5:09:02,  3.70s/it]09/19/2024 07:23:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.021118169650435448, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0575451850891113, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0786633491516113}
 44%|████▎     | 3900/8917 [5:09:24<5:16:40,  3.79s/it] 44%|████▎     | 3901/8917 [5:09:29<5:22:21,  3.86s/it] 44%|████▍     | 3902/8917 [5:09:32<5:13:39,  3.75s/it] 44%|████▍     | 3903/8917 [5:09:36<5:18:23,  3.81s/it] 44%|████▍     | 3904/8917 [5:09:40<5:17:36,  3.80s/it] 44%|████▍     | 3905/8917 [5:09:43<5:12:12,  3.74s/it] 44%|████▍     | 3906/8917 [5:09:47<5:16:03,  3.78s/it] 44%|████▍     | 3907/8917 [5:09:51<5:15:17,  3.78s/it] 44%|████▍     | 3908/8917 [5:09:55<5:13:05,  3.75s/it] 44%|████▍     | 3909/8917 [5:09:59<5:16:11,  3.79s/it] 44%|████▍     | 3910/8917 [5:10:03<5:20:37,  3.84s/it] 44%|████▍     | 3911/8917 [5:10:06<5:15:18,  3.78s/it] 44%|████▍     | 3912/8917 [5:10:10<5:12:34,  3.75s/it] 44%|████▍     | 3913/8917 [5:10:13<5:10:26,  3.72s/it] 44%|████▍     | 3914/8917 [5:10:17<5:16:28,  3.80s/it] 44%|████▍     | 3915/8917 [5:10:21<5:06:19,  3.67s/it] 44%|████▍     | 3916/8917 [5:10:25<5:14:03,  3.77s/it] 44%|████▍     | 3917/8917 [5:10:29<5:14:12,  3.77s/it] 44%|████▍     | 3918/8917 [5:10:32<5:12:08,  3.75s/it] 44%|████▍     | 3919/8917 [5:10:36<5:10:19,  3.73s/it] 44%|████▍     | 3920/8917 [5:10:40<5:07:20,  3.69s/it] 44%|████▍     | 3921/8917 [5:10:43<4:57:31,  3.57s/it] 44%|████▍     | 3922/8917 [5:10:47<5:07:36,  3.69s/it] 44%|████▍     | 3923/8917 [5:10:50<4:59:06,  3.59s/it] 44%|████▍     | 3924/8917 [5:10:54<5:00:37,  3.61s/it] 44%|████▍     | 3925/8917 [5:10:58<5:00:57,  3.62s/it] 44%|████▍     | 3926/8917 [5:11:02<5:11:12,  3.74s/it] 44%|████▍     | 3927/8917 [5:11:05<5:11:20,  3.74s/it] 44%|████▍     | 3928/8917 [5:11:09<5:18:17,  3.83s/it] 44%|████▍     | 3929/8917 [5:11:13<5:14:44,  3.79s/it] 44%|████▍     | 3930/8917 [5:11:17<5:09:21,  3.72s/it] 44%|████▍     | 3931/8917 [5:11:20<5:07:04,  3.70s/it] 44%|████▍     | 3932/8917 [5:11:24<5:07:13,  3.70s/it] 44%|████▍     | 3933/8917 [5:11:28<5:15:23,  3.80s/it] 44%|████▍     | 3934/8917 [5:11:31<5:08:21,  3.71s/it] 44%|████▍     | 3935/8917 [5:11:35<5:15:25,  3.80s/it] 44%|████▍     | 3936/8917 [5:11:39<5:14:35,  3.79s/it] 44%|████▍     | 3937/8917 [5:11:43<5:09:50,  3.73s/it] 44%|████▍     | 3938/8917 [5:11:47<5:09:34,  3.73s/it] 44%|████▍     | 3939/8917 [5:11:50<5:04:20,  3.67s/it] 44%|████▍     | 3940/8917 [5:11:54<5:12:11,  3.76s/it] 44%|████▍     | 3941/8917 [5:11:58<5:05:34,  3.68s/it] 44%|████▍     | 3942/8917 [5:12:02<5:15:21,  3.80s/it] 44%|████▍     | 3943/8917 [5:12:05<5:13:21,  3.78s/it] 44%|████▍     | 3944/8917 [5:12:09<5:08:32,  3.72s/it] 44%|████▍     | 3945/8917 [5:12:13<5:04:53,  3.68s/it] 44%|████▍     | 3946/8917 [5:12:16<5:06:14,  3.70s/it] 44%|████▍     | 3947/8917 [5:12:20<5:05:51,  3.69s/it] 44%|████▍     | 3948/8917 [5:12:24<5:10:36,  3.75s/it] 44%|████▍     | 3949/8917 [5:12:27<5:07:50,  3.72s/it]09/19/2024 07:27:05 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.00999634899199009, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9183199405670166, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9283162951469421}
 44%|████▍     | 3950/8917 [5:12:31<5:10:02,  3.75s/it] 44%|████▍     | 3951/8917 [5:12:35<5:04:10,  3.68s/it] 44%|████▍     | 3952/8917 [5:12:39<5:11:23,  3.76s/it] 44%|████▍     | 3953/8917 [5:12:43<5:15:49,  3.82s/it] 44%|████▍     | 3954/8917 [5:12:46<5:02:57,  3.66s/it] 44%|████▍     | 3955/8917 [5:12:50<5:11:54,  3.77s/it] 44%|████▍     | 3956/8917 [5:12:54<5:12:57,  3.79s/it] 44%|████▍     | 3957/8917 [5:12:57<5:04:41,  3.69s/it] 44%|████▍     | 3958/8917 [5:13:01<5:01:11,  3.64s/it] 44%|████▍     | 3959/8917 [5:13:04<5:01:13,  3.65s/it] 44%|████▍     | 3960/8917 [5:13:08<5:02:05,  3.66s/it] 44%|████▍     | 3961/8917 [5:13:12<5:09:42,  3.75s/it] 44%|████▍     | 3962/8917 [5:13:16<5:13:12,  3.79s/it] 44%|████▍     | 3963/8917 [5:13:20<5:11:36,  3.77s/it] 44%|████▍     | 3964/8917 [5:13:23<5:05:40,  3.70s/it] 44%|████▍     | 3965/8917 [5:13:27<5:03:18,  3.67s/it] 44%|████▍     | 3966/8917 [5:13:31<5:07:51,  3.73s/it] 44%|████▍     | 3967/8917 [5:13:34<5:04:11,  3.69s/it] 44%|████▍     | 3968/8917 [5:13:38<5:01:43,  3.66s/it] 45%|████▍     | 3969/8917 [5:13:42<5:08:57,  3.75s/it] 45%|████▍     | 3970/8917 [5:13:45<5:02:22,  3.67s/it] 45%|████▍     | 3971/8917 [5:13:49<5:06:35,  3.72s/it] 45%|████▍     | 3972/8917 [5:13:53<5:04:23,  3.69s/it] 45%|████▍     | 3973/8917 [5:13:57<5:15:20,  3.83s/it] 45%|████▍     | 3974/8917 [5:14:01<5:09:30,  3.76s/it] 45%|████▍     | 3975/8917 [5:14:04<5:02:35,  3.67s/it] 45%|████▍     | 3976/8917 [5:14:08<5:09:58,  3.76s/it] 45%|████▍     | 3977/8917 [5:14:12<5:15:07,  3.83s/it] 45%|████▍     | 3978/8917 [5:14:15<5:03:20,  3.69s/it] 45%|████▍     | 3979/8917 [5:14:19<5:12:48,  3.80s/it] 45%|████▍     | 3980/8917 [5:14:23<5:01:24,  3.66s/it] 45%|████▍     | 3981/8917 [5:14:27<5:08:20,  3.75s/it] 45%|████▍     | 3982/8917 [5:14:30<5:06:39,  3.73s/it] 45%|████▍     | 3983/8917 [5:14:34<4:58:50,  3.63s/it] 45%|████▍     | 3984/8917 [5:14:37<4:59:19,  3.64s/it] 45%|████▍     | 3985/8917 [5:14:41<4:58:59,  3.64s/it] 45%|████▍     | 3986/8917 [5:14:45<5:01:38,  3.67s/it] 45%|████▍     | 3987/8917 [5:14:49<5:10:43,  3.78s/it] 45%|████▍     | 3988/8917 [5:14:52<5:00:13,  3.65s/it] 45%|████▍     | 3989/8917 [5:14:56<4:58:31,  3.63s/it] 45%|████▍     | 3990/8917 [5:15:00<5:00:23,  3.66s/it] 45%|████▍     | 3991/8917 [5:15:04<5:10:03,  3.78s/it] 45%|████▍     | 3992/8917 [5:15:08<5:17:48,  3.87s/it] 45%|████▍     | 3993/8917 [5:15:11<5:03:19,  3.70s/it] 45%|████▍     | 3994/8917 [5:15:15<5:04:52,  3.72s/it] 45%|████▍     | 3995/8917 [5:15:19<5:05:35,  3.73s/it] 45%|████▍     | 3996/8917 [5:15:22<5:07:59,  3.76s/it] 45%|████▍     | 3997/8917 [5:15:26<5:06:16,  3.74s/it] 45%|████▍     | 3998/8917 [5:15:30<5:03:24,  3.70s/it] 45%|████▍     | 3999/8917 [5:15:33<5:02:59,  3.70s/it]09/19/2024 07:30:10 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 07:30:10 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<04:04,  1.11s/it][A
  1%|          | 2/221 [00:01<02:40,  1.37it/s][A
  1%|▏         | 3/221 [00:02<02:44,  1.33it/s][A
  2%|▏         | 4/221 [00:02<02:00,  1.80it/s][A
  2%|▏         | 5/221 [00:02<01:25,  2.53it/s][A
  3%|▎         | 6/221 [00:02<01:05,  3.28it/s][A
  3%|▎         | 7/221 [00:03<00:58,  3.68it/s][A
  4%|▎         | 8/221 [00:03<01:03,  3.35it/s][A
  4%|▍         | 9/221 [00:03<01:08,  3.11it/s][A
  5%|▍         | 10/221 [00:04<01:21,  2.60it/s][A
  5%|▌         | 12/221 [00:10<06:03,  1.74s/it][A
  6%|▌         | 13/221 [00:10<04:37,  1.33s/it][A
  7%|▋         | 15/221 [00:11<02:54,  1.18it/s][A
  7%|▋         | 16/221 [00:12<02:47,  1.23it/s][A
  8%|▊         | 17/221 [00:13<03:05,  1.10it/s][A
  8%|▊         | 18/221 [00:13<02:30,  1.35it/s][A
  9%|▊         | 19/221 [00:14<02:55,  1.15it/s][A
  9%|▉         | 20/221 [00:14<02:12,  1.52it/s][A
 10%|▉         | 21/221 [00:15<01:54,  1.74it/s][A
 10%|▉         | 22/221 [00:16<02:35,  1.28it/s][A
 11%|█         | 24/221 [00:16<01:39,  1.97it/s][A
 11%|█▏        | 25/221 [00:17<01:36,  2.02it/s][A
 12%|█▏        | 26/221 [00:17<01:24,  2.30it/s][A
 13%|█▎        | 28/221 [00:18<01:14,  2.58it/s][A
 13%|█▎        | 29/221 [00:18<01:04,  2.99it/s][A
 14%|█▎        | 30/221 [00:19<01:22,  2.33it/s][A
 14%|█▍        | 31/221 [00:19<01:19,  2.38it/s][A
 14%|█▍        | 32/221 [00:19<01:03,  2.98it/s][A
 15%|█▍        | 33/221 [00:19<00:59,  3.16it/s][A
 16%|█▌        | 35/221 [00:20<00:42,  4.39it/s][A
 16%|█▋        | 36/221 [00:20<00:45,  4.04it/s][A
 17%|█▋        | 37/221 [00:20<01:01,  3.00it/s][A
 17%|█▋        | 38/221 [00:21<01:13,  2.49it/s][A
 18%|█▊        | 39/221 [00:22<01:16,  2.37it/s][A
 18%|█▊        | 40/221 [00:22<01:22,  2.18it/s][A
 19%|█▊        | 41/221 [00:22<01:06,  2.69it/s][A
 19%|█▉        | 42/221 [00:22<01:01,  2.93it/s][A
 20%|█▉        | 44/221 [00:23<00:42,  4.15it/s][A
 20%|██        | 45/221 [00:28<04:15,  1.45s/it][A
 21%|██        | 46/221 [00:28<03:23,  1.16s/it][A
 21%|██▏       | 47/221 [00:29<02:58,  1.03s/it][A
 22%|██▏       | 48/221 [00:29<02:13,  1.30it/s][A
 22%|██▏       | 49/221 [00:29<01:50,  1.55it/s][A
 23%|██▎       | 50/221 [00:30<01:28,  1.94it/s][A
 23%|██▎       | 51/221 [00:30<01:11,  2.39it/s][A
 24%|██▎       | 52/221 [00:30<00:58,  2.88it/s][A
 24%|██▍       | 53/221 [00:30<00:53,  3.15it/s][A
 24%|██▍       | 54/221 [00:31<01:18,  2.12it/s][A
 25%|██▍       | 55/221 [00:32<01:40,  1.66it/s][A
 25%|██▌       | 56/221 [00:32<01:18,  2.09it/s][A
 26%|██▌       | 57/221 [00:32<01:04,  2.54it/s][A
 27%|██▋       | 59/221 [00:32<00:40,  4.01it/s][A
 27%|██▋       | 60/221 [00:33<00:50,  3.17it/s][A
 28%|██▊       | 61/221 [00:33<00:47,  3.35it/s][A
 28%|██▊       | 62/221 [00:34<00:49,  3.24it/s][A
 29%|██▊       | 63/221 [00:34<00:49,  3.18it/s][A
 29%|██▉       | 64/221 [00:35<01:19,  1.97it/s][A
 29%|██▉       | 65/221 [00:35<01:05,  2.37it/s][A
 30%|██▉       | 66/221 [00:35<01:03,  2.46it/s][A
 30%|███       | 67/221 [00:36<00:55,  2.77it/s][A
 31%|███       | 68/221 [00:36<00:46,  3.30it/s][A
 31%|███       | 69/221 [00:37<01:19,  1.90it/s][A
 32%|███▏      | 70/221 [00:37<01:01,  2.44it/s][A
 32%|███▏      | 71/221 [00:37<01:01,  2.46it/s][A
 33%|███▎      | 72/221 [00:38<00:59,  2.52it/s][A
 33%|███▎      | 73/221 [00:38<01:10,  2.09it/s][A
 33%|███▎      | 74/221 [00:39<00:54,  2.67it/s][A
 34%|███▍      | 75/221 [00:39<00:53,  2.74it/s][A
 34%|███▍      | 76/221 [00:39<00:50,  2.87it/s][A
 35%|███▍      | 77/221 [00:41<01:47,  1.34it/s][A
 35%|███▌      | 78/221 [00:41<01:20,  1.77it/s][A
 36%|███▌      | 79/221 [00:42<01:45,  1.35it/s][A
 36%|███▌      | 80/221 [00:42<01:17,  1.81it/s][A
 37%|███▋      | 81/221 [00:43<01:33,  1.50it/s][A
 37%|███▋      | 82/221 [00:46<03:10,  1.37s/it][A
 38%|███▊      | 83/221 [00:47<02:35,  1.12s/it][A
 38%|███▊      | 84/221 [00:47<01:55,  1.18it/s][A
 39%|███▉      | 86/221 [00:48<01:18,  1.72it/s][A
 39%|███▉      | 87/221 [00:48<01:23,  1.61it/s][A
 40%|███▉      | 88/221 [00:49<01:12,  1.82it/s][A
 40%|████      | 89/221 [00:49<01:02,  2.12it/s][A
 41%|████      | 90/221 [00:49<00:56,  2.32it/s][A
 41%|████      | 91/221 [00:49<00:45,  2.87it/s][A
 42%|████▏     | 92/221 [00:50<00:39,  3.27it/s][A
 42%|████▏     | 93/221 [00:50<00:44,  2.91it/s][A
 43%|████▎     | 94/221 [00:51<01:00,  2.10it/s][A
 43%|████▎     | 95/221 [00:51<00:51,  2.45it/s][A
 43%|████▎     | 96/221 [00:52<01:06,  1.87it/s][A
 44%|████▍     | 97/221 [00:52<00:54,  2.28it/s][A
 44%|████▍     | 98/221 [00:53<01:03,  1.93it/s][A
 45%|████▍     | 99/221 [00:53<00:58,  2.10it/s][A
 45%|████▌     | 100/221 [00:54<01:14,  1.63it/s][A
 46%|████▌     | 101/221 [00:54<00:56,  2.11it/s][A
 46%|████▌     | 102/221 [00:55<01:10,  1.68it/s][A
 47%|████▋     | 103/221 [00:55<00:52,  2.23it/s][A
 47%|████▋     | 104/221 [00:56<00:49,  2.36it/s][A
 48%|████▊     | 105/221 [00:56<00:55,  2.10it/s][A
 48%|████▊     | 106/221 [00:59<02:15,  1.17s/it][A
 48%|████▊     | 107/221 [00:59<01:48,  1.05it/s][A
 49%|████▉     | 108/221 [01:00<01:34,  1.20it/s][A
 49%|████▉     | 109/221 [01:00<01:14,  1.51it/s][A
 50%|█████     | 111/221 [01:01<00:51,  2.14it/s][A
 51%|█████     | 112/221 [01:01<00:45,  2.39it/s][A
 51%|█████     | 113/221 [01:01<00:38,  2.82it/s][A
 52%|█████▏    | 114/221 [01:01<00:31,  3.41it/s][A
 52%|█████▏    | 115/221 [01:02<00:44,  2.40it/s][A
 52%|█████▏    | 116/221 [01:02<00:41,  2.52it/s][A
 53%|█████▎    | 117/221 [01:03<00:43,  2.40it/s][A
 53%|█████▎    | 118/221 [01:03<00:43,  2.35it/s][A
 54%|█████▍    | 119/221 [01:04<00:39,  2.55it/s][A
 54%|█████▍    | 120/221 [01:04<00:32,  3.13it/s][A
 55%|█████▍    | 121/221 [01:04<00:36,  2.76it/s][A
 55%|█████▌    | 122/221 [01:05<00:36,  2.70it/s][A
 56%|█████▌    | 123/221 [01:06<00:56,  1.72it/s][A
 56%|█████▌    | 124/221 [01:06<00:45,  2.14it/s][A
 57%|█████▋    | 125/221 [01:07<00:53,  1.79it/s][A
 57%|█████▋    | 126/221 [01:12<03:10,  2.00s/it][A
 57%|█████▋    | 127/221 [01:13<02:29,  1.59s/it][A
 58%|█████▊    | 128/221 [01:13<01:55,  1.24s/it][A
 58%|█████▊    | 129/221 [01:14<01:44,  1.14s/it][A
 59%|█████▉    | 130/221 [01:14<01:18,  1.15it/s][A
 59%|█████▉    | 131/221 [01:16<01:27,  1.02it/s][A
 60%|█████▉    | 132/221 [01:17<01:40,  1.12s/it][A
 60%|██████    | 133/221 [01:17<01:21,  1.08it/s][A
 61%|██████    | 134/221 [01:19<01:29,  1.03s/it][A
 61%|██████    | 135/221 [01:20<01:35,  1.11s/it][A
 62%|██████▏   | 136/221 [01:20<01:15,  1.12it/s][A
 62%|██████▏   | 137/221 [01:21<01:03,  1.32it/s][A
 62%|██████▏   | 138/221 [01:21<00:57,  1.45it/s][A
 63%|██████▎   | 139/221 [01:22<00:45,  1.81it/s][A
 63%|██████▎   | 140/221 [01:22<00:48,  1.68it/s][A
 64%|██████▍   | 141/221 [01:23<00:41,  1.94it/s][A
 64%|██████▍   | 142/221 [01:23<00:38,  2.08it/s][A
 65%|██████▍   | 143/221 [01:23<00:36,  2.14it/s][A
 65%|██████▌   | 144/221 [01:24<00:28,  2.67it/s][A
 66%|██████▌   | 146/221 [01:24<00:19,  3.89it/s][A
 67%|██████▋   | 148/221 [01:25<00:24,  2.99it/s][A
 67%|██████▋   | 149/221 [01:25<00:28,  2.56it/s][A
 68%|██████▊   | 150/221 [01:26<00:30,  2.31it/s][A
 68%|██████▊   | 151/221 [01:26<00:28,  2.44it/s][A
 69%|██████▉   | 152/221 [01:27<00:27,  2.47it/s][A
 69%|██████▉   | 153/221 [01:27<00:21,  3.11it/s][A
 70%|██████▉   | 154/221 [01:27<00:20,  3.32it/s][A
 70%|███████   | 155/221 [01:27<00:18,  3.49it/s][A
 71%|███████   | 156/221 [01:27<00:17,  3.67it/s][A
 71%|███████   | 157/221 [01:32<01:33,  1.47s/it][A
 71%|███████▏  | 158/221 [01:33<01:21,  1.30s/it][A
 72%|███████▏  | 160/221 [01:33<00:47,  1.29it/s][A
 73%|███████▎  | 161/221 [01:33<00:36,  1.64it/s][A
 74%|███████▍  | 163/221 [01:33<00:23,  2.43it/s][A
 74%|███████▍  | 164/221 [01:34<00:21,  2.70it/s][A
 75%|███████▍  | 165/221 [01:34<00:22,  2.47it/s][A
 75%|███████▌  | 166/221 [01:35<00:24,  2.24it/s][A
 76%|███████▌  | 167/221 [01:35<00:20,  2.64it/s][A
 76%|███████▌  | 168/221 [01:40<01:29,  1.69s/it][A
 76%|███████▋  | 169/221 [01:41<01:12,  1.39s/it][A
 77%|███████▋  | 170/221 [01:41<00:59,  1.17s/it][A
 77%|███████▋  | 171/221 [01:42<00:46,  1.08it/s][A
 78%|███████▊  | 172/221 [01:42<00:36,  1.33it/s][A
 78%|███████▊  | 173/221 [01:42<00:31,  1.53it/s][A
 79%|███████▊  | 174/221 [01:42<00:23,  1.99it/s][A
 79%|███████▉  | 175/221 [01:43<00:20,  2.28it/s][A
 80%|███████▉  | 176/221 [01:43<00:19,  2.25it/s][A
 80%|████████  | 177/221 [01:44<00:18,  2.32it/s][A
 81%|████████  | 178/221 [01:44<00:18,  2.31it/s][A
 81%|████████  | 179/221 [01:44<00:17,  2.41it/s][A
 81%|████████▏ | 180/221 [01:45<00:14,  2.85it/s][A
 82%|████████▏ | 182/221 [01:45<00:11,  3.28it/s][A
 83%|████████▎ | 183/221 [01:46<00:16,  2.32it/s][A
 83%|████████▎ | 184/221 [01:46<00:16,  2.26it/s][A
 84%|████████▎ | 185/221 [01:47<00:14,  2.45it/s][A
 84%|████████▍ | 186/221 [01:47<00:15,  2.23it/s][A
 85%|████████▍ | 187/221 [01:48<00:13,  2.52it/s][A
 85%|████████▌ | 188/221 [01:48<00:10,  3.19it/s][A
 86%|████████▌ | 189/221 [01:48<00:09,  3.39it/s][A
 86%|████████▌ | 190/221 [01:48<00:10,  2.87it/s][A
 86%|████████▋ | 191/221 [01:49<00:08,  3.43it/s][A
 87%|████████▋ | 192/221 [01:49<00:13,  2.16it/s][A
 87%|████████▋ | 193/221 [01:50<00:10,  2.75it/s][A
 88%|████████▊ | 194/221 [01:50<00:10,  2.52it/s][A
 88%|████████▊ | 195/221 [01:50<00:09,  2.68it/s][A
 89%|████████▊ | 196/221 [01:51<00:09,  2.68it/s][A
 89%|████████▉ | 197/221 [01:51<00:07,  3.10it/s][A
 90%|████████▉ | 198/221 [01:51<00:06,  3.39it/s][A
 90%|█████████ | 199/221 [01:51<00:05,  3.74it/s][A
 90%|█████████ | 200/221 [01:52<00:06,  3.06it/s][A
 91%|█████████ | 201/221 [01:53<00:08,  2.27it/s][A
 91%|█████████▏| 202/221 [01:53<00:07,  2.45it/s][A
 92%|█████████▏| 203/221 [01:54<00:12,  1.40it/s][A
 92%|█████████▏| 204/221 [01:54<00:09,  1.80it/s][A
 93%|█████████▎| 206/221 [01:55<00:05,  2.50it/s][A
 94%|█████████▎| 207/221 [01:55<00:04,  3.05it/s][A
 94%|█████████▍| 208/221 [01:55<00:04,  2.95it/s][A
 95%|█████████▌| 210/221 [01:56<00:02,  4.26it/s][A
 95%|█████████▌| 211/221 [01:56<00:03,  3.30it/s][A
 96%|█████████▌| 212/221 [01:56<00:02,  3.19it/s][A
 96%|█████████▋| 213/221 [01:57<00:02,  3.64it/s][A
 97%|█████████▋| 214/221 [01:57<00:02,  3.32it/s][A
 97%|█████████▋| 215/221 [01:57<00:01,  3.16it/s][A
 98%|█████████▊| 216/221 [01:58<00:01,  3.01it/s][A
 98%|█████████▊| 217/221 [02:01<00:04,  1.09s/it][A
 99%|█████████▊| 218/221 [02:01<00:02,  1.15it/s][A
 99%|█████████▉| 219/221 [02:01<00:01,  1.46it/s][A
100%|█████████▉| 220/221 [02:03<00:00,  1.18it/s][A
100%|██████████| 221/221 [02:03<00:00,  1.57it/s][A100%|██████████| 221/221 [02:03<00:00,  1.79it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:23,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:55,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:23<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:51<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:30,  7.17it/s][A
  1%|          | 2/221 [00:00<01:43,  2.11it/s][A
  1%|▏         | 3/221 [00:01<01:21,  2.69it/s][A
  2%|▏         | 4/221 [00:01<01:42,  2.12it/s][A
  2%|▏         | 5/221 [00:02<01:50,  1.96it/s][A
  3%|▎         | 6/221 [00:02<01:29,  2.41it/s][A
  3%|▎         | 7/221 [00:02<01:24,  2.55it/s][A
  4%|▎         | 8/221 [00:03<01:38,  2.15it/s][A
  4%|▍         | 9/221 [00:04<01:53,  1.87it/s][A
  5%|▍         | 10/221 [00:04<02:00,  1.74it/s][A
  5%|▍         | 11/221 [00:05<02:03,  1.70it/s][A
  5%|▌         | 12/221 [00:05<01:47,  1.94it/s][A
  6%|▌         | 13/221 [00:06<01:38,  2.12it/s][A
  6%|▋         | 14/221 [00:06<01:30,  2.29it/s][A
  7%|▋         | 15/221 [00:06<01:13,  2.80it/s][A
  7%|▋         | 16/221 [00:07<01:28,  2.32it/s][A
  8%|▊         | 17/221 [00:08<02:27,  1.38it/s][A
  8%|▊         | 18/221 [00:08<01:55,  1.75it/s][A
  9%|▊         | 19/221 [00:09<02:07,  1.58it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:10<01:37,  2.05it/s][A
 10%|▉         | 22/221 [00:11<01:49,  1.82it/s][A
 10%|█         | 23/221 [00:11<01:37,  2.04it/s][A
 11%|█         | 24/221 [00:11<01:20,  2.46it/s][A
 11%|█▏        | 25/221 [00:12<01:45,  1.85it/s][A
 12%|█▏        | 26/221 [00:13<02:25,  1.34it/s][A
 12%|█▏        | 27/221 [00:13<01:50,  1.76it/s][A
 13%|█▎        | 28/221 [00:15<02:29,  1.29it/s][A
 13%|█▎        | 29/221 [00:15<02:14,  1.43it/s][A
 14%|█▎        | 30/221 [00:16<02:02,  1.56it/s][A
 14%|█▍        | 31/221 [00:16<01:50,  1.72it/s][A
 14%|█▍        | 32/221 [00:17<01:47,  1.77it/s][A
 15%|█▍        | 33/221 [00:17<01:49,  1.72it/s][A
 15%|█▌        | 34/221 [00:18<01:33,  2.01it/s][A
 16%|█▌        | 35/221 [00:18<01:11,  2.62it/s][A
 16%|█▋        | 36/221 [00:18<01:18,  2.35it/s][A
 17%|█▋        | 37/221 [00:19<01:20,  2.29it/s][A
 17%|█▋        | 38/221 [00:20<01:43,  1.77it/s][A
 18%|█▊        | 39/221 [00:20<01:49,  1.67it/s][A
 18%|█▊        | 40/221 [00:21<02:07,  1.42it/s][A
 19%|█▊        | 41/221 [00:21<01:44,  1.72it/s][A
 19%|█▉        | 42/221 [00:22<01:22,  2.18it/s][A
 19%|█▉        | 43/221 [00:22<01:35,  1.87it/s][A
 20%|█▉        | 44/221 [00:23<01:29,  1.97it/s][A
 20%|██        | 45/221 [00:23<01:09,  2.53it/s][A
 21%|██        | 46/221 [00:23<01:08,  2.54it/s][A
 21%|██▏       | 47/221 [00:24<01:14,  2.32it/s][A
 22%|██▏       | 48/221 [00:24<00:59,  2.89it/s][A
 22%|██▏       | 49/221 [00:25<01:38,  1.75it/s][A
 23%|██▎       | 50/221 [00:26<01:53,  1.51it/s][A
 23%|██▎       | 51/221 [00:26<01:36,  1.77it/s][A
 24%|██▎       | 52/221 [00:27<01:21,  2.07it/s][A
 24%|██▍       | 53/221 [00:27<01:17,  2.17it/s][A
 24%|██▍       | 54/221 [00:27<01:17,  2.17it/s][A
 25%|██▍       | 55/221 [00:28<01:03,  2.60it/s][A
 25%|██▌       | 56/221 [00:28<01:23,  1.98it/s][A
 26%|██▌       | 57/221 [00:29<01:09,  2.36it/s][A
 26%|██▌       | 58/221 [00:29<01:16,  2.13it/s][A
 27%|██▋       | 59/221 [00:29<00:58,  2.76it/s][A
 27%|██▋       | 60/221 [00:30<01:02,  2.56it/s][A
 28%|██▊       | 61/221 [00:31<01:24,  1.90it/s][A
 28%|██▊       | 62/221 [00:31<01:17,  2.05it/s][A
 29%|██▊       | 63/221 [00:31<01:09,  2.29it/s][A
 29%|██▉       | 64/221 [00:33<01:49,  1.44it/s][A
 29%|██▉       | 65/221 [00:34<01:57,  1.32it/s][A
 30%|██▉       | 66/221 [00:34<01:46,  1.45it/s][A
 30%|███       | 67/221 [00:34<01:23,  1.85it/s][A
 31%|███       | 68/221 [00:34<01:04,  2.38it/s][A
 31%|███       | 69/221 [00:35<00:52,  2.89it/s][A
 32%|███▏      | 70/221 [00:35<01:03,  2.39it/s][A
 32%|███▏      | 71/221 [00:36<01:04,  2.34it/s][A
 33%|███▎      | 72/221 [00:36<01:04,  2.33it/s][A
 33%|███▎      | 73/221 [00:37<01:16,  1.93it/s][A
 33%|███▎      | 74/221 [00:37<01:11,  2.06it/s][A
 34%|███▍      | 75/221 [00:38<01:04,  2.28it/s][A
 34%|███▍      | 76/221 [00:38<01:05,  2.20it/s][A
 35%|███▍      | 77/221 [00:38<01:01,  2.34it/s][A
 35%|███▌      | 78/221 [00:39<01:09,  2.05it/s][A
 36%|███▌      | 79/221 [00:40<01:31,  1.54it/s][A
 36%|███▌      | 80/221 [00:40<01:11,  1.98it/s][A
 37%|███▋      | 81/221 [00:41<01:06,  2.11it/s][A
 37%|███▋      | 82/221 [00:42<01:48,  1.28it/s][A
 38%|███▊      | 83/221 [00:43<01:42,  1.34it/s][A
 38%|███▊      | 84/221 [00:44<01:50,  1.24it/s][A
 38%|███▊      | 85/221 [00:44<01:26,  1.58it/s][A
 39%|███▉      | 86/221 [00:44<01:20,  1.67it/s][A
 39%|███▉      | 87/221 [00:45<01:14,  1.80it/s][A
 40%|███▉      | 88/221 [00:45<01:01,  2.16it/s][A
 40%|████      | 89/221 [00:46<01:07,  1.95it/s][A
 41%|████      | 90/221 [00:46<01:11,  1.83it/s][A
 41%|████      | 91/221 [00:47<01:05,  1.99it/s][A
 42%|████▏     | 92/221 [00:47<01:03,  2.03it/s][A
 43%|████▎     | 94/221 [00:48<00:40,  3.11it/s][A
 43%|████▎     | 95/221 [00:49<01:01,  2.05it/s][A
 43%|████▎     | 96/221 [00:49<01:00,  2.08it/s][A
 44%|████▍     | 97/221 [00:50<01:01,  2.00it/s][A
 44%|████▍     | 98/221 [00:50<01:11,  1.72it/s][A
 45%|████▍     | 99/221 [00:51<01:10,  1.72it/s][A
 45%|████▌     | 100/221 [00:52<01:12,  1.66it/s][A
 46%|████▌     | 101/221 [00:52<01:08,  1.76it/s][A
 46%|████▌     | 102/221 [00:53<01:27,  1.36it/s][A
 47%|████▋     | 103/221 [00:53<01:05,  1.79it/s][A
 47%|████▋     | 104/221 [00:54<01:19,  1.48it/s][A
 48%|████▊     | 105/221 [00:55<01:06,  1.75it/s][A
 48%|████▊     | 106/221 [00:55<00:53,  2.16it/s][A
 48%|████▊     | 107/221 [00:55<00:51,  2.20it/s][A
 49%|████▉     | 108/221 [00:56<00:51,  2.17it/s][A
 49%|████▉     | 109/221 [00:56<00:42,  2.61it/s][A
 50%|████▉     | 110/221 [00:56<00:45,  2.44it/s][A
 50%|█████     | 111/221 [00:57<00:59,  1.83it/s][A
 51%|█████     | 112/221 [00:58<01:02,  1.75it/s][A
 51%|█████     | 113/221 [00:58<00:51,  2.11it/s][A
 52%|█████▏    | 114/221 [00:59<00:55,  1.92it/s][A
 52%|█████▏    | 115/221 [00:59<01:01,  1.73it/s][A
 52%|█████▏    | 116/221 [01:00<00:51,  2.04it/s][A
 53%|█████▎    | 117/221 [01:00<00:48,  2.15it/s][A
 53%|█████▎    | 118/221 [01:01<01:00,  1.70it/s][A
 54%|█████▍    | 119/221 [01:01<00:52,  1.94it/s][A
 54%|█████▍    | 120/221 [01:02<00:53,  1.88it/s][A
 55%|█████▍    | 121/221 [01:03<01:00,  1.64it/s][A
 55%|█████▌    | 122/221 [01:03<00:54,  1.81it/s][A
 56%|█████▌    | 123/221 [01:04<00:47,  2.04it/s][A
 56%|█████▌    | 124/221 [01:04<00:50,  1.91it/s][A
 57%|█████▋    | 125/221 [01:05<00:55,  1.74it/s][A
 57%|█████▋    | 126/221 [01:05<00:42,  2.24it/s][A
 57%|█████▋    | 127/221 [01:06<00:59,  1.57it/s][A
 58%|█████▊    | 128/221 [01:07<00:58,  1.60it/s][A
 58%|█████▊    | 129/221 [01:07<00:49,  1.87it/s][A
 59%|█████▉    | 130/221 [01:07<00:39,  2.29it/s][A
 59%|█████▉    | 131/221 [01:08<00:37,  2.41it/s][A
 60%|█████▉    | 132/221 [01:08<00:47,  1.86it/s][A
 60%|██████    | 133/221 [01:09<00:58,  1.49it/s][A
 61%|██████    | 134/221 [01:10<00:49,  1.77it/s][A
 61%|██████    | 135/221 [01:10<00:48,  1.78it/s][A
 62%|██████▏   | 136/221 [01:11<00:46,  1.83it/s][A
 62%|██████▏   | 137/221 [01:11<00:45,  1.86it/s][A
 62%|██████▏   | 138/221 [01:12<00:45,  1.82it/s][A
 63%|██████▎   | 139/221 [01:13<01:05,  1.26it/s][A
 63%|██████▎   | 140/221 [01:14<00:58,  1.38it/s][A
 64%|██████▍   | 141/221 [01:14<00:46,  1.73it/s][A
 64%|██████▍   | 142/221 [01:14<00:43,  1.81it/s][A
 65%|██████▍   | 143/221 [01:15<00:37,  2.06it/s][A
 65%|██████▌   | 144/221 [01:15<00:28,  2.68it/s][A
 66%|██████▌   | 145/221 [01:16<00:37,  2.02it/s][A
 66%|██████▌   | 146/221 [01:16<00:29,  2.57it/s][A
 67%|██████▋   | 147/221 [01:16<00:25,  2.91it/s][A
 67%|██████▋   | 148/221 [01:16<00:25,  2.86it/s][A
 68%|██████▊   | 150/221 [01:17<00:20,  3.45it/s][A
 69%|██████▉   | 152/221 [01:18<00:24,  2.80it/s][A
 69%|██████▉   | 153/221 [01:18<00:24,  2.82it/s][A
 70%|██████▉   | 154/221 [01:19<00:29,  2.28it/s][A
 70%|███████   | 155/221 [01:20<00:37,  1.76it/s][A
 71%|███████   | 156/221 [01:20<00:32,  2.01it/s][A
 71%|███████   | 157/221 [01:21<00:30,  2.08it/s][A
 71%|███████▏  | 158/221 [01:21<00:27,  2.31it/s][A
 72%|███████▏  | 160/221 [01:21<00:21,  2.78it/s][A
 73%|███████▎  | 161/221 [01:21<00:17,  3.36it/s][A
 73%|███████▎  | 162/221 [01:22<00:16,  3.50it/s][A
 74%|███████▍  | 163/221 [01:22<00:15,  3.83it/s][A
 74%|███████▍  | 164/221 [01:22<00:15,  3.57it/s][A
 75%|███████▍  | 165/221 [01:22<00:14,  3.87it/s][A
 75%|███████▌  | 166/221 [01:23<00:20,  2.73it/s][A
 76%|███████▌  | 167/221 [01:23<00:17,  3.11it/s][A
 76%|███████▌  | 168/221 [01:24<00:16,  3.17it/s][A
 76%|███████▋  | 169/221 [01:25<00:25,  2.02it/s][A
 77%|███████▋  | 170/221 [01:26<00:37,  1.36it/s][A
 77%|███████▋  | 171/221 [01:26<00:34,  1.46it/s][A
 78%|███████▊  | 172/221 [01:27<00:27,  1.81it/s][A
 78%|███████▊  | 173/221 [01:27<00:23,  2.03it/s][A
 79%|███████▊  | 174/221 [01:28<00:25,  1.82it/s][A
 79%|███████▉  | 175/221 [01:29<00:29,  1.57it/s][A
 80%|███████▉  | 176/221 [01:29<00:26,  1.67it/s][A
 80%|████████  | 177/221 [01:30<00:25,  1.70it/s][A
 81%|████████  | 178/221 [01:30<00:25,  1.69it/s][A
 81%|████████  | 179/221 [01:31<00:23,  1.76it/s][A
 81%|████████▏ | 180/221 [01:31<00:21,  1.88it/s][A
 82%|████████▏ | 181/221 [01:31<00:17,  2.29it/s][A
 82%|████████▏ | 182/221 [01:32<00:19,  2.04it/s][A
 83%|████████▎ | 183/221 [01:32<00:17,  2.24it/s][A
 83%|████████▎ | 184/221 [01:33<00:23,  1.59it/s][A
 84%|████████▎ | 185/221 [01:34<00:23,  1.53it/s][A
 85%|████████▍ | 187/221 [01:35<00:16,  2.07it/s][A
 85%|████████▌ | 188/221 [01:35<00:13,  2.44it/s][A
 86%|████████▌ | 189/221 [01:35<00:13,  2.30it/s][A
 86%|████████▌ | 190/221 [01:36<00:14,  2.19it/s][A
 86%|████████▋ | 191/221 [01:36<00:13,  2.19it/s][A
 87%|████████▋ | 192/221 [01:37<00:12,  2.34it/s][A
 87%|████████▋ | 193/221 [01:37<00:10,  2.64it/s][A
 88%|████████▊ | 194/221 [01:37<00:11,  2.33it/s][A
 88%|████████▊ | 195/221 [01:39<00:20,  1.27it/s][A
 89%|████████▊ | 196/221 [01:40<00:21,  1.17it/s][A
 89%|████████▉ | 197/221 [01:41<00:17,  1.37it/s][A
 90%|████████▉ | 198/221 [01:41<00:13,  1.71it/s][A
 90%|█████████ | 199/221 [01:41<00:11,  1.99it/s][A
 90%|█████████ | 200/221 [01:42<00:14,  1.48it/s][A
 91%|█████████ | 201/221 [01:43<00:11,  1.71it/s][A
 91%|█████████▏| 202/221 [01:43<00:09,  1.98it/s][A
 92%|█████████▏| 203/221 [01:43<00:09,  1.98it/s][A
 92%|█████████▏| 204/221 [01:44<00:11,  1.54it/s][A
 93%|█████████▎| 205/221 [01:45<00:08,  1.91it/s][A
 93%|█████████▎| 206/221 [01:45<00:07,  2.01it/s][A
 94%|█████████▎| 207/221 [01:45<00:05,  2.35it/s][A
 94%|█████████▍| 208/221 [01:47<00:09,  1.42it/s][A
 95%|█████████▍| 209/221 [01:47<00:08,  1.43it/s][A
 95%|█████████▌| 210/221 [01:48<00:05,  1.84it/s][A
 95%|█████████▌| 211/221 [01:48<00:05,  1.91it/s][A
 96%|█████████▌| 212/221 [01:48<00:03,  2.28it/s][A
 96%|█████████▋| 213/221 [01:49<00:04,  1.92it/s][A
 97%|█████████▋| 214/221 [01:50<00:04,  1.72it/s][A
 97%|█████████▋| 215/221 [01:50<00:02,  2.28it/s][A
 98%|█████████▊| 216/221 [01:50<00:02,  2.02it/s][A
 98%|█████████▊| 217/221 [01:51<00:02,  1.92it/s][A
 99%|█████████▊| 218/221 [01:51<00:01,  2.36it/s][A
 99%|█████████▉| 219/221 [01:52<00:00,  2.55it/s][A
100%|█████████▉| 220/221 [01:52<00:00,  2.48it/s][A
100%|██████████| 221/221 [01:52<00:00,  2.43it/s][A100%|██████████| 221/221 [01:52<00:00,  1.96it/s]
09/19/2024 07:39:00 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 3999--===========

09/19/2024 07:39:00 - INFO - __main__ -   {'area_r1': 44.5, 'area_recall': '44.5/74.0/84.0', 'area_ravg': 67.5}
09/19/2024 07:39:00 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 3999--===========

09/19/2024 07:39:00 - INFO - __main__ -   {'forward_r1': 51.5, 'forward_recall': '51.5/79.0/87.6', 'forward_ravg': 72.7}
09/19/2024 07:39:00 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 3999--===========

09/19/2024 07:39:00 - INFO - __main__ -   {'area_video_r1': 48.8, 'area_video_recall': '48.8/78.5/88.0', 'area_video_ravg': 71.8}
09/19/2024 07:39:00 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 07:39:00 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 07:39:00 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 3999--===========

09/19/2024 07:39:00 - INFO - __main__ -   {'area_video_r1': 62.0, 'area_video_recall': '62.0/83.4/88.2', 'area_video_ravg': 77.9, 'area_video_back_r1': 62.4, 'area_video_back_recall': '62.4/85.1/91.1', 'area_video_back_ravg': 79.5}
09/19/2024 07:39:00 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 3499=======

09/19/2024 07:39:00 - INFO - __main__ -   {'area_video_r1': 62.6, 'area_video_recall': '62.6/82.9/88.5', 'area_video_ravg': 78.0, 'area_video_back_r1': 62.0, 'area_video_back_recall': '62.0/85.4/91.2', 'area_video_back_ravg': 79.5}
09/19/2024 07:39:00 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 3999--===========

09/19/2024 07:39:00 - INFO - __main__ -   {'video_r1': 30.5, 'video_recall': '30.5/55.8/68.0', 'video_ravg': 51.4}
09/19/2024 07:39:00 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 07:39:00 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 07:39:00 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 3999--===========

09/19/2024 07:39:00 - INFO - __main__ -   {'video_r1': 59.8, 'video_recall': '59.8/79.9/83.9', 'video_ravg': 74.5}
09/19/2024 07:39:00 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 2999=======

09/19/2024 07:39:00 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/79.5/84.7', 'video_ravg': 74.8}
09/19/2024 07:39:21 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015244932845234871, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9727613925933838, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9880063533782959}
 45%|████▍     | 4000/8917 [5:24:47<230:21:02, 168.65s/it] 45%|████▍     | 4001/8917 [5:24:50<162:31:50, 119.02s/it] 45%|████▍     | 4002/8917 [5:24:53<115:07:27, 84.32s/it]  45%|████▍     | 4003/8917 [5:24:57<82:08:41, 60.18s/it]  45%|████▍     | 4004/8917 [5:25:01<59:06:31, 43.31s/it] 45%|████▍     | 4005/8917 [5:25:05<42:44:46, 31.33s/it] 45%|████▍     | 4006/8917 [5:25:09<31:33:27, 23.13s/it] 45%|████▍     | 4007/8917 [5:25:12<23:38:05, 17.33s/it] 45%|████▍     | 4008/8917 [5:25:16<18:03:54, 13.25s/it] 45%|████▍     | 4009/8917 [5:25:20<14:06:03, 10.34s/it] 45%|████▍     | 4010/8917 [5:25:23<11:16:01,  8.27s/it] 45%|████▍     | 4011/8917 [5:25:27<9:24:16,  6.90s/it]  45%|████▍     | 4012/8917 [5:25:31<8:16:48,  6.08s/it] 45%|████▌     | 4013/8917 [5:25:35<7:22:43,  5.42s/it] 45%|████▌     | 4014/8917 [5:25:39<6:39:18,  4.89s/it] 45%|████▌     | 4015/8917 [5:25:43<6:19:33,  4.65s/it] 45%|████▌     | 4016/8917 [5:25:46<5:55:48,  4.36s/it] 45%|████▌     | 4017/8917 [5:25:50<5:40:16,  4.17s/it] 45%|████▌     | 4018/8917 [5:25:54<5:31:43,  4.06s/it] 45%|████▌     | 4019/8917 [5:25:57<5:20:21,  3.92s/it] 45%|████▌     | 4020/8917 [5:26:01<5:17:44,  3.89s/it] 45%|████▌     | 4021/8917 [5:26:05<5:10:42,  3.81s/it] 45%|████▌     | 4022/8917 [5:26:09<5:07:18,  3.77s/it] 45%|████▌     | 4023/8917 [5:26:12<5:03:58,  3.73s/it] 45%|████▌     | 4024/8917 [5:26:17<5:20:45,  3.93s/it] 45%|████▌     | 4025/8917 [5:26:20<5:07:41,  3.77s/it] 45%|████▌     | 4026/8917 [5:26:24<5:02:41,  3.71s/it] 45%|████▌     | 4027/8917 [5:26:27<5:07:48,  3.78s/it] 45%|████▌     | 4028/8917 [5:26:31<5:04:28,  3.74s/it] 45%|████▌     | 4029/8917 [5:26:35<5:05:50,  3.75s/it] 45%|████▌     | 4030/8917 [5:26:38<4:58:41,  3.67s/it] 45%|████▌     | 4031/8917 [5:26:42<5:03:06,  3.72s/it] 45%|████▌     | 4032/8917 [5:26:46<5:04:51,  3.74s/it] 45%|████▌     | 4033/8917 [5:26:50<5:01:23,  3.70s/it] 45%|████▌     | 4034/8917 [5:26:53<4:55:25,  3.63s/it] 45%|████▌     | 4035/8917 [5:26:57<4:51:51,  3.59s/it] 45%|████▌     | 4036/8917 [5:27:00<4:57:15,  3.65s/it] 45%|████▌     | 4037/8917 [5:27:04<4:55:45,  3.64s/it] 45%|████▌     | 4038/8917 [5:27:08<5:09:34,  3.81s/it] 45%|████▌     | 4039/8917 [5:27:12<5:06:18,  3.77s/it] 45%|████▌     | 4040/8917 [5:27:16<5:04:02,  3.74s/it] 45%|████▌     | 4041/8917 [5:27:20<5:12:44,  3.85s/it] 45%|████▌     | 4042/8917 [5:27:23<5:09:23,  3.81s/it] 45%|████▌     | 4043/8917 [5:27:27<5:12:31,  3.85s/it] 45%|████▌     | 4044/8917 [5:27:31<5:12:22,  3.85s/it] 45%|████▌     | 4045/8917 [5:27:35<5:01:36,  3.71s/it] 45%|████▌     | 4046/8917 [5:27:38<4:56:41,  3.65s/it] 45%|████▌     | 4047/8917 [5:27:42<4:55:10,  3.64s/it] 45%|████▌     | 4048/8917 [5:27:45<4:55:06,  3.64s/it] 45%|████▌     | 4049/8917 [5:27:49<4:51:23,  3.59s/it]09/19/2024 07:42:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0238476749509573, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.343990445137024, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3678381443023682}
 45%|████▌     | 4050/8917 [5:27:53<4:54:50,  3.63s/it] 45%|████▌     | 4051/8917 [5:27:57<5:03:47,  3.75s/it] 45%|████▌     | 4052/8917 [5:28:00<5:08:32,  3.81s/it] 45%|████▌     | 4053/8917 [5:28:04<5:00:49,  3.71s/it] 45%|████▌     | 4054/8917 [5:28:08<4:56:35,  3.66s/it] 45%|████▌     | 4055/8917 [5:28:11<4:45:41,  3.53s/it] 45%|████▌     | 4056/8917 [5:28:14<4:48:40,  3.56s/it] 45%|████▌     | 4057/8917 [5:28:18<4:50:02,  3.58s/it] 46%|████▌     | 4058/8917 [5:28:22<5:05:42,  3.77s/it] 46%|████▌     | 4059/8917 [5:28:26<5:02:49,  3.74s/it] 46%|████▌     | 4060/8917 [5:28:29<4:58:22,  3.69s/it] 46%|████▌     | 4061/8917 [5:28:33<4:56:43,  3.67s/it] 46%|████▌     | 4062/8917 [5:28:37<5:00:59,  3.72s/it] 46%|████▌     | 4063/8917 [5:28:41<5:01:31,  3.73s/it] 46%|████▌     | 4064/8917 [5:28:44<5:02:44,  3.74s/it] 46%|████▌     | 4065/8917 [5:28:48<5:00:04,  3.71s/it] 46%|████▌     | 4066/8917 [5:28:52<4:57:17,  3.68s/it] 46%|████▌     | 4067/8917 [5:28:55<4:48:45,  3.57s/it] 46%|████▌     | 4068/8917 [5:28:59<4:57:51,  3.69s/it] 46%|████▌     | 4069/8917 [5:29:03<5:07:26,  3.80s/it] 46%|████▌     | 4070/8917 [5:29:07<5:08:05,  3.81s/it] 46%|████▌     | 4071/8917 [5:29:11<5:06:27,  3.79s/it] 46%|████▌     | 4072/8917 [5:29:15<5:18:00,  3.94s/it] 46%|████▌     | 4073/8917 [5:29:19<5:17:55,  3.94s/it] 46%|████▌     | 4074/8917 [5:29:22<5:04:48,  3.78s/it] 46%|████▌     | 4075/8917 [5:29:26<5:04:10,  3.77s/it] 46%|████▌     | 4076/8917 [5:29:30<5:09:34,  3.84s/it] 46%|████▌     | 4077/8917 [5:29:34<5:14:32,  3.90s/it] 46%|████▌     | 4078/8917 [5:29:38<5:11:13,  3.86s/it] 46%|████▌     | 4079/8917 [5:29:41<5:06:23,  3.80s/it] 46%|████▌     | 4080/8917 [5:29:45<4:57:10,  3.69s/it] 46%|████▌     | 4081/8917 [5:29:49<5:00:40,  3.73s/it] 46%|████▌     | 4082/8917 [5:29:53<5:07:24,  3.81s/it] 46%|████▌     | 4083/8917 [5:29:56<5:05:02,  3.79s/it] 46%|████▌     | 4084/8917 [5:30:00<5:01:28,  3.74s/it] 46%|████▌     | 4085/8917 [5:30:04<4:57:49,  3.70s/it] 46%|████▌     | 4086/8917 [5:30:07<4:56:42,  3.68s/it] 46%|████▌     | 4087/8917 [5:30:11<5:06:07,  3.80s/it] 46%|████▌     | 4088/8917 [5:30:15<5:01:38,  3.75s/it] 46%|████▌     | 4089/8917 [5:30:19<4:59:39,  3.72s/it] 46%|████▌     | 4090/8917 [5:30:22<5:00:03,  3.73s/it] 46%|████▌     | 4091/8917 [5:30:26<4:55:47,  3.68s/it] 46%|████▌     | 4092/8917 [5:30:30<5:01:55,  3.75s/it] 46%|████▌     | 4093/8917 [5:30:33<4:53:06,  3.65s/it] 46%|████▌     | 4094/8917 [5:30:37<4:54:46,  3.67s/it] 46%|████▌     | 4095/8917 [5:30:41<5:04:48,  3.79s/it] 46%|████▌     | 4096/8917 [5:30:45<4:56:13,  3.69s/it] 46%|████▌     | 4097/8917 [5:30:48<4:52:37,  3.64s/it] 46%|████▌     | 4098/8917 [5:30:52<4:59:13,  3.73s/it] 46%|████▌     | 4099/8917 [5:30:56<4:57:27,  3.70s/it]09/19/2024 07:45:33 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03698423132300377, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3171405792236328, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3541247844696045}
 46%|████▌     | 4100/8917 [5:30:59<4:56:11,  3.69s/it] 46%|████▌     | 4101/8917 [5:31:03<4:48:55,  3.60s/it] 46%|████▌     | 4102/8917 [5:31:06<4:53:00,  3.65s/it] 46%|████▌     | 4103/8917 [5:31:10<4:58:14,  3.72s/it] 46%|████▌     | 4104/8917 [5:31:14<4:55:24,  3.68s/it] 46%|████▌     | 4105/8917 [5:31:18<4:56:49,  3.70s/it] 46%|████▌     | 4106/8917 [5:31:22<5:00:32,  3.75s/it] 46%|████▌     | 4107/8917 [5:31:25<4:57:43,  3.71s/it] 46%|████▌     | 4108/8917 [5:31:29<5:01:45,  3.76s/it] 46%|████▌     | 4109/8917 [5:31:33<4:57:16,  3.71s/it] 46%|████▌     | 4110/8917 [5:31:37<5:03:40,  3.79s/it] 46%|████▌     | 4111/8917 [5:31:41<5:09:53,  3.87s/it] 46%|████▌     | 4112/8917 [5:31:44<4:55:20,  3.69s/it] 46%|████▌     | 4113/8917 [5:31:48<4:58:42,  3.73s/it] 46%|████▌     | 4114/8917 [5:31:52<5:02:16,  3.78s/it] 46%|████▌     | 4115/8917 [5:31:55<4:59:42,  3.74s/it] 46%|████▌     | 4116/8917 [5:31:59<4:56:42,  3.71s/it] 46%|████▌     | 4117/8917 [5:32:03<4:56:55,  3.71s/it] 46%|████▌     | 4118/8917 [5:32:06<4:52:52,  3.66s/it] 46%|████▌     | 4119/8917 [5:32:10<4:58:08,  3.73s/it] 46%|████▌     | 4120/8917 [5:32:14<4:56:02,  3.70s/it] 46%|████▌     | 4121/8917 [5:32:18<5:00:40,  3.76s/it] 46%|████▌     | 4122/8917 [5:32:22<5:04:05,  3.81s/it] 46%|████▌     | 4123/8917 [5:32:25<4:59:53,  3.75s/it] 46%|████▌     | 4124/8917 [5:32:29<5:00:19,  3.76s/it] 46%|████▋     | 4125/8917 [5:32:33<4:56:21,  3.71s/it] 46%|████▋     | 4126/8917 [5:32:37<5:03:52,  3.81s/it] 46%|████▋     | 4127/8917 [5:32:40<4:53:36,  3.68s/it] 46%|████▋     | 4128/8917 [5:32:44<4:59:29,  3.75s/it] 46%|████▋     | 4129/8917 [5:32:48<4:56:18,  3.71s/it] 46%|████▋     | 4130/8917 [5:32:51<4:52:46,  3.67s/it] 46%|████▋     | 4131/8917 [5:32:55<4:58:51,  3.75s/it] 46%|████▋     | 4132/8917 [5:32:59<4:58:17,  3.74s/it] 46%|████▋     | 4133/8917 [5:33:02<4:56:22,  3.72s/it] 46%|████▋     | 4134/8917 [5:33:06<4:59:10,  3.75s/it] 46%|████▋     | 4135/8917 [5:33:10<5:00:04,  3.76s/it] 46%|████▋     | 4136/8917 [5:33:14<4:55:32,  3.71s/it] 46%|████▋     | 4137/8917 [5:33:17<4:56:47,  3.73s/it] 46%|████▋     | 4138/8917 [5:33:21<4:56:46,  3.73s/it] 46%|████▋     | 4139/8917 [5:33:25<4:54:12,  3.69s/it] 46%|████▋     | 4140/8917 [5:33:28<4:53:09,  3.68s/it] 46%|████▋     | 4141/8917 [5:33:32<4:52:05,  3.67s/it] 46%|████▋     | 4142/8917 [5:33:36<4:53:54,  3.69s/it] 46%|████▋     | 4143/8917 [5:33:39<4:46:53,  3.61s/it] 46%|████▋     | 4144/8917 [5:33:43<4:57:38,  3.74s/it] 46%|████▋     | 4145/8917 [5:33:47<5:01:29,  3.79s/it] 46%|████▋     | 4146/8917 [5:33:51<5:09:17,  3.89s/it] 47%|████▋     | 4147/8917 [5:33:55<5:06:46,  3.86s/it] 47%|████▋     | 4148/8917 [5:33:59<5:04:49,  3.83s/it] 47%|████▋     | 4149/8917 [5:34:03<5:03:42,  3.82s/it]09/19/2024 07:48:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01861012727022171, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1104732751846313, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1290833950042725}
 47%|████▋     | 4150/8917 [5:34:06<4:59:55,  3.78s/it] 47%|████▋     | 4151/8917 [5:34:10<5:00:23,  3.78s/it] 47%|████▋     | 4152/8917 [5:34:14<4:54:12,  3.70s/it] 47%|████▋     | 4153/8917 [5:34:18<5:00:25,  3.78s/it] 47%|████▋     | 4154/8917 [5:34:21<4:55:21,  3.72s/it] 47%|████▋     | 4155/8917 [5:34:25<4:49:42,  3.65s/it] 47%|████▋     | 4156/8917 [5:34:28<4:53:34,  3.70s/it] 47%|████▋     | 4157/8917 [5:34:32<4:51:49,  3.68s/it] 47%|████▋     | 4158/8917 [5:34:36<4:55:26,  3.72s/it] 47%|████▋     | 4159/8917 [5:34:40<4:56:13,  3.74s/it] 47%|████▋     | 4160/8917 [5:34:43<4:53:07,  3.70s/it] 47%|████▋     | 4161/8917 [5:34:47<4:58:22,  3.76s/it] 47%|████▋     | 4162/8917 [5:34:51<5:04:16,  3.84s/it] 47%|████▋     | 4163/8917 [5:34:55<5:01:16,  3.80s/it] 47%|████▋     | 4164/8917 [5:34:59<5:05:44,  3.86s/it] 47%|████▋     | 4165/8917 [5:35:02<4:58:14,  3.77s/it] 47%|████▋     | 4166/8917 [5:35:06<4:58:39,  3.77s/it] 47%|████▋     | 4167/8917 [5:35:10<4:54:12,  3.72s/it] 47%|████▋     | 4168/8917 [5:35:14<5:08:08,  3.89s/it] 47%|████▋     | 4169/8917 [5:35:18<5:04:14,  3.84s/it] 47%|████▋     | 4170/8917 [5:35:22<5:06:56,  3.88s/it] 47%|████▋     | 4171/8917 [5:35:26<5:06:58,  3.88s/it] 47%|████▋     | 4172/8917 [5:35:29<5:03:25,  3.84s/it] 47%|████▋     | 4173/8917 [5:35:33<5:03:36,  3.84s/it] 47%|████▋     | 4174/8917 [5:35:37<4:54:30,  3.73s/it] 47%|████▋     | 4175/8917 [5:35:41<4:59:27,  3.79s/it] 47%|████▋     | 4176/8917 [5:35:44<4:56:57,  3.76s/it] 47%|████▋     | 4177/8917 [5:35:48<5:02:58,  3.84s/it] 47%|████▋     | 4178/8917 [5:35:52<4:58:49,  3.78s/it] 47%|████▋     | 4179/8917 [5:35:55<4:48:16,  3.65s/it] 47%|████▋     | 4180/8917 [5:35:59<4:45:43,  3.62s/it] 47%|████▋     | 4181/8917 [5:36:03<4:48:11,  3.65s/it] 47%|████▋     | 4182/8917 [5:36:07<4:55:00,  3.74s/it] 47%|████▋     | 4183/8917 [5:36:10<4:55:59,  3.75s/it] 47%|████▋     | 4184/8917 [5:36:14<4:53:22,  3.72s/it] 47%|████▋     | 4185/8917 [5:36:18<4:48:43,  3.66s/it] 47%|████▋     | 4186/8917 [5:36:22<5:00:22,  3.81s/it] 47%|████▋     | 4187/8917 [5:36:25<4:59:33,  3.80s/it] 47%|████▋     | 4188/8917 [5:36:29<4:56:41,  3.76s/it] 47%|████▋     | 4189/8917 [5:36:32<4:46:19,  3.63s/it] 47%|████▋     | 4190/8917 [5:36:36<4:47:36,  3.65s/it] 47%|████▋     | 4191/8917 [5:36:40<4:47:44,  3.65s/it] 47%|████▋     | 4192/8917 [5:36:44<4:53:21,  3.73s/it] 47%|████▋     | 4193/8917 [5:36:47<4:52:01,  3.71s/it] 47%|████▋     | 4194/8917 [5:36:51<4:49:50,  3.68s/it] 47%|████▋     | 4195/8917 [5:36:55<4:45:55,  3.63s/it] 47%|████▋     | 4196/8917 [5:36:59<4:59:38,  3.81s/it] 47%|████▋     | 4197/8917 [5:37:03<5:08:47,  3.93s/it] 47%|████▋     | 4198/8917 [5:37:06<4:59:41,  3.81s/it] 47%|████▋     | 4199/8917 [5:37:10<4:55:49,  3.76s/it]09/19/2024 07:51:48 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023035110905766487, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3496227264404297, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3726578950881958}
 47%|████▋     | 4200/8917 [5:37:14<4:53:37,  3.73s/it] 47%|████▋     | 4201/8917 [5:37:18<4:58:55,  3.80s/it] 47%|████▋     | 4202/8917 [5:37:22<4:57:28,  3.79s/it] 47%|████▋     | 4203/8917 [5:37:25<4:55:54,  3.77s/it] 47%|████▋     | 4204/8917 [5:37:29<4:49:54,  3.69s/it] 47%|████▋     | 4205/8917 [5:37:33<4:53:47,  3.74s/it] 47%|████▋     | 4206/8917 [5:37:37<5:00:39,  3.83s/it] 47%|████▋     | 4207/8917 [5:37:40<4:54:58,  3.76s/it] 47%|████▋     | 4208/8917 [5:37:44<4:50:57,  3.71s/it] 47%|████▋     | 4209/8917 [5:37:47<4:47:16,  3.66s/it] 47%|████▋     | 4210/8917 [5:37:51<4:53:50,  3.75s/it] 47%|████▋     | 4211/8917 [5:37:55<4:53:54,  3.75s/it] 47%|████▋     | 4212/8917 [5:37:59<4:57:09,  3.79s/it] 47%|████▋     | 4213/8917 [5:38:03<5:01:40,  3.85s/it] 47%|████▋     | 4214/8917 [5:38:07<5:04:35,  3.89s/it] 47%|████▋     | 4215/8917 [5:38:11<5:05:13,  3.89s/it] 47%|████▋     | 4216/8917 [5:38:14<4:56:42,  3.79s/it] 47%|████▋     | 4217/8917 [5:38:18<4:57:21,  3.80s/it] 47%|████▋     | 4218/8917 [5:38:22<4:48:35,  3.68s/it] 47%|████▋     | 4219/8917 [5:38:25<4:46:17,  3.66s/it] 47%|████▋     | 4220/8917 [5:38:29<4:51:12,  3.72s/it] 47%|████▋     | 4221/8917 [5:38:33<4:53:36,  3.75s/it] 47%|████▋     | 4222/8917 [5:38:36<4:42:49,  3.61s/it] 47%|████▋     | 4223/8917 [5:38:40<4:43:16,  3.62s/it] 47%|████▋     | 4224/8917 [5:38:44<4:48:40,  3.69s/it] 47%|████▋     | 4225/8917 [5:38:47<4:46:23,  3.66s/it] 47%|████▋     | 4226/8917 [5:38:51<4:50:22,  3.71s/it] 47%|████▋     | 4227/8917 [5:38:55<5:02:36,  3.87s/it] 47%|████▋     | 4228/8917 [5:38:59<4:58:41,  3.82s/it] 47%|████▋     | 4229/8917 [5:39:03<4:51:14,  3.73s/it] 47%|████▋     | 4230/8917 [5:39:06<4:45:41,  3.66s/it] 47%|████▋     | 4231/8917 [5:39:10<4:43:39,  3.63s/it] 47%|████▋     | 4232/8917 [5:39:14<4:59:36,  3.84s/it] 47%|████▋     | 4233/8917 [5:39:18<4:56:35,  3.80s/it] 47%|████▋     | 4234/8917 [5:39:21<4:52:27,  3.75s/it] 47%|████▋     | 4235/8917 [5:39:25<4:59:11,  3.83s/it] 48%|████▊     | 4236/8917 [5:39:29<4:55:40,  3.79s/it] 48%|████▊     | 4237/8917 [5:39:32<4:46:34,  3.67s/it] 48%|████▊     | 4238/8917 [5:39:36<4:46:39,  3.68s/it] 48%|████▊     | 4239/8917 [5:39:40<4:45:25,  3.66s/it] 48%|████▊     | 4240/8917 [5:39:44<4:48:20,  3.70s/it] 48%|████▊     | 4241/8917 [5:39:47<4:47:40,  3.69s/it] 48%|████▊     | 4242/8917 [5:39:51<4:46:59,  3.68s/it] 48%|████▊     | 4243/8917 [5:39:54<4:42:35,  3.63s/it] 48%|████▊     | 4244/8917 [5:39:59<4:55:52,  3.80s/it] 48%|████▊     | 4245/8917 [5:40:02<4:51:44,  3.75s/it] 48%|████▊     | 4246/8917 [5:40:06<4:56:13,  3.80s/it] 48%|████▊     | 4247/8917 [5:40:10<4:47:05,  3.69s/it] 48%|████▊     | 4248/8917 [5:40:13<4:45:16,  3.67s/it] 48%|████▊     | 4249/8917 [5:40:17<4:43:52,  3.65s/it]09/19/2024 07:54:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02687915973365307, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.076418161392212, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1032973527908325}
 48%|████▊     | 4250/8917 [5:40:21<4:52:47,  3.76s/it] 48%|████▊     | 4251/8917 [5:40:24<4:46:50,  3.69s/it] 48%|████▊     | 4252/8917 [5:40:28<4:51:57,  3.76s/it] 48%|████▊     | 4253/8917 [5:40:32<4:53:33,  3.78s/it] 48%|████▊     | 4254/8917 [5:40:36<4:53:17,  3.77s/it] 48%|████▊     | 4255/8917 [5:40:39<4:46:24,  3.69s/it] 48%|████▊     | 4256/8917 [5:40:43<4:54:32,  3.79s/it] 48%|████▊     | 4257/8917 [5:40:47<4:54:33,  3.79s/it] 48%|████▊     | 4258/8917 [5:40:51<4:48:07,  3.71s/it] 48%|████▊     | 4259/8917 [5:40:54<4:47:27,  3.70s/it] 48%|████▊     | 4260/8917 [5:40:58<4:50:15,  3.74s/it] 48%|████▊     | 4261/8917 [5:41:02<4:50:25,  3.74s/it] 48%|████▊     | 4262/8917 [5:41:06<4:53:16,  3.78s/it] 48%|████▊     | 4263/8917 [5:41:09<4:45:52,  3.69s/it] 48%|████▊     | 4264/8917 [5:41:13<4:52:15,  3.77s/it] 48%|████▊     | 4265/8917 [5:41:17<4:58:44,  3.85s/it] 48%|████▊     | 4266/8917 [5:41:21<4:55:06,  3.81s/it] 48%|████▊     | 4267/8917 [5:41:25<4:54:30,  3.80s/it] 48%|████▊     | 4268/8917 [5:41:29<4:55:41,  3.82s/it] 48%|████▊     | 4269/8917 [5:41:32<4:53:43,  3.79s/it] 48%|████▊     | 4270/8917 [5:41:36<4:50:29,  3.75s/it] 48%|████▊     | 4271/8917 [5:41:40<4:50:50,  3.76s/it] 48%|████▊     | 4272/8917 [5:41:44<4:55:47,  3.82s/it] 48%|████▊     | 4273/8917 [5:41:47<4:47:03,  3.71s/it] 48%|████▊     | 4274/8917 [5:41:51<4:43:48,  3.67s/it] 48%|████▊     | 4275/8917 [5:41:55<4:51:38,  3.77s/it] 48%|████▊     | 4276/8917 [5:41:59<4:52:55,  3.79s/it] 48%|████▊     | 4277/8917 [5:42:02<4:45:27,  3.69s/it] 48%|████▊     | 4278/8917 [5:42:06<4:45:58,  3.70s/it] 48%|████▊     | 4279/8917 [5:42:10<4:51:20,  3.77s/it] 48%|████▊     | 4280/8917 [5:42:13<4:38:50,  3.61s/it] 48%|████▊     | 4281/8917 [5:42:16<4:36:46,  3.58s/it] 48%|████▊     | 4282/8917 [5:42:20<4:45:14,  3.69s/it] 48%|████▊     | 4283/8917 [5:42:24<4:54:03,  3.81s/it] 48%|████▊     | 4284/8917 [5:42:28<4:51:04,  3.77s/it] 48%|████▊     | 4285/8917 [5:42:32<4:54:02,  3.81s/it] 48%|████▊     | 4286/8917 [5:42:35<4:40:52,  3.64s/it] 48%|████▊     | 4287/8917 [5:42:39<4:37:45,  3.60s/it] 48%|████▊     | 4288/8917 [5:42:42<4:35:51,  3.58s/it] 48%|████▊     | 4289/8917 [5:42:46<4:34:37,  3.56s/it] 48%|████▊     | 4290/8917 [5:42:50<4:40:41,  3.64s/it] 48%|████▊     | 4291/8917 [5:42:53<4:33:55,  3.55s/it] 48%|████▊     | 4292/8917 [5:42:57<4:36:02,  3.58s/it] 48%|████▊     | 4293/8917 [5:43:01<4:46:14,  3.71s/it] 48%|████▊     | 4294/8917 [5:43:04<4:47:51,  3.74s/it] 48%|████▊     | 4295/8917 [5:43:08<4:46:37,  3.72s/it] 48%|████▊     | 4296/8917 [5:43:12<4:48:31,  3.75s/it] 48%|████▊     | 4297/8917 [5:43:16<4:43:52,  3.69s/it] 48%|████▊     | 4298/8917 [5:43:19<4:50:48,  3.78s/it] 48%|████▊     | 4299/8917 [5:43:23<4:41:12,  3.65s/it]09/19/2024 07:58:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.021047955378890038, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1495068073272705, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.170554757118225}
 48%|████▊     | 4300/8917 [5:43:27<4:43:13,  3.68s/it] 48%|████▊     | 4301/8917 [5:43:31<4:48:57,  3.76s/it] 48%|████▊     | 4302/8917 [5:43:34<4:39:01,  3.63s/it] 48%|████▊     | 4303/8917 [5:43:38<4:39:49,  3.64s/it] 48%|████▊     | 4304/8917 [5:43:41<4:43:10,  3.68s/it] 48%|████▊     | 4305/8917 [5:43:45<4:49:04,  3.76s/it] 48%|████▊     | 4306/8917 [5:43:49<4:53:16,  3.82s/it] 48%|████▊     | 4307/8917 [5:43:53<4:59:19,  3.90s/it] 48%|████▊     | 4308/8917 [5:43:57<4:50:34,  3.78s/it] 48%|████▊     | 4309/8917 [5:44:00<4:43:49,  3.70s/it] 48%|████▊     | 4310/8917 [5:44:04<4:42:07,  3.67s/it] 48%|████▊     | 4311/8917 [5:44:08<4:47:31,  3.75s/it] 48%|████▊     | 4312/8917 [5:44:11<4:43:58,  3.70s/it] 48%|████▊     | 4313/8917 [5:44:15<4:46:55,  3.74s/it] 48%|████▊     | 4314/8917 [5:44:19<4:42:54,  3.69s/it] 48%|████▊     | 4315/8917 [5:44:23<4:42:53,  3.69s/it] 48%|████▊     | 4316/8917 [5:44:26<4:41:21,  3.67s/it] 48%|████▊     | 4317/8917 [5:44:30<4:40:12,  3.65s/it] 48%|████▊     | 4318/8917 [5:44:34<4:48:17,  3.76s/it] 48%|████▊     | 4319/8917 [5:44:37<4:39:59,  3.65s/it] 48%|████▊     | 4320/8917 [5:44:41<4:46:17,  3.74s/it] 48%|████▊     | 4321/8917 [5:44:45<4:47:41,  3.76s/it] 48%|████▊     | 4322/8917 [5:44:49<4:44:30,  3.72s/it] 48%|████▊     | 4323/8917 [5:44:52<4:46:55,  3.75s/it] 48%|████▊     | 4324/8917 [5:44:56<4:40:11,  3.66s/it] 49%|████▊     | 4325/8917 [5:45:00<4:45:22,  3.73s/it] 49%|████▊     | 4326/8917 [5:45:03<4:44:50,  3.72s/it] 49%|████▊     | 4327/8917 [5:45:07<4:40:27,  3.67s/it] 49%|████▊     | 4328/8917 [5:45:11<4:48:58,  3.78s/it] 49%|████▊     | 4329/8917 [5:45:15<4:52:31,  3.83s/it] 49%|████▊     | 4330/8917 [5:45:19<4:51:12,  3.81s/it] 49%|████▊     | 4331/8917 [5:45:22<4:43:41,  3.71s/it] 49%|████▊     | 4332/8917 [5:45:26<4:43:08,  3.71s/it] 49%|████▊     | 4333/8917 [5:45:29<4:39:46,  3.66s/it] 49%|████▊     | 4334/8917 [5:45:34<4:55:41,  3.87s/it] 49%|████▊     | 4335/8917 [5:45:37<4:51:19,  3.81s/it] 49%|████▊     | 4336/8917 [5:45:41<4:44:13,  3.72s/it] 49%|████▊     | 4337/8917 [5:45:45<4:46:09,  3.75s/it] 49%|████▊     | 4338/8917 [5:45:49<4:47:58,  3.77s/it] 49%|████▊     | 4339/8917 [5:45:52<4:39:16,  3.66s/it] 49%|████▊     | 4340/8917 [5:45:56<4:39:38,  3.67s/it] 49%|████▊     | 4341/8917 [5:45:59<4:39:19,  3.66s/it] 49%|████▊     | 4342/8917 [5:46:03<4:37:28,  3.64s/it] 49%|████▊     | 4343/8917 [5:46:06<4:36:03,  3.62s/it] 49%|████▊     | 4344/8917 [5:46:10<4:31:37,  3.56s/it] 49%|████▊     | 4345/8917 [5:46:14<4:38:03,  3.65s/it] 49%|████▊     | 4346/8917 [5:46:18<4:46:57,  3.77s/it] 49%|████▊     | 4347/8917 [5:46:21<4:43:43,  3.73s/it] 49%|████▉     | 4348/8917 [5:46:25<4:36:55,  3.64s/it] 49%|████▉     | 4349/8917 [5:46:28<4:33:01,  3.59s/it]09/19/2024 08:01:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.022746723145246506, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2289079427719116, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2516546249389648}
 49%|████▉     | 4350/8917 [5:46:32<4:40:36,  3.69s/it] 49%|████▉     | 4351/8917 [5:46:36<4:39:08,  3.67s/it] 49%|████▉     | 4352/8917 [5:46:40<4:47:24,  3.78s/it] 49%|████▉     | 4353/8917 [5:46:44<4:45:00,  3.75s/it] 49%|████▉     | 4354/8917 [5:46:47<4:46:23,  3.77s/it] 49%|████▉     | 4355/8917 [5:46:52<4:54:37,  3.87s/it] 49%|████▉     | 4356/8917 [5:46:55<4:47:31,  3.78s/it] 49%|████▉     | 4357/8917 [5:46:59<4:48:23,  3.79s/it] 49%|████▉     | 4358/8917 [5:47:03<4:46:15,  3.77s/it] 49%|████▉     | 4359/8917 [5:47:07<4:50:36,  3.83s/it] 49%|████▉     | 4360/8917 [5:47:10<4:43:42,  3.74s/it] 49%|████▉     | 4361/8917 [5:47:14<4:40:32,  3.69s/it] 49%|████▉     | 4362/8917 [5:47:17<4:39:38,  3.68s/it] 49%|████▉     | 4363/8917 [5:47:21<4:37:08,  3.65s/it] 49%|████▉     | 4364/8917 [5:47:24<4:33:55,  3.61s/it] 49%|████▉     | 4365/8917 [5:47:28<4:39:40,  3.69s/it] 49%|████▉     | 4366/8917 [5:47:32<4:44:10,  3.75s/it] 49%|████▉     | 4367/8917 [5:47:36<4:46:13,  3.77s/it] 49%|████▉     | 4368/8917 [5:47:40<4:41:51,  3.72s/it] 49%|████▉     | 4369/8917 [5:47:44<4:49:24,  3.82s/it] 49%|████▉     | 4370/8917 [5:47:47<4:46:52,  3.79s/it] 49%|████▉     | 4371/8917 [5:47:51<4:41:32,  3.72s/it] 49%|████▉     | 4372/8917 [5:47:55<4:43:16,  3.74s/it] 49%|████▉     | 4373/8917 [5:47:59<4:43:37,  3.74s/it] 49%|████▉     | 4374/8917 [5:48:02<4:41:18,  3.72s/it] 49%|████▉     | 4375/8917 [5:48:06<4:46:11,  3.78s/it] 49%|████▉     | 4376/8917 [5:48:10<4:42:41,  3.74s/it] 49%|████▉     | 4377/8917 [5:48:14<4:47:49,  3.80s/it] 49%|████▉     | 4378/8917 [5:48:18<4:53:01,  3.87s/it] 49%|████▉     | 4379/8917 [5:48:21<4:43:29,  3.75s/it] 49%|████▉     | 4380/8917 [5:48:25<4:34:59,  3.64s/it] 49%|████▉     | 4381/8917 [5:48:28<4:37:16,  3.67s/it] 49%|████▉     | 4382/8917 [5:48:32<4:40:28,  3.71s/it] 49%|████▉     | 4383/8917 [5:48:36<4:42:39,  3.74s/it] 49%|████▉     | 4384/8917 [5:48:40<4:48:55,  3.82s/it] 49%|████▉     | 4385/8917 [5:48:43<4:37:26,  3.67s/it] 49%|████▉     | 4386/8917 [5:48:47<4:48:50,  3.82s/it] 49%|████▉     | 4387/8917 [5:48:51<4:47:10,  3.80s/it] 49%|████▉     | 4388/8917 [5:48:55<4:39:18,  3.70s/it] 49%|████▉     | 4389/8917 [5:48:59<4:44:12,  3.77s/it] 49%|████▉     | 4390/8917 [5:49:02<4:42:58,  3.75s/it] 49%|████▉     | 4391/8917 [5:49:06<4:34:45,  3.64s/it] 49%|████▉     | 4392/8917 [5:49:09<4:34:57,  3.65s/it] 49%|████▉     | 4393/8917 [5:49:13<4:37:02,  3.67s/it] 49%|████▉     | 4394/8917 [5:49:17<4:43:12,  3.76s/it] 49%|████▉     | 4395/8917 [5:49:21<4:37:34,  3.68s/it] 49%|████▉     | 4396/8917 [5:49:24<4:41:00,  3.73s/it] 49%|████▉     | 4397/8917 [5:49:28<4:35:04,  3.65s/it] 49%|████▉     | 4398/8917 [5:49:32<4:35:51,  3.66s/it] 49%|████▉     | 4399/8917 [5:49:35<4:39:20,  3.71s/it]09/19/2024 08:04:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04114615172147751, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3333840370178223, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3745301961898804}
 49%|████▉     | 4400/8917 [5:49:39<4:47:15,  3.82s/it] 49%|████▉     | 4401/8917 [5:49:43<4:52:04,  3.88s/it] 49%|████▉     | 4402/8917 [5:49:47<4:38:53,  3.71s/it] 49%|████▉     | 4403/8917 [5:49:50<4:36:26,  3.67s/it] 49%|████▉     | 4404/8917 [5:49:54<4:31:30,  3.61s/it] 49%|████▉     | 4405/8917 [5:49:58<4:44:43,  3.79s/it] 49%|████▉     | 4406/8917 [5:50:02<4:44:41,  3.79s/it] 49%|████▉     | 4407/8917 [5:50:05<4:42:08,  3.75s/it] 49%|████▉     | 4408/8917 [5:50:09<4:45:22,  3.80s/it] 49%|████▉     | 4409/8917 [5:50:13<4:38:49,  3.71s/it] 49%|████▉     | 4410/8917 [5:50:17<4:38:03,  3.70s/it] 49%|████▉     | 4411/8917 [5:50:21<4:46:41,  3.82s/it] 49%|████▉     | 4412/8917 [5:50:24<4:43:14,  3.77s/it] 49%|████▉     | 4413/8917 [5:50:28<4:45:41,  3.81s/it] 50%|████▉     | 4414/8917 [5:50:32<4:39:24,  3.72s/it] 50%|████▉     | 4415/8917 [5:50:36<4:46:59,  3.82s/it] 50%|████▉     | 4416/8917 [5:50:39<4:42:52,  3.77s/it] 50%|████▉     | 4417/8917 [5:50:43<4:39:31,  3.73s/it] 50%|████▉     | 4418/8917 [5:50:47<4:36:53,  3.69s/it] 50%|████▉     | 4419/8917 [5:50:50<4:36:37,  3.69s/it] 50%|████▉     | 4420/8917 [5:50:54<4:34:01,  3.66s/it] 50%|████▉     | 4421/8917 [5:50:58<4:35:29,  3.68s/it] 50%|████▉     | 4422/8917 [5:51:01<4:33:45,  3.65s/it] 50%|████▉     | 4423/8917 [5:51:05<4:34:17,  3.66s/it] 50%|████▉     | 4424/8917 [5:51:09<4:41:17,  3.76s/it] 50%|████▉     | 4425/8917 [5:51:13<4:48:06,  3.85s/it] 50%|████▉     | 4426/8917 [5:51:16<4:38:11,  3.72s/it] 50%|████▉     | 4427/8917 [5:51:20<4:31:40,  3.63s/it] 50%|████▉     | 4428/8917 [5:51:23<4:30:47,  3.62s/it] 50%|████▉     | 4429/8917 [5:51:27<4:40:10,  3.75s/it] 50%|████▉     | 4430/8917 [5:51:31<4:31:40,  3.63s/it] 50%|████▉     | 4431/8917 [5:51:35<4:33:32,  3.66s/it] 50%|████▉     | 4432/8917 [5:51:38<4:34:23,  3.67s/it] 50%|████▉     | 4433/8917 [5:51:42<4:37:11,  3.71s/it] 50%|████▉     | 4434/8917 [5:51:46<4:34:25,  3.67s/it] 50%|████▉     | 4435/8917 [5:51:49<4:30:23,  3.62s/it] 50%|████▉     | 4436/8917 [5:51:53<4:32:27,  3.65s/it] 50%|████▉     | 4437/8917 [5:51:57<4:35:52,  3.69s/it] 50%|████▉     | 4438/8917 [5:52:01<4:45:37,  3.83s/it] 50%|████▉     | 4439/8917 [5:52:04<4:39:20,  3.74s/it] 50%|████▉     | 4440/8917 [5:52:08<4:35:56,  3.70s/it] 50%|████▉     | 4441/8917 [5:52:12<4:40:11,  3.76s/it] 50%|████▉     | 4442/8917 [5:52:16<4:43:31,  3.80s/it] 50%|████▉     | 4443/8917 [5:52:19<4:43:00,  3.80s/it] 50%|████▉     | 4444/8917 [5:52:23<4:35:20,  3.69s/it] 50%|████▉     | 4445/8917 [5:52:27<4:36:12,  3.71s/it] 50%|████▉     | 4446/8917 [5:52:31<4:43:52,  3.81s/it] 50%|████▉     | 4447/8917 [5:52:34<4:41:45,  3.78s/it] 50%|████▉     | 4448/8917 [5:52:38<4:39:00,  3.75s/it] 50%|████▉     | 4449/8917 [5:52:42<4:47:14,  3.86s/it]09/19/2024 08:07:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030296161770820618, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1481648683547974, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1784610748291016}
 50%|████▉     | 4450/8917 [5:52:46<4:43:16,  3.80s/it] 50%|████▉     | 4451/8917 [5:52:49<4:37:04,  3.72s/it] 50%|████▉     | 4452/8917 [5:52:53<4:41:59,  3.79s/it] 50%|████▉     | 4453/8917 [5:52:57<4:43:51,  3.82s/it] 50%|████▉     | 4454/8917 [5:53:01<4:39:30,  3.76s/it] 50%|████▉     | 4455/8917 [5:53:05<4:37:36,  3.73s/it] 50%|████▉     | 4456/8917 [5:53:08<4:36:46,  3.72s/it] 50%|████▉     | 4457/8917 [5:53:12<4:27:55,  3.60s/it] 50%|████▉     | 4458/8917 [5:53:15<4:29:17,  3.62s/it] 50%|█████     | 4459/8917 [5:53:19<4:35:40,  3.71s/it] 50%|█████     | 4460/8917 [5:53:23<4:36:10,  3.72s/it] 50%|█████     | 4461/8917 [5:53:26<4:29:38,  3.63s/it] 50%|█████     | 4462/8917 [5:53:30<4:28:17,  3.61s/it] 50%|█████     | 4463/8917 [5:53:34<4:38:56,  3.76s/it] 50%|█████     | 4464/8917 [5:53:38<4:34:44,  3.70s/it] 50%|█████     | 4465/8917 [5:53:42<4:44:16,  3.83s/it] 50%|█████     | 4466/8917 [5:53:46<4:45:23,  3.85s/it] 50%|█████     | 4467/8917 [5:53:49<4:40:33,  3.78s/it] 50%|█████     | 4468/8917 [5:53:53<4:35:33,  3.72s/it] 50%|█████     | 4469/8917 [5:53:56<4:32:56,  3.68s/it] 50%|█████     | 4470/8917 [5:54:00<4:30:47,  3.65s/it] 50%|█████     | 4471/8917 [5:54:03<4:25:27,  3.58s/it] 50%|█████     | 4472/8917 [5:54:07<4:24:45,  3.57s/it] 50%|█████     | 4473/8917 [5:54:11<4:37:57,  3.75s/it] 50%|█████     | 4474/8917 [5:54:15<4:40:31,  3.79s/it] 50%|█████     | 4475/8917 [5:54:19<4:36:41,  3.74s/it] 50%|█████     | 4476/8917 [5:54:22<4:37:28,  3.75s/it] 50%|█████     | 4477/8917 [5:54:26<4:39:21,  3.78s/it] 50%|█████     | 4478/8917 [5:54:30<4:37:19,  3.75s/it] 50%|█████     | 4479/8917 [5:54:34<4:41:22,  3.80s/it] 50%|█████     | 4480/8917 [5:54:37<4:33:52,  3.70s/it] 50%|█████     | 4481/8917 [5:54:41<4:37:03,  3.75s/it] 50%|█████     | 4482/8917 [5:54:45<4:41:49,  3.81s/it] 50%|█████     | 4483/8917 [5:54:49<4:39:02,  3.78s/it] 50%|█████     | 4484/8917 [5:54:52<4:26:40,  3.61s/it] 50%|█████     | 4485/8917 [5:54:56<4:33:29,  3.70s/it] 50%|█████     | 4486/8917 [5:55:00<4:35:25,  3.73s/it] 50%|█████     | 4487/8917 [5:55:04<4:40:36,  3.80s/it] 50%|█████     | 4488/8917 [5:55:07<4:39:19,  3.78s/it] 50%|█████     | 4489/8917 [5:55:11<4:32:37,  3.69s/it] 50%|█████     | 4490/8917 [5:55:15<4:31:25,  3.68s/it] 50%|█████     | 4491/8917 [5:55:18<4:26:09,  3.61s/it] 50%|█████     | 4492/8917 [5:55:22<4:29:44,  3.66s/it] 50%|█████     | 4493/8917 [5:55:25<4:24:17,  3.58s/it] 50%|█████     | 4494/8917 [5:55:29<4:18:48,  3.51s/it] 50%|█████     | 4495/8917 [5:55:33<4:31:14,  3.68s/it] 50%|█████     | 4496/8917 [5:55:36<4:32:18,  3.70s/it] 50%|█████     | 4497/8917 [5:55:40<4:35:27,  3.74s/it] 50%|█████     | 4498/8917 [5:55:44<4:39:01,  3.79s/it] 50%|█████     | 4499/8917 [5:55:48<4:33:00,  3.71s/it]09/19/2024 08:10:24 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 08:10:24 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<06:41,  1.82s/it][A
  1%|          | 2/221 [00:02<03:39,  1.00s/it][A
  1%|▏         | 3/221 [00:03<03:16,  1.11it/s][A
  2%|▏         | 4/221 [00:03<02:19,  1.55it/s][A
  3%|▎         | 6/221 [00:03<01:14,  2.90it/s][A
  3%|▎         | 7/221 [00:03<01:07,  3.19it/s][A
  4%|▎         | 8/221 [00:03<01:06,  3.22it/s][A
  4%|▍         | 9/221 [00:04<01:09,  3.05it/s][A
  5%|▍         | 10/221 [00:04<01:09,  3.02it/s][A
  5%|▌         | 12/221 [00:12<06:38,  1.90s/it][A
  6%|▌         | 13/221 [00:12<05:05,  1.47s/it][A
  7%|▋         | 15/221 [00:12<03:15,  1.05it/s][A
  7%|▋         | 16/221 [00:13<03:03,  1.11it/s][A
  8%|▊         | 17/221 [00:14<03:21,  1.01it/s][A
  8%|▊         | 18/221 [00:15<02:52,  1.17it/s][A
  9%|▊         | 19/221 [00:16<03:19,  1.01it/s][A
  9%|▉         | 20/221 [00:16<02:29,  1.35it/s][A
 10%|▉         | 21/221 [00:16<02:00,  1.66it/s][A
 10%|▉         | 22/221 [00:17<02:21,  1.41it/s][A
 11%|█         | 24/221 [00:18<01:34,  2.09it/s][A
 11%|█▏        | 25/221 [00:18<01:27,  2.24it/s][A
 12%|█▏        | 26/221 [00:18<01:18,  2.50it/s][A
 13%|█▎        | 28/221 [00:19<01:12,  2.68it/s][A
 13%|█▎        | 29/221 [00:19<01:02,  3.06it/s][A
 14%|█▎        | 30/221 [00:20<01:10,  2.70it/s][A
 14%|█▍        | 31/221 [00:20<01:14,  2.55it/s][A
 14%|█▍        | 32/221 [00:20<00:59,  3.19it/s][A
 15%|█▍        | 33/221 [00:21<00:59,  3.17it/s][A
 16%|█▌        | 35/221 [00:21<00:41,  4.53it/s][A
 16%|█▋        | 36/221 [00:21<00:41,  4.44it/s][A
 17%|█▋        | 37/221 [00:22<00:57,  3.21it/s][A
 17%|█▋        | 38/221 [00:22<01:15,  2.41it/s][A
 18%|█▊        | 39/221 [00:23<01:15,  2.41it/s][A
 18%|█▊        | 40/221 [00:23<01:13,  2.45it/s][A
 19%|█▊        | 41/221 [00:23<01:01,  2.93it/s][A
 19%|█▉        | 42/221 [00:24<00:52,  3.42it/s][A
 19%|█▉        | 43/221 [00:24<00:44,  4.04it/s][A
 20%|█▉        | 44/221 [00:24<00:36,  4.88it/s][A
 20%|██        | 45/221 [00:32<07:24,  2.53s/it][A
 21%|██        | 46/221 [00:32<05:23,  1.85s/it][A
 21%|██▏       | 47/221 [00:33<04:12,  1.45s/it][A
 22%|██▏       | 48/221 [00:33<03:03,  1.06s/it][A
 22%|██▏       | 49/221 [00:33<02:25,  1.18it/s][A
 23%|██▎       | 50/221 [00:33<01:53,  1.51it/s][A
 23%|██▎       | 51/221 [00:33<01:27,  1.94it/s][A
 24%|██▎       | 52/221 [00:34<01:11,  2.37it/s][A
 24%|██▍       | 53/221 [00:34<01:02,  2.70it/s][A
 24%|██▍       | 54/221 [00:34<01:09,  2.39it/s][A
 25%|██▍       | 55/221 [00:35<01:35,  1.74it/s][A
 25%|██▌       | 56/221 [00:36<01:14,  2.20it/s][A
 26%|██▌       | 57/221 [00:36<01:01,  2.66it/s][A
 27%|██▋       | 59/221 [00:36<00:40,  3.96it/s][A
 27%|██▋       | 60/221 [00:36<00:46,  3.44it/s][A
 28%|██▊       | 61/221 [00:37<00:44,  3.60it/s][A
 28%|██▊       | 62/221 [00:37<00:45,  3.49it/s][A
 29%|██▊       | 63/221 [00:37<00:38,  4.08it/s][A
 29%|██▉       | 64/221 [00:38<01:07,  2.33it/s][A
 30%|██▉       | 66/221 [00:39<00:55,  2.82it/s][A
 30%|███       | 67/221 [00:39<00:48,  3.14it/s][A
 31%|███       | 68/221 [00:39<00:41,  3.65it/s][A
 31%|███       | 69/221 [00:41<01:37,  1.55it/s][A
 32%|███▏      | 70/221 [00:41<01:16,  1.96it/s][A
 32%|███▏      | 71/221 [00:41<01:03,  2.37it/s][A
 33%|███▎      | 72/221 [00:41<00:57,  2.61it/s][A
 33%|███▎      | 73/221 [00:42<01:06,  2.21it/s][A
 33%|███▎      | 74/221 [00:42<00:51,  2.84it/s][A
 34%|███▍      | 75/221 [00:42<00:49,  2.95it/s][A
 34%|███▍      | 76/221 [00:43<00:50,  2.86it/s][A
 35%|███▍      | 77/221 [00:45<02:30,  1.05s/it][A
 35%|███▌      | 78/221 [00:45<01:49,  1.31it/s][A
 36%|███▌      | 79/221 [00:46<02:01,  1.17it/s][A
 37%|███▋      | 81/221 [00:47<01:31,  1.53it/s][A
 37%|███▋      | 82/221 [00:50<02:52,  1.24s/it][A
 38%|███▊      | 83/221 [00:51<02:28,  1.08s/it][A
 38%|███▊      | 84/221 [00:51<01:56,  1.17it/s][A
 39%|███▉      | 86/221 [00:52<01:17,  1.74it/s][A
 39%|███▉      | 87/221 [00:52<01:21,  1.65it/s][A
 40%|███▉      | 88/221 [00:53<01:12,  1.83it/s][A
 40%|████      | 89/221 [00:53<01:04,  2.06it/s][A
 41%|████      | 90/221 [00:53<00:55,  2.38it/s][A
 41%|████      | 91/221 [00:53<00:44,  2.91it/s][A
 42%|████▏     | 92/221 [00:54<00:39,  3.27it/s][A
 42%|████▏     | 93/221 [00:54<00:42,  3.02it/s][A
 43%|████▎     | 94/221 [00:54<00:45,  2.77it/s][A
 43%|████▎     | 95/221 [00:55<00:42,  2.94it/s][A
 43%|████▎     | 96/221 [00:56<01:02,  2.00it/s][A
 44%|████▍     | 97/221 [00:56<00:51,  2.41it/s][A
 44%|████▍     | 98/221 [00:56<00:59,  2.08it/s][A
 45%|████▍     | 99/221 [00:57<00:56,  2.16it/s][A
 45%|████▌     | 100/221 [00:58<01:23,  1.46it/s][A
 46%|████▌     | 101/221 [00:58<01:04,  1.85it/s][A
 46%|████▌     | 102/221 [00:59<01:18,  1.52it/s][A
 47%|████▋     | 103/221 [00:59<00:57,  2.04it/s][A
 47%|████▋     | 104/221 [01:00<00:52,  2.25it/s][A
 48%|████▊     | 105/221 [01:00<00:54,  2.14it/s][A
 48%|████▊     | 106/221 [01:03<02:26,  1.28s/it][A
 48%|████▊     | 107/221 [01:04<01:56,  1.02s/it][A
 49%|████▉     | 108/221 [01:04<01:37,  1.16it/s][A
 49%|████▉     | 109/221 [01:04<01:14,  1.50it/s][A
 50%|████▉     | 110/221 [01:05<00:56,  1.95it/s][A
 50%|█████     | 111/221 [01:05<00:53,  2.05it/s][A
 51%|█████     | 112/221 [01:05<00:50,  2.16it/s][A
 51%|█████     | 113/221 [01:06<00:41,  2.63it/s][A
 52%|█████▏    | 115/221 [01:06<00:35,  2.96it/s][A
 52%|█████▏    | 116/221 [01:07<00:37,  2.81it/s][A
 53%|█████▎    | 117/221 [01:07<00:41,  2.50it/s][A
 53%|█████▎    | 118/221 [01:08<00:41,  2.47it/s][A
 54%|█████▍    | 119/221 [01:08<00:39,  2.55it/s][A
 54%|█████▍    | 120/221 [01:08<00:31,  3.21it/s][A
 55%|█████▍    | 121/221 [01:08<00:35,  2.85it/s][A
 55%|█████▌    | 122/221 [01:09<00:35,  2.76it/s][A
 56%|█████▌    | 123/221 [01:10<01:04,  1.53it/s][A
 56%|█████▌    | 124/221 [01:10<00:49,  1.97it/s][A
 57%|█████▋    | 125/221 [01:11<01:01,  1.55it/s][A
 57%|█████▋    | 126/221 [01:20<04:35,  2.90s/it][A
 57%|█████▋    | 127/221 [01:20<03:27,  2.21s/it][A
 58%|█████▊    | 128/221 [01:21<02:34,  1.66s/it][A
 58%|█████▊    | 129/221 [01:21<02:05,  1.37s/it][A
 59%|█████▉    | 130/221 [01:21<01:35,  1.05s/it][A
 59%|█████▉    | 131/221 [01:23<01:38,  1.09s/it][A
 60%|█████▉    | 132/221 [01:24<01:51,  1.25s/it][A
 60%|██████    | 133/221 [01:25<01:28,  1.00s/it][A
 61%|██████    | 134/221 [01:26<01:29,  1.03s/it][A
 61%|██████    | 135/221 [01:26<01:15,  1.14it/s][A
 62%|██████▏   | 136/221 [01:27<01:02,  1.36it/s][A
 62%|██████▏   | 137/221 [01:27<00:52,  1.61it/s][A
 62%|██████▏   | 138/221 [01:28<00:50,  1.66it/s][A
 63%|██████▎   | 139/221 [01:28<00:40,  2.02it/s][A
 63%|██████▎   | 140/221 [01:29<00:45,  1.78it/s][A
 64%|██████▍   | 141/221 [01:29<00:40,  1.98it/s][A
 64%|██████▍   | 142/221 [01:29<00:36,  2.17it/s][A
 65%|██████▍   | 143/221 [01:30<00:34,  2.25it/s][A
 65%|██████▌   | 144/221 [01:30<00:30,  2.51it/s][A
 66%|██████▌   | 145/221 [01:30<00:23,  3.20it/s][A
 66%|██████▌   | 146/221 [01:30<00:18,  4.01it/s][A
 67%|██████▋   | 147/221 [01:30<00:15,  4.80it/s][A
 67%|██████▋   | 148/221 [01:32<00:39,  1.85it/s][A
 67%|██████▋   | 149/221 [01:32<00:36,  1.99it/s][A
 68%|██████▊   | 150/221 [01:32<00:32,  2.18it/s][A
 68%|██████▊   | 151/221 [01:33<00:29,  2.37it/s][A
 69%|██████▉   | 152/221 [01:33<00:28,  2.42it/s][A
 69%|██████▉   | 153/221 [01:33<00:21,  3.10it/s][A
 70%|██████▉   | 154/221 [01:33<00:18,  3.63it/s][A
 70%|███████   | 155/221 [01:34<00:17,  3.78it/s][A
 71%|███████   | 156/221 [01:34<00:14,  4.41it/s][A
 71%|███████   | 157/221 [01:37<01:19,  1.25s/it][A
 71%|███████▏  | 158/221 [01:38<01:11,  1.14s/it][A
 72%|███████▏  | 160/221 [01:39<00:43,  1.41it/s][A
 73%|███████▎  | 161/221 [01:39<00:33,  1.79it/s][A
 74%|███████▍  | 163/221 [01:39<00:22,  2.57it/s][A
 74%|███████▍  | 164/221 [01:39<00:19,  2.91it/s][A
 75%|███████▍  | 165/221 [01:40<00:23,  2.34it/s][A
 75%|███████▌  | 166/221 [01:41<00:24,  2.27it/s][A
 76%|███████▌  | 167/221 [01:41<00:19,  2.72it/s][A
 76%|███████▌  | 168/221 [01:46<01:27,  1.65s/it][A
 76%|███████▋  | 169/221 [01:46<01:11,  1.37s/it][A
 77%|███████▋  | 170/221 [01:47<00:56,  1.11s/it][A
 77%|███████▋  | 171/221 [01:47<00:44,  1.13it/s][A
 78%|███████▊  | 172/221 [01:47<00:34,  1.44it/s][A
 78%|███████▊  | 173/221 [01:48<00:28,  1.67it/s][A
 79%|███████▊  | 174/221 [01:48<00:21,  2.22it/s][A
 79%|███████▉  | 175/221 [01:48<00:18,  2.52it/s][A
 80%|███████▉  | 176/221 [01:49<00:20,  2.20it/s][A
 80%|████████  | 177/221 [01:49<00:18,  2.43it/s][A
 81%|████████  | 178/221 [01:49<00:15,  2.72it/s][A
 81%|████████  | 179/221 [01:50<00:15,  2.67it/s][A
 81%|████████▏ | 180/221 [01:50<00:13,  3.09it/s][A
 82%|████████▏ | 181/221 [01:50<00:10,  3.77it/s][A
 82%|████████▏ | 182/221 [01:50<00:11,  3.36it/s][A
 83%|████████▎ | 183/221 [01:51<00:16,  2.33it/s][A
 83%|████████▎ | 184/221 [01:51<00:14,  2.48it/s][A
 84%|████████▎ | 185/221 [01:52<00:13,  2.59it/s][A
 84%|████████▍ | 186/221 [01:52<00:12,  2.85it/s][A
 85%|████████▍ | 187/221 [01:52<00:10,  3.26it/s][A
 85%|████████▌ | 188/221 [01:52<00:08,  3.75it/s][A
 86%|████████▌ | 189/221 [01:53<00:09,  3.38it/s][A
 86%|████████▌ | 190/221 [01:53<00:10,  2.91it/s][A
 86%|████████▋ | 191/221 [01:53<00:08,  3.41it/s][A
 87%|████████▋ | 192/221 [01:54<00:12,  2.42it/s][A
 87%|████████▋ | 193/221 [01:54<00:09,  2.89it/s][A
 88%|████████▊ | 194/221 [01:55<00:11,  2.37it/s][A
 88%|████████▊ | 195/221 [01:55<00:09,  2.62it/s][A
 89%|████████▊ | 196/221 [01:56<00:09,  2.75it/s][A
 89%|████████▉ | 197/221 [01:56<00:07,  3.20it/s][A
 90%|████████▉ | 198/221 [01:56<00:06,  3.53it/s][A
 90%|█████████ | 199/221 [01:56<00:05,  4.12it/s][A
 90%|█████████ | 200/221 [01:57<00:06,  3.29it/s][A
 91%|█████████ | 201/221 [01:57<00:07,  2.55it/s][A
 91%|█████████▏| 202/221 [01:57<00:06,  2.95it/s][A
 92%|█████████▏| 203/221 [01:58<00:08,  2.03it/s][A
 92%|█████████▏| 204/221 [01:58<00:06,  2.50it/s][A
 93%|█████████▎| 206/221 [01:59<00:05,  2.94it/s][A
 94%|█████████▎| 207/221 [01:59<00:04,  3.44it/s][A
 94%|█████████▍| 208/221 [01:59<00:03,  3.51it/s][A
 95%|█████████▌| 210/221 [02:00<00:02,  4.88it/s][A
 95%|█████████▌| 211/221 [02:00<00:02,  3.42it/s][A
 96%|█████████▌| 212/221 [02:00<00:02,  3.43it/s][A
 96%|█████████▋| 213/221 [02:01<00:01,  4.13it/s][A
 97%|█████████▋| 214/221 [02:01<00:02,  3.15it/s][A
 97%|█████████▋| 215/221 [02:02<00:02,  2.71it/s][A
 98%|█████████▊| 216/221 [02:02<00:01,  2.55it/s][A
 98%|█████████▊| 217/221 [02:06<00:05,  1.42s/it][A
 99%|█████████▊| 218/221 [02:06<00:03,  1.13s/it][A
 99%|█████████▉| 219/221 [02:07<00:01,  1.16it/s][A
100%|█████████▉| 220/221 [02:10<00:01,  1.49s/it][A
100%|██████████| 221/221 [02:10<00:00,  1.10s/it][A100%|██████████| 221/221 [02:10<00:00,  1.70it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:35,  1.78it/s][A
 23%|██▎       | 51/221 [00:27<01:33,  1.82it/s][A
 24%|██▎       | 52/221 [00:27<01:31,  1.84it/s][A
 24%|██▍       | 53/221 [00:28<01:30,  1.85it/s][A
 24%|██▍       | 54/221 [00:28<01:29,  1.87it/s][A
 25%|██▍       | 55/221 [00:29<01:28,  1.87it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.88it/s][A
 26%|██▌       | 57/221 [00:30<01:27,  1.88it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:36<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:37<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:46<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:04<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:05<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:23<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:32<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:33<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:51<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:32,  6.74it/s][A
  1%|          | 2/221 [00:01<02:10,  1.67it/s][A
  1%|▏         | 3/221 [00:01<01:39,  2.19it/s][A
  2%|▏         | 4/221 [00:01<01:52,  1.92it/s][A
  2%|▏         | 5/221 [00:02<02:07,  1.70it/s][A
  3%|▎         | 6/221 [00:02<01:43,  2.09it/s][A
  3%|▎         | 7/221 [00:03<01:38,  2.16it/s][A
  4%|▎         | 8/221 [00:04<01:55,  1.85it/s][A
  4%|▍         | 9/221 [00:04<02:15,  1.57it/s][A
  5%|▍         | 10/221 [00:05<01:59,  1.76it/s][A
  5%|▍         | 11/221 [00:05<01:48,  1.94it/s][A
  5%|▌         | 12/221 [00:06<01:38,  2.12it/s][A
  6%|▌         | 13/221 [00:06<01:21,  2.56it/s][A
  6%|▋         | 14/221 [00:06<01:16,  2.71it/s][A
  7%|▋         | 15/221 [00:06<01:12,  2.83it/s][A
  7%|▋         | 16/221 [00:07<01:47,  1.92it/s][A
  8%|▊         | 17/221 [00:08<02:21,  1.45it/s][A
  8%|▊         | 18/221 [00:09<01:51,  1.83it/s][A
  9%|▊         | 19/221 [00:10<02:09,  1.57it/s][A
  9%|▉         | 20/221 [00:10<01:43,  1.94it/s][A
 10%|▉         | 21/221 [00:10<01:44,  1.92it/s][A
 10%|▉         | 22/221 [00:11<02:01,  1.64it/s][A
 10%|█         | 23/221 [00:11<01:47,  1.83it/s][A
 11%|█         | 24/221 [00:12<01:30,  2.17it/s][A
 11%|█▏        | 25/221 [00:13<02:04,  1.58it/s][A
 12%|█▏        | 26/221 [00:14<02:17,  1.42it/s][A
 12%|█▏        | 27/221 [00:14<01:44,  1.86it/s][A
 13%|█▎        | 28/221 [00:15<02:26,  1.31it/s][A
 13%|█▎        | 29/221 [00:15<02:04,  1.54it/s][A
 14%|█▎        | 30/221 [00:16<01:51,  1.72it/s][A
 14%|█▍        | 31/221 [00:16<01:46,  1.79it/s][A
 14%|█▍        | 32/221 [00:17<01:59,  1.58it/s][A
 15%|█▍        | 33/221 [00:19<02:51,  1.10it/s][A
 15%|█▌        | 34/221 [00:19<02:16,  1.37it/s][A
 16%|█▌        | 35/221 [00:19<01:42,  1.81it/s][A
 16%|█▋        | 36/221 [00:20<01:42,  1.81it/s][A
 17%|█▋        | 37/221 [00:20<01:38,  1.87it/s][A
 17%|█▋        | 38/221 [00:21<02:00,  1.52it/s][A
 18%|█▊        | 39/221 [00:22<01:51,  1.63it/s][A
 18%|█▊        | 40/221 [00:23<02:04,  1.45it/s][A
 19%|█▊        | 41/221 [00:23<01:39,  1.81it/s][A
 19%|█▉        | 42/221 [00:23<01:18,  2.27it/s][A
 19%|█▉        | 43/221 [00:24<01:28,  2.01it/s][A
 20%|█▉        | 44/221 [00:24<01:18,  2.25it/s][A
 21%|██        | 46/221 [00:24<00:59,  2.93it/s][A
 21%|██▏       | 47/221 [00:25<01:08,  2.53it/s][A
 22%|██▏       | 48/221 [00:25<00:56,  3.07it/s][A
 22%|██▏       | 49/221 [00:27<01:50,  1.55it/s][A
 23%|██▎       | 50/221 [00:27<01:45,  1.62it/s][A
 23%|██▎       | 51/221 [00:27<01:30,  1.88it/s][A
 24%|██▎       | 52/221 [00:28<01:15,  2.25it/s][A
 24%|██▍       | 53/221 [00:28<01:10,  2.37it/s][A
 24%|██▍       | 54/221 [00:29<01:19,  2.11it/s][A
 25%|██▍       | 55/221 [00:29<01:08,  2.42it/s][A
 25%|██▌       | 56/221 [00:30<01:21,  2.02it/s][A
 26%|██▌       | 57/221 [00:30<01:09,  2.37it/s][A
 26%|██▌       | 58/221 [00:30<01:07,  2.41it/s][A
 27%|██▋       | 59/221 [00:30<00:53,  3.01it/s][A
 27%|██▋       | 60/221 [00:31<01:00,  2.67it/s][A
 28%|██▊       | 61/221 [00:32<01:29,  1.79it/s][A
 28%|██▊       | 62/221 [00:32<01:21,  1.95it/s][A
 29%|██▊       | 63/221 [00:33<01:14,  2.13it/s][A
 29%|██▉       | 64/221 [00:34<02:05,  1.26it/s][A
 29%|██▉       | 65/221 [00:35<01:54,  1.36it/s][A
 30%|██▉       | 66/221 [00:35<01:46,  1.46it/s][A
 30%|███       | 67/221 [00:35<01:20,  1.91it/s][A
 31%|███       | 68/221 [00:36<01:02,  2.44it/s][A
 32%|███▏      | 70/221 [00:36<00:57,  2.63it/s][A
 32%|███▏      | 71/221 [00:37<00:59,  2.54it/s][A
 33%|███▎      | 72/221 [00:37<01:01,  2.44it/s][A
 33%|███▎      | 73/221 [00:38<01:14,  1.98it/s][A
 33%|███▎      | 74/221 [00:38<01:09,  2.11it/s][A
 34%|███▍      | 75/221 [00:39<01:02,  2.34it/s][A
 34%|███▍      | 76/221 [00:39<01:02,  2.33it/s][A
 35%|███▍      | 77/221 [00:39<00:54,  2.64it/s][A
 35%|███▌      | 78/221 [00:40<01:05,  2.18it/s][A
 36%|███▌      | 79/221 [00:41<01:22,  1.71it/s][A
 36%|███▌      | 80/221 [00:41<01:07,  2.08it/s][A
 37%|███▋      | 81/221 [00:42<01:05,  2.15it/s][A
 37%|███▋      | 82/221 [00:43<01:45,  1.32it/s][A
 38%|███▊      | 83/221 [00:44<01:37,  1.42it/s][A
 38%|███▊      | 84/221 [00:45<01:50,  1.24it/s][A
 38%|███▊      | 85/221 [00:45<01:24,  1.61it/s][A
 39%|███▉      | 86/221 [00:45<01:18,  1.72it/s][A
 39%|███▉      | 87/221 [00:46<01:14,  1.79it/s][A
 40%|███▉      | 88/221 [00:46<01:04,  2.06it/s][A
 40%|████      | 89/221 [00:47<01:17,  1.70it/s][A
 41%|████      | 90/221 [00:47<01:13,  1.77it/s][A
 41%|████      | 91/221 [00:48<01:01,  2.12it/s][A
 42%|████▏     | 92/221 [00:48<01:05,  1.98it/s][A
 43%|████▎     | 94/221 [00:49<00:41,  3.05it/s][A
 43%|████▎     | 95/221 [00:50<01:01,  2.04it/s][A
 43%|████▎     | 96/221 [00:50<01:03,  1.96it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.90it/s][A
 44%|████▍     | 98/221 [00:51<01:09,  1.76it/s][A
 45%|████▍     | 99/221 [00:52<01:15,  1.61it/s][A
 45%|████▌     | 100/221 [00:53<01:19,  1.53it/s][A
 46%|████▌     | 101/221 [00:54<01:20,  1.49it/s][A
 46%|████▌     | 102/221 [00:55<01:43,  1.15it/s][A
 47%|████▋     | 103/221 [00:55<01:16,  1.54it/s][A
 47%|████▋     | 104/221 [00:56<01:22,  1.42it/s][A
 48%|████▊     | 105/221 [00:56<01:08,  1.70it/s][A
 48%|████▊     | 106/221 [00:56<00:54,  2.12it/s][A
 48%|████▊     | 107/221 [00:57<00:54,  2.08it/s][A
 49%|████▉     | 108/221 [00:57<00:58,  1.92it/s][A
 49%|████▉     | 109/221 [00:58<00:52,  2.12it/s][A
 50%|████▉     | 110/221 [00:58<00:48,  2.27it/s][A
 50%|█████     | 111/221 [00:59<01:10,  1.56it/s][A
 51%|█████     | 112/221 [01:00<01:12,  1.50it/s][A
 51%|█████     | 113/221 [01:00<01:00,  1.78it/s][A
 52%|█████▏    | 114/221 [01:01<00:52,  2.04it/s][A
 52%|█████▏    | 115/221 [01:01<00:58,  1.80it/s][A
 52%|█████▏    | 116/221 [01:02<00:49,  2.13it/s][A
 53%|█████▎    | 117/221 [01:02<00:45,  2.27it/s][A
 53%|█████▎    | 118/221 [01:03<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:03<00:47,  2.13it/s][A
 54%|█████▍    | 120/221 [01:04<00:49,  2.06it/s][A
 55%|█████▍    | 121/221 [01:04<00:56,  1.76it/s][A
 55%|█████▌    | 122/221 [01:05<00:54,  1.82it/s][A
 56%|█████▌    | 123/221 [01:05<00:47,  2.06it/s][A
 56%|█████▌    | 124/221 [01:06<00:48,  2.02it/s][A
 57%|█████▋    | 125/221 [01:06<00:46,  2.04it/s][A
 57%|█████▋    | 126/221 [01:06<00:36,  2.58it/s][A
 57%|█████▋    | 127/221 [01:07<00:45,  2.05it/s][A
 58%|█████▊    | 128/221 [01:08<00:48,  1.92it/s][A
 58%|█████▊    | 129/221 [01:08<00:45,  2.00it/s][A
 59%|█████▉    | 130/221 [01:08<00:37,  2.44it/s][A
 59%|█████▉    | 131/221 [01:09<00:34,  2.59it/s][A
 60%|█████▉    | 132/221 [01:10<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:54,  1.61it/s][A
 61%|██████    | 134/221 [01:11<00:44,  1.97it/s][A
 61%|██████    | 135/221 [01:11<00:42,  2.03it/s][A
 62%|██████▏   | 136/221 [01:12<00:49,  1.72it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.90it/s][A
 62%|██████▏   | 138/221 [01:13<00:46,  1.80it/s][A
 63%|██████▎   | 139/221 [01:14<01:03,  1.29it/s][A
 63%|██████▎   | 140/221 [01:15<00:59,  1.37it/s][A
 64%|██████▍   | 141/221 [01:15<00:50,  1.59it/s][A
 64%|██████▍   | 142/221 [01:16<00:48,  1.64it/s][A
 65%|██████▍   | 143/221 [01:16<00:35,  2.17it/s][A
 65%|██████▌   | 144/221 [01:16<00:30,  2.54it/s][A
 66%|██████▌   | 145/221 [01:17<00:38,  1.95it/s][A
 66%|██████▌   | 146/221 [01:17<00:31,  2.39it/s][A
 67%|██████▋   | 147/221 [01:17<00:29,  2.51it/s][A
 67%|██████▋   | 148/221 [01:18<00:29,  2.47it/s][A
 67%|██████▋   | 149/221 [01:18<00:23,  3.11it/s][A
 68%|██████▊   | 150/221 [01:18<00:22,  3.17it/s][A
 69%|██████▉   | 152/221 [01:19<00:26,  2.65it/s][A
 69%|██████▉   | 153/221 [01:20<00:26,  2.61it/s][A
 70%|██████▉   | 154/221 [01:20<00:32,  2.07it/s][A
 70%|███████   | 155/221 [01:21<00:36,  1.80it/s][A
 71%|███████   | 156/221 [01:21<00:32,  1.99it/s][A
 71%|███████   | 157/221 [01:22<00:31,  2.04it/s][A
 71%|███████▏  | 158/221 [01:22<00:27,  2.26it/s][A
 72%|███████▏  | 160/221 [01:23<00:25,  2.35it/s][A
 73%|███████▎  | 161/221 [01:23<00:20,  2.87it/s][A
 73%|███████▎  | 162/221 [01:23<00:20,  2.93it/s][A
 74%|███████▍  | 163/221 [01:24<00:18,  3.11it/s][A
 74%|███████▍  | 164/221 [01:24<00:17,  3.17it/s][A
 75%|███████▍  | 165/221 [01:24<00:15,  3.53it/s][A
 75%|███████▌  | 166/221 [01:25<00:21,  2.61it/s][A
 76%|███████▌  | 167/221 [01:25<00:18,  2.96it/s][A
 76%|███████▌  | 168/221 [01:25<00:17,  3.01it/s][A
 76%|███████▋  | 169/221 [01:26<00:27,  1.90it/s][A
 77%|███████▋  | 170/221 [01:27<00:32,  1.55it/s][A
 77%|███████▋  | 171/221 [01:28<00:32,  1.53it/s][A
 78%|███████▊  | 172/221 [01:28<00:25,  1.93it/s][A
 78%|███████▊  | 173/221 [01:29<00:21,  2.20it/s][A
 79%|███████▊  | 174/221 [01:29<00:22,  2.12it/s][A
 79%|███████▉  | 175/221 [01:30<00:28,  1.64it/s][A
 80%|███████▉  | 176/221 [01:30<00:25,  1.79it/s][A
 80%|████████  | 177/221 [01:31<00:24,  1.80it/s][A
 81%|████████  | 178/221 [01:31<00:21,  1.96it/s][A
 81%|████████  | 179/221 [01:32<00:22,  1.85it/s][A
 81%|████████▏ | 180/221 [01:33<00:22,  1.82it/s][A
 82%|████████▏ | 181/221 [01:33<00:19,  2.08it/s][A
 82%|████████▏ | 182/221 [01:33<00:19,  1.98it/s][A
 83%|████████▎ | 183/221 [01:34<00:18,  2.08it/s][A
 83%|████████▎ | 184/221 [01:35<00:23,  1.55it/s][A
 84%|████████▎ | 185/221 [01:36<00:24,  1.47it/s][A
 85%|████████▍ | 187/221 [01:36<00:16,  2.09it/s][A
 85%|████████▌ | 188/221 [01:36<00:13,  2.40it/s][A
 86%|████████▌ | 189/221 [01:37<00:15,  2.13it/s][A
 86%|████████▌ | 190/221 [01:38<00:15,  2.01it/s][A
 86%|████████▋ | 191/221 [01:38<00:15,  1.98it/s][A
 87%|████████▋ | 192/221 [01:38<00:13,  2.17it/s][A
 87%|████████▋ | 193/221 [01:39<00:11,  2.35it/s][A
 88%|████████▊ | 194/221 [01:39<00:13,  2.04it/s][A
 88%|████████▊ | 195/221 [01:41<00:20,  1.24it/s][A
 89%|████████▊ | 196/221 [01:42<00:19,  1.28it/s][A
 89%|████████▉ | 197/221 [01:42<00:16,  1.45it/s][A
 90%|████████▉ | 198/221 [01:43<00:13,  1.71it/s][A
 90%|█████████ | 199/221 [01:43<00:10,  2.11it/s][A
 90%|█████████ | 200/221 [01:43<00:11,  1.78it/s][A
 91%|█████████ | 201/221 [01:44<00:10,  1.95it/s][A
 91%|█████████▏| 202/221 [01:44<00:08,  2.18it/s][A
 92%|█████████▏| 203/221 [01:45<00:07,  2.34it/s][A
 92%|█████████▏| 204/221 [01:45<00:08,  2.04it/s][A
 93%|█████████▎| 205/221 [01:45<00:06,  2.46it/s][A
 93%|█████████▎| 206/221 [01:46<00:05,  2.59it/s][A
 94%|█████████▎| 207/221 [01:46<00:04,  2.86it/s][A
 94%|█████████▍| 208/221 [01:47<00:07,  1.70it/s][A
 95%|█████████▍| 209/221 [01:48<00:07,  1.57it/s][A
 95%|█████████▌| 210/221 [01:48<00:05,  1.93it/s][A
 95%|█████████▌| 211/221 [01:49<00:05,  1.84it/s][A
 96%|█████████▌| 212/221 [01:49<00:04,  2.23it/s][A
 96%|█████████▋| 213/221 [01:50<00:04,  1.90it/s][A
 97%|█████████▋| 214/221 [01:50<00:04,  1.73it/s][A
 97%|█████████▋| 215/221 [01:51<00:02,  2.28it/s][A
 98%|█████████▊| 216/221 [01:51<00:02,  1.88it/s][A
 98%|█████████▊| 217/221 [01:52<00:02,  1.61it/s][A
 99%|█████████▊| 218/221 [01:52<00:01,  2.01it/s][A
 99%|█████████▉| 219/221 [01:53<00:00,  2.02it/s][A
100%|█████████▉| 220/221 [01:53<00:00,  2.07it/s][A
100%|██████████| 221/221 [01:54<00:00,  2.19it/s][A100%|██████████| 221/221 [01:54<00:00,  1.94it/s]
09/19/2024 08:19:20 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 4499--===========

09/19/2024 08:19:20 - INFO - __main__ -   {'area_r1': 45.6, 'area_recall': '45.6/74.3/82.8', 'area_ravg': 67.6}
09/19/2024 08:19:20 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 4499--===========

09/19/2024 08:19:20 - INFO - __main__ -   {'forward_r1': 50.3, 'forward_recall': '50.3/79.4/87.8', 'forward_ravg': 72.5}
09/19/2024 08:19:20 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 4499--===========

09/19/2024 08:19:20 - INFO - __main__ -   {'area_video_r1': 49.0, 'area_video_recall': '49.0/78.1/87.4', 'area_video_ravg': 71.5}
09/19/2024 08:19:20 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 08:19:20 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 08:19:20 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 4499--===========

09/19/2024 08:19:20 - INFO - __main__ -   {'area_video_r1': 61.8, 'area_video_recall': '61.8/82.2/88.5', 'area_video_ravg': 77.5, 'area_video_back_r1': 63.1, 'area_video_back_recall': '63.1/85.2/91.1', 'area_video_back_ravg': 79.8}
09/19/2024 08:19:20 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 3499=======

09/19/2024 08:19:20 - INFO - __main__ -   {'area_video_r1': 62.6, 'area_video_recall': '62.6/82.9/88.5', 'area_video_ravg': 78.0, 'area_video_back_r1': 62.0, 'area_video_back_recall': '62.0/85.4/91.2', 'area_video_back_ravg': 79.5}
09/19/2024 08:19:20 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 4499--===========

09/19/2024 08:19:20 - INFO - __main__ -   {'video_r1': 30.7, 'video_recall': '30.7/55.5/66.7', 'video_ravg': 51.0}
09/19/2024 08:19:20 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 08:19:20 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 08:19:20 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 4499--===========

09/19/2024 08:19:20 - INFO - __main__ -   {'video_r1': 59.3, 'video_recall': '59.3/79.5/85.1', 'video_ravg': 74.6}
09/19/2024 08:19:20 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 2999=======

09/19/2024 08:19:20 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/79.5/84.7', 'video_ravg': 74.8}
09/19/2024 08:19:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02453247830271721, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3226380348205566, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3471704721450806}
 50%|█████     | 4500/8917 [6:05:07<208:54:47, 170.27s/it] 50%|█████     | 4501/8917 [6:05:10<147:23:15, 120.15s/it] 50%|█████     | 4502/8917 [6:05:14<104:43:04, 85.39s/it]  50%|█████     | 4503/8917 [6:05:18<74:38:37, 60.88s/it]  51%|█████     | 4504/8917 [6:05:21<53:35:22, 43.72s/it] 51%|█████     | 4505/8917 [6:05:25<38:54:42, 31.75s/it] 51%|█████     | 4506/8917 [6:05:29<28:33:57, 23.31s/it] 51%|█████     | 4507/8917 [6:05:33<21:27:34, 17.52s/it] 51%|█████     | 4508/8917 [6:05:36<16:21:38, 13.36s/it] 51%|█████     | 4509/8917 [6:05:40<12:47:41, 10.45s/it] 51%|█████     | 4510/8917 [6:05:44<10:19:50,  8.44s/it] 51%|█████     | 4511/8917 [6:05:48<8:48:47,  7.20s/it]  51%|█████     | 4512/8917 [6:05:52<7:31:54,  6.16s/it] 51%|█████     | 4513/8917 [6:05:56<6:37:34,  5.42s/it] 51%|█████     | 4514/8917 [6:05:59<5:56:38,  4.86s/it] 51%|█████     | 4515/8917 [6:06:03<5:41:54,  4.66s/it] 51%|█████     | 4516/8917 [6:06:07<5:23:25,  4.41s/it] 51%|█████     | 4517/8917 [6:06:11<5:07:21,  4.19s/it] 51%|█████     | 4518/8917 [6:06:15<5:00:21,  4.10s/it] 51%|█████     | 4519/8917 [6:06:19<4:56:56,  4.05s/it] 51%|█████     | 4520/8917 [6:06:22<4:50:39,  3.97s/it] 51%|█████     | 4521/8917 [6:06:26<4:48:11,  3.93s/it] 51%|█████     | 4522/8917 [6:06:30<4:46:40,  3.91s/it] 51%|█████     | 4523/8917 [6:06:33<4:32:17,  3.72s/it] 51%|█████     | 4524/8917 [6:06:37<4:35:28,  3.76s/it] 51%|█████     | 4525/8917 [6:06:41<4:43:36,  3.87s/it] 51%|█████     | 4526/8917 [6:06:45<4:44:08,  3.88s/it] 51%|█████     | 4527/8917 [6:06:49<4:42:35,  3.86s/it] 51%|█████     | 4528/8917 [6:06:53<4:36:51,  3.78s/it] 51%|█████     | 4529/8917 [6:06:56<4:30:35,  3.70s/it] 51%|█████     | 4530/8917 [6:07:00<4:35:10,  3.76s/it] 51%|█████     | 4531/8917 [6:07:04<4:47:01,  3.93s/it] 51%|█████     | 4532/8917 [6:07:08<4:45:06,  3.90s/it] 51%|█████     | 4533/8917 [6:07:12<4:40:04,  3.83s/it] 51%|█████     | 4534/8917 [6:07:15<4:31:22,  3.71s/it] 51%|█████     | 4535/8917 [6:07:19<4:27:34,  3.66s/it] 51%|█████     | 4536/8917 [6:07:23<4:27:14,  3.66s/it] 51%|█████     | 4537/8917 [6:07:27<4:37:51,  3.81s/it] 51%|█████     | 4538/8917 [6:07:31<4:36:56,  3.79s/it] 51%|█████     | 4539/8917 [6:07:34<4:38:23,  3.82s/it] 51%|█████     | 4540/8917 [6:07:38<4:39:06,  3.83s/it] 51%|█████     | 4541/8917 [6:07:42<4:37:24,  3.80s/it] 51%|█████     | 4542/8917 [6:07:46<4:34:48,  3.77s/it] 51%|█████     | 4543/8917 [6:07:49<4:31:49,  3.73s/it] 51%|█████     | 4544/8917 [6:07:53<4:33:12,  3.75s/it] 51%|█████     | 4545/8917 [6:07:57<4:29:06,  3.69s/it] 51%|█████     | 4546/8917 [6:08:00<4:24:57,  3.64s/it] 51%|█████     | 4547/8917 [6:08:04<4:26:34,  3.66s/it] 51%|█████     | 4548/8917 [6:08:08<4:33:22,  3.75s/it] 51%|█████     | 4549/8917 [6:08:12<4:36:57,  3.80s/it]09/19/2024 08:22:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029391182586550713, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2681689262390137, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2975600957870483}
 51%|█████     | 4550/8917 [6:08:15<4:27:36,  3.68s/it] 51%|█████     | 4551/8917 [6:08:19<4:26:01,  3.66s/it] 51%|█████     | 4552/8917 [6:08:22<4:26:05,  3.66s/it] 51%|█████     | 4553/8917 [6:08:27<4:36:47,  3.81s/it] 51%|█████     | 4554/8917 [6:08:30<4:35:27,  3.79s/it] 51%|█████     | 4555/8917 [6:08:34<4:27:13,  3.68s/it] 51%|█████     | 4556/8917 [6:08:37<4:24:05,  3.63s/it] 51%|█████     | 4557/8917 [6:08:42<4:38:04,  3.83s/it] 51%|█████     | 4558/8917 [6:08:45<4:31:23,  3.74s/it] 51%|█████     | 4559/8917 [6:08:49<4:28:11,  3.69s/it] 51%|█████     | 4560/8917 [6:08:53<4:37:15,  3.82s/it] 51%|█████     | 4561/8917 [6:08:56<4:31:25,  3.74s/it] 51%|█████     | 4562/8917 [6:09:00<4:28:18,  3.70s/it] 51%|█████     | 4563/8917 [6:09:04<4:29:46,  3.72s/it] 51%|█████     | 4564/8917 [6:09:07<4:28:58,  3.71s/it] 51%|█████     | 4565/8917 [6:09:11<4:29:08,  3.71s/it] 51%|█████     | 4566/8917 [6:09:15<4:32:06,  3.75s/it] 51%|█████     | 4567/8917 [6:09:19<4:28:55,  3.71s/it] 51%|█████     | 4568/8917 [6:09:22<4:31:32,  3.75s/it] 51%|█████     | 4569/8917 [6:09:26<4:33:27,  3.77s/it] 51%|█████▏    | 4570/8917 [6:09:30<4:31:54,  3.75s/it] 51%|█████▏    | 4571/8917 [6:09:34<4:30:41,  3.74s/it] 51%|█████▏    | 4572/8917 [6:09:38<4:33:33,  3.78s/it] 51%|█████▏    | 4573/8917 [6:09:41<4:33:10,  3.77s/it] 51%|█████▏    | 4574/8917 [6:09:45<4:37:20,  3.83s/it] 51%|█████▏    | 4575/8917 [6:09:49<4:35:51,  3.81s/it] 51%|█████▏    | 4576/8917 [6:09:53<4:33:51,  3.79s/it] 51%|█████▏    | 4577/8917 [6:09:56<4:28:16,  3.71s/it] 51%|█████▏    | 4578/8917 [6:10:00<4:19:54,  3.59s/it] 51%|█████▏    | 4579/8917 [6:10:03<4:15:03,  3.53s/it] 51%|█████▏    | 4580/8917 [6:10:07<4:17:50,  3.57s/it] 51%|█████▏    | 4581/8917 [6:10:11<4:24:41,  3.66s/it] 51%|█████▏    | 4582/8917 [6:10:15<4:37:10,  3.84s/it] 51%|█████▏    | 4583/8917 [6:10:18<4:31:03,  3.75s/it] 51%|█████▏    | 4584/8917 [6:10:22<4:33:02,  3.78s/it] 51%|█████▏    | 4585/8917 [6:10:26<4:29:55,  3.74s/it] 51%|█████▏    | 4586/8917 [6:10:29<4:22:51,  3.64s/it] 51%|█████▏    | 4587/8917 [6:10:33<4:25:43,  3.68s/it] 51%|█████▏    | 4588/8917 [6:10:37<4:25:50,  3.68s/it] 51%|█████▏    | 4589/8917 [6:10:41<4:30:46,  3.75s/it] 51%|█████▏    | 4590/8917 [6:10:45<4:37:26,  3.85s/it] 51%|█████▏    | 4591/8917 [6:10:48<4:28:43,  3.73s/it] 51%|█████▏    | 4592/8917 [6:10:52<4:27:35,  3.71s/it] 52%|█████▏    | 4593/8917 [6:10:56<4:31:59,  3.77s/it] 52%|█████▏    | 4594/8917 [6:10:59<4:27:17,  3.71s/it] 52%|█████▏    | 4595/8917 [6:11:03<4:26:13,  3.70s/it] 52%|█████▏    | 4596/8917 [6:11:07<4:27:03,  3.71s/it] 52%|█████▏    | 4597/8917 [6:11:11<4:33:44,  3.80s/it] 52%|█████▏    | 4598/8917 [6:11:14<4:32:08,  3.78s/it] 52%|█████▏    | 4599/8917 [6:11:18<4:34:59,  3.82s/it]09/19/2024 08:25:56 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02202996239066124, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0497263669967651, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.071756362915039}
 52%|█████▏    | 4600/8917 [6:11:22<4:40:29,  3.90s/it] 52%|█████▏    | 4601/8917 [6:11:26<4:30:13,  3.76s/it] 52%|█████▏    | 4602/8917 [6:11:29<4:25:08,  3.69s/it] 52%|█████▏    | 4603/8917 [6:11:33<4:25:10,  3.69s/it] 52%|█████▏    | 4604/8917 [6:11:37<4:30:17,  3.76s/it] 52%|█████▏    | 4605/8917 [6:11:41<4:27:19,  3.72s/it] 52%|█████▏    | 4606/8917 [6:11:44<4:26:18,  3.71s/it] 52%|█████▏    | 4607/8917 [6:11:48<4:24:41,  3.68s/it] 52%|█████▏    | 4608/8917 [6:11:52<4:32:11,  3.79s/it] 52%|█████▏    | 4609/8917 [6:11:55<4:23:52,  3.68s/it] 52%|█████▏    | 4610/8917 [6:11:59<4:27:30,  3.73s/it] 52%|█████▏    | 4611/8917 [6:12:03<4:19:04,  3.61s/it] 52%|█████▏    | 4612/8917 [6:12:06<4:25:44,  3.70s/it] 52%|█████▏    | 4613/8917 [6:12:10<4:23:45,  3.68s/it] 52%|█████▏    | 4614/8917 [6:12:14<4:22:22,  3.66s/it] 52%|█████▏    | 4615/8917 [6:12:17<4:18:09,  3.60s/it] 52%|█████▏    | 4616/8917 [6:12:21<4:17:59,  3.60s/it] 52%|█████▏    | 4617/8917 [6:12:25<4:22:55,  3.67s/it] 52%|█████▏    | 4618/8917 [6:12:29<4:28:31,  3.75s/it] 52%|█████▏    | 4619/8917 [6:12:33<4:33:51,  3.82s/it] 52%|█████▏    | 4620/8917 [6:12:36<4:23:25,  3.68s/it] 52%|█████▏    | 4621/8917 [6:12:40<4:23:00,  3.67s/it] 52%|█████▏    | 4622/8917 [6:12:43<4:21:34,  3.65s/it] 52%|█████▏    | 4623/8917 [6:12:47<4:29:43,  3.77s/it] 52%|█████▏    | 4624/8917 [6:12:51<4:32:28,  3.81s/it] 52%|█████▏    | 4625/8917 [6:12:55<4:34:51,  3.84s/it] 52%|█████▏    | 4626/8917 [6:12:58<4:22:12,  3.67s/it] 52%|█████▏    | 4627/8917 [6:13:02<4:20:50,  3.65s/it] 52%|█████▏    | 4628/8917 [6:13:06<4:27:08,  3.74s/it] 52%|█████▏    | 4629/8917 [6:13:10<4:31:47,  3.80s/it] 52%|█████▏    | 4630/8917 [6:13:13<4:27:43,  3.75s/it] 52%|█████▏    | 4631/8917 [6:13:18<4:41:31,  3.94s/it] 52%|█████▏    | 4632/8917 [6:13:21<4:36:03,  3.87s/it] 52%|█████▏    | 4633/8917 [6:13:25<4:36:08,  3.87s/it] 52%|█████▏    | 4634/8917 [6:13:29<4:27:31,  3.75s/it] 52%|█████▏    | 4635/8917 [6:13:32<4:22:19,  3.68s/it] 52%|█████▏    | 4636/8917 [6:13:36<4:23:17,  3.69s/it] 52%|█████▏    | 4637/8917 [6:13:40<4:21:55,  3.67s/it] 52%|█████▏    | 4638/8917 [6:13:43<4:17:17,  3.61s/it] 52%|█████▏    | 4639/8917 [6:13:47<4:19:38,  3.64s/it] 52%|█████▏    | 4640/8917 [6:13:50<4:19:17,  3.64s/it] 52%|█████▏    | 4641/8917 [6:13:54<4:24:10,  3.71s/it] 52%|█████▏    | 4642/8917 [6:13:58<4:24:18,  3.71s/it] 52%|█████▏    | 4643/8917 [6:14:02<4:23:37,  3.70s/it] 52%|█████▏    | 4644/8917 [6:14:06<4:25:27,  3.73s/it] 52%|█████▏    | 4645/8917 [6:14:09<4:25:09,  3.72s/it] 52%|█████▏    | 4646/8917 [6:14:13<4:24:41,  3.72s/it] 52%|█████▏    | 4647/8917 [6:14:17<4:27:13,  3.75s/it] 52%|█████▏    | 4648/8917 [6:14:20<4:16:57,  3.61s/it] 52%|█████▏    | 4649/8917 [6:14:24<4:24:36,  3.72s/it]09/19/2024 08:29:02 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.022695692256093025, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9812881946563721, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0039838552474976}
 52%|█████▏    | 4650/8917 [6:14:28<4:25:56,  3.74s/it] 52%|█████▏    | 4651/8917 [6:14:31<4:21:37,  3.68s/it] 52%|█████▏    | 4652/8917 [6:14:35<4:20:22,  3.66s/it] 52%|█████▏    | 4653/8917 [6:14:39<4:27:09,  3.76s/it] 52%|█████▏    | 4654/8917 [6:14:43<4:28:53,  3.78s/it] 52%|█████▏    | 4655/8917 [6:14:47<4:27:00,  3.76s/it] 52%|█████▏    | 4656/8917 [6:14:50<4:27:52,  3.77s/it] 52%|█████▏    | 4657/8917 [6:14:54<4:29:36,  3.80s/it] 52%|█████▏    | 4658/8917 [6:14:58<4:27:40,  3.77s/it] 52%|█████▏    | 4659/8917 [6:15:01<4:22:45,  3.70s/it] 52%|█████▏    | 4660/8917 [6:15:05<4:22:23,  3.70s/it] 52%|█████▏    | 4661/8917 [6:15:09<4:20:12,  3.67s/it] 52%|█████▏    | 4662/8917 [6:15:12<4:16:45,  3.62s/it] 52%|█████▏    | 4663/8917 [6:15:16<4:12:55,  3.57s/it] 52%|█████▏    | 4664/8917 [6:15:19<4:18:33,  3.65s/it] 52%|█████▏    | 4665/8917 [6:15:23<4:21:43,  3.69s/it] 52%|█████▏    | 4666/8917 [6:15:27<4:22:46,  3.71s/it] 52%|█████▏    | 4667/8917 [6:15:31<4:20:35,  3.68s/it] 52%|█████▏    | 4668/8917 [6:15:35<4:27:19,  3.77s/it] 52%|█████▏    | 4669/8917 [6:15:38<4:25:14,  3.75s/it] 52%|█████▏    | 4670/8917 [6:15:42<4:24:10,  3.73s/it] 52%|█████▏    | 4671/8917 [6:15:46<4:27:43,  3.78s/it] 52%|█████▏    | 4672/8917 [6:15:50<4:34:36,  3.88s/it] 52%|█████▏    | 4673/8917 [6:15:54<4:32:52,  3.86s/it] 52%|█████▏    | 4674/8917 [6:15:58<4:30:19,  3.82s/it] 52%|█████▏    | 4675/8917 [6:16:01<4:20:33,  3.69s/it] 52%|█████▏    | 4676/8917 [6:16:05<4:20:05,  3.68s/it] 52%|█████▏    | 4677/8917 [6:16:08<4:19:41,  3.67s/it] 52%|█████▏    | 4678/8917 [6:16:12<4:19:47,  3.68s/it] 52%|█████▏    | 4679/8917 [6:16:15<4:16:13,  3.63s/it] 52%|█████▏    | 4680/8917 [6:16:19<4:18:23,  3.66s/it] 52%|█████▏    | 4681/8917 [6:16:23<4:18:02,  3.65s/it] 53%|█████▎    | 4682/8917 [6:16:26<4:17:16,  3.64s/it] 53%|█████▎    | 4683/8917 [6:16:30<4:16:06,  3.63s/it] 53%|█████▎    | 4684/8917 [6:16:34<4:25:38,  3.77s/it] 53%|█████▎    | 4685/8917 [6:16:38<4:32:34,  3.86s/it] 53%|█████▎    | 4686/8917 [6:16:42<4:27:55,  3.80s/it] 53%|█████▎    | 4687/8917 [6:16:46<4:34:07,  3.89s/it] 53%|█████▎    | 4688/8917 [6:16:50<4:40:21,  3.98s/it] 53%|█████▎    | 4689/8917 [6:16:54<4:27:44,  3.80s/it] 53%|█████▎    | 4690/8917 [6:16:57<4:22:20,  3.72s/it] 53%|█████▎    | 4691/8917 [6:17:01<4:20:30,  3.70s/it] 53%|█████▎    | 4692/8917 [6:17:05<4:23:51,  3.75s/it] 53%|█████▎    | 4693/8917 [6:17:08<4:22:53,  3.73s/it] 53%|█████▎    | 4694/8917 [6:17:12<4:24:31,  3.76s/it] 53%|█████▎    | 4695/8917 [6:17:16<4:19:04,  3.68s/it] 53%|█████▎    | 4696/8917 [6:17:19<4:16:52,  3.65s/it] 53%|█████▎    | 4697/8917 [6:17:23<4:17:33,  3.66s/it] 53%|█████▎    | 4698/8917 [6:17:26<4:14:52,  3.62s/it] 53%|█████▎    | 4699/8917 [6:17:30<4:21:25,  3.72s/it]09/19/2024 08:32:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03439741209149361, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.178642749786377, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2130401134490967}
 53%|█████▎    | 4700/8917 [6:17:34<4:20:07,  3.70s/it] 53%|█████▎    | 4701/8917 [6:17:38<4:15:31,  3.64s/it] 53%|█████▎    | 4702/8917 [6:17:41<4:11:11,  3.58s/it] 53%|█████▎    | 4703/8917 [6:17:45<4:17:01,  3.66s/it] 53%|█████▎    | 4704/8917 [6:17:49<4:20:30,  3.71s/it] 53%|█████▎    | 4705/8917 [6:17:53<4:26:32,  3.80s/it] 53%|█████▎    | 4706/8917 [6:17:56<4:14:57,  3.63s/it] 53%|█████▎    | 4707/8917 [6:18:00<4:15:42,  3.64s/it] 53%|█████▎    | 4708/8917 [6:18:04<4:24:13,  3.77s/it] 53%|█████▎    | 4709/8917 [6:18:07<4:16:45,  3.66s/it] 53%|█████▎    | 4710/8917 [6:18:11<4:17:20,  3.67s/it] 53%|█████▎    | 4711/8917 [6:18:15<4:27:51,  3.82s/it] 53%|█████▎    | 4712/8917 [6:18:18<4:14:39,  3.63s/it] 53%|█████▎    | 4713/8917 [6:18:22<4:23:35,  3.76s/it] 53%|█████▎    | 4714/8917 [6:18:26<4:20:11,  3.71s/it] 53%|█████▎    | 4715/8917 [6:18:29<4:19:22,  3.70s/it] 53%|█████▎    | 4716/8917 [6:18:34<4:27:56,  3.83s/it] 53%|█████▎    | 4717/8917 [6:18:37<4:22:51,  3.76s/it] 53%|█████▎    | 4718/8917 [6:18:41<4:22:44,  3.75s/it] 53%|█████▎    | 4719/8917 [6:18:45<4:26:23,  3.81s/it] 53%|█████▎    | 4720/8917 [6:18:49<4:28:11,  3.83s/it] 53%|█████▎    | 4721/8917 [6:18:53<4:27:37,  3.83s/it] 53%|█████▎    | 4722/8917 [6:18:56<4:25:20,  3.80s/it] 53%|█████▎    | 4723/8917 [6:19:00<4:26:52,  3.82s/it] 53%|█████▎    | 4724/8917 [6:19:04<4:24:32,  3.79s/it] 53%|█████▎    | 4725/8917 [6:19:07<4:22:01,  3.75s/it] 53%|█████▎    | 4726/8917 [6:19:12<4:27:48,  3.83s/it] 53%|█████▎    | 4727/8917 [6:19:15<4:21:28,  3.74s/it] 53%|█████▎    | 4728/8917 [6:19:19<4:26:15,  3.81s/it] 53%|█████▎    | 4729/8917 [6:19:23<4:25:05,  3.80s/it] 53%|█████▎    | 4730/8917 [6:19:27<4:24:24,  3.79s/it] 53%|█████▎    | 4731/8917 [6:19:30<4:25:36,  3.81s/it] 53%|█████▎    | 4732/8917 [6:19:34<4:24:57,  3.80s/it] 53%|█████▎    | 4733/8917 [6:19:38<4:20:18,  3.73s/it] 53%|█████▎    | 4734/8917 [6:19:42<4:22:27,  3.76s/it] 53%|█████▎    | 4735/8917 [6:19:45<4:12:42,  3.63s/it] 53%|█████▎    | 4736/8917 [6:19:49<4:18:44,  3.71s/it] 53%|█████▎    | 4737/8917 [6:19:53<4:20:15,  3.74s/it] 53%|█████▎    | 4738/8917 [6:19:56<4:18:50,  3.72s/it] 53%|█████▎    | 4739/8917 [6:20:00<4:13:10,  3.64s/it] 53%|█████▎    | 4740/8917 [6:20:03<4:12:21,  3.63s/it] 53%|█████▎    | 4741/8917 [6:20:07<4:17:16,  3.70s/it] 53%|█████▎    | 4742/8917 [6:20:11<4:17:10,  3.70s/it] 53%|█████▎    | 4743/8917 [6:20:15<4:19:49,  3.73s/it] 53%|█████▎    | 4744/8917 [6:20:18<4:15:19,  3.67s/it] 53%|█████▎    | 4745/8917 [6:20:22<4:18:22,  3.72s/it] 53%|█████▎    | 4746/8917 [6:20:26<4:17:38,  3.71s/it] 53%|█████▎    | 4747/8917 [6:20:30<4:19:20,  3.73s/it] 53%|█████▎    | 4748/8917 [6:20:33<4:15:48,  3.68s/it] 53%|█████▎    | 4749/8917 [6:20:37<4:17:17,  3.70s/it]09/19/2024 08:35:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025370601564645767, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9003773927688599, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9257479906082153}
 53%|█████▎    | 4750/8917 [6:20:41<4:16:56,  3.70s/it] 53%|█████▎    | 4751/8917 [6:20:44<4:15:53,  3.69s/it] 53%|█████▎    | 4752/8917 [6:20:48<4:13:26,  3.65s/it] 53%|█████▎    | 4753/8917 [6:20:51<4:06:20,  3.55s/it] 53%|█████▎    | 4754/8917 [6:20:55<4:11:49,  3.63s/it] 53%|█████▎    | 4755/8917 [6:20:59<4:20:15,  3.75s/it] 53%|█████▎    | 4756/8917 [6:21:02<4:10:55,  3.62s/it] 53%|█████▎    | 4757/8917 [6:21:06<4:16:07,  3.69s/it] 53%|█████▎    | 4758/8917 [6:21:10<4:22:48,  3.79s/it] 53%|█████▎    | 4759/8917 [6:21:14<4:18:13,  3.73s/it] 53%|█████▎    | 4760/8917 [6:21:18<4:19:43,  3.75s/it] 53%|█████▎    | 4761/8917 [6:21:21<4:21:34,  3.78s/it] 53%|█████▎    | 4762/8917 [6:21:25<4:23:17,  3.80s/it] 53%|█████▎    | 4763/8917 [6:21:29<4:20:28,  3.76s/it] 53%|█████▎    | 4764/8917 [6:21:33<4:21:02,  3.77s/it] 53%|█████▎    | 4765/8917 [6:21:36<4:21:02,  3.77s/it] 53%|█████▎    | 4766/8917 [6:21:40<4:16:35,  3.71s/it] 53%|█████▎    | 4767/8917 [6:21:44<4:13:39,  3.67s/it] 53%|█████▎    | 4768/8917 [6:21:48<4:19:54,  3.76s/it] 53%|█████▎    | 4769/8917 [6:21:51<4:17:20,  3.72s/it] 53%|█████▎    | 4770/8917 [6:21:55<4:12:30,  3.65s/it] 54%|█████▎    | 4771/8917 [6:21:58<4:11:50,  3.64s/it] 54%|█████▎    | 4772/8917 [6:22:02<4:12:05,  3.65s/it] 54%|█████▎    | 4773/8917 [6:22:06<4:21:21,  3.78s/it] 54%|█████▎    | 4774/8917 [6:22:10<4:20:40,  3.78s/it] 54%|█████▎    | 4775/8917 [6:22:14<4:24:20,  3.83s/it] 54%|█████▎    | 4776/8917 [6:22:17<4:22:06,  3.80s/it] 54%|█████▎    | 4777/8917 [6:22:21<4:17:10,  3.73s/it] 54%|█████▎    | 4778/8917 [6:22:25<4:16:32,  3.72s/it] 54%|█████▎    | 4779/8917 [6:22:28<4:13:35,  3.68s/it] 54%|█████▎    | 4780/8917 [6:22:32<4:22:46,  3.81s/it] 54%|█████▎    | 4781/8917 [6:22:36<4:18:54,  3.76s/it] 54%|█████▎    | 4782/8917 [6:22:40<4:21:59,  3.80s/it] 54%|█████▎    | 4783/8917 [6:22:44<4:30:10,  3.92s/it] 54%|█████▎    | 4784/8917 [6:22:48<4:27:29,  3.88s/it] 54%|█████▎    | 4785/8917 [6:22:51<4:17:25,  3.74s/it] 54%|█████▎    | 4786/8917 [6:22:55<4:16:06,  3.72s/it] 54%|█████▎    | 4787/8917 [6:22:59<4:23:50,  3.83s/it] 54%|█████▎    | 4788/8917 [6:23:03<4:21:51,  3.81s/it] 54%|█████▎    | 4789/8917 [6:23:07<4:22:30,  3.82s/it] 54%|█████▎    | 4790/8917 [6:23:10<4:16:58,  3.74s/it] 54%|█████▎    | 4791/8917 [6:23:14<4:18:05,  3.75s/it] 54%|█████▎    | 4792/8917 [6:23:18<4:23:06,  3.83s/it] 54%|█████▍    | 4793/8917 [6:23:22<4:24:36,  3.85s/it] 54%|█████▍    | 4794/8917 [6:23:25<4:16:29,  3.73s/it] 54%|█████▍    | 4795/8917 [6:23:29<4:17:48,  3.75s/it] 54%|█████▍    | 4796/8917 [6:23:33<4:21:19,  3.80s/it] 54%|█████▍    | 4797/8917 [6:23:37<4:15:45,  3.72s/it] 54%|█████▍    | 4798/8917 [6:23:41<4:18:42,  3.77s/it] 54%|█████▍    | 4799/8917 [6:23:45<4:22:17,  3.82s/it]09/19/2024 08:38:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.040131356567144394, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3502528667449951, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3903841972351074}
 54%|█████▍    | 4800/8917 [6:23:48<4:18:12,  3.76s/it] 54%|█████▍    | 4801/8917 [6:23:52<4:13:58,  3.70s/it] 54%|█████▍    | 4802/8917 [6:23:55<4:14:35,  3.71s/it] 54%|█████▍    | 4803/8917 [6:23:59<4:16:11,  3.74s/it] 54%|█████▍    | 4804/8917 [6:24:03<4:18:58,  3.78s/it] 54%|█████▍    | 4805/8917 [6:24:07<4:21:17,  3.81s/it] 54%|█████▍    | 4806/8917 [6:24:11<4:18:30,  3.77s/it] 54%|█████▍    | 4807/8917 [6:24:14<4:17:28,  3.76s/it] 54%|█████▍    | 4808/8917 [6:24:18<4:13:30,  3.70s/it] 54%|█████▍    | 4809/8917 [6:24:22<4:15:15,  3.73s/it] 54%|█████▍    | 4810/8917 [6:24:25<4:14:12,  3.71s/it] 54%|█████▍    | 4811/8917 [6:24:29<4:14:48,  3.72s/it] 54%|█████▍    | 4812/8917 [6:24:33<4:19:57,  3.80s/it] 54%|█████▍    | 4813/8917 [6:24:37<4:21:59,  3.83s/it] 54%|█████▍    | 4814/8917 [6:24:41<4:20:04,  3.80s/it] 54%|█████▍    | 4815/8917 [6:24:44<4:16:39,  3.75s/it] 54%|█████▍    | 4816/8917 [6:24:48<4:18:06,  3.78s/it] 54%|█████▍    | 4817/8917 [6:24:52<4:17:55,  3.77s/it] 54%|█████▍    | 4818/8917 [6:24:56<4:17:34,  3.77s/it] 54%|█████▍    | 4819/8917 [6:25:00<4:17:41,  3.77s/it] 54%|█████▍    | 4820/8917 [6:25:03<4:11:34,  3.68s/it] 54%|█████▍    | 4821/8917 [6:25:07<4:12:41,  3.70s/it] 54%|█████▍    | 4822/8917 [6:25:10<4:12:12,  3.70s/it] 54%|█████▍    | 4823/8917 [6:25:14<4:18:12,  3.78s/it] 54%|█████▍    | 4824/8917 [6:25:18<4:09:33,  3.66s/it] 54%|█████▍    | 4825/8917 [6:25:21<4:06:30,  3.61s/it] 54%|█████▍    | 4826/8917 [6:25:26<4:19:14,  3.80s/it] 54%|█████▍    | 4827/8917 [6:25:29<4:19:49,  3.81s/it] 54%|█████▍    | 4828/8917 [6:25:33<4:13:58,  3.73s/it] 54%|█████▍    | 4829/8917 [6:25:37<4:17:03,  3.77s/it] 54%|█████▍    | 4830/8917 [6:25:40<4:10:09,  3.67s/it] 54%|█████▍    | 4831/8917 [6:25:44<4:13:07,  3.72s/it] 54%|█████▍    | 4832/8917 [6:25:48<4:13:26,  3.72s/it] 54%|█████▍    | 4833/8917 [6:25:52<4:16:13,  3.76s/it] 54%|█████▍    | 4834/8917 [6:25:55<4:15:57,  3.76s/it] 54%|█████▍    | 4835/8917 [6:25:59<4:14:41,  3.74s/it] 54%|█████▍    | 4836/8917 [6:26:03<4:10:10,  3.68s/it] 54%|█████▍    | 4837/8917 [6:26:07<4:16:03,  3.77s/it] 54%|█████▍    | 4838/8917 [6:26:10<4:16:01,  3.77s/it] 54%|█████▍    | 4839/8917 [6:26:14<4:08:08,  3.65s/it] 54%|█████▍    | 4840/8917 [6:26:18<4:11:32,  3.70s/it] 54%|█████▍    | 4841/8917 [6:26:21<4:12:39,  3.72s/it] 54%|█████▍    | 4842/8917 [6:26:25<4:15:37,  3.76s/it] 54%|█████▍    | 4843/8917 [6:26:29<4:16:31,  3.78s/it] 54%|█████▍    | 4844/8917 [6:26:33<4:13:54,  3.74s/it] 54%|█████▍    | 4845/8917 [6:26:36<4:08:38,  3.66s/it] 54%|█████▍    | 4846/8917 [6:26:40<4:03:58,  3.60s/it] 54%|█████▍    | 4847/8917 [6:26:44<4:16:04,  3.77s/it] 54%|█████▍    | 4848/8917 [6:26:48<4:18:05,  3.81s/it] 54%|█████▍    | 4849/8917 [6:26:51<4:13:43,  3.74s/it]09/19/2024 08:41:29 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01771583780646324, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0535225868225098, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.071238398551941}
 54%|█████▍    | 4850/8917 [6:26:55<4:08:00,  3.66s/it] 54%|█████▍    | 4851/8917 [6:26:58<4:05:05,  3.62s/it] 54%|█████▍    | 4852/8917 [6:27:02<4:10:00,  3.69s/it] 54%|█████▍    | 4853/8917 [6:27:06<4:21:13,  3.86s/it] 54%|█████▍    | 4854/8917 [6:27:10<4:11:20,  3.71s/it] 54%|█████▍    | 4855/8917 [6:27:13<4:07:12,  3.65s/it] 54%|█████▍    | 4856/8917 [6:27:17<4:09:57,  3.69s/it] 54%|█████▍    | 4857/8917 [6:27:21<4:10:37,  3.70s/it] 54%|█████▍    | 4858/8917 [6:27:25<4:14:08,  3.76s/it] 54%|█████▍    | 4859/8917 [6:27:29<4:18:46,  3.83s/it] 55%|█████▍    | 4860/8917 [6:27:32<4:17:17,  3.81s/it] 55%|█████▍    | 4861/8917 [6:27:36<4:17:06,  3.80s/it] 55%|█████▍    | 4862/8917 [6:27:40<4:13:35,  3.75s/it] 55%|█████▍    | 4863/8917 [6:27:43<4:04:20,  3.62s/it] 55%|█████▍    | 4864/8917 [6:27:47<4:05:01,  3.63s/it] 55%|█████▍    | 4865/8917 [6:27:51<4:11:17,  3.72s/it] 55%|█████▍    | 4866/8917 [6:27:54<4:10:46,  3.71s/it] 55%|█████▍    | 4867/8917 [6:27:58<4:07:15,  3.66s/it] 55%|█████▍    | 4868/8917 [6:28:01<4:03:04,  3.60s/it] 55%|█████▍    | 4869/8917 [6:28:05<4:02:01,  3.59s/it] 55%|█████▍    | 4870/8917 [6:28:09<4:02:49,  3.60s/it] 55%|█████▍    | 4871/8917 [6:28:13<4:09:18,  3.70s/it] 55%|█████▍    | 4872/8917 [6:28:16<4:09:28,  3.70s/it] 55%|█████▍    | 4873/8917 [6:28:20<4:09:45,  3.71s/it] 55%|█████▍    | 4874/8917 [6:28:24<4:08:44,  3.69s/it] 55%|█████▍    | 4875/8917 [6:28:27<4:08:25,  3.69s/it] 55%|█████▍    | 4876/8917 [6:28:31<4:12:53,  3.75s/it] 55%|█████▍    | 4877/8917 [6:28:35<4:16:53,  3.82s/it] 55%|█████▍    | 4878/8917 [6:28:39<4:11:43,  3.74s/it] 55%|█████▍    | 4879/8917 [6:28:43<4:13:59,  3.77s/it] 55%|█████▍    | 4880/8917 [6:28:47<4:17:08,  3.82s/it] 55%|█████▍    | 4881/8917 [6:28:50<4:11:48,  3.74s/it] 55%|█████▍    | 4882/8917 [6:28:54<4:14:55,  3.79s/it] 55%|█████▍    | 4883/8917 [6:28:58<4:15:50,  3.81s/it] 55%|█████▍    | 4884/8917 [6:29:01<4:12:49,  3.76s/it] 55%|█████▍    | 4885/8917 [6:29:05<4:13:40,  3.78s/it] 55%|█████▍    | 4886/8917 [6:29:09<4:12:56,  3.76s/it] 55%|█████▍    | 4887/8917 [6:29:13<4:12:35,  3.76s/it] 55%|█████▍    | 4888/8917 [6:29:17<4:14:56,  3.80s/it] 55%|█████▍    | 4889/8917 [6:29:20<4:08:44,  3.71s/it] 55%|█████▍    | 4890/8917 [6:29:24<4:10:29,  3.73s/it] 55%|█████▍    | 4891/8917 [6:29:28<4:15:57,  3.81s/it] 55%|█████▍    | 4892/8917 [6:29:31<4:09:58,  3.73s/it] 55%|█████▍    | 4893/8917 [6:29:35<4:08:47,  3.71s/it] 55%|█████▍    | 4894/8917 [6:29:39<4:04:57,  3.65s/it] 55%|█████▍    | 4895/8917 [6:29:42<4:00:31,  3.59s/it] 55%|█████▍    | 4896/8917 [6:29:46<4:06:16,  3.67s/it] 55%|█████▍    | 4897/8917 [6:29:50<4:09:22,  3.72s/it] 55%|█████▍    | 4898/8917 [6:29:54<4:13:11,  3.78s/it] 55%|█████▍    | 4899/8917 [6:29:58<4:21:24,  3.90s/it]09/19/2024 08:44:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.022167682647705078, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.732599139213562, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.754766821861267}
 55%|█████▍    | 4900/8917 [6:30:02<4:20:22,  3.89s/it] 55%|█████▍    | 4901/8917 [6:30:05<4:12:17,  3.77s/it] 55%|█████▍    | 4902/8917 [6:30:09<4:11:50,  3.76s/it] 55%|█████▍    | 4903/8917 [6:30:13<4:08:12,  3.71s/it] 55%|█████▍    | 4904/8917 [6:30:17<4:12:51,  3.78s/it] 55%|█████▌    | 4905/8917 [6:30:20<4:12:30,  3.78s/it] 55%|█████▌    | 4906/8917 [6:30:24<4:11:24,  3.76s/it] 55%|█████▌    | 4907/8917 [6:30:28<4:09:01,  3.73s/it] 55%|█████▌    | 4908/8917 [6:30:31<4:07:10,  3.70s/it] 55%|█████▌    | 4909/8917 [6:30:35<4:07:08,  3.70s/it] 55%|█████▌    | 4910/8917 [6:30:39<4:07:42,  3.71s/it] 55%|█████▌    | 4911/8917 [6:30:43<4:16:26,  3.84s/it] 55%|█████▌    | 4912/8917 [6:30:46<4:10:22,  3.75s/it] 55%|█████▌    | 4913/8917 [6:30:50<4:12:41,  3.79s/it] 55%|█████▌    | 4914/8917 [6:30:54<4:15:59,  3.84s/it] 55%|█████▌    | 4915/8917 [6:30:58<4:15:21,  3.83s/it] 55%|█████▌    | 4916/8917 [6:31:02<4:17:11,  3.86s/it] 55%|█████▌    | 4917/8917 [6:31:06<4:10:07,  3.75s/it] 55%|█████▌    | 4918/8917 [6:31:09<4:04:17,  3.67s/it] 55%|█████▌    | 4919/8917 [6:31:13<4:04:46,  3.67s/it] 55%|█████▌    | 4920/8917 [6:31:17<4:11:15,  3.77s/it] 55%|█████▌    | 4921/8917 [6:31:21<4:12:25,  3.79s/it] 55%|█████▌    | 4922/8917 [6:31:24<4:00:37,  3.61s/it] 55%|█████▌    | 4923/8917 [6:31:27<4:00:45,  3.62s/it] 55%|█████▌    | 4924/8917 [6:31:31<4:04:26,  3.67s/it] 55%|█████▌    | 4925/8917 [6:31:35<4:06:16,  3.70s/it] 55%|█████▌    | 4926/8917 [6:31:39<4:11:10,  3.78s/it] 55%|█████▌    | 4927/8917 [6:31:43<4:12:46,  3.80s/it] 55%|█████▌    | 4928/8917 [6:31:47<4:20:54,  3.92s/it] 55%|█████▌    | 4929/8917 [6:31:50<4:12:32,  3.80s/it] 55%|█████▌    | 4930/8917 [6:31:54<4:11:35,  3.79s/it] 55%|█████▌    | 4931/8917 [6:31:58<4:12:32,  3.80s/it] 55%|█████▌    | 4932/8917 [6:32:02<4:10:34,  3.77s/it] 55%|█████▌    | 4933/8917 [6:32:05<4:09:28,  3.76s/it] 55%|█████▌    | 4934/8917 [6:32:09<4:05:35,  3.70s/it] 55%|█████▌    | 4935/8917 [6:32:13<4:07:18,  3.73s/it] 55%|█████▌    | 4936/8917 [6:32:16<4:06:15,  3.71s/it] 55%|█████▌    | 4937/8917 [6:32:20<4:06:47,  3.72s/it] 55%|█████▌    | 4938/8917 [6:32:23<3:57:47,  3.59s/it] 55%|█████▌    | 4939/8917 [6:32:26<3:43:26,  3.37s/it] 55%|█████▌    | 4940/8917 [6:32:29<3:33:26,  3.22s/it] 55%|█████▌    | 4941/8917 [6:32:32<3:26:14,  3.11s/it] 55%|█████▌    | 4942/8917 [6:32:35<3:21:08,  3.04s/it] 55%|█████▌    | 4943/8917 [6:32:38<3:17:49,  2.99s/it] 55%|█████▌    | 4944/8917 [6:32:41<3:15:10,  2.95s/it] 55%|█████▌    | 4945/8917 [6:32:44<3:13:35,  2.92s/it] 55%|█████▌    | 4946/8917 [6:32:46<3:12:20,  2.91s/it] 55%|█████▌    | 4947/8917 [6:32:49<3:11:35,  2.90s/it] 55%|█████▌    | 4948/8917 [6:32:52<3:10:47,  2.88s/it] 56%|█████▌    | 4949/8917 [6:32:55<3:10:35,  2.88s/it]09/19/2024 08:47:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03658633679151535, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4665467739105225, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5031330585479736}
 56%|█████▌    | 4950/8917 [6:32:58<3:10:20,  2.88s/it] 56%|█████▌    | 4951/8917 [6:33:01<3:09:56,  2.87s/it] 56%|█████▌    | 4952/8917 [6:33:04<3:09:43,  2.87s/it] 56%|█████▌    | 4953/8917 [6:33:06<3:09:35,  2.87s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
 56%|█████▌    | 4954/8917 [6:33:51<16:54:18, 15.36s/it] 56%|█████▌    | 4955/8917 [6:33:57<13:49:21, 12.56s/it] 56%|█████▌    | 4956/8917 [6:34:04<12:03:13, 10.96s/it] 56%|█████▌    | 4957/8917 [6:34:09<9:57:38,  9.06s/it]  56%|█████▌    | 4958/8917 [6:34:14<8:39:10,  7.87s/it] 56%|█████▌    | 4959/8917 [6:34:20<8:00:07,  7.28s/it] 56%|█████▌    | 4960/8917 [6:34:25<7:27:50,  6.79s/it] 56%|█████▌    | 4961/8917 [6:34:30<6:34:58,  5.99s/it] 56%|█████▌    | 4962/8917 [6:34:33<5:52:44,  5.35s/it] 56%|█████▌    | 4963/8917 [6:34:37<5:22:03,  4.89s/it] 56%|█████▌    | 4964/8917 [6:34:41<4:58:17,  4.53s/it] 56%|█████▌    | 4965/8917 [6:34:44<4:36:28,  4.20s/it] 56%|█████▌    | 4966/8917 [6:34:48<4:24:57,  4.02s/it] 56%|█████▌    | 4967/8917 [6:34:52<4:20:05,  3.95s/it] 56%|█████▌    | 4968/8917 [6:34:55<4:14:14,  3.86s/it] 56%|█████▌    | 4969/8917 [6:34:59<4:14:15,  3.86s/it] 56%|█████▌    | 4970/8917 [6:35:03<4:10:34,  3.81s/it] 56%|█████▌    | 4971/8917 [6:35:07<4:05:11,  3.73s/it] 56%|█████▌    | 4972/8917 [6:35:10<4:07:04,  3.76s/it] 56%|█████▌    | 4973/8917 [6:35:14<4:04:29,  3.72s/it] 56%|█████▌    | 4974/8917 [6:35:18<4:04:09,  3.72s/it] 56%|█████▌    | 4975/8917 [6:35:22<4:09:19,  3.79s/it] 56%|█████▌    | 4976/8917 [6:35:25<4:05:27,  3.74s/it] 56%|█████▌    | 4977/8917 [6:35:29<4:05:33,  3.74s/it] 56%|█████▌    | 4978/8917 [6:35:33<4:10:57,  3.82s/it] 56%|█████▌    | 4979/8917 [6:35:37<4:06:31,  3.76s/it] 56%|█████▌    | 4980/8917 [6:35:40<4:04:56,  3.73s/it] 56%|█████▌    | 4981/8917 [6:35:44<4:05:46,  3.75s/it] 56%|█████▌    | 4982/8917 [6:35:48<4:09:45,  3.81s/it] 56%|█████▌    | 4983/8917 [6:35:52<4:16:47,  3.92s/it] 56%|█████▌    | 4984/8917 [6:35:56<4:05:38,  3.75s/it] 56%|█████▌    | 4985/8917 [6:35:59<4:06:00,  3.75s/it] 56%|█████▌    | 4986/8917 [6:36:03<4:07:16,  3.77s/it] 56%|█████▌    | 4987/8917 [6:36:07<4:03:01,  3.71s/it] 56%|█████▌    | 4988/8917 [6:36:10<3:53:52,  3.57s/it] 56%|█████▌    | 4989/8917 [6:36:14<3:58:28,  3.64s/it] 56%|█████▌    | 4990/8917 [6:36:18<4:01:59,  3.70s/it] 56%|█████▌    | 4991/8917 [6:36:22<4:07:51,  3.79s/it] 56%|█████▌    | 4992/8917 [6:36:25<4:08:47,  3.80s/it] 56%|█████▌    | 4993/8917 [6:36:29<4:01:38,  3.69s/it] 56%|█████▌    | 4994/8917 [6:36:32<3:57:33,  3.63s/it] 56%|█████▌    | 4995/8917 [6:36:36<4:00:17,  3.68s/it] 56%|█████▌    | 4996/8917 [6:36:40<4:02:58,  3.72s/it] 56%|█████▌    | 4997/8917 [6:36:44<4:05:36,  3.76s/it] 56%|█████▌    | 4998/8917 [6:36:48<4:09:48,  3.82s/it] 56%|█████▌    | 4999/8917 [6:36:51<4:01:18,  3.70s/it]09/19/2024 08:51:28 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 08:51:28 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:02<08:10,  2.23s/it][A
  1%|          | 2/221 [00:02<04:45,  1.30s/it][A
  1%|▏         | 3/221 [00:03<04:06,  1.13s/it][A
  2%|▏         | 4/221 [00:04<02:45,  1.31it/s][A
  3%|▎         | 6/221 [00:04<01:31,  2.34it/s][A
  3%|▎         | 7/221 [00:04<01:24,  2.53it/s][A
  4%|▎         | 8/221 [00:05<01:27,  2.44it/s][A
  4%|▍         | 9/221 [00:05<01:30,  2.35it/s][A
  5%|▍         | 10/221 [00:05<01:28,  2.39it/s][A
  5%|▌         | 12/221 [00:11<05:21,  1.54s/it][A
  6%|▌         | 13/221 [00:11<04:10,  1.20s/it][A
  7%|▋         | 15/221 [00:12<02:43,  1.26it/s][A
  7%|▋         | 16/221 [00:13<02:46,  1.23it/s][A
  8%|▊         | 17/221 [00:14<03:18,  1.03it/s][A
  8%|▊         | 18/221 [00:14<02:45,  1.22it/s][A
  9%|▊         | 19/221 [00:16<03:31,  1.05s/it][A
  9%|▉         | 20/221 [00:16<02:37,  1.27it/s][A
 10%|▉         | 21/221 [00:17<02:11,  1.53it/s][A
 10%|▉         | 22/221 [00:17<02:18,  1.44it/s][A
 11%|█         | 24/221 [00:18<01:32,  2.14it/s][A
 11%|█▏        | 25/221 [00:18<01:26,  2.27it/s][A
 12%|█▏        | 26/221 [00:18<01:21,  2.39it/s][A
 13%|█▎        | 28/221 [00:19<01:16,  2.52it/s][A
 13%|█▎        | 29/221 [00:19<01:05,  2.93it/s][A
 14%|█▎        | 30/221 [00:20<01:12,  2.63it/s][A
 14%|█▍        | 31/221 [00:20<01:16,  2.50it/s][A
 15%|█▍        | 33/221 [00:21<01:00,  3.13it/s][A
 16%|█▌        | 35/221 [00:21<00:44,  4.22it/s][A
 16%|█▋        | 36/221 [00:21<00:43,  4.24it/s][A
 17%|█▋        | 37/221 [00:22<00:57,  3.20it/s][A
 17%|█▋        | 38/221 [00:22<01:05,  2.78it/s][A
 18%|█▊        | 39/221 [00:23<01:16,  2.37it/s][A
 18%|█▊        | 40/221 [00:23<01:18,  2.32it/s][A
 19%|█▊        | 41/221 [00:23<01:06,  2.72it/s][A
 19%|█▉        | 42/221 [00:24<00:56,  3.14it/s][A
 20%|█▉        | 44/221 [00:24<00:38,  4.64it/s][A
 20%|██        | 45/221 [00:27<02:27,  1.19it/s][A
 21%|██        | 46/221 [00:27<02:00,  1.45it/s][A
 21%|██▏       | 47/221 [00:27<01:53,  1.54it/s][A
 22%|██▏       | 48/221 [00:28<01:28,  1.96it/s][A
 22%|██▏       | 49/221 [00:28<01:20,  2.14it/s][A
 23%|██▎       | 50/221 [00:28<01:10,  2.44it/s][A
 23%|██▎       | 51/221 [00:28<01:02,  2.74it/s][A
 24%|██▎       | 52/221 [00:29<00:55,  3.02it/s][A
 24%|██▍       | 53/221 [00:29<00:55,  3.00it/s][A
 24%|██▍       | 54/221 [00:30<01:11,  2.32it/s][A
 25%|██▍       | 55/221 [00:31<01:30,  1.84it/s][A
 25%|██▌       | 56/221 [00:31<01:11,  2.32it/s][A
 26%|██▌       | 57/221 [00:31<01:06,  2.46it/s][A
 27%|██▋       | 59/221 [00:31<00:43,  3.70it/s][A
 27%|██▋       | 60/221 [00:32<00:48,  3.33it/s][A
 28%|██▊       | 61/221 [00:32<00:43,  3.64it/s][A
 28%|██▊       | 62/221 [00:32<00:44,  3.55it/s][A
 29%|██▊       | 63/221 [00:32<00:39,  3.97it/s][A
 29%|██▉       | 64/221 [00:33<01:15,  2.08it/s][A
 29%|██▉       | 65/221 [00:34<00:59,  2.61it/s][A
 30%|██▉       | 66/221 [00:34<01:05,  2.38it/s][A
 30%|███       | 67/221 [00:34<00:57,  2.70it/s][A
 31%|███       | 68/221 [00:35<00:50,  3.04it/s][A
 31%|███       | 69/221 [00:36<01:57,  1.30it/s][A
 32%|███▏      | 70/221 [00:36<01:28,  1.72it/s][A
 32%|███▏      | 71/221 [00:37<01:16,  1.95it/s][A
 33%|███▎      | 72/221 [00:37<01:11,  2.09it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.88it/s][A
 33%|███▎      | 74/221 [00:38<00:59,  2.46it/s][A
 34%|███▍      | 75/221 [00:38<00:54,  2.69it/s][A
 34%|███▍      | 76/221 [00:39<00:56,  2.56it/s][A
 35%|███▍      | 77/221 [00:41<02:28,  1.03s/it][A
 35%|███▌      | 78/221 [00:41<01:49,  1.31it/s][A
 36%|███▌      | 79/221 [00:42<01:58,  1.20it/s][A
 37%|███▋      | 81/221 [00:43<01:28,  1.58it/s][A
 37%|███▋      | 82/221 [00:48<03:47,  1.63s/it][A
 38%|███▊      | 83/221 [00:48<03:06,  1.35s/it][A
 38%|███▊      | 84/221 [00:49<02:24,  1.05s/it][A
 39%|███▉      | 86/221 [00:49<01:32,  1.46it/s][A
 39%|███▉      | 87/221 [00:50<01:24,  1.59it/s][A
 40%|███▉      | 88/221 [00:50<01:12,  1.84it/s][A
 40%|████      | 89/221 [00:50<01:03,  2.07it/s][A
 41%|████      | 90/221 [00:50<00:55,  2.38it/s][A
 41%|████      | 91/221 [00:51<00:45,  2.85it/s][A
 42%|████▏     | 92/221 [00:51<00:39,  3.26it/s][A
 42%|████▏     | 93/221 [00:51<00:41,  3.09it/s][A
 43%|████▎     | 94/221 [00:52<00:58,  2.17it/s][A
 43%|████▎     | 95/221 [00:52<00:52,  2.40it/s][A
 43%|████▎     | 96/221 [00:53<01:07,  1.85it/s][A
 44%|████▍     | 97/221 [00:53<00:56,  2.19it/s][A
 44%|████▍     | 98/221 [00:54<01:04,  1.90it/s][A
 45%|████▍     | 99/221 [00:54<00:59,  2.04it/s][A
 45%|████▌     | 100/221 [00:56<01:26,  1.40it/s][A
 46%|████▌     | 101/221 [00:56<01:05,  1.82it/s][A
 46%|████▌     | 102/221 [00:57<01:13,  1.61it/s][A
 47%|████▋     | 103/221 [00:57<00:54,  2.15it/s][A
 47%|████▋     | 104/221 [00:57<00:49,  2.37it/s][A
 48%|████▊     | 105/221 [00:58<00:54,  2.14it/s][A
 48%|████▊     | 106/221 [00:59<01:36,  1.19it/s][A
 48%|████▊     | 107/221 [01:00<01:22,  1.37it/s][A
 49%|████▉     | 108/221 [01:00<01:14,  1.52it/s][A
 49%|████▉     | 109/221 [01:01<01:03,  1.75it/s][A
 50%|████▉     | 110/221 [01:01<00:51,  2.16it/s][A
 50%|█████     | 111/221 [01:01<00:47,  2.32it/s][A
 51%|█████     | 112/221 [01:02<00:44,  2.46it/s][A
 51%|█████     | 113/221 [01:02<00:37,  2.92it/s][A
 52%|█████▏    | 115/221 [01:02<00:33,  3.17it/s][A
 52%|█████▏    | 116/221 [01:03<00:34,  3.07it/s][A
 53%|█████▎    | 117/221 [01:03<00:39,  2.66it/s][A
 53%|█████▎    | 118/221 [01:04<00:39,  2.61it/s][A
 54%|█████▍    | 119/221 [01:04<00:35,  2.85it/s][A
 55%|█████▍    | 121/221 [01:04<00:30,  3.24it/s][A
 55%|█████▌    | 122/221 [01:05<00:33,  2.95it/s][A
 56%|█████▌    | 123/221 [01:06<00:52,  1.88it/s][A
 56%|█████▌    | 124/221 [01:06<00:44,  2.19it/s][A
 57%|█████▋    | 125/221 [01:07<00:49,  1.95it/s][A
 57%|█████▋    | 126/221 [01:13<03:21,  2.12s/it][A
 57%|█████▋    | 127/221 [01:13<02:33,  1.64s/it][A
 58%|█████▊    | 128/221 [01:14<02:00,  1.29s/it][A
 58%|█████▊    | 129/221 [01:15<01:41,  1.10s/it][A
 59%|█████▉    | 130/221 [01:15<01:16,  1.19it/s][A
 59%|█████▉    | 131/221 [01:16<01:28,  1.02it/s][A
 60%|█████▉    | 132/221 [01:18<01:40,  1.12s/it][A
 60%|██████    | 133/221 [01:18<01:21,  1.08it/s][A
 61%|██████    | 134/221 [01:19<01:22,  1.06it/s][A
 61%|██████    | 135/221 [01:20<01:12,  1.18it/s][A
 62%|██████▏   | 136/221 [01:20<01:00,  1.41it/s][A
 62%|██████▏   | 137/221 [01:20<00:50,  1.65it/s][A
 62%|██████▏   | 138/221 [01:21<00:47,  1.75it/s][A
 63%|██████▎   | 139/221 [01:21<00:38,  2.10it/s][A
 63%|██████▎   | 140/221 [01:22<00:42,  1.91it/s][A
 64%|██████▍   | 141/221 [01:22<00:37,  2.14it/s][A
 64%|██████▍   | 142/221 [01:22<00:34,  2.32it/s][A
 65%|██████▍   | 143/221 [01:23<00:33,  2.31it/s][A
 65%|██████▌   | 144/221 [01:23<00:27,  2.77it/s][A
 66%|██████▌   | 146/221 [01:23<00:18,  4.13it/s][A
 67%|██████▋   | 148/221 [01:24<00:28,  2.54it/s][A
 67%|██████▋   | 149/221 [01:25<00:27,  2.66it/s][A
 68%|██████▊   | 150/221 [01:25<00:28,  2.50it/s][A
 68%|██████▊   | 151/221 [01:25<00:24,  2.88it/s][A
 69%|██████▉   | 152/221 [01:26<00:23,  2.95it/s][A
 69%|██████▉   | 153/221 [01:26<00:19,  3.52it/s][A
 70%|██████▉   | 154/221 [01:26<00:18,  3.58it/s][A
 70%|███████   | 155/221 [01:26<00:16,  4.01it/s][A
 71%|███████   | 156/221 [01:27<00:17,  3.78it/s][A
 71%|███████   | 157/221 [01:30<01:16,  1.20s/it][A
 71%|███████▏  | 158/221 [01:31<01:09,  1.10s/it][A
 72%|███████▏  | 160/221 [01:31<00:41,  1.49it/s][A
 73%|███████▎  | 161/221 [01:31<00:32,  1.85it/s][A
 74%|███████▍  | 163/221 [01:32<00:21,  2.66it/s][A
 74%|███████▍  | 164/221 [01:32<00:19,  2.99it/s][A
 75%|███████▍  | 165/221 [01:32<00:20,  2.74it/s][A
 75%|███████▌  | 166/221 [01:33<00:23,  2.36it/s][A
 76%|███████▌  | 167/221 [01:33<00:20,  2.68it/s][A
 76%|███████▌  | 168/221 [01:37<01:11,  1.34s/it][A
 76%|███████▋  | 169/221 [01:38<00:59,  1.15s/it][A
 77%|███████▋  | 170/221 [01:38<00:48,  1.06it/s][A
 77%|███████▋  | 171/221 [01:39<00:38,  1.31it/s][A
 78%|███████▊  | 172/221 [01:39<00:30,  1.58it/s][A
 78%|███████▊  | 173/221 [01:39<00:26,  1.80it/s][A
 79%|███████▊  | 174/221 [01:39<00:20,  2.29it/s][A
 79%|███████▉  | 175/221 [01:40<00:18,  2.44it/s][A
 80%|███████▉  | 176/221 [01:40<00:19,  2.28it/s][A
 80%|████████  | 177/221 [01:41<00:17,  2.44it/s][A
 81%|████████  | 178/221 [01:41<00:17,  2.46it/s][A
 81%|████████  | 179/221 [01:41<00:15,  2.67it/s][A
 81%|████████▏ | 180/221 [01:41<00:13,  3.11it/s][A
 82%|████████▏ | 181/221 [01:42<00:10,  3.71it/s][A
 82%|████████▏ | 182/221 [01:42<00:12,  3.07it/s][A
 83%|████████▎ | 183/221 [01:43<00:16,  2.29it/s][A
 83%|████████▎ | 184/221 [01:43<00:16,  2.31it/s][A
 84%|████████▎ | 185/221 [01:44<00:14,  2.44it/s][A
 84%|████████▍ | 186/221 [01:44<00:14,  2.47it/s][A
 85%|████████▍ | 187/221 [01:44<00:12,  2.68it/s][A
 85%|████████▌ | 188/221 [01:44<00:10,  3.19it/s][A
 86%|████████▌ | 189/221 [01:45<00:09,  3.23it/s][A
 86%|████████▌ | 190/221 [01:45<00:10,  2.99it/s][A
 86%|████████▋ | 191/221 [01:45<00:09,  3.27it/s][A
 87%|████████▋ | 192/221 [01:46<00:12,  2.25it/s][A
 87%|████████▋ | 193/221 [01:46<00:10,  2.77it/s][A
 88%|████████▊ | 194/221 [01:47<00:11,  2.31it/s][A
 88%|████████▊ | 195/221 [01:47<00:09,  2.67it/s][A
 89%|████████▊ | 196/221 [01:47<00:08,  3.09it/s][A
 89%|████████▉ | 197/221 [01:48<00:07,  3.38it/s][A
 90%|████████▉ | 198/221 [01:48<00:06,  3.69it/s][A
 90%|█████████ | 199/221 [01:48<00:05,  3.95it/s][A
 90%|█████████ | 200/221 [01:49<00:07,  2.89it/s][A
 91%|█████████ | 201/221 [01:49<00:10,  1.91it/s][A
 91%|█████████▏| 202/221 [01:50<00:08,  2.19it/s][A
 92%|█████████▏| 203/221 [01:51<00:13,  1.30it/s][A
 92%|█████████▏| 204/221 [01:52<00:10,  1.62it/s][A
 93%|█████████▎| 206/221 [01:52<00:07,  2.00it/s][A
 94%|█████████▍| 208/221 [01:53<00:05,  2.49it/s][A
 95%|█████████▌| 210/221 [01:53<00:03,  3.40it/s][A
 95%|█████████▌| 211/221 [01:54<00:03,  2.88it/s][A
 96%|█████████▌| 212/221 [01:54<00:03,  2.97it/s][A
 96%|█████████▋| 213/221 [01:54<00:02,  3.47it/s][A
 97%|█████████▋| 214/221 [01:54<00:02,  3.06it/s][A
 97%|█████████▋| 215/221 [01:55<00:02,  2.83it/s][A
 98%|█████████▊| 216/221 [01:55<00:01,  2.86it/s][A
 98%|█████████▊| 217/221 [01:57<00:03,  1.19it/s][A
 99%|█████████▊| 218/221 [01:58<00:02,  1.43it/s][A
 99%|█████████▉| 219/221 [01:58<00:01,  1.67it/s][A
100%|█████████▉| 220/221 [01:59<00:00,  1.37it/s][A
100%|██████████| 221/221 [01:59<00:00,  1.77it/s][A100%|██████████| 221/221 [01:59<00:00,  1.85it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:32,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:05,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:32,  6.74it/s][A
  1%|          | 2/221 [00:01<02:34,  1.42it/s][A
  1%|▏         | 3/221 [00:01<01:46,  2.05it/s][A
  2%|▏         | 4/221 [00:02<01:59,  1.81it/s][A
  2%|▏         | 5/221 [00:02<02:10,  1.65it/s][A
  3%|▎         | 6/221 [00:03<01:49,  1.97it/s][A
  3%|▎         | 7/221 [00:03<01:44,  2.04it/s][A
  4%|▎         | 8/221 [00:04<02:09,  1.64it/s][A
  4%|▍         | 9/221 [00:05<02:17,  1.54it/s][A
  5%|▍         | 10/221 [00:05<02:08,  1.65it/s][A
  5%|▍         | 11/221 [00:06<01:49,  1.93it/s][A
  5%|▌         | 12/221 [00:06<01:39,  2.11it/s][A
  6%|▌         | 13/221 [00:06<01:20,  2.58it/s][A
  6%|▋         | 14/221 [00:06<01:02,  3.31it/s][A
  7%|▋         | 15/221 [00:06<01:00,  3.43it/s][A
  7%|▋         | 16/221 [00:07<01:30,  2.27it/s][A
  8%|▊         | 17/221 [00:08<02:19,  1.46it/s][A
  8%|▊         | 18/221 [00:09<01:50,  1.84it/s][A
  9%|▊         | 19/221 [00:09<02:03,  1.63it/s][A
  9%|▉         | 20/221 [00:10<01:42,  1.96it/s][A
 10%|▉         | 21/221 [00:10<01:39,  2.00it/s][A
 10%|▉         | 22/221 [00:11<02:15,  1.47it/s][A
 10%|█         | 23/221 [00:12<01:51,  1.78it/s][A
 11%|█         | 24/221 [00:12<01:40,  1.96it/s][A
 11%|█▏        | 25/221 [00:13<02:04,  1.57it/s][A
 12%|█▏        | 26/221 [00:14<02:17,  1.42it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:15<02:40,  1.20it/s][A
 13%|█▎        | 29/221 [00:16<02:05,  1.52it/s][A
 14%|█▎        | 30/221 [00:16<01:59,  1.60it/s][A
 14%|█▍        | 31/221 [00:17<01:47,  1.76it/s][A
 14%|█▍        | 32/221 [00:17<01:59,  1.58it/s][A
 15%|█▍        | 33/221 [00:19<02:36,  1.21it/s][A
 15%|█▌        | 34/221 [00:19<02:01,  1.54it/s][A
 16%|█▋        | 36/221 [00:20<01:33,  1.98it/s][A
 17%|█▋        | 37/221 [00:20<01:29,  2.06it/s][A
 17%|█▋        | 38/221 [00:21<01:52,  1.62it/s][A
 18%|█▊        | 39/221 [00:22<02:00,  1.51it/s][A
 18%|█▊        | 40/221 [00:23<02:14,  1.35it/s][A
 19%|█▊        | 41/221 [00:23<01:51,  1.62it/s][A
 19%|█▉        | 42/221 [00:23<01:27,  2.04it/s][A
 19%|█▉        | 43/221 [00:24<01:35,  1.86it/s][A
 20%|█▉        | 44/221 [00:24<01:27,  2.01it/s][A
 21%|██        | 46/221 [00:25<01:01,  2.82it/s][A
 21%|██▏       | 47/221 [00:25<01:12,  2.39it/s][A
 22%|██▏       | 48/221 [00:25<00:59,  2.91it/s][A
 22%|██▏       | 49/221 [00:27<01:47,  1.61it/s][A
 23%|██▎       | 50/221 [00:27<01:40,  1.70it/s][A
 23%|██▎       | 51/221 [00:28<01:28,  1.93it/s][A
 24%|██▎       | 52/221 [00:28<01:17,  2.17it/s][A
 24%|██▍       | 53/221 [00:28<01:11,  2.35it/s][A
 24%|██▍       | 54/221 [00:29<01:12,  2.29it/s][A
 25%|██▍       | 55/221 [00:29<00:58,  2.83it/s][A
 25%|██▌       | 56/221 [00:30<01:16,  2.17it/s][A
 26%|██▌       | 57/221 [00:30<01:07,  2.42it/s][A
 26%|██▌       | 58/221 [00:30<00:58,  2.76it/s][A
 27%|██▋       | 59/221 [00:30<00:47,  3.42it/s][A
 27%|██▋       | 60/221 [00:31<00:58,  2.76it/s][A
 28%|██▊       | 61/221 [00:32<01:25,  1.87it/s][A
 28%|██▊       | 62/221 [00:32<01:18,  2.03it/s][A
 29%|██▊       | 63/221 [00:33<01:18,  2.01it/s][A
 29%|██▉       | 64/221 [00:34<02:06,  1.24it/s][A
 29%|██▉       | 65/221 [00:35<02:15,  1.15it/s][A
 30%|██▉       | 66/221 [00:36<02:00,  1.29it/s][A
 30%|███       | 67/221 [00:36<01:30,  1.71it/s][A
 31%|███       | 68/221 [00:36<01:10,  2.18it/s][A
 31%|███       | 69/221 [00:36<00:53,  2.82it/s][A
 32%|███▏      | 70/221 [00:37<01:05,  2.31it/s][A
 32%|███▏      | 71/221 [00:38<01:16,  1.97it/s][A
 33%|███▎      | 72/221 [00:38<01:16,  1.96it/s][A
 33%|███▎      | 73/221 [00:39<01:27,  1.68it/s][A
 33%|███▎      | 74/221 [00:39<01:14,  1.97it/s][A
 34%|███▍      | 75/221 [00:39<01:02,  2.32it/s][A
 34%|███▍      | 76/221 [00:40<01:03,  2.28it/s][A
 35%|███▍      | 77/221 [00:40<00:56,  2.54it/s][A
 35%|███▌      | 78/221 [00:41<01:08,  2.10it/s][A
 36%|███▌      | 79/221 [00:42<01:33,  1.51it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.90it/s][A
 37%|███▋      | 81/221 [00:43<01:11,  1.96it/s][A
 37%|███▋      | 82/221 [00:44<01:35,  1.45it/s][A
 38%|███▊      | 83/221 [00:44<01:19,  1.74it/s][A
 38%|███▊      | 84/221 [00:45<01:40,  1.37it/s][A
 38%|███▊      | 85/221 [00:45<01:14,  1.84it/s][A
 39%|███▉      | 86/221 [00:46<01:08,  1.97it/s][A
 39%|███▉      | 87/221 [00:46<01:04,  2.07it/s][A
 40%|███▉      | 88/221 [00:46<00:57,  2.32it/s][A
 40%|████      | 89/221 [00:47<01:08,  1.93it/s][A
 41%|████      | 90/221 [00:48<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<00:57,  2.28it/s][A
 42%|████▏     | 92/221 [00:48<01:01,  2.10it/s][A
 43%|████▎     | 94/221 [00:49<00:40,  3.14it/s][A
 43%|████▎     | 95/221 [00:50<01:00,  2.10it/s][A
 43%|████▎     | 96/221 [00:50<01:02,  2.00it/s][A
 44%|████▍     | 97/221 [00:51<01:09,  1.79it/s][A
 44%|████▍     | 98/221 [00:52<01:14,  1.66it/s][A
 45%|████▍     | 99/221 [00:52<01:17,  1.57it/s][A
 45%|████▌     | 100/221 [00:53<01:20,  1.51it/s][A
 46%|████▌     | 101/221 [00:54<01:26,  1.38it/s][A
 46%|████▌     | 102/221 [00:55<01:44,  1.14it/s][A
 47%|████▋     | 103/221 [00:55<01:19,  1.48it/s][A
 47%|████▋     | 104/221 [00:56<01:20,  1.45it/s][A
 48%|████▊     | 105/221 [00:56<01:08,  1.69it/s][A
 48%|████▊     | 106/221 [00:57<00:58,  1.96it/s][A
 48%|████▊     | 107/221 [00:57<00:54,  2.08it/s][A
 49%|████▉     | 108/221 [00:58<00:57,  1.98it/s][A
 49%|████▉     | 109/221 [00:58<00:52,  2.15it/s][A
 50%|████▉     | 110/221 [00:59<00:47,  2.33it/s][A
 50%|█████     | 111/221 [01:00<01:11,  1.55it/s][A
 51%|█████     | 112/221 [01:00<01:08,  1.60it/s][A
 51%|█████     | 113/221 [01:01<00:56,  1.91it/s][A
 52%|█████▏    | 114/221 [01:01<00:47,  2.25it/s][A
 52%|█████▏    | 115/221 [01:01<00:51,  2.04it/s][A
 52%|█████▏    | 116/221 [01:02<00:48,  2.17it/s][A
 53%|█████▎    | 117/221 [01:02<00:42,  2.42it/s][A
 53%|█████▎    | 118/221 [01:03<00:54,  1.88it/s][A
 54%|█████▍    | 119/221 [01:03<00:51,  1.96it/s][A
 54%|█████▍    | 120/221 [01:04<00:49,  2.03it/s][A
 55%|█████▍    | 121/221 [01:04<00:52,  1.91it/s][A
 55%|█████▌    | 122/221 [01:05<00:51,  1.91it/s][A
 56%|█████▌    | 123/221 [01:05<00:48,  2.03it/s][A
 56%|█████▌    | 124/221 [01:06<00:55,  1.75it/s][A
 57%|█████▋    | 125/221 [01:07<00:51,  1.87it/s][A
 57%|█████▋    | 126/221 [01:07<00:39,  2.38it/s][A
 57%|█████▋    | 127/221 [01:08<00:52,  1.78it/s][A
 58%|█████▊    | 128/221 [01:08<00:55,  1.67it/s][A
 58%|█████▊    | 129/221 [01:09<00:50,  1.83it/s][A
 59%|█████▉    | 130/221 [01:09<00:39,  2.30it/s][A
 59%|█████▉    | 131/221 [01:09<00:39,  2.28it/s][A
 60%|█████▉    | 132/221 [01:10<00:50,  1.75it/s][A
 60%|██████    | 133/221 [01:11<00:57,  1.54it/s][A
 61%|██████    | 134/221 [01:11<00:47,  1.85it/s][A
 61%|██████    | 135/221 [01:12<00:45,  1.87it/s][A
 62%|██████▏   | 136/221 [01:13<00:51,  1.65it/s][A
 62%|██████▏   | 137/221 [01:13<00:48,  1.73it/s][A
 62%|██████▏   | 138/221 [01:14<00:52,  1.58it/s][A
 63%|██████▎   | 139/221 [01:15<01:10,  1.16it/s][A
 63%|██████▎   | 140/221 [01:16<01:04,  1.25it/s][A
 64%|██████▍   | 141/221 [01:16<00:52,  1.52it/s][A
 64%|██████▍   | 142/221 [01:17<00:49,  1.60it/s][A
 65%|██████▍   | 143/221 [01:17<00:38,  2.05it/s][A
 65%|██████▌   | 144/221 [01:17<00:33,  2.29it/s][A
 66%|██████▌   | 145/221 [01:18<00:40,  1.87it/s][A
 66%|██████▌   | 146/221 [01:18<00:32,  2.31it/s][A
 67%|██████▋   | 147/221 [01:19<00:29,  2.51it/s][A
 67%|██████▋   | 148/221 [01:19<00:29,  2.47it/s][A
 68%|██████▊   | 150/221 [01:19<00:22,  3.22it/s][A
 69%|██████▉   | 152/221 [01:20<00:22,  3.03it/s][A
 69%|██████▉   | 153/221 [01:20<00:22,  3.04it/s][A
 70%|██████▉   | 154/221 [01:21<00:30,  2.23it/s][A
 70%|███████   | 155/221 [01:22<00:36,  1.79it/s][A
 71%|███████   | 156/221 [01:23<00:36,  1.79it/s][A
 71%|███████   | 157/221 [01:23<00:34,  1.87it/s][A
 71%|███████▏  | 158/221 [01:23<00:28,  2.19it/s][A
 72%|███████▏  | 160/221 [01:24<00:23,  2.57it/s][A
 73%|███████▎  | 161/221 [01:24<00:19,  3.07it/s][A
 73%|███████▎  | 162/221 [01:25<00:20,  2.90it/s][A
 74%|███████▍  | 163/221 [01:25<00:18,  3.18it/s][A
 74%|███████▍  | 164/221 [01:25<00:17,  3.22it/s][A
 75%|███████▍  | 165/221 [01:25<00:15,  3.57it/s][A
 75%|███████▌  | 166/221 [01:26<00:21,  2.53it/s][A
 76%|███████▌  | 167/221 [01:26<00:19,  2.84it/s][A
 76%|███████▌  | 168/221 [01:27<00:18,  2.94it/s][A
 76%|███████▋  | 169/221 [01:28<00:31,  1.67it/s][A
 77%|███████▋  | 170/221 [01:28<00:32,  1.58it/s][A
 77%|███████▋  | 171/221 [01:29<00:30,  1.66it/s][A
 78%|███████▊  | 172/221 [01:29<00:24,  2.03it/s][A
 78%|███████▊  | 173/221 [01:30<00:21,  2.23it/s][A
 79%|███████▊  | 174/221 [01:30<00:21,  2.21it/s][A
 79%|███████▉  | 175/221 [01:31<00:24,  1.86it/s][A
 80%|███████▉  | 176/221 [01:31<00:22,  1.97it/s][A
 80%|████████  | 177/221 [01:32<00:21,  2.01it/s][A
 81%|████████  | 178/221 [01:32<00:22,  1.92it/s][A
 81%|████████  | 179/221 [01:33<00:23,  1.82it/s][A
 81%|████████▏ | 180/221 [01:33<00:21,  1.91it/s][A
 82%|████████▏ | 181/221 [01:34<00:17,  2.32it/s][A
 82%|████████▏ | 182/221 [01:34<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:35<00:18,  2.00it/s][A
 83%|████████▎ | 184/221 [01:36<00:28,  1.29it/s][A
 84%|████████▎ | 185/221 [01:37<00:28,  1.24it/s][A
 84%|████████▍ | 186/221 [01:37<00:20,  1.68it/s][A
 85%|████████▍ | 187/221 [01:38<00:18,  1.79it/s][A
 85%|████████▌ | 188/221 [01:38<00:14,  2.29it/s][A
 86%|████████▌ | 189/221 [01:38<00:13,  2.30it/s][A
 86%|████████▌ | 190/221 [01:39<00:14,  2.16it/s][A
 86%|████████▋ | 191/221 [01:39<00:15,  1.95it/s][A
 87%|████████▋ | 192/221 [01:40<00:13,  2.20it/s][A
 87%|████████▋ | 193/221 [01:40<00:11,  2.51it/s][A
 88%|████████▊ | 194/221 [01:41<00:13,  2.05it/s][A
 88%|████████▊ | 195/221 [01:42<00:20,  1.27it/s][A
 89%|████████▊ | 196/221 [01:43<00:18,  1.32it/s][A
 89%|████████▉ | 197/221 [01:43<00:16,  1.42it/s][A
 90%|████████▉ | 198/221 [01:44<00:14,  1.62it/s][A
 90%|█████████ | 199/221 [01:44<00:12,  1.82it/s][A
 90%|█████████ | 200/221 [01:45<00:13,  1.56it/s][A
 91%|█████████ | 201/221 [01:45<00:11,  1.73it/s][A
 91%|█████████▏| 202/221 [01:46<00:09,  1.95it/s][A
 92%|█████████▏| 203/221 [01:46<00:09,  1.84it/s][A
 92%|█████████▏| 204/221 [01:47<00:09,  1.88it/s][A
 93%|█████████▎| 205/221 [01:47<00:07,  2.26it/s][A
 93%|█████████▎| 206/221 [01:48<00:06,  2.47it/s][A
 94%|█████████▎| 207/221 [01:48<00:05,  2.48it/s][A
 94%|█████████▍| 208/221 [01:49<00:08,  1.50it/s][A
 95%|█████████▍| 209/221 [01:50<00:07,  1.63it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.96it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.74it/s][A
 96%|█████████▌| 212/221 [01:51<00:04,  2.12it/s][A
 96%|█████████▋| 213/221 [01:51<00:03,  2.02it/s][A
 97%|█████████▋| 214/221 [01:52<00:03,  1.76it/s][A
 97%|█████████▋| 215/221 [01:52<00:02,  2.29it/s][A
 98%|█████████▊| 216/221 [01:53<00:02,  1.95it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.58it/s][A
 99%|█████████▊| 218/221 [01:54<00:01,  1.94it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.86it/s][A
100%|█████████▉| 220/221 [01:55<00:00,  1.99it/s][A
100%|██████████| 221/221 [01:56<00:00,  2.13it/s][A100%|██████████| 221/221 [01:56<00:00,  1.90it/s]
09/19/2024 09:00:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 4999--===========

09/19/2024 09:00:18 - INFO - __main__ -   {'area_r1': 45.2, 'area_recall': '45.2/74.5/83.4', 'area_ravg': 67.7}
09/19/2024 09:00:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 4999--===========

09/19/2024 09:00:18 - INFO - __main__ -   {'forward_r1': 49.9, 'forward_recall': '49.9/78.6/87.7', 'forward_ravg': 72.1}
09/19/2024 09:00:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 4999--===========

09/19/2024 09:00:18 - INFO - __main__ -   {'area_video_r1': 48.8, 'area_video_recall': '48.8/78.6/87.4', 'area_video_ravg': 71.6}
09/19/2024 09:00:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 09:00:18 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 09:00:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 4999--===========

09/19/2024 09:00:18 - INFO - __main__ -   {'area_video_r1': 62.9, 'area_video_recall': '62.9/83.7/89.5', 'area_video_ravg': 78.7, 'area_video_back_r1': 63.6, 'area_video_back_recall': '63.6/85.5/91.4', 'area_video_back_ravg': 80.2}
09/19/2024 09:00:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 4999=======

09/19/2024 09:00:18 - INFO - __main__ -   {'area_video_r1': 62.9, 'area_video_recall': '62.9/83.7/89.5', 'area_video_ravg': 78.7, 'area_video_back_r1': 63.6, 'area_video_back_recall': '63.6/85.5/91.4', 'area_video_back_ravg': 80.2}
09/19/2024 09:00:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 4999--===========

09/19/2024 09:00:18 - INFO - __main__ -   {'video_r1': 31.9, 'video_recall': '31.9/55.7/68.0', 'video_ravg': 51.8}
09/19/2024 09:00:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 09:00:18 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 09:00:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 4999--===========

09/19/2024 09:00:18 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/80.4/84.7', 'video_ravg': 75.1}
09/19/2024 09:00:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 4999=======

09/19/2024 09:00:18 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/80.4/84.7', 'video_ravg': 75.1}
09/19/2024 09:00:48 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.012764778919517994, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0994172096252441, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1121820211410522}
 56%|█████▌    | 5000/8917 [6:46:14<186:34:54, 171.48s/it] 56%|█████▌    | 5001/8917 [6:46:18<131:42:42, 121.08s/it] 56%|█████▌    | 5002/8917 [6:46:21<93:22:30, 85.86s/it]   56%|█████▌    | 5003/8917 [6:46:25<66:29:53, 61.16s/it] 56%|█████▌    | 5004/8917 [6:46:29<47:44:35, 43.92s/it] 56%|█████▌    | 5005/8917 [6:46:32<34:37:53, 31.87s/it] 56%|█████▌    | 5006/8917 [6:46:36<25:27:09, 23.43s/it] 56%|█████▌    | 5007/8917 [6:46:40<19:04:13, 17.56s/it] 56%|█████▌    | 5008/8917 [6:46:44<14:34:20, 13.42s/it] 56%|█████▌    | 5009/8917 [6:46:48<11:28:14, 10.57s/it] 56%|█████▌    | 5010/8917 [6:46:51<9:15:46,  8.54s/it]  56%|█████▌    | 5011/8917 [6:46:55<7:42:25,  7.10s/it] 56%|█████▌    | 5012/8917 [6:46:59<6:40:55,  6.16s/it] 56%|█████▌    | 5013/8917 [6:47:03<5:57:21,  5.49s/it] 56%|█████▌    | 5014/8917 [6:47:07<5:20:52,  4.93s/it] 56%|█████▌    | 5015/8917 [6:47:10<4:54:38,  4.53s/it] 56%|█████▋    | 5016/8917 [6:47:14<4:39:58,  4.31s/it] 56%|█████▋    | 5017/8917 [6:47:18<4:25:27,  4.08s/it] 56%|█████▋    | 5018/8917 [6:47:22<4:26:18,  4.10s/it] 56%|█████▋    | 5019/8917 [6:47:26<4:22:34,  4.04s/it] 56%|█████▋    | 5020/8917 [6:47:29<4:13:31,  3.90s/it] 56%|█████▋    | 5021/8917 [6:47:33<4:09:20,  3.84s/it] 56%|█████▋    | 5022/8917 [6:47:36<4:02:27,  3.73s/it] 56%|█████▋    | 5023/8917 [6:47:40<3:57:26,  3.66s/it] 56%|█████▋    | 5024/8917 [6:47:44<3:57:30,  3.66s/it] 56%|█████▋    | 5025/8917 [6:47:48<4:02:54,  3.74s/it] 56%|█████▋    | 5026/8917 [6:47:51<3:55:17,  3.63s/it] 56%|█████▋    | 5027/8917 [6:47:54<3:51:48,  3.58s/it] 56%|█████▋    | 5028/8917 [6:47:58<3:52:42,  3.59s/it] 56%|█████▋    | 5029/8917 [6:48:02<3:52:42,  3.59s/it] 56%|█████▋    | 5030/8917 [6:48:05<3:54:37,  3.62s/it] 56%|█████▋    | 5031/8917 [6:48:09<3:57:10,  3.66s/it] 56%|█████▋    | 5032/8917 [6:48:13<4:01:53,  3.74s/it] 56%|█████▋    | 5033/8917 [6:48:17<4:03:15,  3.76s/it] 56%|█████▋    | 5034/8917 [6:48:21<4:06:54,  3.82s/it] 56%|█████▋    | 5035/8917 [6:48:25<4:09:57,  3.86s/it] 56%|█████▋    | 5036/8917 [6:48:28<4:05:40,  3.80s/it] 56%|█████▋    | 5037/8917 [6:48:32<4:05:26,  3.80s/it] 56%|█████▋    | 5038/8917 [6:48:36<4:02:46,  3.76s/it] 57%|█████▋    | 5039/8917 [6:48:40<4:06:17,  3.81s/it] 57%|█████▋    | 5040/8917 [6:48:43<4:06:31,  3.82s/it] 57%|█████▋    | 5041/8917 [6:48:47<4:04:07,  3.78s/it] 57%|█████▋    | 5042/8917 [6:48:51<4:01:32,  3.74s/it] 57%|█████▋    | 5043/8917 [6:48:55<4:06:41,  3.82s/it] 57%|█████▋    | 5044/8917 [6:48:59<4:04:31,  3.79s/it] 57%|█████▋    | 5045/8917 [6:49:02<3:53:35,  3.62s/it] 57%|█████▋    | 5046/8917 [6:49:06<3:56:34,  3.67s/it] 57%|█████▋    | 5047/8917 [6:49:10<4:02:51,  3.77s/it] 57%|█████▋    | 5048/8917 [6:49:13<3:58:35,  3.70s/it] 57%|█████▋    | 5049/8917 [6:49:16<3:52:34,  3.61s/it]09/19/2024 09:03:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015749575570225716, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9773918986320496, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9931414723396301}
 57%|█████▋    | 5050/8917 [6:49:20<3:55:31,  3.65s/it] 57%|█████▋    | 5051/8917 [6:49:24<3:53:00,  3.62s/it] 57%|█████▋    | 5052/8917 [6:49:28<3:59:33,  3.72s/it] 57%|█████▋    | 5053/8917 [6:49:32<4:03:21,  3.78s/it] 57%|█████▋    | 5054/8917 [6:49:36<4:06:50,  3.83s/it] 57%|█████▋    | 5055/8917 [6:49:39<4:04:47,  3.80s/it] 57%|█████▋    | 5056/8917 [6:49:43<4:08:26,  3.86s/it] 57%|█████▋    | 5057/8917 [6:49:47<4:09:45,  3.88s/it] 57%|█████▋    | 5058/8917 [6:49:51<4:12:25,  3.92s/it] 57%|█████▋    | 5059/8917 [6:49:55<4:06:48,  3.84s/it] 57%|█████▋    | 5060/8917 [6:49:58<4:01:08,  3.75s/it] 57%|█████▋    | 5061/8917 [6:50:02<3:55:02,  3.66s/it] 57%|█████▋    | 5062/8917 [6:50:06<4:00:26,  3.74s/it] 57%|█████▋    | 5063/8917 [6:50:10<4:00:40,  3.75s/it] 57%|█████▋    | 5064/8917 [6:50:14<4:10:57,  3.91s/it] 57%|█████▋    | 5065/8917 [6:50:17<4:01:31,  3.76s/it] 57%|█████▋    | 5066/8917 [6:50:21<3:57:12,  3.70s/it] 57%|█████▋    | 5067/8917 [6:50:25<3:59:01,  3.73s/it] 57%|█████▋    | 5068/8917 [6:50:28<3:57:07,  3.70s/it] 57%|█████▋    | 5069/8917 [6:50:32<4:02:09,  3.78s/it] 57%|█████▋    | 5070/8917 [6:50:36<4:05:40,  3.83s/it] 57%|█████▋    | 5071/8917 [6:50:40<4:00:38,  3.75s/it] 57%|█████▋    | 5072/8917 [6:50:43<3:58:57,  3.73s/it] 57%|█████▋    | 5073/8917 [6:50:47<3:59:00,  3.73s/it] 57%|█████▋    | 5074/8917 [6:50:51<3:57:02,  3.70s/it] 57%|█████▋    | 5075/8917 [6:50:54<3:56:07,  3.69s/it] 57%|█████▋    | 5076/8917 [6:50:58<3:59:12,  3.74s/it] 57%|█████▋    | 5077/8917 [6:51:02<3:54:23,  3.66s/it] 57%|█████▋    | 5078/8917 [6:51:06<3:55:34,  3.68s/it] 57%|█████▋    | 5079/8917 [6:51:09<3:53:01,  3.64s/it] 57%|█████▋    | 5080/8917 [6:51:13<3:51:53,  3.63s/it] 57%|█████▋    | 5081/8917 [6:51:17<3:56:34,  3.70s/it] 57%|█████▋    | 5082/8917 [6:51:21<4:02:30,  3.79s/it] 57%|█████▋    | 5083/8917 [6:51:24<4:00:55,  3.77s/it] 57%|█████▋    | 5084/8917 [6:51:28<3:56:33,  3.70s/it] 57%|█████▋    | 5085/8917 [6:51:31<3:53:04,  3.65s/it] 57%|█████▋    | 5086/8917 [6:51:36<4:03:05,  3.81s/it] 57%|█████▋    | 5087/8917 [6:51:39<3:57:44,  3.72s/it] 57%|█████▋    | 5088/8917 [6:51:43<4:04:16,  3.83s/it] 57%|█████▋    | 5089/8917 [6:51:47<4:05:24,  3.85s/it] 57%|█████▋    | 5090/8917 [6:51:50<3:52:45,  3.65s/it] 57%|█████▋    | 5091/8917 [6:51:54<3:58:42,  3.74s/it] 57%|█████▋    | 5092/8917 [6:51:58<4:03:04,  3.81s/it] 57%|█████▋    | 5093/8917 [6:52:02<4:03:59,  3.83s/it] 57%|█████▋    | 5094/8917 [6:52:06<4:00:27,  3.77s/it] 57%|█████▋    | 5095/8917 [6:52:09<3:56:51,  3.72s/it] 57%|█████▋    | 5096/8917 [6:52:13<4:01:34,  3.79s/it] 57%|█████▋    | 5097/8917 [6:52:17<3:56:10,  3.71s/it] 57%|█████▋    | 5098/8917 [6:52:20<3:55:01,  3.69s/it] 57%|█████▋    | 5099/8917 [6:52:24<3:57:33,  3.73s/it]09/19/2024 09:07:02 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018505625426769257, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2736519575119019, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.292157530784607}
 57%|█████▋    | 5100/8917 [6:52:28<4:02:34,  3.81s/it] 57%|█████▋    | 5101/8917 [6:52:32<3:54:21,  3.68s/it] 57%|█████▋    | 5102/8917 [6:52:36<4:04:20,  3.84s/it] 57%|█████▋    | 5103/8917 [6:52:40<4:12:52,  3.98s/it] 57%|█████▋    | 5104/8917 [6:52:44<4:03:14,  3.83s/it] 57%|█████▋    | 5105/8917 [6:52:47<3:58:08,  3.75s/it] 57%|█████▋    | 5106/8917 [6:52:51<3:54:33,  3.69s/it] 57%|█████▋    | 5107/8917 [6:52:55<3:58:18,  3.75s/it] 57%|█████▋    | 5108/8917 [6:52:58<3:55:15,  3.71s/it] 57%|█████▋    | 5109/8917 [6:53:02<3:52:27,  3.66s/it] 57%|█████▋    | 5110/8917 [6:53:05<3:49:57,  3.62s/it] 57%|█████▋    | 5111/8917 [6:53:09<3:52:32,  3.67s/it] 57%|█████▋    | 5112/8917 [6:53:13<3:57:28,  3.74s/it] 57%|█████▋    | 5113/8917 [6:53:17<3:54:44,  3.70s/it] 57%|█████▋    | 5114/8917 [6:53:20<3:55:33,  3.72s/it] 57%|█████▋    | 5115/8917 [6:53:24<4:00:18,  3.79s/it] 57%|█████▋    | 5116/8917 [6:53:28<4:01:08,  3.81s/it] 57%|█████▋    | 5117/8917 [6:53:31<3:52:22,  3.67s/it] 57%|█████▋    | 5118/8917 [6:53:36<3:58:43,  3.77s/it] 57%|█████▋    | 5119/8917 [6:53:39<3:56:52,  3.74s/it] 57%|█████▋    | 5120/8917 [6:53:43<3:56:48,  3.74s/it] 57%|█████▋    | 5121/8917 [6:53:47<4:01:30,  3.82s/it] 57%|█████▋    | 5122/8917 [6:53:50<3:52:18,  3.67s/it] 57%|█████▋    | 5123/8917 [6:53:54<3:54:33,  3.71s/it] 57%|█████▋    | 5124/8917 [6:53:58<3:58:15,  3.77s/it] 57%|█████▋    | 5125/8917 [6:54:02<3:58:34,  3.77s/it] 57%|█████▋    | 5126/8917 [6:54:05<3:57:57,  3.77s/it] 57%|█████▋    | 5127/8917 [6:54:09<3:54:23,  3.71s/it] 58%|█████▊    | 5128/8917 [6:54:13<3:52:16,  3.68s/it] 58%|█████▊    | 5129/8917 [6:54:17<3:56:52,  3.75s/it] 58%|█████▊    | 5130/8917 [6:54:21<4:07:43,  3.92s/it] 58%|█████▊    | 5131/8917 [6:54:24<3:59:29,  3.80s/it] 58%|█████▊    | 5132/8917 [6:54:28<3:58:40,  3.78s/it] 58%|█████▊    | 5133/8917 [6:54:32<3:54:46,  3.72s/it] 58%|█████▊    | 5134/8917 [6:54:35<3:53:56,  3.71s/it] 58%|█████▊    | 5135/8917 [6:54:39<3:45:08,  3.57s/it] 58%|█████▊    | 5136/8917 [6:54:42<3:49:24,  3.64s/it] 58%|█████▊    | 5137/8917 [6:54:46<3:49:46,  3.65s/it] 58%|█████▊    | 5138/8917 [6:54:50<3:49:14,  3.64s/it] 58%|█████▊    | 5139/8917 [6:54:53<3:47:51,  3.62s/it] 58%|█████▊    | 5140/8917 [6:54:57<3:55:55,  3.75s/it] 58%|█████▊    | 5141/8917 [6:55:01<3:59:49,  3.81s/it] 58%|█████▊    | 5142/8917 [6:55:05<3:55:07,  3.74s/it] 58%|█████▊    | 5143/8917 [6:55:08<3:49:07,  3.64s/it] 58%|█████▊    | 5144/8917 [6:55:12<3:54:38,  3.73s/it] 58%|█████▊    | 5145/8917 [6:55:16<4:01:48,  3.85s/it] 58%|█████▊    | 5146/8917 [6:55:20<3:58:40,  3.80s/it] 58%|█████▊    | 5147/8917 [6:55:24<3:55:16,  3.74s/it] 58%|█████▊    | 5148/8917 [6:55:27<3:51:00,  3.68s/it] 58%|█████▊    | 5149/8917 [6:55:31<3:47:14,  3.62s/it]09/19/2024 09:10:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.009620319120585918, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0702964067459106, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0799167156219482}
 58%|█████▊    | 5150/8917 [6:55:34<3:43:27,  3.56s/it] 58%|█████▊    | 5151/8917 [6:55:38<3:55:30,  3.75s/it] 58%|█████▊    | 5152/8917 [6:55:42<3:58:58,  3.81s/it] 58%|█████▊    | 5153/8917 [6:55:46<3:54:48,  3.74s/it] 58%|█████▊    | 5154/8917 [6:55:49<3:48:20,  3.64s/it] 58%|█████▊    | 5155/8917 [6:55:53<3:53:09,  3.72s/it] 58%|█████▊    | 5156/8917 [6:55:57<3:48:09,  3.64s/it] 58%|█████▊    | 5157/8917 [6:56:00<3:52:06,  3.70s/it] 58%|█████▊    | 5158/8917 [6:56:04<3:52:20,  3.71s/it] 58%|█████▊    | 5159/8917 [6:56:08<3:47:50,  3.64s/it] 58%|█████▊    | 5160/8917 [6:56:11<3:49:15,  3.66s/it] 58%|█████▊    | 5161/8917 [6:56:15<3:49:43,  3.67s/it] 58%|█████▊    | 5162/8917 [6:56:19<3:51:39,  3.70s/it] 58%|█████▊    | 5163/8917 [6:56:23<3:53:38,  3.73s/it] 58%|█████▊    | 5164/8917 [6:56:27<3:58:17,  3.81s/it] 58%|█████▊    | 5165/8917 [6:56:30<3:58:34,  3.82s/it] 58%|█████▊    | 5166/8917 [6:56:34<3:50:12,  3.68s/it] 58%|█████▊    | 5167/8917 [6:56:37<3:44:42,  3.60s/it] 58%|█████▊    | 5168/8917 [6:56:41<3:52:56,  3.73s/it] 58%|█████▊    | 5169/8917 [6:56:45<3:54:59,  3.76s/it] 58%|█████▊    | 5170/8917 [6:56:49<3:52:18,  3.72s/it] 58%|█████▊    | 5171/8917 [6:56:53<3:56:45,  3.79s/it] 58%|█████▊    | 5172/8917 [6:56:56<3:53:02,  3.73s/it] 58%|█████▊    | 5173/8917 [6:57:00<3:56:25,  3.79s/it] 58%|█████▊    | 5174/8917 [6:57:04<3:49:15,  3.68s/it] 58%|█████▊    | 5175/8917 [6:57:08<3:53:22,  3.74s/it] 58%|█████▊    | 5176/8917 [6:57:11<3:56:27,  3.79s/it] 58%|█████▊    | 5177/8917 [6:57:15<3:56:42,  3.80s/it] 58%|█████▊    | 5178/8917 [6:57:19<3:50:35,  3.70s/it] 58%|█████▊    | 5179/8917 [6:57:23<3:53:32,  3.75s/it] 58%|█████▊    | 5180/8917 [6:57:26<3:54:58,  3.77s/it] 58%|█████▊    | 5181/8917 [6:57:30<3:50:10,  3.70s/it] 58%|█████▊    | 5182/8917 [6:57:33<3:44:25,  3.61s/it] 58%|█████▊    | 5183/8917 [6:57:37<3:46:10,  3.63s/it] 58%|█████▊    | 5184/8917 [6:57:41<3:47:55,  3.66s/it] 58%|█████▊    | 5185/8917 [6:57:45<3:53:14,  3.75s/it] 58%|█████▊    | 5186/8917 [6:57:49<3:54:27,  3.77s/it] 58%|█████▊    | 5187/8917 [6:57:52<3:52:13,  3.74s/it] 58%|█████▊    | 5188/8917 [6:57:56<3:54:37,  3.78s/it] 58%|█████▊    | 5189/8917 [6:57:59<3:48:40,  3.68s/it] 58%|█████▊    | 5190/8917 [6:58:03<3:50:05,  3.70s/it] 58%|█████▊    | 5191/8917 [6:58:07<3:51:18,  3.72s/it] 58%|█████▊    | 5192/8917 [6:58:11<3:47:36,  3.67s/it] 58%|█████▊    | 5193/8917 [6:58:15<3:54:02,  3.77s/it] 58%|█████▊    | 5194/8917 [6:58:18<3:53:43,  3.77s/it] 58%|█████▊    | 5195/8917 [6:58:22<3:46:35,  3.65s/it] 58%|█████▊    | 5196/8917 [6:58:26<3:51:51,  3.74s/it] 58%|█████▊    | 5197/8917 [6:58:29<3:52:28,  3.75s/it] 58%|█████▊    | 5198/8917 [6:58:33<3:49:27,  3.70s/it] 58%|█████▊    | 5199/8917 [6:58:36<3:44:25,  3.62s/it]09/19/2024 09:13:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01718885451555252, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2859933376312256, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3031822443008423}
 58%|█████▊    | 5200/8917 [6:58:40<3:47:59,  3.68s/it] 58%|█████▊    | 5201/8917 [6:58:44<3:43:17,  3.61s/it] 58%|█████▊    | 5202/8917 [6:58:47<3:44:04,  3.62s/it] 58%|█████▊    | 5203/8917 [6:58:51<3:44:12,  3.62s/it] 58%|█████▊    | 5204/8917 [6:58:55<3:51:15,  3.74s/it] 58%|█████▊    | 5205/8917 [6:58:59<3:50:34,  3.73s/it] 58%|█████▊    | 5206/8917 [6:59:03<3:55:39,  3.81s/it] 58%|█████▊    | 5207/8917 [6:59:07<3:56:34,  3.83s/it] 58%|█████▊    | 5208/8917 [6:59:10<3:56:21,  3.82s/it] 58%|█████▊    | 5209/8917 [6:59:14<3:49:10,  3.71s/it] 58%|█████▊    | 5210/8917 [6:59:17<3:47:23,  3.68s/it] 58%|█████▊    | 5211/8917 [6:59:21<3:48:19,  3.70s/it] 58%|█████▊    | 5212/8917 [6:59:25<3:48:27,  3.70s/it] 58%|█████▊    | 5213/8917 [6:59:29<3:51:54,  3.76s/it] 58%|█████▊    | 5214/8917 [6:59:32<3:48:27,  3.70s/it] 58%|█████▊    | 5215/8917 [6:59:36<3:49:51,  3.73s/it] 58%|█████▊    | 5216/8917 [6:59:40<3:54:13,  3.80s/it] 59%|█████▊    | 5217/8917 [6:59:44<3:48:18,  3.70s/it] 59%|█████▊    | 5218/8917 [6:59:47<3:42:03,  3.60s/it] 59%|█████▊    | 5219/8917 [6:59:51<3:43:15,  3.62s/it] 59%|█████▊    | 5220/8917 [6:59:54<3:47:53,  3.70s/it] 59%|█████▊    | 5221/8917 [6:59:58<3:49:55,  3.73s/it] 59%|█████▊    | 5222/8917 [7:00:02<3:51:50,  3.76s/it] 59%|█████▊    | 5223/8917 [7:00:06<3:47:41,  3.70s/it] 59%|█████▊    | 5224/8917 [7:00:09<3:49:56,  3.74s/it] 59%|█████▊    | 5225/8917 [7:00:13<3:49:37,  3.73s/it] 59%|█████▊    | 5226/8917 [7:00:17<3:51:35,  3.76s/it] 59%|█████▊    | 5227/8917 [7:00:20<3:44:19,  3.65s/it] 59%|█████▊    | 5228/8917 [7:00:24<3:38:43,  3.56s/it] 59%|█████▊    | 5229/8917 [7:00:28<3:44:46,  3.66s/it] 59%|█████▊    | 5230/8917 [7:00:31<3:47:33,  3.70s/it] 59%|█████▊    | 5231/8917 [7:00:35<3:46:07,  3.68s/it] 59%|█████▊    | 5232/8917 [7:00:39<3:47:00,  3.70s/it] 59%|█████▊    | 5233/8917 [7:00:43<3:48:05,  3.71s/it] 59%|█████▊    | 5234/8917 [7:00:46<3:41:30,  3.61s/it] 59%|█████▊    | 5235/8917 [7:00:50<3:46:39,  3.69s/it] 59%|█████▊    | 5236/8917 [7:00:54<3:54:00,  3.81s/it] 59%|█████▊    | 5237/8917 [7:00:58<3:51:33,  3.78s/it] 59%|█████▊    | 5238/8917 [7:01:02<3:53:26,  3.81s/it] 59%|█████▉    | 5239/8917 [7:01:05<3:50:58,  3.77s/it] 59%|█████▉    | 5240/8917 [7:01:09<3:46:44,  3.70s/it] 59%|█████▉    | 5241/8917 [7:01:13<3:52:56,  3.80s/it] 59%|█████▉    | 5242/8917 [7:01:16<3:51:01,  3.77s/it] 59%|█████▉    | 5243/8917 [7:01:20<3:42:14,  3.63s/it] 59%|█████▉    | 5244/8917 [7:01:24<3:45:35,  3.69s/it] 59%|█████▉    | 5245/8917 [7:01:27<3:49:22,  3.75s/it] 59%|█████▉    | 5246/8917 [7:01:31<3:48:43,  3.74s/it] 59%|█████▉    | 5247/8917 [7:01:35<3:47:45,  3.72s/it] 59%|█████▉    | 5248/8917 [7:01:39<3:47:58,  3.73s/it] 59%|█████▉    | 5249/8917 [7:01:42<3:43:18,  3.65s/it]09/19/2024 09:16:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04274766147136688, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4804518222808838, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.523199439048767}
 59%|█████▉    | 5250/8917 [7:01:46<3:48:01,  3.73s/it] 59%|█████▉    | 5251/8917 [7:01:50<3:47:38,  3.73s/it] 59%|█████▉    | 5252/8917 [7:01:54<3:48:48,  3.75s/it] 59%|█████▉    | 5253/8917 [7:01:57<3:47:02,  3.72s/it] 59%|█████▉    | 5254/8917 [7:02:01<3:45:38,  3.70s/it] 59%|█████▉    | 5255/8917 [7:02:05<3:46:15,  3.71s/it] 59%|█████▉    | 5256/8917 [7:02:08<3:46:34,  3.71s/it] 59%|█████▉    | 5257/8917 [7:02:12<3:46:22,  3.71s/it] 59%|█████▉    | 5258/8917 [7:02:16<3:45:03,  3.69s/it] 59%|█████▉    | 5259/8917 [7:02:19<3:45:55,  3.71s/it] 59%|█████▉    | 5260/8917 [7:02:23<3:43:06,  3.66s/it] 59%|█████▉    | 5261/8917 [7:02:27<3:44:40,  3.69s/it] 59%|█████▉    | 5262/8917 [7:02:30<3:46:18,  3.72s/it] 59%|█████▉    | 5263/8917 [7:02:34<3:44:41,  3.69s/it] 59%|█████▉    | 5264/8917 [7:02:38<3:45:05,  3.70s/it] 59%|█████▉    | 5265/8917 [7:02:42<3:53:16,  3.83s/it] 59%|█████▉    | 5266/8917 [7:02:46<3:48:54,  3.76s/it] 59%|█████▉    | 5267/8917 [7:02:49<3:48:40,  3.76s/it] 59%|█████▉    | 5268/8917 [7:02:53<3:47:43,  3.74s/it] 59%|█████▉    | 5269/8917 [7:02:57<3:47:13,  3.74s/it] 59%|█████▉    | 5270/8917 [7:03:00<3:41:01,  3.64s/it] 59%|█████▉    | 5271/8917 [7:03:04<3:46:47,  3.73s/it] 59%|█████▉    | 5272/8917 [7:03:08<3:42:38,  3.66s/it] 59%|█████▉    | 5273/8917 [7:03:11<3:41:27,  3.65s/it] 59%|█████▉    | 5274/8917 [7:03:15<3:40:15,  3.63s/it] 59%|█████▉    | 5275/8917 [7:03:18<3:35:27,  3.55s/it] 59%|█████▉    | 5276/8917 [7:03:22<3:43:00,  3.67s/it] 59%|█████▉    | 5277/8917 [7:03:26<3:47:18,  3.75s/it] 59%|█████▉    | 5278/8917 [7:03:30<3:43:41,  3.69s/it] 59%|█████▉    | 5279/8917 [7:03:33<3:45:20,  3.72s/it] 59%|█████▉    | 5280/8917 [7:03:37<3:45:34,  3.72s/it] 59%|█████▉    | 5281/8917 [7:03:41<3:40:14,  3.63s/it] 59%|█████▉    | 5282/8917 [7:03:44<3:41:51,  3.66s/it] 59%|█████▉    | 5283/8917 [7:03:48<3:43:34,  3.69s/it] 59%|█████▉    | 5284/8917 [7:03:52<3:43:05,  3.68s/it] 59%|█████▉    | 5285/8917 [7:03:56<3:46:27,  3.74s/it] 59%|█████▉    | 5286/8917 [7:03:59<3:41:24,  3.66s/it] 59%|█████▉    | 5287/8917 [7:04:03<3:39:51,  3.63s/it] 59%|█████▉    | 5288/8917 [7:04:06<3:39:39,  3.63s/it] 59%|█████▉    | 5289/8917 [7:04:10<3:41:02,  3.66s/it] 59%|█████▉    | 5290/8917 [7:04:14<3:46:00,  3.74s/it] 59%|█████▉    | 5291/8917 [7:04:17<3:42:31,  3.68s/it] 59%|█████▉    | 5292/8917 [7:04:21<3:44:26,  3.72s/it] 59%|█████▉    | 5293/8917 [7:04:25<3:44:50,  3.72s/it] 59%|█████▉    | 5294/8917 [7:04:29<3:51:11,  3.83s/it] 59%|█████▉    | 5295/8917 [7:04:32<3:44:30,  3.72s/it] 59%|█████▉    | 5296/8917 [7:04:36<3:40:00,  3.65s/it] 59%|█████▉    | 5297/8917 [7:04:40<3:42:54,  3.69s/it] 59%|█████▉    | 5298/8917 [7:04:43<3:43:39,  3.71s/it] 59%|█████▉    | 5299/8917 [7:04:47<3:48:51,  3.80s/it]09/19/2024 09:19:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.017886310815811157, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1160129308700562, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.133899211883545}
 59%|█████▉    | 5300/8917 [7:04:52<3:52:36,  3.86s/it] 59%|█████▉    | 5301/8917 [7:04:55<3:45:22,  3.74s/it] 59%|█████▉    | 5302/8917 [7:04:58<3:36:50,  3.60s/it] 59%|█████▉    | 5303/8917 [7:05:02<3:34:52,  3.57s/it] 59%|█████▉    | 5304/8917 [7:05:06<3:40:55,  3.67s/it] 59%|█████▉    | 5305/8917 [7:05:09<3:40:59,  3.67s/it] 60%|█████▉    | 5306/8917 [7:05:13<3:39:38,  3.65s/it] 60%|█████▉    | 5307/8917 [7:05:17<3:39:06,  3.64s/it] 60%|█████▉    | 5308/8917 [7:05:20<3:37:11,  3.61s/it] 60%|█████▉    | 5309/8917 [7:05:24<3:39:24,  3.65s/it] 60%|█████▉    | 5310/8917 [7:05:28<3:43:35,  3.72s/it] 60%|█████▉    | 5311/8917 [7:05:31<3:43:41,  3.72s/it] 60%|█████▉    | 5312/8917 [7:05:36<3:53:03,  3.88s/it] 60%|█████▉    | 5313/8917 [7:05:39<3:46:34,  3.77s/it] 60%|█████▉    | 5314/8917 [7:05:43<3:47:55,  3.80s/it] 60%|█████▉    | 5315/8917 [7:05:47<3:52:17,  3.87s/it] 60%|█████▉    | 5316/8917 [7:05:51<3:49:49,  3.83s/it] 60%|█████▉    | 5317/8917 [7:05:54<3:45:28,  3.76s/it] 60%|█████▉    | 5318/8917 [7:05:58<3:50:50,  3.85s/it] 60%|█████▉    | 5319/8917 [7:06:02<3:45:35,  3.76s/it] 60%|█████▉    | 5320/8917 [7:06:06<3:42:49,  3.72s/it] 60%|█████▉    | 5321/8917 [7:06:10<3:51:27,  3.86s/it] 60%|█████▉    | 5322/8917 [7:06:13<3:47:17,  3.79s/it] 60%|█████▉    | 5323/8917 [7:06:17<3:47:19,  3.80s/it] 60%|█████▉    | 5324/8917 [7:06:21<3:44:09,  3.74s/it] 60%|█████▉    | 5325/8917 [7:06:25<3:41:56,  3.71s/it] 60%|█████▉    | 5326/8917 [7:06:28<3:37:05,  3.63s/it] 60%|█████▉    | 5327/8917 [7:06:32<3:35:43,  3.61s/it] 60%|█████▉    | 5328/8917 [7:06:35<3:42:15,  3.72s/it] 60%|█████▉    | 5329/8917 [7:06:39<3:44:03,  3.75s/it] 60%|█████▉    | 5330/8917 [7:06:43<3:41:56,  3.71s/it] 60%|█████▉    | 5331/8917 [7:06:47<3:40:03,  3.68s/it] 60%|█████▉    | 5332/8917 [7:06:50<3:37:51,  3.65s/it] 60%|█████▉    | 5333/8917 [7:06:54<3:36:11,  3.62s/it] 60%|█████▉    | 5334/8917 [7:06:57<3:38:32,  3.66s/it] 60%|█████▉    | 5335/8917 [7:07:01<3:37:31,  3.64s/it] 60%|█████▉    | 5336/8917 [7:07:05<3:40:47,  3.70s/it] 60%|█████▉    | 5337/8917 [7:07:09<3:50:26,  3.86s/it] 60%|█████▉    | 5338/8917 [7:07:13<3:42:42,  3.73s/it] 60%|█████▉    | 5339/8917 [7:07:16<3:44:38,  3.77s/it] 60%|█████▉    | 5340/8917 [7:07:20<3:49:34,  3.85s/it] 60%|█████▉    | 5341/8917 [7:07:24<3:49:53,  3.86s/it] 60%|█████▉    | 5342/8917 [7:07:28<3:45:36,  3.79s/it] 60%|█████▉    | 5343/8917 [7:07:32<3:46:24,  3.80s/it] 60%|█████▉    | 5344/8917 [7:07:36<3:47:27,  3.82s/it] 60%|█████▉    | 5345/8917 [7:07:39<3:43:53,  3.76s/it] 60%|█████▉    | 5346/8917 [7:07:43<3:47:48,  3.83s/it] 60%|█████▉    | 5347/8917 [7:07:47<3:45:45,  3.79s/it] 60%|█████▉    | 5348/8917 [7:07:50<3:40:37,  3.71s/it] 60%|█████▉    | 5349/8917 [7:07:55<3:47:51,  3.83s/it]09/19/2024 09:22:32 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032349709421396255, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0676251649856567, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0999748706817627}
 60%|█████▉    | 5350/8917 [7:07:59<3:51:07,  3.89s/it] 60%|██████    | 5351/8917 [7:08:02<3:50:23,  3.88s/it] 60%|██████    | 5352/8917 [7:08:06<3:49:46,  3.87s/it] 60%|██████    | 5353/8917 [7:08:10<3:49:29,  3.86s/it] 60%|██████    | 5354/8917 [7:08:14<3:49:23,  3.86s/it] 60%|██████    | 5355/8917 [7:08:18<3:46:36,  3.82s/it] 60%|██████    | 5356/8917 [7:08:21<3:45:46,  3.80s/it] 60%|██████    | 5357/8917 [7:08:25<3:41:38,  3.74s/it] 60%|██████    | 5358/8917 [7:08:29<3:41:12,  3.73s/it] 60%|██████    | 5359/8917 [7:08:32<3:38:33,  3.69s/it] 60%|██████    | 5360/8917 [7:08:36<3:39:22,  3.70s/it] 60%|██████    | 5361/8917 [7:08:40<3:39:57,  3.71s/it] 60%|██████    | 5362/8917 [7:08:44<3:41:58,  3.75s/it] 60%|██████    | 5363/8917 [7:08:47<3:39:52,  3.71s/it] 60%|██████    | 5364/8917 [7:08:51<3:40:16,  3.72s/it] 60%|██████    | 5365/8917 [7:08:55<3:41:17,  3.74s/it] 60%|██████    | 5366/8917 [7:08:59<3:47:35,  3.85s/it] 60%|██████    | 5367/8917 [7:09:03<3:46:35,  3.83s/it] 60%|██████    | 5368/8917 [7:09:07<3:46:13,  3.82s/it] 60%|██████    | 5369/8917 [7:09:10<3:41:06,  3.74s/it] 60%|██████    | 5370/8917 [7:09:14<3:47:58,  3.86s/it] 60%|██████    | 5371/8917 [7:09:18<3:39:32,  3.71s/it] 60%|██████    | 5372/8917 [7:09:21<3:41:42,  3.75s/it] 60%|██████    | 5373/8917 [7:09:25<3:41:52,  3.76s/it] 60%|██████    | 5374/8917 [7:09:29<3:39:54,  3.72s/it] 60%|██████    | 5375/8917 [7:09:33<3:39:59,  3.73s/it] 60%|██████    | 5376/8917 [7:09:37<3:47:09,  3.85s/it] 60%|██████    | 5377/8917 [7:09:41<3:52:41,  3.94s/it] 60%|██████    | 5378/8917 [7:09:44<3:41:36,  3.76s/it] 60%|██████    | 5379/8917 [7:09:48<3:36:23,  3.67s/it] 60%|██████    | 5380/8917 [7:09:51<3:38:24,  3.70s/it] 60%|██████    | 5381/8917 [7:09:55<3:44:43,  3.81s/it] 60%|██████    | 5382/8917 [7:09:59<3:47:17,  3.86s/it] 60%|██████    | 5383/8917 [7:10:03<3:43:01,  3.79s/it] 60%|██████    | 5384/8917 [7:10:07<3:42:00,  3.77s/it] 60%|██████    | 5385/8917 [7:10:10<3:39:45,  3.73s/it] 60%|██████    | 5386/8917 [7:10:14<3:37:40,  3.70s/it] 60%|██████    | 5387/8917 [7:10:18<3:38:30,  3.71s/it] 60%|██████    | 5388/8917 [7:10:22<3:43:50,  3.81s/it] 60%|██████    | 5389/8917 [7:10:25<3:40:42,  3.75s/it] 60%|██████    | 5390/8917 [7:10:29<3:41:07,  3.76s/it] 60%|██████    | 5391/8917 [7:10:33<3:44:10,  3.81s/it] 60%|██████    | 5392/8917 [7:10:37<3:39:34,  3.74s/it] 60%|██████    | 5393/8917 [7:10:40<3:35:33,  3.67s/it] 60%|██████    | 5394/8917 [7:10:44<3:40:50,  3.76s/it] 61%|██████    | 5395/8917 [7:10:48<3:45:12,  3.84s/it] 61%|██████    | 5396/8917 [7:10:52<3:40:06,  3.75s/it] 61%|██████    | 5397/8917 [7:10:56<3:40:11,  3.75s/it] 61%|██████    | 5398/8917 [7:10:59<3:37:12,  3.70s/it] 61%|██████    | 5399/8917 [7:11:03<3:40:57,  3.77s/it]09/19/2024 09:25:41 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020192187279462814, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.519792914390564, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5399850606918335}
 61%|██████    | 5400/8917 [7:11:07<3:38:11,  3.72s/it] 61%|██████    | 5401/8917 [7:11:10<3:35:06,  3.67s/it] 61%|██████    | 5402/8917 [7:11:14<3:36:30,  3.70s/it] 61%|██████    | 5403/8917 [7:11:18<3:37:25,  3.71s/it] 61%|██████    | 5404/8917 [7:11:21<3:36:52,  3.70s/it] 61%|██████    | 5405/8917 [7:11:25<3:39:48,  3.76s/it] 61%|██████    | 5406/8917 [7:11:29<3:35:09,  3.68s/it] 61%|██████    | 5407/8917 [7:11:33<3:36:11,  3.70s/it] 61%|██████    | 5408/8917 [7:11:36<3:36:47,  3.71s/it] 61%|██████    | 5409/8917 [7:11:40<3:30:35,  3.60s/it] 61%|██████    | 5410/8917 [7:11:44<3:38:58,  3.75s/it] 61%|██████    | 5411/8917 [7:11:48<3:40:52,  3.78s/it] 61%|██████    | 5412/8917 [7:11:51<3:40:54,  3.78s/it] 61%|██████    | 5413/8917 [7:11:55<3:38:24,  3.74s/it] 61%|██████    | 5414/8917 [7:11:59<3:43:04,  3.82s/it] 61%|██████    | 5415/8917 [7:12:03<3:42:02,  3.80s/it] 61%|██████    | 5416/8917 [7:12:06<3:38:02,  3.74s/it] 61%|██████    | 5417/8917 [7:12:10<3:43:54,  3.84s/it] 61%|██████    | 5418/8917 [7:12:14<3:37:06,  3.72s/it] 61%|██████    | 5419/8917 [7:12:18<3:37:19,  3.73s/it] 61%|██████    | 5420/8917 [7:12:21<3:35:41,  3.70s/it] 61%|██████    | 5421/8917 [7:12:25<3:40:51,  3.79s/it] 61%|██████    | 5422/8917 [7:12:29<3:35:53,  3.71s/it] 61%|██████    | 5423/8917 [7:12:32<3:31:04,  3.62s/it] 61%|██████    | 5424/8917 [7:12:36<3:35:52,  3.71s/it] 61%|██████    | 5425/8917 [7:12:40<3:34:58,  3.69s/it] 61%|██████    | 5426/8917 [7:12:43<3:29:41,  3.60s/it] 61%|██████    | 5427/8917 [7:12:47<3:40:49,  3.80s/it] 61%|██████    | 5428/8917 [7:12:51<3:43:18,  3.84s/it] 61%|██████    | 5429/8917 [7:12:55<3:35:47,  3.71s/it] 61%|██████    | 5430/8917 [7:12:59<3:38:41,  3.76s/it] 61%|██████    | 5431/8917 [7:13:02<3:37:09,  3.74s/it] 61%|██████    | 5432/8917 [7:13:06<3:35:19,  3.71s/it] 61%|██████    | 5433/8917 [7:13:10<3:35:06,  3.70s/it] 61%|██████    | 5434/8917 [7:13:13<3:35:11,  3.71s/it] 61%|██████    | 5435/8917 [7:13:17<3:36:10,  3.72s/it] 61%|██████    | 5436/8917 [7:13:21<3:41:28,  3.82s/it] 61%|██████    | 5437/8917 [7:13:25<3:42:48,  3.84s/it] 61%|██████    | 5438/8917 [7:13:29<3:43:29,  3.85s/it] 61%|██████    | 5439/8917 [7:13:33<3:39:51,  3.79s/it] 61%|██████    | 5440/8917 [7:13:36<3:41:48,  3.83s/it] 61%|██████    | 5441/8917 [7:13:40<3:41:54,  3.83s/it] 61%|██████    | 5442/8917 [7:13:44<3:35:16,  3.72s/it] 61%|██████    | 5443/8917 [7:13:47<3:32:31,  3.67s/it] 61%|██████    | 5444/8917 [7:13:51<3:32:53,  3.68s/it] 61%|██████    | 5445/8917 [7:13:55<3:32:40,  3.68s/it] 61%|██████    | 5446/8917 [7:13:58<3:29:58,  3.63s/it] 61%|██████    | 5447/8917 [7:14:02<3:40:46,  3.82s/it] 61%|██████    | 5448/8917 [7:14:06<3:40:08,  3.81s/it] 61%|██████    | 5449/8917 [7:14:10<3:40:40,  3.82s/it]09/19/2024 09:28:47 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020186197012662888, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0959396362304688, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1161258220672607}
 61%|██████    | 5450/8917 [7:14:14<3:34:24,  3.71s/it] 61%|██████    | 5451/8917 [7:14:17<3:31:38,  3.66s/it] 61%|██████    | 5452/8917 [7:14:21<3:38:47,  3.79s/it] 61%|██████    | 5453/8917 [7:14:25<3:36:08,  3.74s/it] 61%|██████    | 5454/8917 [7:14:29<3:39:01,  3.79s/it] 61%|██████    | 5455/8917 [7:14:33<3:40:54,  3.83s/it] 61%|██████    | 5456/8917 [7:14:36<3:35:06,  3.73s/it] 61%|██████    | 5457/8917 [7:14:40<3:38:51,  3.80s/it] 61%|██████    | 5458/8917 [7:14:44<3:33:52,  3.71s/it] 61%|██████    | 5459/8917 [7:14:47<3:36:15,  3.75s/it] 61%|██████    | 5460/8917 [7:14:51<3:32:42,  3.69s/it] 61%|██████    | 5461/8917 [7:14:55<3:34:54,  3.73s/it] 61%|██████▏   | 5462/8917 [7:14:59<3:41:35,  3.85s/it] 61%|██████▏   | 5463/8917 [7:15:03<3:37:16,  3.77s/it] 61%|██████▏   | 5464/8917 [7:15:06<3:34:55,  3.73s/it] 61%|██████▏   | 5465/8917 [7:15:10<3:32:38,  3.70s/it] 61%|██████▏   | 5466/8917 [7:15:14<3:33:11,  3.71s/it] 61%|██████▏   | 5467/8917 [7:15:17<3:35:51,  3.75s/it] 61%|██████▏   | 5468/8917 [7:15:21<3:36:37,  3.77s/it] 61%|██████▏   | 5469/8917 [7:15:25<3:38:46,  3.81s/it] 61%|██████▏   | 5470/8917 [7:15:29<3:37:48,  3.79s/it] 61%|██████▏   | 5471/8917 [7:15:32<3:33:48,  3.72s/it] 61%|██████▏   | 5472/8917 [7:15:36<3:36:52,  3.78s/it] 61%|██████▏   | 5473/8917 [7:15:40<3:33:56,  3.73s/it] 61%|██████▏   | 5474/8917 [7:15:44<3:35:40,  3.76s/it] 61%|██████▏   | 5475/8917 [7:15:47<3:33:19,  3.72s/it] 61%|██████▏   | 5476/8917 [7:15:51<3:30:05,  3.66s/it] 61%|██████▏   | 5477/8917 [7:15:55<3:30:13,  3.67s/it] 61%|██████▏   | 5478/8917 [7:15:59<3:34:46,  3.75s/it] 61%|██████▏   | 5479/8917 [7:16:02<3:33:15,  3.72s/it] 61%|██████▏   | 5480/8917 [7:16:06<3:31:23,  3.69s/it] 61%|██████▏   | 5481/8917 [7:16:09<3:28:18,  3.64s/it] 61%|██████▏   | 5482/8917 [7:16:13<3:24:18,  3.57s/it] 61%|██████▏   | 5483/8917 [7:16:17<3:29:16,  3.66s/it] 62%|██████▏   | 5484/8917 [7:16:20<3:29:15,  3.66s/it] 62%|██████▏   | 5485/8917 [7:16:25<3:44:17,  3.92s/it] 62%|██████▏   | 5486/8917 [7:16:28<3:36:40,  3.79s/it] 62%|██████▏   | 5487/8917 [7:16:32<3:32:55,  3.72s/it] 62%|██████▏   | 5488/8917 [7:16:36<3:33:39,  3.74s/it] 62%|██████▏   | 5489/8917 [7:16:39<3:31:00,  3.69s/it] 62%|██████▏   | 5490/8917 [7:16:43<3:27:19,  3.63s/it] 62%|██████▏   | 5491/8917 [7:16:46<3:29:26,  3.67s/it] 62%|██████▏   | 5492/8917 [7:16:50<3:32:52,  3.73s/it] 62%|██████▏   | 5493/8917 [7:16:54<3:29:46,  3.68s/it] 62%|██████▏   | 5494/8917 [7:16:58<3:29:28,  3.67s/it] 62%|██████▏   | 5495/8917 [7:17:01<3:30:43,  3.69s/it] 62%|██████▏   | 5496/8917 [7:17:05<3:31:27,  3.71s/it] 62%|██████▏   | 5497/8917 [7:17:09<3:35:57,  3.79s/it] 62%|██████▏   | 5498/8917 [7:17:13<3:37:19,  3.81s/it] 62%|██████▏   | 5499/8917 [7:17:16<3:33:35,  3.75s/it]09/19/2024 09:31:53 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 09:31:53 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<05:08,  1.40s/it][A
  1%|          | 2/221 [00:01<03:21,  1.09it/s][A
  1%|▏         | 3/221 [00:02<03:16,  1.11it/s][A
  2%|▏         | 4/221 [00:03<02:18,  1.56it/s][A
  2%|▏         | 5/221 [00:03<01:36,  2.23it/s][A
  3%|▎         | 6/221 [00:03<01:11,  2.99it/s][A
  3%|▎         | 7/221 [00:03<01:03,  3.39it/s][A
  4%|▎         | 8/221 [00:03<01:13,  2.89it/s][A
  4%|▍         | 9/221 [00:04<01:14,  2.84it/s][A
  5%|▍         | 10/221 [00:04<01:22,  2.57it/s][A
  5%|▍         | 11/221 [00:04<01:03,  3.31it/s][A
  5%|▌         | 12/221 [00:12<08:53,  2.55s/it][A
  6%|▌         | 13/221 [00:12<06:26,  1.86s/it][A
  7%|▋         | 15/221 [00:13<03:49,  1.12s/it][A
  7%|▋         | 16/221 [00:14<03:22,  1.01it/s][A
  8%|▊         | 17/221 [00:15<03:31,  1.04s/it][A
  8%|▊         | 18/221 [00:15<02:50,  1.19it/s][A
  9%|▊         | 19/221 [00:16<03:12,  1.05it/s][A
  9%|▉         | 20/221 [00:16<02:23,  1.40it/s][A
 10%|▉         | 21/221 [00:17<02:04,  1.61it/s][A
 10%|▉         | 22/221 [00:18<02:11,  1.51it/s][A
 11%|█         | 24/221 [00:18<01:28,  2.22it/s][A
 11%|█▏        | 25/221 [00:18<01:22,  2.38it/s][A
 12%|█▏        | 26/221 [00:19<01:17,  2.52it/s][A
 13%|█▎        | 28/221 [00:19<01:06,  2.88it/s][A
 13%|█▎        | 29/221 [00:19<00:59,  3.21it/s][A
 14%|█▎        | 30/221 [00:20<01:14,  2.58it/s][A
 14%|█▍        | 31/221 [00:20<01:14,  2.54it/s][A
 15%|█▍        | 33/221 [00:21<00:58,  3.19it/s][A
 16%|█▌        | 35/221 [00:21<00:48,  3.86it/s][A
 16%|█▋        | 36/221 [00:21<00:47,  3.89it/s][A
 17%|█▋        | 37/221 [00:22<01:04,  2.86it/s][A
 17%|█▋        | 38/221 [00:23<01:10,  2.58it/s][A
 18%|█▊        | 39/221 [00:23<01:32,  1.96it/s][A
 18%|█▊        | 40/221 [00:24<01:29,  2.02it/s][A
 19%|█▊        | 41/221 [00:24<01:12,  2.48it/s][A
 19%|█▉        | 42/221 [00:24<01:01,  2.93it/s][A
 19%|█▉        | 43/221 [00:24<00:51,  3.48it/s][A
 20%|█▉        | 44/221 [00:24<00:41,  4.24it/s][A
 20%|██        | 45/221 [00:27<02:54,  1.01it/s][A
 21%|██        | 46/221 [00:28<02:19,  1.25it/s][A
 21%|██▏       | 47/221 [00:28<02:04,  1.40it/s][A
 22%|██▏       | 48/221 [00:28<01:33,  1.85it/s][A
 22%|██▏       | 49/221 [00:29<01:23,  2.06it/s][A
 23%|██▎       | 50/221 [00:29<01:11,  2.38it/s][A
 23%|██▎       | 51/221 [00:29<00:59,  2.84it/s][A
 24%|██▎       | 52/221 [00:29<00:50,  3.34it/s][A
 24%|██▍       | 53/221 [00:30<00:50,  3.31it/s][A
 24%|██▍       | 54/221 [00:30<01:08,  2.42it/s][A
 25%|██▍       | 55/221 [00:31<01:39,  1.67it/s][A
 25%|██▌       | 56/221 [00:31<01:18,  2.09it/s][A
 26%|██▌       | 57/221 [00:32<01:08,  2.41it/s][A
 27%|██▋       | 59/221 [00:32<00:42,  3.78it/s][A
 27%|██▋       | 60/221 [00:32<00:45,  3.56it/s][A
 28%|██▊       | 61/221 [00:32<00:41,  3.84it/s][A
 28%|██▊       | 62/221 [00:33<00:44,  3.55it/s][A
 29%|██▊       | 63/221 [00:33<00:41,  3.85it/s][A
 29%|██▉       | 64/221 [00:34<01:21,  1.92it/s][A
 29%|██▉       | 65/221 [00:34<01:05,  2.39it/s][A
 30%|██▉       | 66/221 [00:35<01:11,  2.16it/s][A
 30%|███       | 67/221 [00:35<01:01,  2.50it/s][A
 31%|███       | 68/221 [00:35<00:49,  3.10it/s][A
 31%|███       | 69/221 [00:37<01:55,  1.31it/s][A
 32%|███▏      | 70/221 [00:37<01:25,  1.76it/s][A
 32%|███▏      | 71/221 [00:38<01:17,  1.94it/s][A
 33%|███▎      | 72/221 [00:38<01:09,  2.15it/s][A
 33%|███▎      | 73/221 [00:39<01:20,  1.84it/s][A
 33%|███▎      | 74/221 [00:39<01:01,  2.39it/s][A
 34%|███▍      | 75/221 [00:39<00:57,  2.54it/s][A
 34%|███▍      | 76/221 [00:39<00:55,  2.59it/s][A
 35%|███▍      | 77/221 [00:42<02:17,  1.05it/s][A
 35%|███▌      | 78/221 [00:42<01:43,  1.39it/s][A
 36%|███▌      | 79/221 [00:43<01:47,  1.33it/s][A
 37%|███▋      | 81/221 [00:43<01:15,  1.86it/s][A
 37%|███▋      | 82/221 [00:46<02:32,  1.09s/it][A
 38%|███▊      | 83/221 [00:47<02:14,  1.03it/s][A
 38%|███▊      | 84/221 [00:47<01:47,  1.27it/s][A
 39%|███▉      | 86/221 [00:47<01:12,  1.86it/s][A
 39%|███▉      | 87/221 [00:48<01:09,  1.92it/s][A
 40%|███▉      | 88/221 [00:48<01:01,  2.15it/s][A
 40%|████      | 89/221 [00:49<00:57,  2.32it/s][A
 41%|████      | 90/221 [00:49<00:50,  2.61it/s][A
 41%|████      | 91/221 [00:49<00:42,  3.08it/s][A
 42%|████▏     | 92/221 [00:49<00:39,  3.28it/s][A
 42%|████▏     | 93/221 [00:50<00:39,  3.24it/s][A
 43%|████▎     | 94/221 [00:50<00:53,  2.37it/s][A
 43%|████▎     | 95/221 [00:51<00:49,  2.53it/s][A
 43%|████▎     | 96/221 [00:51<01:05,  1.90it/s][A
 44%|████▍     | 97/221 [00:52<00:54,  2.28it/s][A
 44%|████▍     | 98/221 [00:52<00:55,  2.20it/s][A
 45%|████▍     | 99/221 [00:52<00:51,  2.37it/s][A
 45%|████▌     | 100/221 [00:53<01:07,  1.78it/s][A
 46%|████▌     | 101/221 [00:54<00:52,  2.27it/s][A
 46%|████▌     | 102/221 [00:54<01:07,  1.77it/s][A
 47%|████▋     | 104/221 [00:55<00:48,  2.43it/s][A
 48%|████▊     | 105/221 [00:55<00:50,  2.30it/s][A
 48%|████▊     | 106/221 [00:59<02:13,  1.16s/it][A
 48%|████▊     | 107/221 [00:59<01:46,  1.07it/s][A
 49%|████▉     | 108/221 [00:59<01:31,  1.23it/s][A
 49%|████▉     | 109/221 [01:00<01:17,  1.44it/s][A
 50%|████▉     | 110/221 [01:00<01:00,  1.83it/s][A
 50%|█████     | 111/221 [01:00<00:55,  1.98it/s][A
 51%|█████     | 112/221 [01:01<00:52,  2.07it/s][A
 51%|█████     | 113/221 [01:01<00:42,  2.53it/s][A
 52%|█████▏    | 114/221 [01:01<00:32,  3.24it/s][A
 52%|█████▏    | 115/221 [01:02<00:41,  2.55it/s][A
 52%|█████▏    | 116/221 [01:02<00:40,  2.60it/s][A
 53%|█████▎    | 117/221 [01:02<00:40,  2.55it/s][A
 53%|█████▎    | 118/221 [01:03<00:41,  2.51it/s][A
 54%|█████▍    | 119/221 [01:03<00:36,  2.78it/s][A
 54%|█████▍    | 120/221 [01:03<00:30,  3.32it/s][A
 55%|█████▍    | 121/221 [01:04<00:36,  2.73it/s][A
 55%|█████▌    | 122/221 [01:04<00:37,  2.62it/s][A
 56%|█████▌    | 123/221 [01:06<01:24,  1.16it/s][A
 56%|█████▌    | 124/221 [01:07<01:07,  1.44it/s][A
 57%|█████▋    | 125/221 [01:07<01:09,  1.38it/s][A
 57%|█████▋    | 126/221 [01:15<04:14,  2.68s/it][A
 57%|█████▋    | 127/221 [01:15<03:11,  2.03s/it][A
 58%|█████▊    | 128/221 [01:15<02:22,  1.53s/it][A
 58%|█████▊    | 129/221 [01:16<01:55,  1.25s/it][A
 59%|█████▉    | 130/221 [01:16<01:26,  1.05it/s][A
 59%|█████▉    | 131/221 [01:18<01:32,  1.03s/it][A
 60%|█████▉    | 132/221 [01:19<01:37,  1.10s/it][A
 60%|██████    | 133/221 [01:19<01:21,  1.08it/s][A
 61%|██████    | 134/221 [01:20<01:16,  1.14it/s][A
 61%|██████    | 135/221 [01:21<01:11,  1.21it/s][A
 62%|██████▏   | 136/221 [01:21<00:58,  1.46it/s][A
 62%|██████▏   | 137/221 [01:22<00:50,  1.65it/s][A
 62%|██████▏   | 138/221 [01:22<00:46,  1.78it/s][A
 63%|██████▎   | 139/221 [01:22<00:38,  2.12it/s][A
 63%|██████▎   | 140/221 [01:23<00:42,  1.90it/s][A
 64%|██████▍   | 141/221 [01:23<00:39,  2.04it/s][A
 64%|██████▍   | 142/221 [01:24<00:36,  2.17it/s][A
 65%|██████▍   | 143/221 [01:24<00:33,  2.30it/s][A
 65%|██████▌   | 144/221 [01:24<00:26,  2.85it/s][A
 66%|██████▌   | 145/221 [01:24<00:21,  3.59it/s][A
 66%|██████▌   | 146/221 [01:25<00:17,  4.17it/s][A
 67%|██████▋   | 148/221 [01:26<00:28,  2.56it/s][A
 67%|██████▋   | 149/221 [01:26<00:25,  2.81it/s][A
 68%|██████▊   | 150/221 [01:26<00:27,  2.55it/s][A
 68%|██████▊   | 151/221 [01:27<00:25,  2.72it/s][A
 69%|██████▉   | 152/221 [01:27<00:25,  2.73it/s][A
 69%|██████▉   | 153/221 [01:27<00:19,  3.41it/s][A
 70%|██████▉   | 154/221 [01:27<00:18,  3.63it/s][A
 70%|███████   | 155/221 [01:28<00:16,  3.93it/s][A
 71%|███████   | 156/221 [01:28<00:17,  3.78it/s][A
 71%|███████   | 157/221 [01:32<01:34,  1.48s/it][A
 71%|███████▏  | 158/221 [01:33<01:19,  1.27s/it][A
 72%|███████▏  | 159/221 [01:33<00:57,  1.07it/s][A
 72%|███████▏  | 160/221 [01:33<00:43,  1.41it/s][A
 73%|███████▎  | 162/221 [01:33<00:24,  2.42it/s][A
 74%|███████▍  | 163/221 [01:34<00:21,  2.75it/s][A
 74%|███████▍  | 164/221 [01:34<00:17,  3.24it/s][A
 75%|███████▍  | 165/221 [01:34<00:18,  2.97it/s][A
 75%|███████▌  | 166/221 [01:35<00:21,  2.50it/s][A
 76%|███████▌  | 167/221 [01:35<00:20,  2.69it/s][A
 76%|███████▌  | 168/221 [01:38<00:58,  1.11s/it][A
 76%|███████▋  | 169/221 [01:38<00:46,  1.11it/s][A
 77%|███████▋  | 170/221 [01:39<00:40,  1.27it/s][A
 77%|███████▋  | 171/221 [01:39<00:33,  1.51it/s][A
 78%|███████▊  | 172/221 [01:40<00:26,  1.86it/s][A
 78%|███████▊  | 173/221 [01:40<00:22,  2.09it/s][A
 79%|███████▊  | 174/221 [01:40<00:17,  2.72it/s][A
 79%|███████▉  | 175/221 [01:40<00:16,  2.85it/s][A
 80%|███████▉  | 176/221 [01:41<00:15,  2.84it/s][A
 80%|████████  | 177/221 [01:41<00:14,  2.97it/s][A
 81%|████████  | 178/221 [01:41<00:14,  3.02it/s][A
 81%|████████  | 179/221 [01:42<00:13,  3.13it/s][A
 81%|████████▏ | 180/221 [01:42<00:11,  3.52it/s][A
 82%|████████▏ | 181/221 [01:42<00:09,  4.11it/s][A
 82%|████████▏ | 182/221 [01:42<00:11,  3.44it/s][A
 83%|████████▎ | 183/221 [01:43<00:15,  2.43it/s][A
 83%|████████▎ | 184/221 [01:43<00:14,  2.50it/s][A
 84%|████████▎ | 185/221 [01:44<00:14,  2.48it/s][A
 84%|████████▍ | 186/221 [01:44<00:13,  2.68it/s][A
 85%|████████▍ | 187/221 [01:44<00:11,  2.88it/s][A
 85%|████████▌ | 188/221 [01:45<00:09,  3.31it/s][A
 86%|████████▌ | 189/221 [01:45<00:09,  3.26it/s][A
 86%|████████▌ | 190/221 [01:45<00:11,  2.70it/s][A
 86%|████████▋ | 191/221 [01:46<00:09,  3.10it/s][A
 87%|████████▋ | 192/221 [01:47<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:47<00:11,  2.41it/s][A
 88%|████████▊ | 194/221 [01:47<00:11,  2.25it/s][A
 88%|████████▊ | 195/221 [01:47<00:09,  2.80it/s][A
 89%|████████▊ | 196/221 [01:48<00:08,  2.84it/s][A
 89%|████████▉ | 197/221 [01:48<00:08,  2.97it/s][A
 90%|████████▉ | 198/221 [01:48<00:07,  3.10it/s][A
 90%|█████████ | 199/221 [01:49<00:06,  3.49it/s][A
 90%|█████████ | 200/221 [01:49<00:06,  3.07it/s][A
 91%|█████████ | 201/221 [01:50<00:10,  1.91it/s][A
 91%|█████████▏| 202/221 [01:50<00:08,  2.35it/s][A
 92%|█████████▏| 203/221 [01:51<00:11,  1.51it/s][A
 92%|█████████▏| 204/221 [01:52<00:09,  1.85it/s][A
 93%|█████████▎| 206/221 [01:52<00:05,  2.56it/s][A
 94%|█████████▍| 208/221 [01:53<00:04,  3.06it/s][A
 95%|█████████▌| 210/221 [01:53<00:02,  4.02it/s][A
 95%|█████████▌| 211/221 [01:53<00:03,  3.33it/s][A
 96%|█████████▌| 212/221 [01:54<00:02,  3.46it/s][A
 96%|█████████▋| 213/221 [01:54<00:02,  3.76it/s][A
 97%|█████████▋| 214/221 [01:54<00:02,  3.25it/s][A
 97%|█████████▋| 215/221 [01:55<00:02,  2.62it/s][A
 98%|█████████▊| 216/221 [01:55<00:01,  2.61it/s][A
 98%|█████████▊| 217/221 [01:57<00:03,  1.23it/s][A
 99%|█████████▊| 218/221 [01:57<00:01,  1.53it/s][A
 99%|█████████▉| 219/221 [01:58<00:01,  1.79it/s][A
100%|█████████▉| 220/221 [02:03<00:01,  1.89s/it][A
100%|██████████| 221/221 [02:03<00:00,  1.38s/it][A100%|██████████| 221/221 [02:03<00:00,  1.79it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:55,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:30,  7.16it/s][A
  1%|          | 2/221 [00:01<02:07,  1.71it/s][A
  1%|▏         | 3/221 [00:01<01:30,  2.41it/s][A
  2%|▏         | 4/221 [00:01<01:47,  2.02it/s][A
  2%|▏         | 5/221 [00:02<02:00,  1.79it/s][A
  3%|▎         | 6/221 [00:02<01:49,  1.97it/s][A
  3%|▎         | 7/221 [00:03<01:40,  2.13it/s][A
  4%|▎         | 8/221 [00:04<02:08,  1.65it/s][A
  4%|▍         | 9/221 [00:04<02:13,  1.59it/s][A
  5%|▍         | 10/221 [00:05<01:47,  1.97it/s][A
  5%|▍         | 11/221 [00:05<01:35,  2.21it/s][A
  5%|▌         | 12/221 [00:05<01:30,  2.30it/s][A
  6%|▌         | 13/221 [00:06<01:15,  2.75it/s][A
  6%|▋         | 14/221 [00:06<01:01,  3.38it/s][A
  7%|▋         | 15/221 [00:06<01:10,  2.90it/s][A
  7%|▋         | 16/221 [00:07<01:40,  2.04it/s][A
  8%|▊         | 17/221 [00:08<02:11,  1.56it/s][A
  8%|▊         | 18/221 [00:08<01:44,  1.95it/s][A
  9%|▊         | 19/221 [00:09<02:11,  1.53it/s][A
  9%|▉         | 20/221 [00:09<01:50,  1.83it/s][A
 10%|▉         | 21/221 [00:10<01:50,  1.80it/s][A
 10%|▉         | 22/221 [00:11<02:23,  1.38it/s][A
 10%|█         | 23/221 [00:11<01:52,  1.76it/s][A
 11%|█         | 24/221 [00:12<01:41,  1.94it/s][A
 11%|█▏        | 25/221 [00:13<02:08,  1.53it/s][A
 12%|█▏        | 26/221 [00:14<02:17,  1.42it/s][A
 12%|█▏        | 27/221 [00:14<01:43,  1.87it/s][A
 13%|█▎        | 28/221 [00:15<02:36,  1.23it/s][A
 13%|█▎        | 29/221 [00:15<02:03,  1.55it/s][A
 14%|█▎        | 30/221 [00:16<02:00,  1.59it/s][A
 14%|█▍        | 31/221 [00:16<01:47,  1.77it/s][A
 14%|█▍        | 32/221 [00:17<01:56,  1.62it/s][A
 15%|█▍        | 33/221 [00:18<02:09,  1.45it/s][A
 15%|█▌        | 34/221 [00:18<01:43,  1.81it/s][A
 16%|█▌        | 35/221 [00:18<01:23,  2.22it/s][A
 16%|█▋        | 36/221 [00:19<01:40,  1.84it/s][A
 17%|█▋        | 37/221 [00:20<01:35,  1.92it/s][A
 17%|█▋        | 38/221 [00:21<02:00,  1.52it/s][A
 18%|█▊        | 39/221 [00:22<02:09,  1.41it/s][A
 18%|█▊        | 40/221 [00:23<02:44,  1.10it/s][A
 19%|█▊        | 41/221 [00:23<02:06,  1.42it/s][A
 19%|█▉        | 42/221 [00:23<01:38,  1.82it/s][A
 19%|█▉        | 43/221 [00:24<01:42,  1.74it/s][A
 20%|█▉        | 44/221 [00:25<01:45,  1.67it/s][A
 21%|██        | 46/221 [00:25<01:11,  2.44it/s][A
 21%|██▏       | 47/221 [00:26<01:18,  2.21it/s][A
 22%|██▏       | 48/221 [00:26<01:04,  2.69it/s][A
 22%|██▏       | 49/221 [00:27<01:30,  1.91it/s][A
 23%|██▎       | 50/221 [00:27<01:22,  2.09it/s][A
 23%|██▎       | 51/221 [00:27<01:15,  2.24it/s][A
 24%|██▎       | 52/221 [00:28<01:06,  2.53it/s][A
 24%|██▍       | 53/221 [00:28<01:06,  2.52it/s][A
 24%|██▍       | 54/221 [00:28<01:08,  2.43it/s][A
 25%|██▍       | 55/221 [00:29<00:54,  3.02it/s][A
 25%|██▌       | 56/221 [00:29<01:12,  2.29it/s][A
 26%|██▌       | 57/221 [00:30<01:03,  2.59it/s][A
 26%|██▌       | 58/221 [00:30<00:53,  3.06it/s][A
 27%|██▋       | 60/221 [00:30<00:50,  3.21it/s][A
 28%|██▊       | 61/221 [00:31<01:11,  2.25it/s][A
 28%|██▊       | 62/221 [00:32<01:09,  2.30it/s][A
 29%|██▊       | 63/221 [00:32<01:18,  2.02it/s][A
 29%|██▉       | 64/221 [00:34<02:09,  1.21it/s][A
 29%|██▉       | 65/221 [00:35<02:25,  1.07it/s][A
 30%|██▉       | 66/221 [00:36<02:05,  1.23it/s][A
 30%|███       | 67/221 [00:36<01:40,  1.53it/s][A
 31%|███       | 68/221 [00:36<01:18,  1.94it/s][A
 32%|███▏      | 70/221 [00:37<01:04,  2.34it/s][A
 32%|███▏      | 71/221 [00:37<01:09,  2.15it/s][A
 33%|███▎      | 72/221 [00:38<01:09,  2.14it/s][A
 33%|███▎      | 73/221 [00:38<01:15,  1.96it/s][A
 33%|███▎      | 74/221 [00:39<01:08,  2.14it/s][A
 34%|███▍      | 75/221 [00:39<01:01,  2.38it/s][A
 34%|███▍      | 76/221 [00:40<01:01,  2.37it/s][A
 35%|███▍      | 77/221 [00:40<00:54,  2.65it/s][A
 35%|███▌      | 78/221 [00:40<01:03,  2.25it/s][A
 36%|███▌      | 79/221 [00:41<01:30,  1.56it/s][A
 36%|███▌      | 80/221 [00:42<01:10,  2.00it/s][A
 37%|███▋      | 81/221 [00:42<01:07,  2.06it/s][A
 37%|███▋      | 82/221 [00:43<01:27,  1.60it/s][A
 38%|███▊      | 83/221 [00:43<01:10,  1.95it/s][A
 38%|███▊      | 84/221 [00:45<01:51,  1.23it/s][A
 39%|███▉      | 86/221 [00:45<01:14,  1.80it/s][A
 39%|███▉      | 87/221 [00:46<01:12,  1.85it/s][A
 40%|███▉      | 88/221 [00:46<01:05,  2.05it/s][A
 40%|████      | 89/221 [00:47<01:16,  1.72it/s][A
 41%|████      | 90/221 [00:48<01:19,  1.65it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:49<01:07,  1.91it/s][A
 43%|████▎     | 94/221 [00:49<00:44,  2.86it/s][A
 43%|████▎     | 95/221 [00:49<00:51,  2.42it/s][A
 43%|████▎     | 96/221 [00:50<00:52,  2.37it/s][A
 44%|████▍     | 97/221 [00:51<01:02,  1.99it/s][A
 44%|████▍     | 98/221 [00:51<01:10,  1.74it/s][A
 45%|████▍     | 99/221 [00:52<01:11,  1.71it/s][A
 45%|████▌     | 100/221 [00:53<01:19,  1.52it/s][A
 46%|████▌     | 101/221 [00:54<01:25,  1.40it/s][A
 46%|████▌     | 102/221 [00:55<01:41,  1.18it/s][A
 47%|████▋     | 103/221 [00:55<01:19,  1.49it/s][A
 47%|████▋     | 104/221 [00:56<01:17,  1.51it/s][A
 48%|████▊     | 105/221 [00:56<01:07,  1.72it/s][A
 48%|████▊     | 106/221 [00:56<00:58,  1.97it/s][A
 48%|████▊     | 107/221 [00:57<00:54,  2.11it/s][A
 49%|████▉     | 108/221 [00:58<01:03,  1.78it/s][A
 49%|████▉     | 109/221 [00:58<00:57,  1.95it/s][A
 50%|████▉     | 110/221 [00:58<00:53,  2.07it/s][A
 50%|█████     | 111/221 [01:00<01:24,  1.30it/s][A
 51%|█████     | 112/221 [01:00<01:17,  1.41it/s][A
 51%|█████     | 113/221 [01:01<01:02,  1.74it/s][A
 52%|█████▏    | 114/221 [01:01<00:58,  1.82it/s][A
 52%|█████▏    | 115/221 [01:02<01:05,  1.62it/s][A
 52%|█████▏    | 116/221 [01:02<00:56,  1.87it/s][A
 53%|█████▎    | 117/221 [01:03<00:49,  2.10it/s][A
 53%|█████▎    | 118/221 [01:03<00:53,  1.91it/s][A
 54%|█████▍    | 119/221 [01:04<00:47,  2.17it/s][A
 54%|█████▍    | 120/221 [01:04<00:47,  2.12it/s][A
 55%|█████▍    | 121/221 [01:05<00:48,  2.06it/s][A
 55%|█████▌    | 122/221 [01:05<00:51,  1.92it/s][A
 56%|█████▌    | 123/221 [01:06<00:49,  1.99it/s][A
 56%|█████▌    | 124/221 [01:07<01:01,  1.58it/s][A
 57%|█████▋    | 125/221 [01:07<00:57,  1.67it/s][A
 57%|█████▋    | 126/221 [01:07<00:44,  2.15it/s][A
 57%|█████▋    | 127/221 [01:08<00:58,  1.62it/s][A
 58%|█████▊    | 128/221 [01:09<00:54,  1.69it/s][A
 58%|█████▊    | 129/221 [01:09<00:52,  1.77it/s][A
 59%|█████▉    | 130/221 [01:09<00:41,  2.20it/s][A
 59%|█████▉    | 131/221 [01:10<00:42,  2.12it/s][A
 60%|█████▉    | 132/221 [01:11<00:53,  1.66it/s][A
 60%|██████    | 133/221 [01:12<01:00,  1.46it/s][A
 61%|██████    | 134/221 [01:12<00:50,  1.74it/s][A
 61%|██████    | 135/221 [01:13<00:45,  1.87it/s][A
 62%|██████▏   | 136/221 [01:13<00:49,  1.72it/s][A
 62%|██████▏   | 137/221 [01:14<00:48,  1.72it/s][A
 62%|██████▏   | 138/221 [01:14<00:51,  1.62it/s][A
 63%|██████▎   | 139/221 [01:16<01:11,  1.14it/s][A
 63%|██████▎   | 140/221 [01:17<01:10,  1.15it/s][A
 64%|██████▍   | 141/221 [01:17<00:58,  1.37it/s][A
 64%|██████▍   | 142/221 [01:18<00:53,  1.49it/s][A
 65%|██████▍   | 143/221 [01:18<00:42,  1.85it/s][A
 65%|██████▌   | 144/221 [01:18<00:37,  2.06it/s][A
 66%|██████▌   | 145/221 [01:19<00:42,  1.81it/s][A
 66%|██████▌   | 146/221 [01:19<00:33,  2.22it/s][A
 67%|██████▋   | 147/221 [01:20<00:33,  2.24it/s][A
 67%|██████▋   | 148/221 [01:20<00:32,  2.26it/s][A
 68%|██████▊   | 150/221 [01:21<00:24,  2.96it/s][A
 69%|██████▉   | 152/221 [01:21<00:19,  3.58it/s][A
 69%|██████▉   | 153/221 [01:21<00:19,  3.50it/s][A
 70%|██████▉   | 154/221 [01:22<00:24,  2.71it/s][A
 70%|███████   | 155/221 [01:23<00:36,  1.80it/s][A
 71%|███████   | 156/221 [01:23<00:33,  1.96it/s][A
 71%|███████   | 157/221 [01:24<00:32,  1.98it/s][A
 71%|███████▏  | 158/221 [01:24<00:29,  2.16it/s][A
 72%|███████▏  | 160/221 [01:25<00:22,  2.70it/s][A
 73%|███████▎  | 161/221 [01:25<00:19,  3.10it/s][A
 73%|███████▎  | 162/221 [01:25<00:20,  2.90it/s][A
 74%|███████▍  | 163/221 [01:26<00:17,  3.23it/s][A
 74%|███████▍  | 164/221 [01:26<00:16,  3.36it/s][A
 75%|███████▍  | 165/221 [01:26<00:14,  3.75it/s][A
 75%|███████▌  | 166/221 [01:27<00:20,  2.70it/s][A
 76%|███████▌  | 167/221 [01:27<00:18,  2.88it/s][A
 76%|███████▌  | 168/221 [01:27<00:16,  3.20it/s][A
 76%|███████▋  | 169/221 [01:28<00:29,  1.75it/s][A
 77%|███████▋  | 170/221 [01:29<00:29,  1.72it/s][A
 77%|███████▋  | 171/221 [01:30<00:29,  1.71it/s][A
 78%|███████▊  | 172/221 [01:30<00:23,  2.07it/s][A
 78%|███████▊  | 173/221 [01:30<00:21,  2.22it/s][A
 79%|███████▊  | 174/221 [01:31<00:20,  2.24it/s][A
 79%|███████▉  | 175/221 [01:31<00:22,  2.06it/s][A
 80%|███████▉  | 176/221 [01:32<00:20,  2.22it/s][A
 80%|████████  | 177/221 [01:32<00:20,  2.19it/s][A
 81%|████████  | 178/221 [01:32<00:19,  2.16it/s][A
 81%|████████  | 179/221 [01:33<00:20,  2.02it/s][A
 81%|████████▏ | 180/221 [01:33<00:17,  2.39it/s][A
 82%|████████▏ | 181/221 [01:34<00:15,  2.58it/s][A
 82%|████████▏ | 182/221 [01:34<00:17,  2.18it/s][A
 83%|████████▎ | 183/221 [01:35<00:17,  2.18it/s][A
 83%|████████▎ | 184/221 [01:36<00:26,  1.42it/s][A
 84%|████████▎ | 185/221 [01:37<00:25,  1.40it/s][A
 85%|████████▍ | 187/221 [01:37<00:18,  1.84it/s][A
 85%|████████▌ | 188/221 [01:38<00:14,  2.23it/s][A
 86%|████████▌ | 189/221 [01:38<00:14,  2.17it/s][A
 86%|████████▌ | 190/221 [01:39<00:15,  2.05it/s][A
 86%|████████▋ | 191/221 [01:39<00:16,  1.80it/s][A
 87%|████████▋ | 192/221 [01:40<00:14,  1.97it/s][A
 87%|████████▋ | 193/221 [01:40<00:12,  2.28it/s][A
 88%|████████▊ | 194/221 [01:41<00:13,  1.97it/s][A
 88%|████████▊ | 195/221 [01:42<00:20,  1.29it/s][A
 89%|████████▊ | 196/221 [01:43<00:17,  1.43it/s][A
 89%|████████▉ | 197/221 [01:43<00:17,  1.36it/s][A
 90%|████████▉ | 198/221 [01:44<00:14,  1.53it/s][A
 90%|█████████ | 199/221 [01:44<00:12,  1.77it/s][A
 90%|█████████ | 200/221 [01:45<00:14,  1.47it/s][A
 91%|█████████ | 201/221 [01:46<00:12,  1.55it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.83it/s][A
 92%|█████████▏| 203/221 [01:47<00:10,  1.75it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  2.02it/s][A
 93%|█████████▎| 205/221 [01:47<00:06,  2.39it/s][A
 93%|█████████▎| 206/221 [01:48<00:05,  2.64it/s][A
 94%|█████████▎| 207/221 [01:48<00:04,  3.15it/s][A
 94%|█████████▍| 208/221 [01:49<00:08,  1.48it/s][A
 95%|█████████▍| 209/221 [01:50<00:07,  1.67it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.93it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.78it/s][A
 96%|█████████▌| 212/221 [01:51<00:04,  2.15it/s][A
 96%|█████████▋| 213/221 [01:51<00:03,  2.11it/s][A
 97%|█████████▋| 214/221 [01:52<00:03,  1.93it/s][A
 97%|█████████▋| 215/221 [01:52<00:02,  2.47it/s][A
 98%|█████████▊| 216/221 [01:53<00:02,  2.06it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.79it/s][A
 99%|█████████▊| 218/221 [01:54<00:01,  2.22it/s][A
 99%|█████████▉| 219/221 [01:54<00:00,  2.15it/s][A
100%|█████████▉| 220/221 [01:55<00:00,  2.37it/s][A
100%|██████████| 221/221 [01:55<00:00,  2.23it/s][A100%|██████████| 221/221 [01:55<00:00,  1.91it/s]
09/19/2024 09:40:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 5499--===========

09/19/2024 09:40:37 - INFO - __main__ -   {'area_r1': 45.8, 'area_recall': '45.8/74.4/83.9', 'area_ravg': 68.1}
09/19/2024 09:40:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 5499--===========

09/19/2024 09:40:37 - INFO - __main__ -   {'forward_r1': 50.6, 'forward_recall': '50.6/78.8/87.3', 'forward_ravg': 72.2}
09/19/2024 09:40:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 5499--===========

09/19/2024 09:40:37 - INFO - __main__ -   {'area_video_r1': 48.9, 'area_video_recall': '48.9/79.2/87.7', 'area_video_ravg': 71.9}
09/19/2024 09:40:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 09:40:37 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 09:40:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 5499--===========

09/19/2024 09:40:37 - INFO - __main__ -   {'area_video_r1': 63.5, 'area_video_recall': '63.5/83.7/89.0', 'area_video_ravg': 78.7, 'area_video_back_r1': 62.9, 'area_video_back_recall': '62.9/85.1/91.1', 'area_video_back_ravg': 79.7}
09/19/2024 09:40:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 5499=======

09/19/2024 09:40:37 - INFO - __main__ -   {'area_video_r1': 63.5, 'area_video_recall': '63.5/83.7/89.0', 'area_video_ravg': 78.7, 'area_video_back_r1': 62.9, 'area_video_back_recall': '62.9/85.1/91.1', 'area_video_back_ravg': 79.7}
09/19/2024 09:40:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 5499--===========

09/19/2024 09:40:37 - INFO - __main__ -   {'video_r1': 30.4, 'video_recall': '30.4/55.5/66.2', 'video_ravg': 50.7}
09/19/2024 09:40:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 09:40:37 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 09:40:37 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 5499--===========

09/19/2024 09:40:37 - INFO - __main__ -   {'video_r1': 60.4, 'video_recall': '60.4/79.9/84.0', 'video_ravg': 74.8}
09/19/2024 09:40:37 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 5499=======

09/19/2024 09:40:37 - INFO - __main__ -   {'video_r1': 60.4, 'video_recall': '60.4/79.9/84.0', 'video_ravg': 74.8}
09/19/2024 09:41:08 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02665841393172741, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2730467319488525, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.299705147743225}
 62%|██████▏   | 5500/8917 [7:26:34<161:11:00, 169.82s/it] 62%|██████▏   | 5501/8917 [7:26:37<113:40:33, 119.80s/it] 62%|██████▏   | 5502/8917 [7:26:40<80:34:01, 84.93s/it]   62%|██████▏   | 5503/8917 [7:26:44<57:31:18, 60.66s/it] 62%|██████▏   | 5504/8917 [7:26:48<41:19:31, 43.59s/it] 62%|██████▏   | 5505/8917 [7:26:52<29:55:52, 31.58s/it] 62%|██████▏   | 5506/8917 [7:26:56<22:05:29, 23.32s/it] 62%|██████▏   | 5507/8917 [7:26:59<16:29:58, 17.42s/it] 62%|██████▏   | 5508/8917 [7:27:03<12:32:02, 13.24s/it] 62%|██████▏   | 5509/8917 [7:27:07<9:49:42, 10.38s/it]  62%|██████▏   | 5510/8917 [7:27:10<7:55:53,  8.38s/it] 62%|██████▏   | 5511/8917 [7:27:14<6:41:31,  7.07s/it] 62%|██████▏   | 5512/8917 [7:27:18<5:39:14,  5.98s/it] 62%|██████▏   | 5513/8917 [7:27:22<5:00:44,  5.30s/it] 62%|██████▏   | 5514/8917 [7:27:26<4:42:36,  4.98s/it] 62%|██████▏   | 5515/8917 [7:27:29<4:18:29,  4.56s/it] 62%|██████▏   | 5516/8917 [7:27:33<4:04:28,  4.31s/it] 62%|██████▏   | 5517/8917 [7:27:37<3:52:46,  4.11s/it] 62%|██████▏   | 5518/8917 [7:27:41<3:46:51,  4.00s/it] 62%|██████▏   | 5519/8917 [7:27:44<3:39:46,  3.88s/it] 62%|██████▏   | 5520/8917 [7:27:48<3:35:36,  3.81s/it] 62%|██████▏   | 5521/8917 [7:27:51<3:31:21,  3.73s/it] 62%|██████▏   | 5522/8917 [7:27:55<3:29:12,  3.70s/it] 62%|██████▏   | 5523/8917 [7:27:59<3:32:38,  3.76s/it] 62%|██████▏   | 5524/8917 [7:28:03<3:37:39,  3.85s/it] 62%|██████▏   | 5525/8917 [7:28:07<3:37:55,  3.85s/it] 62%|██████▏   | 5526/8917 [7:28:11<3:38:59,  3.87s/it] 62%|██████▏   | 5527/8917 [7:28:15<3:39:22,  3.88s/it] 62%|██████▏   | 5528/8917 [7:28:18<3:36:46,  3.84s/it] 62%|██████▏   | 5529/8917 [7:28:23<3:45:13,  3.99s/it] 62%|██████▏   | 5530/8917 [7:28:26<3:36:55,  3.84s/it] 62%|██████▏   | 5531/8917 [7:28:30<3:29:55,  3.72s/it] 62%|██████▏   | 5532/8917 [7:28:34<3:36:54,  3.84s/it] 62%|██████▏   | 5533/8917 [7:28:37<3:32:24,  3.77s/it] 62%|██████▏   | 5534/8917 [7:28:41<3:25:52,  3.65s/it] 62%|██████▏   | 5535/8917 [7:28:44<3:25:24,  3.64s/it] 62%|██████▏   | 5536/8917 [7:28:48<3:20:25,  3.56s/it] 62%|██████▏   | 5537/8917 [7:28:52<3:27:05,  3.68s/it] 62%|██████▏   | 5538/8917 [7:28:56<3:33:23,  3.79s/it] 62%|██████▏   | 5539/8917 [7:28:59<3:34:00,  3.80s/it] 62%|██████▏   | 5540/8917 [7:29:03<3:34:15,  3.81s/it] 62%|██████▏   | 5541/8917 [7:29:07<3:31:28,  3.76s/it] 62%|██████▏   | 5542/8917 [7:29:11<3:30:09,  3.74s/it] 62%|██████▏   | 5543/8917 [7:29:14<3:28:02,  3.70s/it] 62%|██████▏   | 5544/8917 [7:29:18<3:28:20,  3.71s/it] 62%|██████▏   | 5545/8917 [7:29:22<3:30:44,  3.75s/it] 62%|██████▏   | 5546/8917 [7:29:25<3:27:22,  3.69s/it] 62%|██████▏   | 5547/8917 [7:29:29<3:23:31,  3.62s/it] 62%|██████▏   | 5548/8917 [7:29:32<3:21:07,  3.58s/it] 62%|██████▏   | 5549/8917 [7:29:36<3:24:53,  3.65s/it]09/19/2024 09:44:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.048179104924201965, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.7280189990997314, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.776198148727417}
 62%|██████▏   | 5550/8917 [7:29:40<3:23:53,  3.63s/it] 62%|██████▏   | 5551/8917 [7:29:44<3:33:17,  3.80s/it] 62%|██████▏   | 5552/8917 [7:29:48<3:34:05,  3.82s/it] 62%|██████▏   | 5553/8917 [7:29:51<3:30:50,  3.76s/it] 62%|██████▏   | 5554/8917 [7:29:55<3:27:56,  3.71s/it] 62%|██████▏   | 5555/8917 [7:29:59<3:28:13,  3.72s/it] 62%|██████▏   | 5556/8917 [7:30:03<3:30:47,  3.76s/it] 62%|██████▏   | 5557/8917 [7:30:06<3:29:17,  3.74s/it] 62%|██████▏   | 5558/8917 [7:30:10<3:25:16,  3.67s/it] 62%|██████▏   | 5559/8917 [7:30:13<3:24:10,  3.65s/it] 62%|██████▏   | 5560/8917 [7:30:17<3:27:17,  3.70s/it] 62%|██████▏   | 5561/8917 [7:30:21<3:29:26,  3.74s/it] 62%|██████▏   | 5562/8917 [7:30:25<3:28:03,  3.72s/it] 62%|██████▏   | 5563/8917 [7:30:28<3:27:45,  3.72s/it] 62%|██████▏   | 5564/8917 [7:30:33<3:36:31,  3.87s/it] 62%|██████▏   | 5565/8917 [7:30:36<3:30:04,  3.76s/it] 62%|██████▏   | 5566/8917 [7:30:40<3:33:47,  3.83s/it] 62%|██████▏   | 5567/8917 [7:30:44<3:34:52,  3.85s/it] 62%|██████▏   | 5568/8917 [7:30:47<3:25:42,  3.69s/it] 62%|██████▏   | 5569/8917 [7:30:51<3:26:22,  3.70s/it] 62%|██████▏   | 5570/8917 [7:30:55<3:34:15,  3.84s/it] 62%|██████▏   | 5571/8917 [7:30:59<3:30:21,  3.77s/it] 62%|██████▏   | 5572/8917 [7:31:03<3:29:13,  3.75s/it] 62%|██████▏   | 5573/8917 [7:31:06<3:28:28,  3.74s/it] 63%|██████▎   | 5574/8917 [7:31:10<3:28:24,  3.74s/it] 63%|██████▎   | 5575/8917 [7:31:14<3:26:42,  3.71s/it] 63%|██████▎   | 5576/8917 [7:31:17<3:25:48,  3.70s/it] 63%|██████▎   | 5577/8917 [7:31:21<3:29:02,  3.76s/it] 63%|██████▎   | 5578/8917 [7:31:25<3:31:22,  3.80s/it] 63%|██████▎   | 5579/8917 [7:31:29<3:27:08,  3.72s/it] 63%|██████▎   | 5580/8917 [7:31:33<3:29:38,  3.77s/it] 63%|██████▎   | 5581/8917 [7:31:36<3:30:42,  3.79s/it] 63%|██████▎   | 5582/8917 [7:31:40<3:26:01,  3.71s/it] 63%|██████▎   | 5583/8917 [7:31:44<3:31:39,  3.81s/it] 63%|██████▎   | 5584/8917 [7:31:48<3:30:53,  3.80s/it] 63%|██████▎   | 5585/8917 [7:31:51<3:28:05,  3.75s/it] 63%|██████▎   | 5586/8917 [7:31:55<3:25:25,  3.70s/it] 63%|██████▎   | 5587/8917 [7:31:59<3:24:54,  3.69s/it] 63%|██████▎   | 5588/8917 [7:32:03<3:28:54,  3.77s/it] 63%|██████▎   | 5589/8917 [7:32:06<3:29:43,  3.78s/it] 63%|██████▎   | 5590/8917 [7:32:10<3:28:00,  3.75s/it] 63%|██████▎   | 5591/8917 [7:32:14<3:26:10,  3.72s/it] 63%|██████▎   | 5592/8917 [7:32:17<3:23:47,  3.68s/it] 63%|██████▎   | 5593/8917 [7:32:21<3:16:20,  3.54s/it] 63%|██████▎   | 5594/8917 [7:32:24<3:16:35,  3.55s/it] 63%|██████▎   | 5595/8917 [7:32:28<3:25:36,  3.71s/it] 63%|██████▎   | 5596/8917 [7:32:32<3:32:22,  3.84s/it] 63%|██████▎   | 5597/8917 [7:32:36<3:34:08,  3.87s/it] 63%|██████▎   | 5598/8917 [7:32:40<3:37:22,  3.93s/it] 63%|██████▎   | 5599/8917 [7:32:44<3:32:42,  3.85s/it]09/19/2024 09:47:22 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.014761949889361858, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0537328720092773, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0684947967529297}
 63%|██████▎   | 5600/8917 [7:32:48<3:35:02,  3.89s/it] 63%|██████▎   | 5601/8917 [7:32:52<3:31:00,  3.82s/it] 63%|██████▎   | 5602/8917 [7:32:55<3:28:36,  3.78s/it] 63%|██████▎   | 5603/8917 [7:32:59<3:23:52,  3.69s/it] 63%|██████▎   | 5604/8917 [7:33:03<3:25:22,  3.72s/it] 63%|██████▎   | 5605/8917 [7:33:06<3:25:23,  3.72s/it] 63%|██████▎   | 5606/8917 [7:33:10<3:20:21,  3.63s/it] 63%|██████▎   | 5607/8917 [7:33:14<3:23:24,  3.69s/it] 63%|██████▎   | 5608/8917 [7:33:17<3:20:13,  3.63s/it] 63%|██████▎   | 5609/8917 [7:33:21<3:27:30,  3.76s/it] 63%|██████▎   | 5610/8917 [7:33:25<3:29:05,  3.79s/it] 63%|██████▎   | 5611/8917 [7:33:29<3:25:51,  3.74s/it] 63%|██████▎   | 5612/8917 [7:33:32<3:22:55,  3.68s/it] 63%|██████▎   | 5613/8917 [7:33:36<3:27:27,  3.77s/it] 63%|██████▎   | 5614/8917 [7:33:40<3:23:38,  3.70s/it] 63%|██████▎   | 5615/8917 [7:33:43<3:18:15,  3.60s/it] 63%|██████▎   | 5616/8917 [7:33:47<3:23:28,  3.70s/it] 63%|██████▎   | 5617/8917 [7:33:51<3:26:49,  3.76s/it] 63%|██████▎   | 5618/8917 [7:33:54<3:23:49,  3.71s/it] 63%|██████▎   | 5619/8917 [7:33:58<3:23:36,  3.70s/it] 63%|██████▎   | 5620/8917 [7:34:02<3:24:57,  3.73s/it] 63%|██████▎   | 5621/8917 [7:34:05<3:19:00,  3.62s/it] 63%|██████▎   | 5622/8917 [7:34:09<3:26:22,  3.76s/it] 63%|██████▎   | 5623/8917 [7:34:13<3:31:55,  3.86s/it] 63%|██████▎   | 5624/8917 [7:34:17<3:28:38,  3.80s/it] 63%|██████▎   | 5625/8917 [7:34:21<3:27:29,  3.78s/it] 63%|██████▎   | 5626/8917 [7:34:24<3:21:19,  3.67s/it] 63%|██████▎   | 5627/8917 [7:34:28<3:22:37,  3.70s/it] 63%|██████▎   | 5628/8917 [7:34:32<3:26:08,  3.76s/it] 63%|██████▎   | 5629/8917 [7:34:36<3:24:58,  3.74s/it] 63%|██████▎   | 5630/8917 [7:34:39<3:23:05,  3.71s/it] 63%|██████▎   | 5631/8917 [7:34:43<3:20:13,  3.66s/it] 63%|██████▎   | 5632/8917 [7:34:46<3:20:35,  3.66s/it] 63%|██████▎   | 5633/8917 [7:34:50<3:22:41,  3.70s/it] 63%|██████▎   | 5634/8917 [7:34:54<3:23:53,  3.73s/it] 63%|██████▎   | 5635/8917 [7:34:58<3:20:24,  3.66s/it] 63%|██████▎   | 5636/8917 [7:35:02<3:29:21,  3.83s/it] 63%|██████▎   | 5637/8917 [7:35:06<3:27:48,  3.80s/it] 63%|██████▎   | 5638/8917 [7:35:09<3:29:26,  3.83s/it] 63%|██████▎   | 5639/8917 [7:35:13<3:24:56,  3.75s/it] 63%|██████▎   | 5640/8917 [7:35:17<3:22:47,  3.71s/it] 63%|██████▎   | 5641/8917 [7:35:21<3:28:10,  3.81s/it] 63%|██████▎   | 5642/8917 [7:35:24<3:22:45,  3.71s/it] 63%|██████▎   | 5643/8917 [7:35:28<3:20:55,  3.68s/it] 63%|██████▎   | 5644/8917 [7:35:31<3:18:23,  3.64s/it] 63%|██████▎   | 5645/8917 [7:35:35<3:18:53,  3.65s/it] 63%|██████▎   | 5646/8917 [7:35:38<3:14:11,  3.56s/it] 63%|██████▎   | 5647/8917 [7:35:42<3:17:06,  3.62s/it] 63%|██████▎   | 5648/8917 [7:35:46<3:28:51,  3.83s/it] 63%|██████▎   | 5649/8917 [7:35:51<3:35:10,  3.95s/it]09/19/2024 09:50:28 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.009723576717078686, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.915479302406311, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9252029061317444}
 63%|██████▎   | 5650/8917 [7:35:54<3:28:04,  3.82s/it] 63%|██████▎   | 5651/8917 [7:35:57<3:18:52,  3.65s/it] 63%|██████▎   | 5652/8917 [7:36:01<3:23:37,  3.74s/it] 63%|██████▎   | 5653/8917 [7:36:05<3:25:47,  3.78s/it] 63%|██████▎   | 5654/8917 [7:36:09<3:24:25,  3.76s/it] 63%|██████▎   | 5655/8917 [7:36:13<3:23:31,  3.74s/it] 63%|██████▎   | 5656/8917 [7:36:17<3:25:49,  3.79s/it] 63%|██████▎   | 5657/8917 [7:36:20<3:28:49,  3.84s/it] 63%|██████▎   | 5658/8917 [7:36:24<3:24:18,  3.76s/it] 63%|██████▎   | 5659/8917 [7:36:28<3:28:57,  3.85s/it] 63%|██████▎   | 5660/8917 [7:36:32<3:22:28,  3.73s/it] 63%|██████▎   | 5661/8917 [7:36:35<3:16:46,  3.63s/it] 63%|██████▎   | 5662/8917 [7:36:39<3:19:23,  3.68s/it] 64%|██████▎   | 5663/8917 [7:36:43<3:22:48,  3.74s/it] 64%|██████▎   | 5664/8917 [7:36:46<3:23:52,  3.76s/it] 64%|██████▎   | 5665/8917 [7:36:50<3:21:28,  3.72s/it] 64%|██████▎   | 5666/8917 [7:36:54<3:28:30,  3.85s/it] 64%|██████▎   | 5667/8917 [7:36:58<3:26:46,  3.82s/it] 64%|██████▎   | 5668/8917 [7:37:02<3:27:31,  3.83s/it] 64%|██████▎   | 5669/8917 [7:37:06<3:28:48,  3.86s/it] 64%|██████▎   | 5670/8917 [7:37:09<3:20:05,  3.70s/it] 64%|██████▎   | 5671/8917 [7:37:13<3:18:35,  3.67s/it] 64%|██████▎   | 5672/8917 [7:37:16<3:18:25,  3.67s/it] 64%|██████▎   | 5673/8917 [7:37:20<3:21:12,  3.72s/it] 64%|██████▎   | 5674/8917 [7:37:24<3:18:22,  3.67s/it] 64%|██████▎   | 5675/8917 [7:37:28<3:24:02,  3.78s/it] 64%|██████▎   | 5676/8917 [7:37:31<3:17:57,  3.66s/it] 64%|██████▎   | 5677/8917 [7:37:35<3:25:23,  3.80s/it] 64%|██████▎   | 5678/8917 [7:37:39<3:26:08,  3.82s/it] 64%|██████▎   | 5679/8917 [7:37:43<3:20:22,  3.71s/it] 64%|██████▎   | 5680/8917 [7:37:46<3:19:54,  3.71s/it] 64%|██████▎   | 5681/8917 [7:37:50<3:22:10,  3.75s/it] 64%|██████▎   | 5682/8917 [7:37:54<3:24:56,  3.80s/it] 64%|██████▎   | 5683/8917 [7:37:58<3:21:55,  3.75s/it] 64%|██████▎   | 5684/8917 [7:38:01<3:19:25,  3.70s/it] 64%|██████▍   | 5685/8917 [7:38:05<3:25:03,  3.81s/it] 64%|██████▍   | 5686/8917 [7:38:09<3:25:20,  3.81s/it] 64%|██████▍   | 5687/8917 [7:38:13<3:26:06,  3.83s/it] 64%|██████▍   | 5688/8917 [7:38:17<3:25:15,  3.81s/it] 64%|██████▍   | 5689/8917 [7:38:21<3:26:27,  3.84s/it] 64%|██████▍   | 5690/8917 [7:38:24<3:23:12,  3.78s/it] 64%|██████▍   | 5691/8917 [7:38:28<3:21:37,  3.75s/it] 64%|██████▍   | 5692/8917 [7:38:31<3:15:42,  3.64s/it] 64%|██████▍   | 5693/8917 [7:38:35<3:15:04,  3.63s/it] 64%|██████▍   | 5694/8917 [7:38:39<3:19:39,  3.72s/it] 64%|██████▍   | 5695/8917 [7:38:43<3:19:46,  3.72s/it] 64%|██████▍   | 5696/8917 [7:38:46<3:19:42,  3.72s/it] 64%|██████▍   | 5697/8917 [7:38:51<3:27:00,  3.86s/it] 64%|██████▍   | 5698/8917 [7:38:54<3:25:33,  3.83s/it] 64%|██████▍   | 5699/8917 [7:38:58<3:24:21,  3.81s/it]09/19/2024 09:53:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01120070181787014, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9285187125205994, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9397194385528564}
 64%|██████▍   | 5700/8917 [7:39:02<3:19:27,  3.72s/it] 64%|██████▍   | 5701/8917 [7:39:06<3:25:27,  3.83s/it] 64%|██████▍   | 5702/8917 [7:39:09<3:23:00,  3.79s/it] 64%|██████▍   | 5703/8917 [7:39:13<3:23:13,  3.79s/it] 64%|██████▍   | 5704/8917 [7:39:17<3:24:55,  3.83s/it] 64%|██████▍   | 5705/8917 [7:39:21<3:23:12,  3.80s/it] 64%|██████▍   | 5706/8917 [7:39:25<3:23:20,  3.80s/it] 64%|██████▍   | 5707/8917 [7:39:29<3:25:43,  3.85s/it] 64%|██████▍   | 5708/8917 [7:39:32<3:22:38,  3.79s/it] 64%|██████▍   | 5709/8917 [7:39:36<3:18:16,  3.71s/it] 64%|██████▍   | 5710/8917 [7:39:39<3:15:55,  3.67s/it] 64%|██████▍   | 5711/8917 [7:39:43<3:13:32,  3.62s/it] 64%|██████▍   | 5712/8917 [7:39:47<3:17:11,  3.69s/it] 64%|██████▍   | 5713/8917 [7:39:51<3:21:46,  3.78s/it] 64%|██████▍   | 5714/8917 [7:39:54<3:21:57,  3.78s/it] 64%|██████▍   | 5715/8917 [7:39:58<3:21:11,  3.77s/it] 64%|██████▍   | 5716/8917 [7:40:02<3:19:42,  3.74s/it] 64%|██████▍   | 5717/8917 [7:40:06<3:18:17,  3.72s/it] 64%|██████▍   | 5718/8917 [7:40:09<3:17:41,  3.71s/it] 64%|██████▍   | 5719/8917 [7:40:13<3:20:25,  3.76s/it] 64%|██████▍   | 5720/8917 [7:40:17<3:21:01,  3.77s/it] 64%|██████▍   | 5721/8917 [7:40:21<3:18:59,  3.74s/it] 64%|██████▍   | 5722/8917 [7:40:24<3:19:55,  3.75s/it] 64%|██████▍   | 5723/8917 [7:40:28<3:19:09,  3.74s/it] 64%|██████▍   | 5724/8917 [7:40:32<3:22:03,  3.80s/it] 64%|██████▍   | 5725/8917 [7:40:36<3:20:14,  3.76s/it] 64%|██████▍   | 5726/8917 [7:40:39<3:17:08,  3.71s/it] 64%|██████▍   | 5727/8917 [7:40:43<3:21:35,  3.79s/it] 64%|██████▍   | 5728/8917 [7:40:47<3:18:29,  3.73s/it] 64%|██████▍   | 5729/8917 [7:40:51<3:18:56,  3.74s/it] 64%|██████▍   | 5730/8917 [7:40:54<3:20:50,  3.78s/it] 64%|██████▍   | 5731/8917 [7:40:58<3:22:38,  3.82s/it] 64%|██████▍   | 5732/8917 [7:41:02<3:20:30,  3.78s/it] 64%|██████▍   | 5733/8917 [7:41:06<3:17:50,  3.73s/it] 64%|██████▍   | 5734/8917 [7:41:10<3:20:48,  3.79s/it] 64%|██████▍   | 5735/8917 [7:41:13<3:17:57,  3.73s/it] 64%|██████▍   | 5736/8917 [7:41:17<3:14:04,  3.66s/it] 64%|██████▍   | 5737/8917 [7:41:20<3:14:58,  3.68s/it] 64%|██████▍   | 5738/8917 [7:41:24<3:12:59,  3.64s/it] 64%|██████▍   | 5739/8917 [7:41:28<3:19:39,  3.77s/it] 64%|██████▍   | 5740/8917 [7:41:32<3:22:42,  3.83s/it] 64%|██████▍   | 5741/8917 [7:41:36<3:19:15,  3.76s/it] 64%|██████▍   | 5742/8917 [7:41:40<3:21:24,  3.81s/it] 64%|██████▍   | 5743/8917 [7:41:43<3:17:58,  3.74s/it] 64%|██████▍   | 5744/8917 [7:41:46<3:11:44,  3.63s/it] 64%|██████▍   | 5745/8917 [7:41:50<3:11:54,  3.63s/it] 64%|██████▍   | 5746/8917 [7:41:54<3:16:43,  3.72s/it] 64%|██████▍   | 5747/8917 [7:41:58<3:20:18,  3.79s/it] 64%|██████▍   | 5748/8917 [7:42:02<3:27:40,  3.93s/it] 64%|██████▍   | 5749/8917 [7:42:06<3:20:33,  3.80s/it]09/19/2024 09:56:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02388516068458557, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2844632863998413, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3083484172821045}
 64%|██████▍   | 5750/8917 [7:42:10<3:23:42,  3.86s/it] 64%|██████▍   | 5751/8917 [7:42:13<3:20:04,  3.79s/it] 65%|██████▍   | 5752/8917 [7:42:17<3:14:45,  3.69s/it] 65%|██████▍   | 5753/8917 [7:42:21<3:20:21,  3.80s/it] 65%|██████▍   | 5754/8917 [7:42:25<3:18:14,  3.76s/it] 65%|██████▍   | 5755/8917 [7:42:28<3:20:20,  3.80s/it] 65%|██████▍   | 5756/8917 [7:42:32<3:20:56,  3.81s/it] 65%|██████▍   | 5757/8917 [7:42:36<3:12:31,  3.66s/it] 65%|██████▍   | 5758/8917 [7:42:40<3:21:50,  3.83s/it] 65%|██████▍   | 5759/8917 [7:42:44<3:21:35,  3.83s/it] 65%|██████▍   | 5760/8917 [7:42:47<3:16:06,  3.73s/it] 65%|██████▍   | 5761/8917 [7:42:51<3:17:23,  3.75s/it] 65%|██████▍   | 5762/8917 [7:42:55<3:15:43,  3.72s/it] 65%|██████▍   | 5763/8917 [7:42:59<3:18:37,  3.78s/it] 65%|██████▍   | 5764/8917 [7:43:02<3:17:59,  3.77s/it] 65%|██████▍   | 5765/8917 [7:43:06<3:19:19,  3.79s/it] 65%|██████▍   | 5766/8917 [7:43:10<3:15:32,  3.72s/it] 65%|██████▍   | 5767/8917 [7:43:13<3:13:20,  3.68s/it] 65%|██████▍   | 5768/8917 [7:43:17<3:17:13,  3.76s/it] 65%|██████▍   | 5769/8917 [7:43:21<3:13:22,  3.69s/it] 65%|██████▍   | 5770/8917 [7:43:25<3:17:21,  3.76s/it] 65%|██████▍   | 5771/8917 [7:43:28<3:16:14,  3.74s/it] 65%|██████▍   | 5772/8917 [7:43:32<3:14:09,  3.70s/it] 65%|██████▍   | 5773/8917 [7:43:36<3:19:16,  3.80s/it] 65%|██████▍   | 5774/8917 [7:43:40<3:16:04,  3.74s/it] 65%|██████▍   | 5775/8917 [7:43:43<3:11:31,  3.66s/it] 65%|██████▍   | 5776/8917 [7:43:47<3:14:50,  3.72s/it] 65%|██████▍   | 5777/8917 [7:43:51<3:13:35,  3.70s/it] 65%|██████▍   | 5778/8917 [7:43:55<3:18:39,  3.80s/it] 65%|██████▍   | 5779/8917 [7:43:58<3:17:12,  3.77s/it] 65%|██████▍   | 5780/8917 [7:44:02<3:22:50,  3.88s/it] 65%|██████▍   | 5781/8917 [7:44:06<3:16:14,  3.75s/it] 65%|██████▍   | 5782/8917 [7:44:10<3:13:33,  3.70s/it] 65%|██████▍   | 5783/8917 [7:44:13<3:15:41,  3.75s/it] 65%|██████▍   | 5784/8917 [7:44:17<3:17:22,  3.78s/it] 65%|██████▍   | 5785/8917 [7:44:21<3:11:55,  3.68s/it] 65%|██████▍   | 5786/8917 [7:44:24<3:09:32,  3.63s/it] 65%|██████▍   | 5787/8917 [7:44:28<3:13:24,  3.71s/it] 65%|██████▍   | 5788/8917 [7:44:32<3:11:48,  3.68s/it] 65%|██████▍   | 5789/8917 [7:44:35<3:10:54,  3.66s/it] 65%|██████▍   | 5790/8917 [7:44:39<3:09:17,  3.63s/it] 65%|██████▍   | 5791/8917 [7:44:43<3:11:40,  3.68s/it] 65%|██████▍   | 5792/8917 [7:44:47<3:15:11,  3.75s/it] 65%|██████▍   | 5793/8917 [7:44:51<3:18:17,  3.81s/it] 65%|██████▍   | 5794/8917 [7:44:54<3:18:11,  3.81s/it] 65%|██████▍   | 5795/8917 [7:44:58<3:19:56,  3.84s/it] 65%|██████▍   | 5796/8917 [7:45:02<3:13:21,  3.72s/it] 65%|██████▌   | 5797/8917 [7:45:05<3:07:42,  3.61s/it] 65%|██████▌   | 5798/8917 [7:45:09<3:12:57,  3.71s/it] 65%|██████▌   | 5799/8917 [7:45:13<3:12:52,  3.71s/it]09/19/2024 09:59:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03285292908549309, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.4743971824645996, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5072500705718994}
 65%|██████▌   | 5800/8917 [7:45:16<3:12:23,  3.70s/it] 65%|██████▌   | 5801/8917 [7:45:20<3:18:48,  3.83s/it] 65%|██████▌   | 5802/8917 [7:45:24<3:14:39,  3.75s/it] 65%|██████▌   | 5803/8917 [7:45:28<3:12:17,  3.71s/it] 65%|██████▌   | 5804/8917 [7:45:31<3:13:05,  3.72s/it] 65%|██████▌   | 5805/8917 [7:45:35<3:08:04,  3.63s/it] 65%|██████▌   | 5806/8917 [7:45:39<3:11:35,  3.70s/it] 65%|██████▌   | 5807/8917 [7:45:42<3:10:00,  3.67s/it] 65%|██████▌   | 5808/8917 [7:45:46<3:14:22,  3.75s/it] 65%|██████▌   | 5809/8917 [7:45:50<3:14:32,  3.76s/it] 65%|██████▌   | 5810/8917 [7:45:54<3:13:57,  3.75s/it] 65%|██████▌   | 5811/8917 [7:45:57<3:09:22,  3.66s/it] 65%|██████▌   | 5812/8917 [7:46:01<3:08:20,  3.64s/it] 65%|██████▌   | 5813/8917 [7:46:04<3:08:37,  3.65s/it] 65%|██████▌   | 5814/8917 [7:46:08<3:10:45,  3.69s/it] 65%|██████▌   | 5815/8917 [7:46:12<3:12:49,  3.73s/it] 65%|██████▌   | 5816/8917 [7:46:16<3:08:42,  3.65s/it] 65%|██████▌   | 5817/8917 [7:46:19<3:08:00,  3.64s/it] 65%|██████▌   | 5818/8917 [7:46:23<3:08:57,  3.66s/it] 65%|██████▌   | 5819/8917 [7:46:26<3:05:03,  3.58s/it] 65%|██████▌   | 5820/8917 [7:46:30<3:02:45,  3.54s/it] 65%|██████▌   | 5821/8917 [7:46:34<3:13:42,  3.75s/it] 65%|██████▌   | 5822/8917 [7:46:37<3:08:54,  3.66s/it] 65%|██████▌   | 5823/8917 [7:46:41<3:12:36,  3.74s/it] 65%|██████▌   | 5824/8917 [7:46:45<3:13:38,  3.76s/it] 65%|██████▌   | 5825/8917 [7:46:49<3:13:10,  3.75s/it] 65%|██████▌   | 5826/8917 [7:46:52<3:10:43,  3.70s/it] 65%|██████▌   | 5827/8917 [7:46:56<3:10:41,  3.70s/it] 65%|██████▌   | 5828/8917 [7:47:00<3:09:16,  3.68s/it] 65%|██████▌   | 5829/8917 [7:47:03<3:07:32,  3.64s/it] 65%|██████▌   | 5830/8917 [7:47:07<3:08:09,  3.66s/it] 65%|██████▌   | 5831/8917 [7:47:11<3:07:44,  3.65s/it] 65%|██████▌   | 5832/8917 [7:47:14<3:10:17,  3.70s/it] 65%|██████▌   | 5833/8917 [7:47:18<3:14:01,  3.77s/it] 65%|██████▌   | 5834/8917 [7:47:22<3:12:32,  3.75s/it] 65%|██████▌   | 5835/8917 [7:47:26<3:10:03,  3.70s/it] 65%|██████▌   | 5836/8917 [7:47:29<3:08:32,  3.67s/it] 65%|██████▌   | 5837/8917 [7:47:33<3:08:48,  3.68s/it] 65%|██████▌   | 5838/8917 [7:47:37<3:07:58,  3.66s/it] 65%|██████▌   | 5839/8917 [7:47:40<3:06:30,  3.64s/it] 65%|██████▌   | 5840/8917 [7:47:43<3:01:00,  3.53s/it] 66%|██████▌   | 5841/8917 [7:47:47<3:06:33,  3.64s/it] 66%|██████▌   | 5842/8917 [7:47:51<3:13:50,  3.78s/it] 66%|██████▌   | 5843/8917 [7:47:55<3:13:34,  3.78s/it] 66%|██████▌   | 5844/8917 [7:47:59<3:12:58,  3.77s/it] 66%|██████▌   | 5845/8917 [7:48:03<3:11:23,  3.74s/it] 66%|██████▌   | 5846/8917 [7:48:07<3:14:53,  3.81s/it] 66%|██████▌   | 5847/8917 [7:48:10<3:12:55,  3.77s/it] 66%|██████▌   | 5848/8917 [7:48:14<3:08:32,  3.69s/it] 66%|██████▌   | 5849/8917 [7:48:18<3:10:22,  3.72s/it]09/19/2024 10:02:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.030699649825692177, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3816505670547485, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.4123501777648926}
 66%|██████▌   | 5850/8917 [7:48:21<3:08:14,  3.68s/it] 66%|██████▌   | 5851/8917 [7:48:25<3:12:07,  3.76s/it] 66%|██████▌   | 5852/8917 [7:48:28<3:06:24,  3.65s/it] 66%|██████▌   | 5853/8917 [7:48:32<3:09:16,  3.71s/it] 66%|██████▌   | 5854/8917 [7:48:36<3:12:33,  3.77s/it] 66%|██████▌   | 5855/8917 [7:48:40<3:07:59,  3.68s/it] 66%|██████▌   | 5856/8917 [7:48:44<3:09:19,  3.71s/it] 66%|██████▌   | 5857/8917 [7:48:47<3:11:27,  3.75s/it] 66%|██████▌   | 5858/8917 [7:48:51<3:09:13,  3.71s/it] 66%|██████▌   | 5859/8917 [7:48:55<3:08:51,  3.71s/it] 66%|██████▌   | 5860/8917 [7:48:58<3:10:06,  3.73s/it] 66%|██████▌   | 5861/8917 [7:49:02<3:11:54,  3.77s/it] 66%|██████▌   | 5862/8917 [7:49:06<3:09:13,  3.72s/it] 66%|██████▌   | 5863/8917 [7:49:10<3:10:39,  3.75s/it] 66%|██████▌   | 5864/8917 [7:49:13<3:10:33,  3.75s/it] 66%|██████▌   | 5865/8917 [7:49:17<3:08:41,  3.71s/it] 66%|██████▌   | 5866/8917 [7:49:21<3:15:44,  3.85s/it] 66%|██████▌   | 5867/8917 [7:49:25<3:13:16,  3.80s/it] 66%|██████▌   | 5868/8917 [7:49:29<3:09:23,  3.73s/it] 66%|██████▌   | 5869/8917 [7:49:32<3:05:24,  3.65s/it] 66%|██████▌   | 5870/8917 [7:49:36<3:06:11,  3.67s/it] 66%|██████▌   | 5871/8917 [7:49:39<3:04:05,  3.63s/it] 66%|██████▌   | 5872/8917 [7:49:43<3:08:28,  3.71s/it] 66%|██████▌   | 5873/8917 [7:49:47<3:06:03,  3.67s/it] 66%|██████▌   | 5874/8917 [7:49:50<3:05:17,  3.65s/it] 66%|██████▌   | 5875/8917 [7:49:54<3:05:36,  3.66s/it] 66%|██████▌   | 5876/8917 [7:49:58<3:07:09,  3.69s/it] 66%|██████▌   | 5877/8917 [7:50:02<3:07:51,  3.71s/it] 66%|██████▌   | 5878/8917 [7:50:05<3:03:08,  3.62s/it] 66%|██████▌   | 5879/8917 [7:50:09<3:05:30,  3.66s/it] 66%|██████▌   | 5880/8917 [7:50:12<3:05:12,  3.66s/it] 66%|██████▌   | 5881/8917 [7:50:16<3:08:59,  3.73s/it] 66%|██████▌   | 5882/8917 [7:50:20<3:10:21,  3.76s/it] 66%|██████▌   | 5883/8917 [7:50:24<3:09:55,  3.76s/it] 66%|██████▌   | 5884/8917 [7:50:27<3:08:37,  3.73s/it] 66%|██████▌   | 5885/8917 [7:50:31<3:04:28,  3.65s/it] 66%|██████▌   | 5886/8917 [7:50:35<3:09:54,  3.76s/it] 66%|██████▌   | 5887/8917 [7:50:39<3:07:00,  3.70s/it] 66%|██████▌   | 5888/8917 [7:50:42<3:10:27,  3.77s/it] 66%|██████▌   | 5889/8917 [7:50:46<3:11:43,  3.80s/it] 66%|██████▌   | 5890/8917 [7:50:50<3:11:17,  3.79s/it] 66%|██████▌   | 5891/8917 [7:50:54<3:09:49,  3.76s/it] 66%|██████▌   | 5892/8917 [7:50:57<3:07:47,  3.72s/it] 66%|██████▌   | 5893/8917 [7:51:01<3:07:42,  3.72s/it] 66%|██████▌   | 5894/8917 [7:51:05<3:08:55,  3.75s/it] 66%|██████▌   | 5895/8917 [7:51:09<3:10:42,  3.79s/it] 66%|██████▌   | 5896/8917 [7:51:13<3:09:45,  3.77s/it] 66%|██████▌   | 5897/8917 [7:51:16<3:05:47,  3.69s/it] 66%|██████▌   | 5898/8917 [7:51:20<3:08:02,  3.74s/it] 66%|██████▌   | 5899/8917 [7:51:24<3:12:08,  3.82s/it]09/19/2024 10:06:02 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025424813851714134, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1782163381576538, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2036411762237549}
 66%|██████▌   | 5900/8917 [7:51:28<3:09:38,  3.77s/it] 66%|██████▌   | 5901/8917 [7:51:31<3:06:35,  3.71s/it] 66%|██████▌   | 5902/8917 [7:51:35<3:03:54,  3.66s/it] 66%|██████▌   | 5903/8917 [7:51:38<3:00:47,  3.60s/it] 66%|██████▌   | 5904/8917 [7:51:42<3:06:49,  3.72s/it] 66%|██████▌   | 5905/8917 [7:51:46<3:06:54,  3.72s/it] 66%|██████▌   | 5906/8917 [7:51:50<3:09:35,  3.78s/it] 66%|██████▌   | 5907/8917 [7:51:53<3:07:56,  3.75s/it] 66%|██████▋   | 5908/8917 [7:51:57<3:07:32,  3.74s/it] 66%|██████▋   | 5909/8917 [7:52:01<3:12:38,  3.84s/it] 66%|██████▋   | 5910/8917 [7:52:05<3:09:35,  3.78s/it] 66%|██████▋   | 5911/8917 [7:52:09<3:10:39,  3.81s/it] 66%|██████▋   | 5912/8917 [7:52:13<3:14:36,  3.89s/it] 66%|██████▋   | 5913/8917 [7:52:16<3:09:44,  3.79s/it] 66%|██████▋   | 5914/8917 [7:52:20<3:03:05,  3.66s/it] 66%|██████▋   | 5915/8917 [7:52:23<3:03:22,  3.67s/it] 66%|██████▋   | 5916/8917 [7:52:28<3:11:09,  3.82s/it] 66%|██████▋   | 5917/8917 [7:52:31<3:08:51,  3.78s/it] 66%|██████▋   | 5918/8917 [7:52:35<3:03:50,  3.68s/it] 66%|██████▋   | 5919/8917 [7:52:39<3:05:58,  3.72s/it] 66%|██████▋   | 5920/8917 [7:52:42<3:05:29,  3.71s/it] 66%|██████▋   | 5921/8917 [7:52:46<3:05:40,  3.72s/it] 66%|██████▋   | 5922/8917 [7:52:50<3:09:14,  3.79s/it] 66%|██████▋   | 5923/8917 [7:52:54<3:11:32,  3.84s/it] 66%|██████▋   | 5924/8917 [7:52:57<3:05:38,  3.72s/it] 66%|██████▋   | 5925/8917 [7:53:01<3:03:42,  3.68s/it] 66%|██████▋   | 5926/8917 [7:53:04<3:00:53,  3.63s/it] 66%|██████▋   | 5927/8917 [7:53:08<2:58:50,  3.59s/it] 66%|██████▋   | 5928/8917 [7:53:12<3:07:02,  3.75s/it] 66%|██████▋   | 5929/8917 [7:53:16<3:08:47,  3.79s/it] 67%|██████▋   | 5930/8917 [7:53:20<3:05:16,  3.72s/it] 67%|██████▋   | 5931/8917 [7:53:24<3:11:23,  3.85s/it] 67%|██████▋   | 5932/8917 [7:53:27<3:10:38,  3.83s/it] 67%|██████▋   | 5933/8917 [7:53:31<3:08:05,  3.78s/it] 67%|██████▋   | 5934/8917 [7:53:35<3:08:36,  3.79s/it] 67%|██████▋   | 5935/8917 [7:53:39<3:07:03,  3.76s/it] 67%|██████▋   | 5936/8917 [7:53:42<3:07:52,  3.78s/it] 67%|██████▋   | 5937/8917 [7:53:46<3:10:47,  3.84s/it] 67%|██████▋   | 5938/8917 [7:53:50<3:09:34,  3.82s/it] 67%|██████▋   | 5939/8917 [7:53:54<3:05:13,  3.73s/it] 67%|██████▋   | 5940/8917 [7:53:57<3:02:36,  3.68s/it] 67%|██████▋   | 5941/8917 [7:54:01<3:00:39,  3.64s/it] 67%|██████▋   | 5942/8917 [7:54:05<3:01:26,  3.66s/it] 67%|██████▋   | 5943/8917 [7:54:08<3:00:42,  3.65s/it] 67%|██████▋   | 5944/8917 [7:54:12<2:58:31,  3.60s/it] 67%|██████▋   | 5945/8917 [7:54:15<3:00:30,  3.64s/it] 67%|██████▋   | 5946/8917 [7:54:19<2:59:29,  3.62s/it] 67%|██████▋   | 5947/8917 [7:54:23<3:04:31,  3.73s/it] 67%|██████▋   | 5948/8917 [7:54:27<3:03:02,  3.70s/it] 67%|██████▋   | 5949/8917 [7:54:30<2:58:49,  3.62s/it]09/19/2024 10:09:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.009925845079123974, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0625123977661133, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0724382400512695}
 67%|██████▋   | 5950/8917 [7:54:34<3:00:59,  3.66s/it] 67%|██████▋   | 5951/8917 [7:54:38<3:03:13,  3.71s/it] 67%|██████▋   | 5952/8917 [7:54:41<2:56:55,  3.58s/it] 67%|██████▋   | 5953/8917 [7:54:45<2:59:20,  3.63s/it] 67%|██████▋   | 5954/8917 [7:54:48<2:56:40,  3.58s/it] 67%|██████▋   | 5955/8917 [7:54:52<3:01:21,  3.67s/it] 67%|██████▋   | 5956/8917 [7:54:56<3:04:52,  3.75s/it] 67%|██████▋   | 5957/8917 [7:55:00<3:02:50,  3.71s/it] 67%|██████▋   | 5958/8917 [7:55:03<3:01:22,  3.68s/it] 67%|██████▋   | 5959/8917 [7:55:07<3:02:53,  3.71s/it] 67%|██████▋   | 5960/8917 [7:55:10<2:57:22,  3.60s/it] 67%|██████▋   | 5961/8917 [7:55:14<3:01:11,  3.68s/it] 67%|██████▋   | 5962/8917 [7:55:18<2:57:31,  3.60s/it] 67%|██████▋   | 5963/8917 [7:55:22<3:02:44,  3.71s/it] 67%|██████▋   | 5964/8917 [7:55:25<2:57:18,  3.60s/it] 67%|██████▋   | 5965/8917 [7:55:29<3:02:41,  3.71s/it] 67%|██████▋   | 5966/8917 [7:55:33<3:06:02,  3.78s/it] 67%|██████▋   | 5967/8917 [7:55:36<2:59:30,  3.65s/it] 67%|██████▋   | 5968/8917 [7:55:40<2:59:55,  3.66s/it] 67%|██████▋   | 5969/8917 [7:55:43<2:59:49,  3.66s/it] 67%|██████▋   | 5970/8917 [7:55:48<3:06:44,  3.80s/it] 67%|██████▋   | 5971/8917 [7:55:51<3:07:30,  3.82s/it] 67%|██████▋   | 5972/8917 [7:55:55<3:03:18,  3.73s/it] 67%|██████▋   | 5973/8917 [7:55:59<3:00:44,  3.68s/it] 67%|██████▋   | 5974/8917 [7:56:03<3:07:24,  3.82s/it] 67%|██████▋   | 5975/8917 [7:56:06<3:02:10,  3.72s/it] 67%|██████▋   | 5976/8917 [7:56:10<2:59:59,  3.67s/it] 67%|██████▋   | 5977/8917 [7:56:13<2:59:00,  3.65s/it] 67%|██████▋   | 5978/8917 [7:56:17<2:56:09,  3.60s/it] 67%|██████▋   | 5979/8917 [7:56:20<2:54:04,  3.55s/it] 67%|██████▋   | 5980/8917 [7:56:24<2:59:16,  3.66s/it] 67%|██████▋   | 5981/8917 [7:56:28<3:02:12,  3.72s/it] 67%|██████▋   | 5982/8917 [7:56:32<2:59:29,  3.67s/it] 67%|██████▋   | 5983/8917 [7:56:35<2:59:20,  3.67s/it] 67%|██████▋   | 5984/8917 [7:56:39<2:54:28,  3.57s/it] 67%|██████▋   | 5985/8917 [7:56:42<2:58:34,  3.65s/it] 67%|██████▋   | 5986/8917 [7:56:46<3:00:56,  3.70s/it] 67%|██████▋   | 5987/8917 [7:56:50<2:56:42,  3.62s/it] 67%|██████▋   | 5988/8917 [7:56:53<2:58:34,  3.66s/it] 67%|██████▋   | 5989/8917 [7:56:57<2:55:40,  3.60s/it] 67%|██████▋   | 5990/8917 [7:57:01<3:01:43,  3.73s/it] 67%|██████▋   | 5991/8917 [7:57:04<2:58:08,  3.65s/it] 67%|██████▋   | 5992/8917 [7:57:08<3:00:57,  3.71s/it] 67%|██████▋   | 5993/8917 [7:57:12<3:05:54,  3.81s/it] 67%|██████▋   | 5994/8917 [7:57:16<3:03:08,  3.76s/it] 67%|██████▋   | 5995/8917 [7:57:20<3:05:43,  3.81s/it] 67%|██████▋   | 5996/8917 [7:57:23<3:00:27,  3.71s/it] 67%|██████▋   | 5997/8917 [7:57:27<3:00:34,  3.71s/it] 67%|██████▋   | 5998/8917 [7:57:30<2:55:44,  3.61s/it] 67%|██████▋   | 5999/8917 [7:57:34<2:53:10,  3.56s/it]09/19/2024 10:12:10 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 10:12:10 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<03:35,  1.02it/s][A
  1%|          | 2/221 [00:01<02:35,  1.41it/s][A
  1%|▏         | 3/221 [00:02<02:31,  1.44it/s][A
  2%|▏         | 4/221 [00:02<01:47,  2.01it/s][A
  3%|▎         | 6/221 [00:02<01:00,  3.53it/s][A
  3%|▎         | 7/221 [00:02<00:59,  3.62it/s][A
  4%|▎         | 8/221 [00:03<01:06,  3.19it/s][A
  4%|▍         | 9/221 [00:03<01:09,  3.07it/s][A
  5%|▍         | 10/221 [00:03<01:10,  3.01it/s][A
  5%|▌         | 12/221 [00:11<06:31,  1.87s/it][A
  6%|▌         | 13/221 [00:11<05:11,  1.50s/it][A
  7%|▋         | 15/221 [00:12<03:23,  1.01it/s][A
  7%|▋         | 16/221 [00:12<03:02,  1.12it/s][A
  8%|▊         | 17/221 [00:14<03:55,  1.16s/it][A
  8%|▊         | 18/221 [00:15<03:17,  1.03it/s][A
  9%|▊         | 19/221 [00:16<03:54,  1.16s/it][A
  9%|▉         | 20/221 [00:17<02:54,  1.15it/s][A
 10%|▉         | 21/221 [00:17<02:21,  1.41it/s][A
 10%|▉         | 22/221 [00:18<02:21,  1.41it/s][A
 11%|█         | 24/221 [00:18<01:33,  2.10it/s][A
 11%|█▏        | 25/221 [00:18<01:24,  2.33it/s][A
 12%|█▏        | 26/221 [00:19<01:18,  2.49it/s][A
 13%|█▎        | 28/221 [00:19<01:09,  2.79it/s][A
 13%|█▎        | 29/221 [00:19<01:03,  3.04it/s][A
 14%|█▎        | 30/221 [00:20<01:22,  2.32it/s][A
 14%|█▍        | 31/221 [00:21<01:22,  2.32it/s][A
 14%|█▍        | 32/221 [00:21<01:04,  2.92it/s][A
 15%|█▍        | 33/221 [00:21<01:03,  2.98it/s][A
 16%|█▌        | 35/221 [00:21<00:47,  3.93it/s][A
 16%|█▋        | 36/221 [00:21<00:44,  4.13it/s][A
 17%|█▋        | 37/221 [00:22<00:56,  3.24it/s][A
 17%|█▋        | 38/221 [00:22<01:07,  2.72it/s][A
 18%|█▊        | 39/221 [00:23<01:18,  2.32it/s][A
 18%|█▊        | 40/221 [00:24<01:20,  2.26it/s][A
 19%|█▊        | 41/221 [00:24<01:05,  2.73it/s][A
 19%|█▉        | 42/221 [00:24<00:53,  3.34it/s][A
 20%|█▉        | 44/221 [00:24<00:37,  4.75it/s][A
 20%|██        | 45/221 [00:29<03:52,  1.32s/it][A
 21%|██        | 46/221 [00:29<03:00,  1.03s/it][A
 21%|██▏       | 47/221 [00:29<02:33,  1.13it/s][A
 22%|██▏       | 48/221 [00:30<01:55,  1.50it/s][A
 22%|██▏       | 49/221 [00:30<01:39,  1.74it/s][A
 23%|██▎       | 50/221 [00:30<01:25,  2.00it/s][A
 23%|██▎       | 51/221 [00:30<01:08,  2.49it/s][A
 24%|██▎       | 52/221 [00:31<00:56,  2.99it/s][A
 24%|██▍       | 53/221 [00:31<00:55,  3.03it/s][A
 24%|██▍       | 54/221 [00:32<01:23,  2.01it/s][A
 25%|██▍       | 55/221 [00:33<01:44,  1.59it/s][A
 25%|██▌       | 56/221 [00:33<01:21,  2.03it/s][A
 26%|██▌       | 57/221 [00:33<01:05,  2.49it/s][A
 27%|██▋       | 59/221 [00:33<00:43,  3.76it/s][A
 27%|██▋       | 60/221 [00:34<00:47,  3.42it/s][A
 28%|██▊       | 61/221 [00:34<00:44,  3.62it/s][A
 28%|██▊       | 62/221 [00:34<00:46,  3.44it/s][A
 29%|██▊       | 63/221 [00:35<00:44,  3.52it/s][A
 29%|██▉       | 64/221 [00:35<01:14,  2.11it/s][A
 29%|██▉       | 65/221 [00:36<01:00,  2.60it/s][A
 30%|██▉       | 66/221 [00:36<01:07,  2.31it/s][A
 30%|███       | 67/221 [00:36<00:59,  2.60it/s][A
 31%|███       | 68/221 [00:37<00:48,  3.18it/s][A
 31%|███       | 69/221 [00:39<02:13,  1.14it/s][A
 32%|███▏      | 70/221 [00:39<01:39,  1.52it/s][A
 32%|███▏      | 71/221 [00:39<01:22,  1.82it/s][A
 33%|███▎      | 72/221 [00:40<01:22,  1.81it/s][A
 33%|███▎      | 73/221 [00:40<01:22,  1.78it/s][A
 33%|███▎      | 74/221 [00:41<01:03,  2.32it/s][A
 34%|███▍      | 75/221 [00:41<00:59,  2.45it/s][A
 34%|███▍      | 76/221 [00:41<00:56,  2.54it/s][A
 35%|███▍      | 77/221 [00:43<01:58,  1.22it/s][A
 35%|███▌      | 78/221 [00:43<01:28,  1.61it/s][A
 36%|███▌      | 79/221 [00:44<01:39,  1.43it/s][A
 36%|███▌      | 80/221 [00:44<01:13,  1.91it/s][A
 37%|███▋      | 81/221 [00:45<01:07,  2.09it/s][A
 37%|███▋      | 82/221 [00:47<02:45,  1.19s/it][A
 38%|███▊      | 83/221 [00:48<02:22,  1.03s/it][A
 38%|███▊      | 84/221 [00:48<01:48,  1.27it/s][A
 39%|███▉      | 86/221 [00:49<01:09,  1.93it/s][A
 39%|███▉      | 87/221 [00:49<01:10,  1.90it/s][A
 40%|███▉      | 88/221 [00:49<00:58,  2.26it/s][A
 40%|████      | 89/221 [00:50<00:53,  2.45it/s][A
 41%|████      | 90/221 [00:50<00:49,  2.63it/s][A
 41%|████      | 91/221 [00:50<00:40,  3.21it/s][A
 42%|████▏     | 92/221 [00:50<00:35,  3.63it/s][A
 42%|████▏     | 93/221 [00:51<00:40,  3.15it/s][A
 43%|████▎     | 94/221 [00:51<00:45,  2.77it/s][A
 43%|████▎     | 95/221 [00:52<00:44,  2.81it/s][A
 43%|████▎     | 96/221 [00:52<01:02,  2.01it/s][A
 44%|████▍     | 97/221 [00:53<00:48,  2.56it/s][A
 44%|████▍     | 98/221 [00:53<00:51,  2.37it/s][A
 45%|████▍     | 99/221 [00:54<00:51,  2.38it/s][A
 45%|████▌     | 100/221 [00:55<01:11,  1.70it/s][A
 46%|████▌     | 101/221 [00:55<00:54,  2.18it/s][A
 46%|████▌     | 102/221 [00:56<01:12,  1.65it/s][A
 47%|████▋     | 103/221 [00:56<00:53,  2.19it/s][A
 47%|████▋     | 104/221 [00:56<00:48,  2.41it/s][A
 48%|████▊     | 105/221 [00:56<00:48,  2.39it/s][A
 48%|████▊     | 106/221 [00:59<01:44,  1.10it/s][A
 48%|████▊     | 107/221 [00:59<01:24,  1.35it/s][A
 49%|████▉     | 108/221 [00:59<01:13,  1.53it/s][A
 49%|████▉     | 109/221 [01:00<01:03,  1.76it/s][A
 50%|████▉     | 110/221 [01:00<00:48,  2.28it/s][A
 50%|█████     | 111/221 [01:00<00:46,  2.39it/s][A
 51%|█████     | 112/221 [01:01<00:41,  2.61it/s][A
 51%|█████     | 113/221 [01:01<00:34,  3.12it/s][A
 52%|█████▏    | 115/221 [01:01<00:33,  3.13it/s][A
 52%|█████▏    | 116/221 [01:02<00:34,  3.07it/s][A
 53%|█████▎    | 117/221 [01:02<00:35,  2.92it/s][A
 53%|█████▎    | 118/221 [01:02<00:36,  2.81it/s][A
 54%|█████▍    | 119/221 [01:03<00:33,  3.07it/s][A
 54%|█████▍    | 120/221 [01:03<00:28,  3.54it/s][A
 55%|█████▍    | 121/221 [01:03<00:31,  3.19it/s][A
 55%|█████▌    | 122/221 [01:04<00:36,  2.71it/s][A
 56%|█████▌    | 123/221 [01:05<01:04,  1.51it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.87it/s][A
 57%|█████▋    | 125/221 [01:06<00:57,  1.68it/s][A
 57%|█████▋    | 126/221 [01:15<04:53,  3.09s/it][A
 57%|█████▋    | 127/221 [01:16<03:36,  2.30s/it][A
 58%|█████▊    | 128/221 [01:16<02:41,  1.73s/it][A
 58%|█████▊    | 129/221 [01:16<02:04,  1.36s/it][A
 59%|█████▉    | 130/221 [01:17<01:34,  1.04s/it][A
 59%|█████▉    | 131/221 [01:18<01:35,  1.06s/it][A
 60%|█████▉    | 132/221 [01:19<01:43,  1.17s/it][A
 60%|██████    | 133/221 [01:20<01:29,  1.02s/it][A
 61%|██████    | 134/221 [01:21<01:30,  1.04s/it][A
 61%|██████    | 135/221 [01:22<01:30,  1.05s/it][A
 62%|██████▏   | 136/221 [01:22<01:11,  1.19it/s][A
 62%|██████▏   | 137/221 [01:23<00:58,  1.43it/s][A
 62%|██████▏   | 138/221 [01:23<00:52,  1.59it/s][A
 63%|██████▎   | 139/221 [01:23<00:40,  2.01it/s][A
 63%|██████▎   | 140/221 [01:24<00:41,  1.93it/s][A
 64%|██████▍   | 141/221 [01:24<00:37,  2.16it/s][A
 64%|██████▍   | 142/221 [01:25<00:33,  2.33it/s][A
 65%|██████▍   | 143/221 [01:25<00:31,  2.48it/s][A
 65%|██████▌   | 144/221 [01:25<00:25,  2.99it/s][A
 66%|██████▌   | 145/221 [01:25<00:20,  3.66it/s][A
 66%|██████▌   | 146/221 [01:25<00:16,  4.50it/s][A
 67%|██████▋   | 148/221 [01:27<00:36,  2.01it/s][A
 67%|██████▋   | 149/221 [01:27<00:31,  2.29it/s][A
 68%|██████▊   | 150/221 [01:28<00:30,  2.30it/s][A
 68%|██████▊   | 151/221 [01:28<00:28,  2.48it/s][A
 69%|██████▉   | 152/221 [01:29<00:29,  2.34it/s][A
 69%|██████▉   | 153/221 [01:29<00:22,  2.96it/s][A
 70%|██████▉   | 154/221 [01:29<00:19,  3.38it/s][A
 70%|███████   | 155/221 [01:29<00:17,  3.88it/s][A
 71%|███████   | 156/221 [01:29<00:16,  3.96it/s][A
 71%|███████   | 157/221 [01:34<01:47,  1.68s/it][A
 71%|███████▏  | 158/221 [01:35<01:26,  1.37s/it][A
 72%|███████▏  | 159/221 [01:35<01:02,  1.01s/it][A
 72%|███████▏  | 160/221 [01:35<00:46,  1.32it/s][A
 73%|███████▎  | 162/221 [01:35<00:25,  2.28it/s][A
 74%|███████▍  | 163/221 [01:36<00:23,  2.49it/s][A
 74%|███████▍  | 164/221 [01:36<00:19,  2.90it/s][A
 75%|███████▍  | 165/221 [01:36<00:19,  2.84it/s][A
 75%|███████▌  | 166/221 [01:37<00:23,  2.29it/s][A
 76%|███████▌  | 167/221 [01:37<00:20,  2.65it/s][A
 76%|███████▌  | 168/221 [01:40<00:50,  1.04it/s][A
 76%|███████▋  | 169/221 [01:40<00:43,  1.18it/s][A
 77%|███████▋  | 170/221 [01:41<00:36,  1.38it/s][A
 77%|███████▋  | 171/221 [01:41<00:31,  1.60it/s][A
 78%|███████▊  | 172/221 [01:41<00:25,  1.96it/s][A
 78%|███████▊  | 173/221 [01:41<00:21,  2.19it/s][A
 79%|███████▊  | 174/221 [01:42<00:16,  2.77it/s][A
 79%|███████▉  | 175/221 [01:42<00:15,  3.05it/s][A
 80%|███████▉  | 176/221 [01:42<00:14,  3.02it/s][A
 80%|████████  | 177/221 [01:42<00:13,  3.24it/s][A
 81%|████████  | 178/221 [01:43<00:13,  3.18it/s][A
 81%|████████  | 179/221 [01:43<00:12,  3.25it/s][A
 81%|████████▏ | 180/221 [01:43<00:11,  3.59it/s][A
 82%|████████▏ | 181/221 [01:43<00:09,  4.22it/s][A
 82%|████████▏ | 182/221 [01:44<00:11,  3.40it/s][A
 83%|████████▎ | 183/221 [01:45<00:15,  2.49it/s][A
 83%|████████▎ | 184/221 [01:45<00:15,  2.46it/s][A
 84%|████████▎ | 185/221 [01:45<00:14,  2.47it/s][A
 84%|████████▍ | 186/221 [01:46<00:13,  2.62it/s][A
 85%|████████▍ | 187/221 [01:46<00:11,  2.90it/s][A
 85%|████████▌ | 188/221 [01:46<00:10,  3.28it/s][A
 86%|████████▌ | 189/221 [01:46<00:09,  3.29it/s][A
 86%|████████▌ | 190/221 [01:47<00:10,  2.93it/s][A
 86%|████████▋ | 191/221 [01:47<00:08,  3.39it/s][A
 87%|████████▋ | 192/221 [01:48<00:11,  2.56it/s][A
 87%|████████▋ | 193/221 [01:48<00:08,  3.14it/s][A
 88%|████████▊ | 194/221 [01:48<00:10,  2.68it/s][A
 88%|████████▊ | 195/221 [01:48<00:08,  3.22it/s][A
 89%|████████▊ | 196/221 [01:49<00:08,  3.08it/s][A
 89%|████████▉ | 197/221 [01:49<00:07,  3.26it/s][A
 90%|████████▉ | 198/221 [01:49<00:06,  3.59it/s][A
 90%|█████████ | 199/221 [01:50<00:05,  4.04it/s][A
 90%|█████████ | 200/221 [01:50<00:07,  2.77it/s][A
 91%|█████████ | 201/221 [01:51<00:09,  2.11it/s][A
 91%|█████████▏| 202/221 [01:51<00:07,  2.58it/s][A
 92%|█████████▏| 203/221 [01:52<00:09,  1.93it/s][A
 92%|█████████▏| 204/221 [01:52<00:07,  2.17it/s][A
 93%|█████████▎| 206/221 [01:53<00:05,  2.86it/s][A
 94%|█████████▎| 207/221 [01:53<00:04,  3.44it/s][A
 94%|█████████▍| 208/221 [01:53<00:03,  3.44it/s][A
 95%|█████████▍| 209/221 [01:53<00:02,  4.17it/s][A
 95%|█████████▌| 210/221 [01:53<00:02,  4.13it/s][A
 95%|█████████▌| 211/221 [01:54<00:03,  3.14it/s][A
 96%|█████████▌| 212/221 [01:54<00:02,  3.19it/s][A
 96%|█████████▋| 213/221 [01:54<00:02,  3.70it/s][A
 97%|█████████▋| 214/221 [01:55<00:02,  3.10it/s][A
 97%|█████████▋| 215/221 [01:55<00:02,  2.72it/s][A
 98%|█████████▊| 216/221 [01:56<00:01,  2.85it/s][A
 98%|█████████▊| 217/221 [01:59<00:05,  1.40s/it][A
 99%|█████████▊| 218/221 [02:00<00:03,  1.09s/it][A
 99%|█████████▉| 219/221 [02:00<00:01,  1.19it/s][A
100%|█████████▉| 220/221 [02:03<00:01,  1.40s/it][A
100%|██████████| 221/221 [02:03<00:00,  1.03s/it][A100%|██████████| 221/221 [02:03<00:00,  1.79it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:51,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:34,  6.39it/s][A
  1%|          | 2/221 [00:01<02:15,  1.62it/s][A
  1%|▏         | 3/221 [00:01<01:33,  2.33it/s][A
  2%|▏         | 4/221 [00:01<01:45,  2.06it/s][A
  2%|▏         | 5/221 [00:02<01:50,  1.95it/s][A
  3%|▎         | 6/221 [00:02<01:42,  2.09it/s][A
  3%|▎         | 7/221 [00:03<01:34,  2.27it/s][A
  4%|▎         | 8/221 [00:04<02:30,  1.42it/s][A
  4%|▍         | 9/221 [00:05<02:39,  1.33it/s][A
  5%|▍         | 10/221 [00:05<01:59,  1.77it/s][A
  5%|▍         | 11/221 [00:05<01:45,  1.99it/s][A
  5%|▌         | 12/221 [00:06<01:36,  2.16it/s][A
  6%|▌         | 13/221 [00:06<01:18,  2.64it/s][A
  6%|▋         | 14/221 [00:06<01:03,  3.26it/s][A
  7%|▋         | 15/221 [00:06<01:00,  3.43it/s][A
  7%|▋         | 16/221 [00:07<01:29,  2.29it/s][A
  8%|▊         | 17/221 [00:08<02:05,  1.63it/s][A
  8%|▊         | 18/221 [00:08<01:40,  2.02it/s][A
  9%|▊         | 19/221 [00:09<02:09,  1.56it/s][A
  9%|▉         | 20/221 [00:10<01:57,  1.71it/s][A
 10%|▉         | 21/221 [00:10<01:52,  1.77it/s][A
 10%|▉         | 22/221 [00:12<02:34,  1.29it/s][A
 10%|█         | 23/221 [00:12<02:01,  1.62it/s][A
 11%|█         | 24/221 [00:12<01:48,  1.81it/s][A
 11%|█▏        | 25/221 [00:13<02:09,  1.51it/s][A
 12%|█▏        | 26/221 [00:14<02:16,  1.43it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:15<02:31,  1.27it/s][A
 13%|█▎        | 29/221 [00:16<02:00,  1.59it/s][A
 14%|█▎        | 30/221 [00:16<01:56,  1.64it/s][A
 14%|█▍        | 31/221 [00:17<01:56,  1.63it/s][A
 14%|█▍        | 32/221 [00:18<02:16,  1.39it/s][A
 15%|█▍        | 33/221 [00:19<02:17,  1.36it/s][A
 15%|█▌        | 34/221 [00:19<01:47,  1.74it/s][A
 16%|█▌        | 35/221 [00:19<01:29,  2.08it/s][A
 16%|█▋        | 36/221 [00:20<01:41,  1.82it/s][A
 17%|█▋        | 37/221 [00:20<01:33,  1.97it/s][A
 17%|█▋        | 38/221 [00:21<02:06,  1.45it/s][A
 18%|█▊        | 39/221 [00:22<01:59,  1.52it/s][A
 18%|█▊        | 40/221 [00:23<02:36,  1.16it/s][A
 19%|█▊        | 41/221 [00:23<01:58,  1.52it/s][A
 19%|█▉        | 42/221 [00:24<01:35,  1.86it/s][A
 19%|█▉        | 43/221 [00:24<01:25,  2.09it/s][A
 20%|█▉        | 44/221 [00:25<01:36,  1.84it/s][A
 21%|██        | 46/221 [00:25<01:06,  2.65it/s][A
 21%|██▏       | 47/221 [00:26<01:16,  2.28it/s][A
 22%|██▏       | 48/221 [00:26<01:01,  2.79it/s][A
 22%|██▏       | 49/221 [00:27<01:32,  1.86it/s][A
 23%|██▎       | 50/221 [00:27<01:26,  1.97it/s][A
 23%|██▎       | 51/221 [00:28<01:20,  2.11it/s][A
 24%|██▎       | 52/221 [00:28<01:09,  2.43it/s][A
 24%|██▍       | 53/221 [00:28<01:11,  2.34it/s][A
 24%|██▍       | 54/221 [00:29<01:14,  2.24it/s][A
 25%|██▍       | 55/221 [00:29<01:00,  2.74it/s][A
 25%|██▌       | 56/221 [00:30<01:12,  2.29it/s][A
 26%|██▌       | 57/221 [00:30<01:05,  2.50it/s][A
 26%|██▌       | 58/221 [00:30<00:54,  2.98it/s][A
 27%|██▋       | 60/221 [00:31<00:55,  2.88it/s][A
 28%|██▊       | 61/221 [00:32<01:15,  2.12it/s][A
 28%|██▊       | 62/221 [00:32<01:12,  2.21it/s][A
 29%|██▊       | 63/221 [00:33<01:28,  1.78it/s][A
 29%|██▉       | 64/221 [00:34<02:09,  1.22it/s][A
 29%|██▉       | 65/221 [00:36<02:26,  1.06it/s][A
 30%|██▉       | 66/221 [00:36<02:06,  1.22it/s][A
 30%|███       | 67/221 [00:36<01:37,  1.57it/s][A
 31%|███       | 68/221 [00:37<01:19,  1.93it/s][A
 31%|███       | 69/221 [00:37<01:03,  2.41it/s][A
 32%|███▏      | 70/221 [00:37<01:10,  2.15it/s][A
 32%|███▏      | 71/221 [00:38<01:11,  2.09it/s][A
 33%|███▎      | 72/221 [00:39<01:21,  1.82it/s][A
 33%|███▎      | 73/221 [00:39<01:24,  1.75it/s][A
 33%|███▎      | 74/221 [00:40<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:40<01:07,  2.17it/s][A
 34%|███▍      | 76/221 [00:40<01:03,  2.27it/s][A
 35%|███▍      | 77/221 [00:41<00:55,  2.61it/s][A
 35%|███▌      | 78/221 [00:41<01:06,  2.16it/s][A
 36%|███▌      | 79/221 [00:42<01:28,  1.61it/s][A
 36%|███▌      | 80/221 [00:42<01:08,  2.05it/s][A
 37%|███▋      | 81/221 [00:43<01:08,  2.04it/s][A
 37%|███▋      | 82/221 [00:44<01:34,  1.47it/s][A
 38%|███▊      | 83/221 [00:44<01:15,  1.82it/s][A
 38%|███▊      | 84/221 [00:46<01:48,  1.26it/s][A
 38%|███▊      | 85/221 [00:46<01:21,  1.68it/s][A
 39%|███▉      | 86/221 [00:46<01:18,  1.72it/s][A
 39%|███▉      | 87/221 [00:47<01:18,  1.71it/s][A
 40%|███▉      | 88/221 [00:47<01:07,  1.98it/s][A
 40%|████      | 89/221 [00:48<01:15,  1.74it/s][A
 41%|████      | 90/221 [00:49<01:16,  1.72it/s][A
 41%|████      | 91/221 [00:49<01:06,  1.96it/s][A
 42%|████▏     | 92/221 [00:49<01:03,  2.02it/s][A
 43%|████▎     | 94/221 [00:50<00:42,  2.97it/s][A
 43%|████▎     | 95/221 [00:50<00:45,  2.78it/s][A
 43%|████▎     | 96/221 [00:51<00:46,  2.70it/s][A
 44%|████▍     | 97/221 [00:51<00:52,  2.35it/s][A
 44%|████▍     | 98/221 [00:52<01:03,  1.93it/s][A
 45%|████▍     | 99/221 [00:52<01:05,  1.85it/s][A
 45%|████▌     | 100/221 [00:53<01:17,  1.57it/s][A
 46%|████▌     | 101/221 [00:54<01:29,  1.34it/s][A
 46%|████▌     | 102/221 [00:55<01:41,  1.17it/s][A
 47%|████▋     | 103/221 [00:56<01:18,  1.50it/s][A
 47%|████▋     | 104/221 [00:56<01:16,  1.54it/s][A
 48%|████▊     | 105/221 [00:57<01:05,  1.77it/s][A
 48%|████▊     | 106/221 [00:57<00:52,  2.17it/s][A
 48%|████▊     | 107/221 [00:57<00:52,  2.15it/s][A
 49%|████▉     | 108/221 [00:58<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:58<00:55,  2.01it/s][A
 50%|████▉     | 110/221 [00:59<00:53,  2.06it/s][A
 50%|█████     | 111/221 [01:00<01:13,  1.50it/s][A
 51%|█████     | 112/221 [01:01<01:08,  1.60it/s][A
 51%|█████     | 113/221 [01:01<00:56,  1.91it/s][A
 52%|█████▏    | 114/221 [01:01<00:48,  2.19it/s][A
 52%|█████▏    | 115/221 [01:02<00:58,  1.81it/s][A
 52%|█████▏    | 116/221 [01:02<00:50,  2.10it/s][A
 53%|█████▎    | 117/221 [01:02<00:43,  2.38it/s][A
 53%|█████▎    | 118/221 [01:03<00:47,  2.15it/s][A
 54%|█████▍    | 119/221 [01:03<00:46,  2.19it/s][A
 54%|█████▍    | 120/221 [01:04<00:47,  2.11it/s][A
 55%|█████▍    | 121/221 [01:04<00:46,  2.13it/s][A
 55%|█████▌    | 122/221 [01:05<00:52,  1.88it/s][A
 56%|█████▌    | 123/221 [01:06<00:47,  2.04it/s][A
 56%|█████▌    | 124/221 [01:06<01:00,  1.59it/s][A
 57%|█████▋    | 125/221 [01:07<00:52,  1.82it/s][A
 57%|█████▋    | 126/221 [01:07<00:41,  2.30it/s][A
 57%|█████▋    | 127/221 [01:08<00:58,  1.60it/s][A
 58%|█████▊    | 128/221 [01:09<00:58,  1.60it/s][A
 58%|█████▊    | 129/221 [01:09<00:59,  1.55it/s][A
 59%|█████▉    | 130/221 [01:10<00:47,  1.92it/s][A
 59%|█████▉    | 131/221 [01:10<00:46,  1.92it/s][A
 60%|█████▉    | 132/221 [01:11<00:56,  1.59it/s][A
 60%|██████    | 133/221 [01:12<01:05,  1.34it/s][A
 61%|██████    | 134/221 [01:12<00:56,  1.55it/s][A
 61%|██████    | 135/221 [01:13<00:56,  1.52it/s][A
 62%|██████▏   | 136/221 [01:14<01:01,  1.39it/s][A
 62%|██████▏   | 137/221 [01:15<00:58,  1.44it/s][A
 62%|██████▏   | 138/221 [01:15<01:00,  1.37it/s][A
 63%|██████▎   | 139/221 [01:17<01:12,  1.13it/s][A
 63%|██████▎   | 140/221 [01:18<01:10,  1.15it/s][A
 64%|██████▍   | 141/221 [01:18<00:58,  1.38it/s][A
 64%|██████▍   | 142/221 [01:18<00:51,  1.54it/s][A
 65%|██████▍   | 143/221 [01:19<00:39,  1.96it/s][A
 65%|██████▌   | 144/221 [01:19<00:35,  2.17it/s][A
 66%|██████▌   | 145/221 [01:19<00:34,  2.20it/s][A
 66%|██████▌   | 146/221 [01:20<00:27,  2.70it/s][A
 67%|██████▋   | 147/221 [01:20<00:30,  2.45it/s][A
 67%|██████▋   | 148/221 [01:20<00:30,  2.37it/s][A
 68%|██████▊   | 150/221 [01:21<00:22,  3.18it/s][A
 68%|██████▊   | 151/221 [01:21<00:19,  3.58it/s][A
 69%|██████▉   | 152/221 [01:21<00:20,  3.42it/s][A
 69%|██████▉   | 153/221 [01:22<00:18,  3.60it/s][A
 70%|██████▉   | 154/221 [01:22<00:27,  2.42it/s][A
 70%|███████   | 155/221 [01:23<00:37,  1.78it/s][A
 71%|███████   | 156/221 [01:24<00:32,  2.00it/s][A
 71%|███████   | 157/221 [01:24<00:34,  1.86it/s][A
 71%|███████▏  | 158/221 [01:25<00:29,  2.13it/s][A
 72%|███████▏  | 160/221 [01:25<00:23,  2.63it/s][A
 73%|███████▎  | 161/221 [01:25<00:19,  3.08it/s][A
 73%|███████▎  | 162/221 [01:26<00:20,  2.92it/s][A
 74%|███████▍  | 163/221 [01:26<00:17,  3.40it/s][A
 74%|███████▍  | 164/221 [01:26<00:17,  3.32it/s][A
 75%|███████▍  | 165/221 [01:26<00:15,  3.56it/s][A
 75%|███████▌  | 166/221 [01:27<00:21,  2.61it/s][A
 76%|███████▌  | 167/221 [01:27<00:20,  2.65it/s][A
 76%|███████▌  | 168/221 [01:28<00:17,  3.04it/s][A
 76%|███████▋  | 169/221 [01:29<00:29,  1.75it/s][A
 77%|███████▋  | 170/221 [01:29<00:28,  1.80it/s][A
 77%|███████▋  | 171/221 [01:30<00:28,  1.73it/s][A
 78%|███████▊  | 172/221 [01:30<00:23,  2.10it/s][A
 78%|███████▊  | 173/221 [01:30<00:20,  2.29it/s][A
 79%|███████▊  | 174/221 [01:31<00:20,  2.32it/s][A
 79%|███████▉  | 175/221 [01:31<00:20,  2.28it/s][A
 80%|███████▉  | 176/221 [01:32<00:19,  2.27it/s][A
 80%|████████  | 177/221 [01:32<00:19,  2.26it/s][A
 81%|████████  | 178/221 [01:33<00:20,  2.10it/s][A
 81%|████████  | 179/221 [01:34<00:23,  1.76it/s][A
 81%|████████▏ | 180/221 [01:34<00:18,  2.26it/s][A
 82%|████████▏ | 181/221 [01:34<00:14,  2.69it/s][A
 82%|████████▏ | 182/221 [01:35<00:17,  2.28it/s][A
 83%|████████▎ | 183/221 [01:35<00:18,  2.08it/s][A
 83%|████████▎ | 184/221 [01:36<00:25,  1.45it/s][A
 84%|████████▎ | 185/221 [01:37<00:23,  1.51it/s][A
 84%|████████▍ | 186/221 [01:37<00:17,  1.96it/s][A
 85%|████████▍ | 187/221 [01:38<00:18,  1.83it/s][A
 85%|████████▌ | 188/221 [01:38<00:14,  2.34it/s][A
 86%|████████▌ | 189/221 [01:38<00:14,  2.15it/s][A
 86%|████████▌ | 190/221 [01:39<00:13,  2.26it/s][A
 86%|████████▋ | 191/221 [01:40<00:16,  1.83it/s][A
 87%|████████▋ | 192/221 [01:40<00:14,  2.00it/s][A
 87%|████████▋ | 193/221 [01:40<00:12,  2.29it/s][A
 88%|████████▊ | 194/221 [01:41<00:14,  1.88it/s][A
 88%|████████▊ | 195/221 [01:43<00:21,  1.19it/s][A
 89%|████████▊ | 196/221 [01:43<00:18,  1.37it/s][A
 89%|████████▉ | 197/221 [01:44<00:17,  1.36it/s][A
 90%|████████▉ | 198/221 [01:44<00:15,  1.45it/s][A
 90%|█████████ | 199/221 [01:45<00:12,  1.73it/s][A
 90%|█████████ | 200/221 [01:46<00:14,  1.45it/s][A
 91%|█████████ | 201/221 [01:46<00:13,  1.53it/s][A
 91%|█████████▏| 202/221 [01:47<00:10,  1.76it/s][A
 92%|█████████▏| 203/221 [01:47<00:10,  1.73it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  2.11it/s][A
 93%|█████████▎| 205/221 [01:48<00:06,  2.44it/s][A
 93%|█████████▎| 206/221 [01:48<00:06,  2.43it/s][A
 94%|█████████▎| 207/221 [01:48<00:04,  2.84it/s][A
 94%|█████████▍| 208/221 [01:50<00:08,  1.51it/s][A
 95%|█████████▍| 209/221 [01:50<00:07,  1.61it/s][A
 95%|█████████▌| 210/221 [01:51<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:06,  1.62it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.99it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.77it/s][A
 97%|█████████▋| 214/221 [01:53<00:04,  1.67it/s][A
 97%|█████████▋| 215/221 [01:53<00:02,  2.21it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.90it/s][A
 98%|█████████▊| 217/221 [01:55<00:02,  1.57it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.96it/s][A
 99%|█████████▉| 219/221 [01:55<00:00,  2.04it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  2.28it/s][A
100%|██████████| 221/221 [01:56<00:00,  2.20it/s][A100%|██████████| 221/221 [01:56<00:00,  1.90it/s]
09/19/2024 10:20:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 5999--===========

09/19/2024 10:20:57 - INFO - __main__ -   {'area_r1': 45.9, 'area_recall': '45.9/74.1/83.8', 'area_ravg': 67.9}
09/19/2024 10:20:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 5999--===========

09/19/2024 10:20:57 - INFO - __main__ -   {'forward_r1': 49.7, 'forward_recall': '49.7/78.6/88.0', 'forward_ravg': 72.1}
09/19/2024 10:20:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 5999--===========

09/19/2024 10:20:57 - INFO - __main__ -   {'area_video_r1': 48.3, 'area_video_recall': '48.3/77.9/87.6', 'area_video_ravg': 71.3}
09/19/2024 10:20:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 10:20:57 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 10:20:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 5999--===========

09/19/2024 10:20:57 - INFO - __main__ -   {'area_video_r1': 63.0, 'area_video_recall': '63.0/82.9/89.4', 'area_video_ravg': 78.4, 'area_video_back_r1': 64.1, 'area_video_back_recall': '64.1/86.0/91.6', 'area_video_back_ravg': 80.6}
09/19/2024 10:20:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 5499=======

09/19/2024 10:20:57 - INFO - __main__ -   {'area_video_r1': 63.5, 'area_video_recall': '63.5/83.7/89.0', 'area_video_ravg': 78.7, 'area_video_back_r1': 62.9, 'area_video_back_recall': '62.9/85.1/91.1', 'area_video_back_ravg': 79.7}
09/19/2024 10:20:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 5999--===========

09/19/2024 10:20:57 - INFO - __main__ -   {'video_r1': 31.1, 'video_recall': '31.1/54.9/66.0', 'video_ravg': 50.6}
09/19/2024 10:20:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 10:20:57 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 10:20:57 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 5999--===========

09/19/2024 10:20:57 - INFO - __main__ -   {'video_r1': 60.2, 'video_recall': '60.2/80.1/84.8', 'video_ravg': 75.0}
09/19/2024 10:20:57 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 5499=======

09/19/2024 10:20:57 - INFO - __main__ -   {'video_r1': 60.4, 'video_recall': '60.4/79.9/84.0', 'video_ravg': 74.8}
09/19/2024 10:21:17 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01929926872253418, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.967465877532959, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9867651462554932}
 67%|██████▋   | 6000/8917 [8:06:43<135:31:01, 167.25s/it] 67%|██████▋   | 6001/8917 [8:06:46<95:37:20, 118.05s/it]  67%|██████▋   | 6002/8917 [8:06:50<67:44:37, 83.66s/it]  67%|██████▋   | 6003/8917 [8:06:54<48:19:37, 59.70s/it] 67%|██████▋   | 6004/8917 [8:06:57<34:43:05, 42.91s/it] 67%|██████▋   | 6005/8917 [8:07:01<25:12:23, 31.16s/it] 67%|██████▋   | 6006/8917 [8:07:05<18:31:20, 22.91s/it] 67%|██████▋   | 6007/8917 [8:07:08<13:53:30, 17.19s/it] 67%|██████▋   | 6008/8917 [8:07:12<10:39:19, 13.19s/it] 67%|██████▋   | 6009/8917 [8:07:16<8:19:25, 10.30s/it]  67%|██████▋   | 6010/8917 [8:07:20<6:42:57,  8.32s/it] 67%|██████▋   | 6011/8917 [8:07:23<5:36:31,  6.95s/it] 67%|██████▋   | 6012/8917 [8:07:27<4:46:41,  5.92s/it] 67%|██████▋   | 6013/8917 [8:07:30<4:12:21,  5.21s/it] 67%|██████▋   | 6014/8917 [8:07:34<3:50:40,  4.77s/it] 67%|██████▋   | 6015/8917 [8:07:38<3:35:50,  4.46s/it] 67%|██████▋   | 6016/8917 [8:07:42<3:27:08,  4.28s/it] 67%|██████▋   | 6017/8917 [8:07:46<3:20:46,  4.15s/it] 67%|██████▋   | 6018/8917 [8:07:50<3:20:36,  4.15s/it] 68%|██████▊   | 6019/8917 [8:07:53<3:08:15,  3.90s/it] 68%|██████▊   | 6020/8917 [8:07:57<3:11:10,  3.96s/it] 68%|██████▊   | 6021/8917 [8:08:01<3:03:23,  3.80s/it] 68%|██████▊   | 6022/8917 [8:08:04<3:00:36,  3.74s/it] 68%|██████▊   | 6023/8917 [8:08:08<3:04:28,  3.82s/it] 68%|██████▊   | 6024/8917 [8:08:12<3:03:21,  3.80s/it] 68%|██████▊   | 6025/8917 [8:08:16<3:01:25,  3.76s/it] 68%|██████▊   | 6026/8917 [8:08:19<2:58:22,  3.70s/it] 68%|██████▊   | 6027/8917 [8:08:23<3:03:37,  3.81s/it] 68%|██████▊   | 6028/8917 [8:08:27<3:04:33,  3.83s/it] 68%|██████▊   | 6029/8917 [8:08:31<3:00:18,  3.75s/it] 68%|██████▊   | 6030/8917 [8:08:34<2:57:41,  3.69s/it] 68%|██████▊   | 6031/8917 [8:08:38<2:56:44,  3.67s/it] 68%|██████▊   | 6032/8917 [8:08:42<3:00:20,  3.75s/it] 68%|██████▊   | 6033/8917 [8:08:46<3:02:51,  3.80s/it] 68%|██████▊   | 6034/8917 [8:08:49<2:59:55,  3.74s/it] 68%|██████▊   | 6035/8917 [8:08:53<3:01:55,  3.79s/it] 68%|██████▊   | 6036/8917 [8:08:57<2:56:44,  3.68s/it] 68%|██████▊   | 6037/8917 [8:09:00<2:56:15,  3.67s/it] 68%|██████▊   | 6038/8917 [8:09:05<3:03:32,  3.83s/it] 68%|██████▊   | 6039/8917 [8:09:08<3:01:22,  3.78s/it] 68%|██████▊   | 6040/8917 [8:09:12<3:01:21,  3.78s/it] 68%|██████▊   | 6041/8917 [8:09:16<3:01:17,  3.78s/it] 68%|██████▊   | 6042/8917 [8:09:19<2:57:17,  3.70s/it] 68%|██████▊   | 6043/8917 [8:09:23<2:56:39,  3.69s/it] 68%|██████▊   | 6044/8917 [8:09:27<2:58:07,  3.72s/it] 68%|██████▊   | 6045/8917 [8:09:31<3:00:59,  3.78s/it] 68%|██████▊   | 6046/8917 [8:09:34<2:58:59,  3.74s/it] 68%|██████▊   | 6047/8917 [8:09:39<3:06:23,  3.90s/it] 68%|██████▊   | 6048/8917 [8:09:42<3:00:30,  3.78s/it] 68%|██████▊   | 6049/8917 [8:09:46<2:57:35,  3.72s/it]09/19/2024 10:24:23 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029617026448249817, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2516590356826782, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2812761068344116}
 68%|██████▊   | 6050/8917 [8:09:49<2:56:07,  3.69s/it] 68%|██████▊   | 6051/8917 [8:09:53<2:51:06,  3.58s/it] 68%|██████▊   | 6052/8917 [8:09:57<2:57:03,  3.71s/it] 68%|██████▊   | 6053/8917 [8:10:01<3:00:16,  3.78s/it] 68%|██████▊   | 6054/8917 [8:10:04<2:59:42,  3.77s/it] 68%|██████▊   | 6055/8917 [8:10:08<3:01:55,  3.81s/it] 68%|██████▊   | 6056/8917 [8:10:12<2:59:03,  3.76s/it] 68%|██████▊   | 6057/8917 [8:10:16<2:58:23,  3.74s/it] 68%|██████▊   | 6058/8917 [8:10:19<2:52:19,  3.62s/it] 68%|██████▊   | 6059/8917 [8:10:22<2:49:17,  3.55s/it] 68%|██████▊   | 6060/8917 [8:10:26<2:53:20,  3.64s/it] 68%|██████▊   | 6061/8917 [8:10:30<2:50:10,  3.57s/it] 68%|██████▊   | 6062/8917 [8:10:34<2:55:56,  3.70s/it] 68%|██████▊   | 6063/8917 [8:10:38<3:00:17,  3.79s/it] 68%|██████▊   | 6064/8917 [8:10:41<2:56:50,  3.72s/it] 68%|██████▊   | 6065/8917 [8:10:45<2:52:35,  3.63s/it] 68%|██████▊   | 6066/8917 [8:10:48<2:54:28,  3.67s/it] 68%|██████▊   | 6067/8917 [8:10:52<2:57:46,  3.74s/it] 68%|██████▊   | 6068/8917 [8:10:56<2:58:18,  3.76s/it] 68%|██████▊   | 6069/8917 [8:11:00<3:01:26,  3.82s/it] 68%|██████▊   | 6070/8917 [8:11:04<3:01:20,  3.82s/it] 68%|██████▊   | 6071/8917 [8:11:07<2:57:44,  3.75s/it] 68%|██████▊   | 6072/8917 [8:11:11<2:58:36,  3.77s/it] 68%|██████▊   | 6073/8917 [8:11:15<2:55:55,  3.71s/it] 68%|██████▊   | 6074/8917 [8:11:18<2:55:13,  3.70s/it] 68%|██████▊   | 6075/8917 [8:11:22<2:55:59,  3.72s/it] 68%|██████▊   | 6076/8917 [8:11:26<2:54:15,  3.68s/it] 68%|██████▊   | 6077/8917 [8:11:29<2:54:24,  3.68s/it] 68%|██████▊   | 6078/8917 [8:11:33<2:58:38,  3.78s/it] 68%|██████▊   | 6079/8917 [8:11:37<2:59:01,  3.78s/it] 68%|██████▊   | 6080/8917 [8:11:41<2:55:44,  3.72s/it] 68%|██████▊   | 6081/8917 [8:11:44<2:54:10,  3.68s/it] 68%|██████▊   | 6082/8917 [8:11:48<2:52:32,  3.65s/it] 68%|██████▊   | 6083/8917 [8:11:52<2:53:02,  3.66s/it] 68%|██████▊   | 6084/8917 [8:11:55<2:52:13,  3.65s/it] 68%|██████▊   | 6085/8917 [8:11:59<2:51:57,  3.64s/it] 68%|██████▊   | 6086/8917 [8:12:03<2:55:57,  3.73s/it] 68%|██████▊   | 6087/8917 [8:12:07<2:57:40,  3.77s/it] 68%|██████▊   | 6088/8917 [8:12:11<3:00:21,  3.83s/it] 68%|██████▊   | 6089/8917 [8:12:14<2:55:29,  3.72s/it] 68%|██████▊   | 6090/8917 [8:12:18<2:56:28,  3.75s/it] 68%|██████▊   | 6091/8917 [8:12:22<2:58:23,  3.79s/it] 68%|██████▊   | 6092/8917 [8:12:25<2:55:50,  3.73s/it] 68%|██████▊   | 6093/8917 [8:12:29<2:53:52,  3.69s/it] 68%|██████▊   | 6094/8917 [8:12:33<2:58:13,  3.79s/it] 68%|██████▊   | 6095/8917 [8:12:36<2:53:07,  3.68s/it] 68%|██████▊   | 6096/8917 [8:12:40<2:54:35,  3.71s/it] 68%|██████▊   | 6097/8917 [8:12:44<2:52:38,  3.67s/it] 68%|██████▊   | 6098/8917 [8:12:47<2:47:16,  3.56s/it] 68%|██████▊   | 6099/8917 [8:12:51<2:52:16,  3.67s/it]09/19/2024 10:27:29 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020309515297412872, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3645083904266357, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3848179578781128}
 68%|██████▊   | 6100/8917 [8:12:55<2:53:10,  3.69s/it] 68%|██████▊   | 6101/8917 [8:12:58<2:52:56,  3.69s/it] 68%|██████▊   | 6102/8917 [8:13:02<2:50:36,  3.64s/it] 68%|██████▊   | 6103/8917 [8:13:06<2:52:42,  3.68s/it] 68%|██████▊   | 6104/8917 [8:13:10<3:00:00,  3.84s/it] 68%|██████▊   | 6105/8917 [8:13:14<2:59:09,  3.82s/it] 68%|██████▊   | 6106/8917 [8:13:18<2:58:20,  3.81s/it] 68%|██████▊   | 6107/8917 [8:13:21<2:57:01,  3.78s/it] 68%|██████▊   | 6108/8917 [8:13:25<2:56:53,  3.78s/it] 69%|██████▊   | 6109/8917 [8:13:29<2:54:54,  3.74s/it] 69%|██████▊   | 6110/8917 [8:13:32<2:55:06,  3.74s/it] 69%|██████▊   | 6111/8917 [8:13:36<2:52:43,  3.69s/it] 69%|██████▊   | 6112/8917 [8:13:40<2:49:52,  3.63s/it] 69%|██████▊   | 6113/8917 [8:13:43<2:50:59,  3.66s/it] 69%|██████▊   | 6114/8917 [8:13:47<2:55:06,  3.75s/it] 69%|██████▊   | 6115/8917 [8:13:51<2:58:16,  3.82s/it] 69%|██████▊   | 6116/8917 [8:13:55<2:55:06,  3.75s/it] 69%|██████▊   | 6117/8917 [8:13:58<2:53:02,  3.71s/it] 69%|██████▊   | 6118/8917 [8:14:02<2:55:59,  3.77s/it] 69%|██████▊   | 6119/8917 [8:14:06<2:50:35,  3.66s/it] 69%|██████▊   | 6120/8917 [8:14:10<2:52:40,  3.70s/it] 69%|██████▊   | 6121/8917 [8:14:13<2:55:55,  3.78s/it] 69%|██████▊   | 6122/8917 [8:14:17<2:57:55,  3.82s/it] 69%|██████▊   | 6123/8917 [8:14:21<2:52:47,  3.71s/it] 69%|██████▊   | 6124/8917 [8:14:25<2:52:21,  3.70s/it] 69%|██████▊   | 6125/8917 [8:14:28<2:54:41,  3.75s/it] 69%|██████▊   | 6126/8917 [8:14:32<2:52:13,  3.70s/it] 69%|██████▊   | 6127/8917 [8:14:36<2:53:35,  3.73s/it] 69%|██████▊   | 6128/8917 [8:14:40<2:56:50,  3.80s/it] 69%|██████▊   | 6129/8917 [8:14:43<2:54:01,  3.75s/it] 69%|██████▊   | 6130/8917 [8:14:47<2:46:11,  3.58s/it] 69%|██████▉   | 6131/8917 [8:14:50<2:48:55,  3.64s/it] 69%|██████▉   | 6132/8917 [8:14:54<2:48:13,  3.62s/it] 69%|██████▉   | 6133/8917 [8:14:58<2:53:40,  3.74s/it] 69%|██████▉   | 6134/8917 [8:15:02<2:57:16,  3.82s/it] 69%|██████▉   | 6135/8917 [8:15:06<2:56:47,  3.81s/it] 69%|██████▉   | 6136/8917 [8:15:09<2:50:05,  3.67s/it] 69%|██████▉   | 6137/8917 [8:15:13<2:51:04,  3.69s/it] 69%|██████▉   | 6138/8917 [8:15:17<2:54:19,  3.76s/it] 69%|██████▉   | 6139/8917 [8:15:20<2:51:16,  3.70s/it] 69%|██████▉   | 6140/8917 [8:15:24<2:50:45,  3.69s/it] 69%|██████▉   | 6141/8917 [8:15:28<2:56:01,  3.80s/it] 69%|██████▉   | 6142/8917 [8:15:32<2:54:53,  3.78s/it] 69%|██████▉   | 6143/8917 [8:15:36<2:58:08,  3.85s/it] 69%|██████▉   | 6144/8917 [8:15:39<2:54:20,  3.77s/it] 69%|██████▉   | 6145/8917 [8:15:43<2:56:13,  3.81s/it] 69%|██████▉   | 6146/8917 [8:15:47<2:53:34,  3.76s/it] 69%|██████▉   | 6147/8917 [8:15:50<2:47:15,  3.62s/it] 69%|██████▉   | 6148/8917 [8:15:54<2:51:37,  3.72s/it] 69%|██████▉   | 6149/8917 [8:15:58<2:47:46,  3.64s/it]09/19/2024 10:30:35 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.028404388576745987, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.040663480758667, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0690678358078003}
 69%|██████▉   | 6150/8917 [8:16:02<2:53:52,  3.77s/it] 69%|██████▉   | 6151/8917 [8:16:06<2:54:49,  3.79s/it] 69%|██████▉   | 6152/8917 [8:16:09<2:52:47,  3.75s/it] 69%|██████▉   | 6153/8917 [8:16:13<2:49:18,  3.68s/it] 69%|██████▉   | 6154/8917 [8:16:16<2:51:17,  3.72s/it] 69%|██████▉   | 6155/8917 [8:16:20<2:53:09,  3.76s/it] 69%|██████▉   | 6156/8917 [8:16:24<2:49:38,  3.69s/it] 69%|██████▉   | 6157/8917 [8:16:27<2:45:44,  3.60s/it] 69%|██████▉   | 6158/8917 [8:16:31<2:51:19,  3.73s/it] 69%|██████▉   | 6159/8917 [8:16:35<2:52:28,  3.75s/it] 69%|██████▉   | 6160/8917 [8:16:39<2:50:16,  3.71s/it] 69%|██████▉   | 6161/8917 [8:16:43<2:54:41,  3.80s/it] 69%|██████▉   | 6162/8917 [8:16:46<2:52:50,  3.76s/it] 69%|██████▉   | 6163/8917 [8:16:50<2:51:53,  3.75s/it] 69%|██████▉   | 6164/8917 [8:16:54<2:49:25,  3.69s/it] 69%|██████▉   | 6165/8917 [8:16:58<2:52:30,  3.76s/it] 69%|██████▉   | 6166/8917 [8:17:01<2:51:52,  3.75s/it] 69%|██████▉   | 6167/8917 [8:17:05<2:51:18,  3.74s/it] 69%|██████▉   | 6168/8917 [8:17:08<2:47:30,  3.66s/it] 69%|██████▉   | 6169/8917 [8:17:12<2:48:56,  3.69s/it] 69%|██████▉   | 6170/8917 [8:17:16<2:54:18,  3.81s/it] 69%|██████▉   | 6171/8917 [8:17:20<2:54:09,  3.81s/it] 69%|██████▉   | 6172/8917 [8:17:24<2:53:22,  3.79s/it] 69%|██████▉   | 6173/8917 [8:17:27<2:50:09,  3.72s/it] 69%|██████▉   | 6174/8917 [8:17:31<2:51:42,  3.76s/it] 69%|██████▉   | 6175/8917 [8:17:36<2:58:03,  3.90s/it] 69%|██████▉   | 6176/8917 [8:17:39<2:57:56,  3.89s/it] 69%|██████▉   | 6177/8917 [8:17:43<2:54:19,  3.82s/it] 69%|██████▉   | 6178/8917 [8:17:46<2:48:36,  3.69s/it] 69%|██████▉   | 6179/8917 [8:17:50<2:48:03,  3.68s/it] 69%|██████▉   | 6180/8917 [8:17:54<2:49:38,  3.72s/it] 69%|██████▉   | 6181/8917 [8:17:58<2:52:36,  3.79s/it] 69%|██████▉   | 6182/8917 [8:18:01<2:50:22,  3.74s/it] 69%|██████▉   | 6183/8917 [8:18:05<2:49:18,  3.72s/it] 69%|██████▉   | 6184/8917 [8:18:09<2:50:27,  3.74s/it] 69%|██████▉   | 6185/8917 [8:18:13<2:52:49,  3.80s/it] 69%|██████▉   | 6186/8917 [8:18:17<2:52:20,  3.79s/it] 69%|██████▉   | 6187/8917 [8:18:20<2:49:36,  3.73s/it] 69%|██████▉   | 6188/8917 [8:18:24<2:45:19,  3.63s/it] 69%|██████▉   | 6189/8917 [8:18:27<2:40:41,  3.53s/it] 69%|██████▉   | 6190/8917 [8:18:31<2:44:30,  3.62s/it] 69%|██████▉   | 6191/8917 [8:18:35<2:47:52,  3.70s/it] 69%|██████▉   | 6192/8917 [8:18:38<2:49:27,  3.73s/it] 69%|██████▉   | 6193/8917 [8:18:42<2:51:36,  3.78s/it] 69%|██████▉   | 6194/8917 [8:18:46<2:48:51,  3.72s/it] 69%|██████▉   | 6195/8917 [8:18:50<2:53:04,  3.81s/it] 69%|██████▉   | 6196/8917 [8:18:54<2:51:00,  3.77s/it] 69%|██████▉   | 6197/8917 [8:18:57<2:49:17,  3.73s/it] 70%|██████▉   | 6198/8917 [8:19:01<2:48:21,  3.72s/it] 70%|██████▉   | 6199/8917 [8:19:05<2:46:50,  3.68s/it]09/19/2024 10:33:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02277197130024433, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0737618207931519, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0965337753295898}
 70%|██████▉   | 6200/8917 [8:19:08<2:48:16,  3.72s/it] 70%|██████▉   | 6201/8917 [8:19:12<2:47:43,  3.71s/it] 70%|██████▉   | 6202/8917 [8:19:16<2:48:03,  3.71s/it] 70%|██████▉   | 6203/8917 [8:19:19<2:47:55,  3.71s/it] 70%|██████▉   | 6204/8917 [8:19:23<2:49:34,  3.75s/it] 70%|██████▉   | 6205/8917 [8:19:27<2:46:50,  3.69s/it] 70%|██████▉   | 6206/8917 [8:19:31<2:50:12,  3.77s/it] 70%|██████▉   | 6207/8917 [8:19:34<2:47:34,  3.71s/it] 70%|██████▉   | 6208/8917 [8:19:39<2:55:11,  3.88s/it] 70%|██████▉   | 6209/8917 [8:19:42<2:53:36,  3.85s/it] 70%|██████▉   | 6210/8917 [8:19:46<2:47:07,  3.70s/it] 70%|██████▉   | 6211/8917 [8:19:49<2:45:56,  3.68s/it] 70%|██████▉   | 6212/8917 [8:19:53<2:49:46,  3.77s/it] 70%|██████▉   | 6213/8917 [8:19:57<2:48:27,  3.74s/it] 70%|██████▉   | 6214/8917 [8:20:01<2:52:32,  3.83s/it] 70%|██████▉   | 6215/8917 [8:20:05<2:49:34,  3.77s/it] 70%|██████▉   | 6216/8917 [8:20:08<2:48:26,  3.74s/it] 70%|██████▉   | 6217/8917 [8:20:12<2:44:45,  3.66s/it] 70%|██████▉   | 6218/8917 [8:20:16<2:49:32,  3.77s/it] 70%|██████▉   | 6219/8917 [8:20:20<2:48:48,  3.75s/it] 70%|██████▉   | 6220/8917 [8:20:23<2:48:13,  3.74s/it] 70%|██████▉   | 6221/8917 [8:20:27<2:52:14,  3.83s/it] 70%|██████▉   | 6222/8917 [8:20:31<2:51:05,  3.81s/it] 70%|██████▉   | 6223/8917 [8:20:35<2:48:15,  3.75s/it] 70%|██████▉   | 6224/8917 [8:20:38<2:48:26,  3.75s/it] 70%|██████▉   | 6225/8917 [8:20:42<2:47:23,  3.73s/it] 70%|██████▉   | 6226/8917 [8:20:46<2:49:53,  3.79s/it] 70%|██████▉   | 6227/8917 [8:20:50<2:51:07,  3.82s/it] 70%|██████▉   | 6228/8917 [8:20:54<2:47:09,  3.73s/it] 70%|██████▉   | 6229/8917 [8:20:57<2:49:40,  3.79s/it] 70%|██████▉   | 6230/8917 [8:21:02<2:54:04,  3.89s/it] 70%|██████▉   | 6231/8917 [8:21:05<2:52:12,  3.85s/it] 70%|██████▉   | 6232/8917 [8:21:09<2:48:40,  3.77s/it] 70%|██████▉   | 6233/8917 [8:21:12<2:42:19,  3.63s/it] 70%|██████▉   | 6234/8917 [8:21:16<2:44:20,  3.68s/it] 70%|██████▉   | 6235/8917 [8:21:20<2:46:18,  3.72s/it] 70%|██████▉   | 6236/8917 [8:21:23<2:44:22,  3.68s/it] 70%|██████▉   | 6237/8917 [8:21:27<2:46:11,  3.72s/it] 70%|██████▉   | 6238/8917 [8:21:31<2:47:07,  3.74s/it] 70%|██████▉   | 6239/8917 [8:21:35<2:46:40,  3.73s/it] 70%|██████▉   | 6240/8917 [8:21:38<2:44:36,  3.69s/it] 70%|██████▉   | 6241/8917 [8:21:42<2:43:29,  3.67s/it] 70%|███████   | 6242/8917 [8:21:46<2:44:38,  3.69s/it] 70%|███████   | 6243/8917 [8:21:49<2:44:38,  3.69s/it] 70%|███████   | 6244/8917 [8:21:53<2:44:24,  3.69s/it] 70%|███████   | 6245/8917 [8:21:57<2:43:00,  3.66s/it] 70%|███████   | 6246/8917 [8:22:00<2:42:57,  3.66s/it] 70%|███████   | 6247/8917 [8:22:04<2:45:13,  3.71s/it] 70%|███████   | 6248/8917 [8:22:08<2:47:57,  3.78s/it] 70%|███████   | 6249/8917 [8:22:11<2:42:51,  3.66s/it]09/19/2024 10:36:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.026399893686175346, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3616617918014526, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3880616426467896}
 70%|███████   | 6250/8917 [8:22:15<2:47:33,  3.77s/it] 70%|███████   | 6251/8917 [8:22:19<2:45:08,  3.72s/it] 70%|███████   | 6252/8917 [8:22:23<2:46:04,  3.74s/it] 70%|███████   | 6253/8917 [8:22:27<2:50:58,  3.85s/it] 70%|███████   | 6254/8917 [8:22:31<2:48:59,  3.81s/it] 70%|███████   | 6255/8917 [8:22:34<2:45:30,  3.73s/it] 70%|███████   | 6256/8917 [8:22:38<2:47:58,  3.79s/it] 70%|███████   | 6257/8917 [8:22:42<2:47:07,  3.77s/it] 70%|███████   | 6258/8917 [8:22:45<2:44:36,  3.71s/it] 70%|███████   | 6259/8917 [8:22:49<2:42:16,  3.66s/it] 70%|███████   | 6260/8917 [8:22:53<2:41:48,  3.65s/it] 70%|███████   | 6261/8917 [8:22:56<2:41:49,  3.66s/it] 70%|███████   | 6262/8917 [8:23:00<2:40:04,  3.62s/it] 70%|███████   | 6263/8917 [8:23:04<2:46:46,  3.77s/it] 70%|███████   | 6264/8917 [8:23:08<2:47:59,  3.80s/it] 70%|███████   | 6265/8917 [8:23:11<2:45:02,  3.73s/it] 70%|███████   | 6266/8917 [8:23:15<2:39:58,  3.62s/it] 70%|███████   | 6267/8917 [8:23:18<2:39:03,  3.60s/it] 70%|███████   | 6268/8917 [8:23:22<2:43:51,  3.71s/it] 70%|███████   | 6269/8917 [8:23:26<2:46:44,  3.78s/it] 70%|███████   | 6270/8917 [8:23:30<2:43:54,  3.72s/it] 70%|███████   | 6271/8917 [8:23:34<2:46:19,  3.77s/it] 70%|███████   | 6272/8917 [8:23:38<2:54:39,  3.96s/it] 70%|███████   | 6273/8917 [8:23:42<2:49:41,  3.85s/it] 70%|███████   | 6274/8917 [8:23:45<2:48:37,  3.83s/it] 70%|███████   | 6275/8917 [8:23:49<2:47:18,  3.80s/it] 70%|███████   | 6276/8917 [8:23:53<2:46:54,  3.79s/it] 70%|███████   | 6277/8917 [8:23:57<2:44:10,  3.73s/it] 70%|███████   | 6278/8917 [8:24:00<2:44:21,  3.74s/it] 70%|███████   | 6279/8917 [8:24:04<2:45:03,  3.75s/it] 70%|███████   | 6280/8917 [8:24:08<2:44:07,  3.73s/it] 70%|███████   | 6281/8917 [8:24:11<2:41:36,  3.68s/it] 70%|███████   | 6282/8917 [8:24:15<2:43:31,  3.72s/it] 70%|███████   | 6283/8917 [8:24:19<2:46:41,  3.80s/it] 70%|███████   | 6284/8917 [8:24:23<2:50:45,  3.89s/it] 70%|███████   | 6285/8917 [8:24:27<2:44:24,  3.75s/it] 70%|███████   | 6286/8917 [8:24:30<2:43:13,  3.72s/it] 71%|███████   | 6287/8917 [8:24:34<2:43:33,  3.73s/it] 71%|███████   | 6288/8917 [8:24:38<2:49:00,  3.86s/it] 71%|███████   | 6289/8917 [8:24:42<2:46:48,  3.81s/it] 71%|███████   | 6290/8917 [8:24:46<2:46:18,  3.80s/it] 71%|███████   | 6291/8917 [8:24:50<2:47:59,  3.84s/it] 71%|███████   | 6292/8917 [8:24:53<2:47:34,  3.83s/it] 71%|███████   | 6293/8917 [8:24:57<2:48:10,  3.85s/it] 71%|███████   | 6294/8917 [8:25:01<2:40:40,  3.68s/it] 71%|███████   | 6295/8917 [8:25:05<2:43:47,  3.75s/it] 71%|███████   | 6296/8917 [8:25:09<2:47:52,  3.84s/it] 71%|███████   | 6297/8917 [8:25:12<2:44:42,  3.77s/it] 71%|███████   | 6298/8917 [8:25:16<2:43:05,  3.74s/it] 71%|███████   | 6299/8917 [8:25:20<2:47:48,  3.85s/it]09/19/2024 10:39:57 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.016225988045334816, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1917729377746582, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2079988718032837}
 71%|███████   | 6300/8917 [8:25:24<2:44:57,  3.78s/it] 71%|███████   | 6301/8917 [8:25:27<2:43:19,  3.75s/it] 71%|███████   | 6302/8917 [8:25:31<2:41:56,  3.72s/it] 71%|███████   | 6303/8917 [8:25:34<2:38:12,  3.63s/it] 71%|███████   | 6304/8917 [8:25:38<2:39:41,  3.67s/it] 71%|███████   | 6305/8917 [8:25:42<2:40:41,  3.69s/it] 71%|███████   | 6306/8917 [8:25:46<2:42:21,  3.73s/it] 71%|███████   | 6307/8917 [8:25:49<2:41:03,  3.70s/it] 71%|███████   | 6308/8917 [8:25:53<2:39:24,  3.67s/it] 71%|███████   | 6309/8917 [8:25:57<2:47:05,  3.84s/it] 71%|███████   | 6310/8917 [8:26:01<2:43:56,  3.77s/it] 71%|███████   | 6311/8917 [8:26:04<2:43:56,  3.77s/it] 71%|███████   | 6312/8917 [8:26:08<2:44:11,  3.78s/it] 71%|███████   | 6313/8917 [8:26:12<2:42:39,  3.75s/it] 71%|███████   | 6314/8917 [8:26:16<2:46:12,  3.83s/it] 71%|███████   | 6315/8917 [8:26:20<2:42:53,  3.76s/it] 71%|███████   | 6316/8917 [8:26:23<2:38:52,  3.66s/it] 71%|███████   | 6317/8917 [8:26:27<2:37:21,  3.63s/it] 71%|███████   | 6318/8917 [8:26:30<2:40:43,  3.71s/it] 71%|███████   | 6319/8917 [8:26:34<2:41:09,  3.72s/it] 71%|███████   | 6320/8917 [8:26:38<2:47:37,  3.87s/it] 71%|███████   | 6321/8917 [8:26:42<2:45:45,  3.83s/it] 71%|███████   | 6322/8917 [8:26:46<2:47:22,  3.87s/it] 71%|███████   | 6323/8917 [8:26:50<2:46:24,  3.85s/it] 71%|███████   | 6324/8917 [8:26:53<2:42:28,  3.76s/it] 71%|███████   | 6325/8917 [8:26:57<2:43:07,  3.78s/it] 71%|███████   | 6326/8917 [8:27:01<2:41:11,  3.73s/it] 71%|███████   | 6327/8917 [8:27:05<2:43:44,  3.79s/it] 71%|███████   | 6328/8917 [8:27:09<2:42:29,  3.77s/it] 71%|███████   | 6329/8917 [8:27:12<2:42:15,  3.76s/it] 71%|███████   | 6330/8917 [8:27:16<2:45:40,  3.84s/it] 71%|███████   | 6331/8917 [8:27:20<2:41:00,  3.74s/it] 71%|███████   | 6332/8917 [8:27:24<2:42:39,  3.78s/it] 71%|███████   | 6333/8917 [8:27:27<2:41:27,  3.75s/it] 71%|███████   | 6334/8917 [8:27:31<2:37:36,  3.66s/it] 71%|███████   | 6335/8917 [8:27:35<2:39:11,  3.70s/it] 71%|███████   | 6336/8917 [8:27:38<2:38:19,  3.68s/it] 71%|███████   | 6337/8917 [8:27:41<2:32:06,  3.54s/it] 71%|███████   | 6338/8917 [8:27:45<2:35:17,  3.61s/it] 71%|███████   | 6339/8917 [8:27:50<2:46:38,  3.88s/it] 71%|███████   | 6340/8917 [8:27:53<2:41:02,  3.75s/it] 71%|███████   | 6341/8917 [8:27:57<2:39:14,  3.71s/it] 71%|███████   | 6342/8917 [8:28:01<2:40:52,  3.75s/it] 71%|███████   | 6343/8917 [8:28:04<2:41:24,  3.76s/it] 71%|███████   | 6344/8917 [8:28:08<2:42:11,  3.78s/it] 71%|███████   | 6345/8917 [8:28:12<2:40:44,  3.75s/it] 71%|███████   | 6346/8917 [8:28:16<2:41:09,  3.76s/it] 71%|███████   | 6347/8917 [8:28:19<2:39:40,  3.73s/it] 71%|███████   | 6348/8917 [8:28:23<2:38:05,  3.69s/it] 71%|███████   | 6349/8917 [8:28:27<2:36:17,  3.65s/it]09/19/2024 10:43:04 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.034515369683504105, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.800095558166504, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.834610939025879}
 71%|███████   | 6350/8917 [8:28:30<2:37:02,  3.67s/it] 71%|███████   | 6351/8917 [8:28:34<2:41:21,  3.77s/it] 71%|███████   | 6352/8917 [8:28:38<2:37:38,  3.69s/it] 71%|███████   | 6353/8917 [8:28:42<2:40:11,  3.75s/it] 71%|███████▏  | 6354/8917 [8:28:45<2:38:27,  3.71s/it] 71%|███████▏  | 6355/8917 [8:28:49<2:36:51,  3.67s/it] 71%|███████▏  | 6356/8917 [8:28:54<2:49:23,  3.97s/it] 71%|███████▏  | 6357/8917 [8:28:57<2:48:29,  3.95s/it] 71%|███████▏  | 6358/8917 [8:29:01<2:40:49,  3.77s/it] 71%|███████▏  | 6359/8917 [8:29:04<2:38:47,  3.72s/it] 71%|███████▏  | 6360/8917 [8:29:08<2:39:08,  3.73s/it] 71%|███████▏  | 6361/8917 [8:29:12<2:34:38,  3.63s/it] 71%|███████▏  | 6362/8917 [8:29:16<2:39:29,  3.75s/it] 71%|███████▏  | 6363/8917 [8:29:19<2:41:36,  3.80s/it] 71%|███████▏  | 6364/8917 [8:29:23<2:39:54,  3.76s/it] 71%|███████▏  | 6365/8917 [8:29:27<2:43:45,  3.85s/it] 71%|███████▏  | 6366/8917 [8:29:31<2:48:28,  3.96s/it] 71%|███████▏  | 6367/8917 [8:29:35<2:43:38,  3.85s/it] 71%|███████▏  | 6368/8917 [8:29:38<2:36:08,  3.68s/it] 71%|███████▏  | 6369/8917 [8:29:42<2:36:37,  3.69s/it] 71%|███████▏  | 6370/8917 [8:29:46<2:36:28,  3.69s/it] 71%|███████▏  | 6371/8917 [8:29:49<2:32:31,  3.59s/it] 71%|███████▏  | 6372/8917 [8:29:53<2:33:22,  3.62s/it] 71%|███████▏  | 6373/8917 [8:29:56<2:33:59,  3.63s/it] 71%|███████▏  | 6374/8917 [8:30:00<2:36:36,  3.70s/it] 71%|███████▏  | 6375/8917 [8:30:04<2:36:39,  3.70s/it] 72%|███████▏  | 6376/8917 [8:30:08<2:38:17,  3.74s/it] 72%|███████▏  | 6377/8917 [8:30:12<2:38:09,  3.74s/it] 72%|███████▏  | 6378/8917 [8:30:15<2:35:13,  3.67s/it] 72%|███████▏  | 6379/8917 [8:30:18<2:30:15,  3.55s/it] 72%|███████▏  | 6380/8917 [8:30:22<2:33:33,  3.63s/it] 72%|███████▏  | 6381/8917 [8:30:26<2:35:31,  3.68s/it] 72%|███████▏  | 6382/8917 [8:30:30<2:38:44,  3.76s/it] 72%|███████▏  | 6383/8917 [8:30:33<2:33:28,  3.63s/it] 72%|███████▏  | 6384/8917 [8:30:37<2:32:43,  3.62s/it] 72%|███████▏  | 6385/8917 [8:30:41<2:36:10,  3.70s/it] 72%|███████▏  | 6386/8917 [8:30:44<2:37:24,  3.73s/it] 72%|███████▏  | 6387/8917 [8:30:48<2:35:48,  3.69s/it] 72%|███████▏  | 6388/8917 [8:30:52<2:33:55,  3.65s/it] 72%|███████▏  | 6389/8917 [8:30:56<2:37:04,  3.73s/it] 72%|███████▏  | 6390/8917 [8:30:59<2:33:37,  3.65s/it] 72%|███████▏  | 6391/8917 [8:31:03<2:35:58,  3.70s/it] 72%|███████▏  | 6392/8917 [8:31:07<2:35:38,  3.70s/it] 72%|███████▏  | 6393/8917 [8:31:10<2:34:54,  3.68s/it] 72%|███████▏  | 6394/8917 [8:31:14<2:31:29,  3.60s/it] 72%|███████▏  | 6395/8917 [8:31:17<2:31:47,  3.61s/it] 72%|███████▏  | 6396/8917 [8:31:21<2:31:49,  3.61s/it] 72%|███████▏  | 6397/8917 [8:31:25<2:35:04,  3.69s/it] 72%|███████▏  | 6398/8917 [8:31:28<2:35:46,  3.71s/it] 72%|███████▏  | 6399/8917 [8:31:32<2:33:48,  3.66s/it]09/19/2024 10:46:10 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.010985848493874073, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0443319082260132, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.055317759513855}
 72%|███████▏  | 6400/8917 [8:31:36<2:33:35,  3.66s/it] 72%|███████▏  | 6401/8917 [8:31:40<2:37:21,  3.75s/it] 72%|███████▏  | 6402/8917 [8:31:44<2:38:38,  3.78s/it] 72%|███████▏  | 6403/8917 [8:31:47<2:35:13,  3.70s/it] 72%|███████▏  | 6404/8917 [8:31:51<2:34:29,  3.69s/it] 72%|███████▏  | 6405/8917 [8:31:55<2:37:14,  3.76s/it] 72%|███████▏  | 6406/8917 [8:31:58<2:33:53,  3.68s/it] 72%|███████▏  | 6407/8917 [8:32:02<2:34:20,  3.69s/it] 72%|███████▏  | 6408/8917 [8:32:06<2:34:47,  3.70s/it] 72%|███████▏  | 6409/8917 [8:32:09<2:36:48,  3.75s/it] 72%|███████▏  | 6410/8917 [8:32:13<2:37:30,  3.77s/it] 72%|███████▏  | 6411/8917 [8:32:17<2:37:30,  3.77s/it] 72%|███████▏  | 6412/8917 [8:32:21<2:37:29,  3.77s/it] 72%|███████▏  | 6413/8917 [8:32:25<2:37:09,  3.77s/it] 72%|███████▏  | 6414/8917 [8:32:28<2:34:49,  3.71s/it] 72%|███████▏  | 6415/8917 [8:32:32<2:36:10,  3.75s/it] 72%|███████▏  | 6416/8917 [8:32:36<2:35:15,  3.72s/it] 72%|███████▏  | 6417/8917 [8:32:39<2:32:43,  3.67s/it] 72%|███████▏  | 6418/8917 [8:32:43<2:33:27,  3.68s/it] 72%|███████▏  | 6419/8917 [8:32:47<2:33:36,  3.69s/it] 72%|███████▏  | 6420/8917 [8:32:50<2:33:13,  3.68s/it] 72%|███████▏  | 6421/8917 [8:32:54<2:36:39,  3.77s/it] 72%|███████▏  | 6422/8917 [8:32:58<2:36:08,  3.76s/it] 72%|███████▏  | 6423/8917 [8:33:01<2:33:44,  3.70s/it] 72%|███████▏  | 6424/8917 [8:33:05<2:35:26,  3.74s/it] 72%|███████▏  | 6425/8917 [8:33:09<2:35:31,  3.74s/it] 72%|███████▏  | 6426/8917 [8:33:13<2:36:17,  3.76s/it] 72%|███████▏  | 6427/8917 [8:33:16<2:32:28,  3.67s/it] 72%|███████▏  | 6428/8917 [8:33:20<2:36:08,  3.76s/it] 72%|███████▏  | 6429/8917 [8:33:24<2:31:59,  3.67s/it] 72%|███████▏  | 6430/8917 [8:33:27<2:30:31,  3.63s/it] 72%|███████▏  | 6431/8917 [8:33:31<2:34:37,  3.73s/it] 72%|███████▏  | 6432/8917 [8:33:35<2:34:51,  3.74s/it] 72%|███████▏  | 6433/8917 [8:33:39<2:35:46,  3.76s/it] 72%|███████▏  | 6434/8917 [8:33:43<2:35:14,  3.75s/it] 72%|███████▏  | 6435/8917 [8:33:46<2:34:32,  3.74s/it] 72%|███████▏  | 6436/8917 [8:33:50<2:32:13,  3.68s/it] 72%|███████▏  | 6437/8917 [8:33:54<2:33:47,  3.72s/it] 72%|███████▏  | 6438/8917 [8:33:58<2:36:06,  3.78s/it] 72%|███████▏  | 6439/8917 [8:34:01<2:36:50,  3.80s/it] 72%|███████▏  | 6440/8917 [8:34:05<2:35:24,  3.76s/it] 72%|███████▏  | 6441/8917 [8:34:09<2:36:44,  3.80s/it] 72%|███████▏  | 6442/8917 [8:34:13<2:33:37,  3.72s/it] 72%|███████▏  | 6443/8917 [8:34:16<2:36:01,  3.78s/it] 72%|███████▏  | 6444/8917 [8:34:20<2:34:02,  3.74s/it] 72%|███████▏  | 6445/8917 [8:34:24<2:34:49,  3.76s/it] 72%|███████▏  | 6446/8917 [8:34:27<2:29:42,  3.64s/it] 72%|███████▏  | 6447/8917 [8:34:31<2:27:41,  3.59s/it] 72%|███████▏  | 6448/8917 [8:34:35<2:33:30,  3.73s/it] 72%|███████▏  | 6449/8917 [8:34:39<2:40:42,  3.91s/it]09/19/2024 10:49:16 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.013897870667278767, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9758803844451904, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9897782802581787}
 72%|███████▏  | 6450/8917 [8:34:43<2:35:07,  3.77s/it] 72%|███████▏  | 6451/8917 [8:34:46<2:35:12,  3.78s/it] 72%|███████▏  | 6452/8917 [8:34:50<2:35:36,  3.79s/it] 72%|███████▏  | 6453/8917 [8:34:54<2:30:48,  3.67s/it] 72%|███████▏  | 6454/8917 [8:34:58<2:35:14,  3.78s/it] 72%|███████▏  | 6455/8917 [8:35:01<2:36:07,  3.80s/it] 72%|███████▏  | 6456/8917 [8:35:05<2:36:10,  3.81s/it] 72%|███████▏  | 6457/8917 [8:35:09<2:30:06,  3.66s/it] 72%|███████▏  | 6458/8917 [8:35:12<2:30:03,  3.66s/it] 72%|███████▏  | 6459/8917 [8:35:16<2:34:22,  3.77s/it] 72%|███████▏  | 6460/8917 [8:35:20<2:35:23,  3.79s/it] 72%|███████▏  | 6461/8917 [8:35:24<2:32:16,  3.72s/it] 72%|███████▏  | 6462/8917 [8:35:28<2:34:40,  3.78s/it] 72%|███████▏  | 6463/8917 [8:35:31<2:29:33,  3.66s/it] 72%|███████▏  | 6464/8917 [8:35:35<2:32:06,  3.72s/it] 73%|███████▎  | 6465/8917 [8:35:39<2:32:08,  3.72s/it] 73%|███████▎  | 6466/8917 [8:35:42<2:33:11,  3.75s/it] 73%|███████▎  | 6467/8917 [8:35:46<2:30:47,  3.69s/it] 73%|███████▎  | 6468/8917 [8:35:50<2:31:54,  3.72s/it] 73%|███████▎  | 6469/8917 [8:35:53<2:30:51,  3.70s/it] 73%|███████▎  | 6470/8917 [8:35:57<2:32:56,  3.75s/it] 73%|███████▎  | 6471/8917 [8:36:01<2:34:06,  3.78s/it] 73%|███████▎  | 6472/8917 [8:36:05<2:32:21,  3.74s/it] 73%|███████▎  | 6473/8917 [8:36:08<2:29:54,  3.68s/it] 73%|███████▎  | 6474/8917 [8:36:12<2:30:55,  3.71s/it] 73%|███████▎  | 6475/8917 [8:36:16<2:30:18,  3.69s/it] 73%|███████▎  | 6476/8917 [8:36:20<2:34:04,  3.79s/it] 73%|███████▎  | 6477/8917 [8:36:23<2:29:47,  3.68s/it] 73%|███████▎  | 6478/8917 [8:36:27<2:27:40,  3.63s/it] 73%|███████▎  | 6479/8917 [8:36:31<2:34:29,  3.80s/it] 73%|███████▎  | 6480/8917 [8:36:35<2:35:44,  3.83s/it] 73%|███████▎  | 6481/8917 [8:36:38<2:31:07,  3.72s/it] 73%|███████▎  | 6482/8917 [8:36:42<2:33:58,  3.79s/it] 73%|███████▎  | 6483/8917 [8:36:46<2:30:13,  3.70s/it] 73%|███████▎  | 6484/8917 [8:36:49<2:27:40,  3.64s/it] 73%|███████▎  | 6485/8917 [8:36:53<2:29:27,  3.69s/it] 73%|███████▎  | 6486/8917 [8:36:57<2:28:18,  3.66s/it] 73%|███████▎  | 6487/8917 [8:37:00<2:31:08,  3.73s/it] 73%|███████▎  | 6488/8917 [8:37:04<2:32:04,  3.76s/it] 73%|███████▎  | 6489/8917 [8:37:08<2:28:56,  3.68s/it] 73%|███████▎  | 6490/8917 [8:37:12<2:31:05,  3.74s/it] 73%|███████▎  | 6491/8917 [8:37:16<2:33:44,  3.80s/it] 73%|███████▎  | 6492/8917 [8:37:19<2:31:44,  3.75s/it] 73%|███████▎  | 6493/8917 [8:37:23<2:28:55,  3.69s/it] 73%|███████▎  | 6494/8917 [8:37:27<2:29:31,  3.70s/it] 73%|███████▎  | 6495/8917 [8:37:30<2:29:22,  3.70s/it] 73%|███████▎  | 6496/8917 [8:37:34<2:30:09,  3.72s/it] 73%|███████▎  | 6497/8917 [8:37:38<2:28:42,  3.69s/it] 73%|███████▎  | 6498/8917 [8:37:41<2:28:11,  3.68s/it] 73%|███████▎  | 6499/8917 [8:37:45<2:30:38,  3.74s/it]09/19/2024 10:52:22 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 10:52:22 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<03:26,  1.06it/s][A
  1%|          | 2/221 [00:01<02:19,  1.57it/s][A
  1%|▏         | 3/221 [00:02<02:27,  1.48it/s][A
  2%|▏         | 4/221 [00:02<01:53,  1.91it/s][A
  2%|▏         | 5/221 [00:02<01:22,  2.62it/s][A
  3%|▎         | 6/221 [00:02<01:02,  3.45it/s][A
  3%|▎         | 7/221 [00:02<00:53,  3.97it/s][A
  4%|▎         | 8/221 [00:03<01:15,  2.81it/s][A
  4%|▍         | 9/221 [00:03<01:15,  2.81it/s][A
  5%|▍         | 10/221 [00:04<01:22,  2.57it/s][A
  5%|▍         | 11/221 [00:04<01:05,  3.23it/s][A
  5%|▌         | 12/221 [00:10<07:39,  2.20s/it][A
  6%|▌         | 13/221 [00:11<05:40,  1.64s/it][A
  6%|▋         | 14/221 [00:11<04:05,  1.19s/it][A
  7%|▋         | 15/221 [00:11<03:22,  1.02it/s][A
  7%|▋         | 16/221 [00:12<03:03,  1.12it/s][A
  8%|▊         | 17/221 [00:13<03:14,  1.05it/s][A
  8%|▊         | 18/221 [00:14<02:41,  1.26it/s][A
  9%|▊         | 19/221 [00:16<04:42,  1.40s/it][A
  9%|▉         | 20/221 [00:16<03:23,  1.01s/it][A
 10%|▉         | 21/221 [00:17<02:41,  1.24it/s][A
 10%|▉         | 22/221 [00:18<02:36,  1.27it/s][A
 11%|█         | 24/221 [00:18<01:43,  1.91it/s][A
 11%|█▏        | 25/221 [00:18<01:30,  2.17it/s][A
 12%|█▏        | 26/221 [00:19<01:22,  2.37it/s][A
 13%|█▎        | 28/221 [00:19<01:13,  2.62it/s][A
 13%|█▎        | 29/221 [00:19<01:07,  2.87it/s][A
 14%|█▎        | 30/221 [00:20<01:26,  2.21it/s][A
 14%|█▍        | 31/221 [00:21<01:27,  2.18it/s][A
 14%|█▍        | 32/221 [00:21<01:08,  2.75it/s][A
 15%|█▍        | 33/221 [00:21<01:10,  2.68it/s][A
 16%|█▌        | 35/221 [00:21<00:47,  3.95it/s][A
 16%|█▋        | 36/221 [00:22<00:43,  4.21it/s][A
 17%|█▋        | 37/221 [00:22<00:51,  3.55it/s][A
 17%|█▋        | 38/221 [00:23<01:02,  2.92it/s][A
 18%|█▊        | 39/221 [00:23<01:18,  2.31it/s][A
 18%|█▊        | 40/221 [00:24<01:26,  2.09it/s][A
 19%|█▊        | 41/221 [00:24<01:11,  2.51it/s][A
 19%|█▉        | 42/221 [00:24<00:58,  3.07it/s][A
 19%|█▉        | 43/221 [00:24<00:46,  3.84it/s][A
 20%|█▉        | 44/221 [00:24<00:37,  4.67it/s][A
 20%|██        | 45/221 [00:27<02:33,  1.15it/s][A
 21%|██        | 46/221 [00:27<01:59,  1.46it/s][A
 21%|██▏       | 47/221 [00:27<01:47,  1.62it/s][A
 22%|██▏       | 48/221 [00:28<01:20,  2.14it/s][A
 22%|██▏       | 49/221 [00:28<01:12,  2.37it/s][A
 23%|██▎       | 50/221 [00:28<01:07,  2.52it/s][A
 23%|██▎       | 51/221 [00:28<00:55,  3.08it/s][A
 24%|██▎       | 52/221 [00:29<00:50,  3.37it/s][A
 24%|██▍       | 53/221 [00:29<00:54,  3.08it/s][A
 24%|██▍       | 54/221 [00:30<01:17,  2.17it/s][A
 25%|██▍       | 55/221 [00:31<01:43,  1.61it/s][A
 25%|██▌       | 56/221 [00:31<01:21,  2.03it/s][A
 26%|██▌       | 57/221 [00:31<01:06,  2.46it/s][A
 27%|██▋       | 59/221 [00:31<00:45,  3.59it/s][A
 27%|██▋       | 60/221 [00:32<00:46,  3.47it/s][A
 28%|██▊       | 61/221 [00:32<00:44,  3.63it/s][A
 28%|██▊       | 62/221 [00:32<00:45,  3.51it/s][A
 29%|██▊       | 63/221 [00:33<00:42,  3.70it/s][A
 29%|██▉       | 64/221 [00:34<01:19,  1.97it/s][A
 29%|██▉       | 65/221 [00:34<01:03,  2.45it/s][A
 30%|██▉       | 66/221 [00:34<01:10,  2.19it/s][A
 30%|███       | 67/221 [00:35<01:04,  2.39it/s][A
 31%|███       | 68/221 [00:35<00:51,  2.95it/s][A
 31%|███       | 69/221 [00:37<01:55,  1.32it/s][A
 32%|███▏      | 70/221 [00:37<01:25,  1.77it/s][A
 32%|███▏      | 71/221 [00:37<01:11,  2.10it/s][A
 33%|███▎      | 72/221 [00:37<01:12,  2.05it/s][A
 33%|███▎      | 73/221 [00:38<01:11,  2.07it/s][A
 33%|███▎      | 74/221 [00:38<00:55,  2.65it/s][A
 34%|███▍      | 75/221 [00:38<00:52,  2.78it/s][A
 34%|███▍      | 76/221 [00:39<00:49,  2.92it/s][A
 35%|███▍      | 77/221 [00:40<01:35,  1.51it/s][A
 35%|███▌      | 78/221 [00:40<01:13,  1.94it/s][A
 36%|███▌      | 79/221 [00:41<01:26,  1.64it/s][A
 36%|███▌      | 80/221 [00:41<01:06,  2.13it/s][A
 37%|███▋      | 81/221 [00:42<01:02,  2.23it/s][A
 37%|███▋      | 82/221 [00:45<03:00,  1.30s/it][A
 38%|███▊      | 83/221 [00:46<02:32,  1.11s/it][A
 38%|███▊      | 84/221 [00:46<01:58,  1.16it/s][A
 39%|███▉      | 86/221 [00:46<01:15,  1.79it/s][A
 39%|███▉      | 87/221 [00:47<01:12,  1.84it/s][A
 40%|███▉      | 88/221 [00:47<01:01,  2.17it/s][A
 40%|████      | 89/221 [00:47<00:56,  2.35it/s][A
 41%|████      | 90/221 [00:48<00:49,  2.64it/s][A
 41%|████      | 91/221 [00:48<00:40,  3.20it/s][A
 42%|████▏     | 92/221 [00:48<00:38,  3.34it/s][A
 42%|████▏     | 93/221 [00:48<00:43,  2.97it/s][A
 43%|████▎     | 94/221 [00:49<00:54,  2.31it/s][A
 43%|████▎     | 95/221 [00:50<00:53,  2.34it/s][A
 43%|████▎     | 96/221 [00:50<01:05,  1.90it/s][A
 44%|████▍     | 97/221 [00:50<00:52,  2.37it/s][A
 44%|████▍     | 98/221 [00:51<00:58,  2.12it/s][A
 45%|████▍     | 99/221 [00:51<00:55,  2.19it/s][A
 45%|████▌     | 100/221 [00:53<01:15,  1.61it/s][A
 46%|████▌     | 101/221 [00:53<00:57,  2.08it/s][A
 46%|████▌     | 102/221 [00:53<01:09,  1.72it/s][A
 47%|████▋     | 103/221 [00:54<00:51,  2.29it/s][A
 47%|████▋     | 104/221 [00:54<00:49,  2.36it/s][A
 48%|████▊     | 105/221 [00:54<00:50,  2.28it/s][A
 48%|████▊     | 106/221 [00:56<01:27,  1.31it/s][A
 48%|████▊     | 107/221 [00:56<01:12,  1.57it/s][A
 49%|████▉     | 108/221 [00:57<01:08,  1.66it/s][A
 49%|████▉     | 109/221 [00:57<00:58,  1.90it/s][A
 50%|████▉     | 110/221 [00:57<00:46,  2.40it/s][A
 50%|█████     | 111/221 [00:58<00:47,  2.32it/s][A
 51%|█████     | 112/221 [00:58<00:43,  2.52it/s][A
 51%|█████     | 113/221 [00:58<00:37,  2.88it/s][A
 52%|█████▏    | 115/221 [00:59<00:34,  3.08it/s][A
 52%|█████▏    | 116/221 [00:59<00:35,  2.97it/s][A
 53%|█████▎    | 117/221 [01:00<00:36,  2.83it/s][A
 53%|█████▎    | 118/221 [01:00<00:39,  2.64it/s][A
 54%|█████▍    | 119/221 [01:00<00:33,  3.03it/s][A
 54%|█████▍    | 120/221 [01:01<00:28,  3.51it/s][A
 55%|█████▍    | 121/221 [01:01<00:36,  2.76it/s][A
 55%|█████▌    | 122/221 [01:02<00:39,  2.48it/s][A
 56%|█████▌    | 123/221 [01:03<01:16,  1.28it/s][A
 56%|█████▌    | 124/221 [01:03<00:59,  1.64it/s][A
 57%|█████▋    | 125/221 [01:04<01:04,  1.49it/s][A
 57%|█████▋    | 126/221 [01:13<04:54,  3.10s/it][A
 57%|█████▋    | 127/221 [01:14<03:38,  2.33s/it][A
 58%|█████▊    | 128/221 [01:14<02:42,  1.75s/it][A
 58%|█████▊    | 129/221 [01:14<02:06,  1.37s/it][A
 59%|█████▉    | 130/221 [01:15<01:34,  1.04s/it][A
 59%|█████▉    | 131/221 [01:16<01:32,  1.03s/it][A
 60%|█████▉    | 132/221 [01:17<01:44,  1.17s/it][A
 60%|██████    | 133/221 [01:18<01:28,  1.00s/it][A
 61%|██████    | 134/221 [01:19<01:20,  1.08it/s][A
 61%|██████    | 135/221 [01:19<01:13,  1.17it/s][A
 62%|██████▏   | 136/221 [01:20<01:00,  1.41it/s][A
 62%|██████▏   | 137/221 [01:20<00:49,  1.69it/s][A
 62%|██████▏   | 138/221 [01:20<00:45,  1.82it/s][A
 63%|██████▎   | 139/221 [01:21<00:37,  2.17it/s][A
 63%|██████▎   | 140/221 [01:21<00:38,  2.13it/s][A
 64%|██████▍   | 141/221 [01:22<00:36,  2.20it/s][A
 64%|██████▍   | 142/221 [01:22<00:32,  2.42it/s][A
 65%|██████▍   | 143/221 [01:22<00:30,  2.54it/s][A
 65%|██████▌   | 144/221 [01:22<00:24,  3.11it/s][A
 66%|██████▌   | 145/221 [01:23<00:19,  3.90it/s][A
 66%|██████▌   | 146/221 [01:23<00:15,  4.75it/s][A
 67%|██████▋   | 147/221 [01:23<00:13,  5.60it/s][A
 67%|██████▋   | 148/221 [01:25<00:52,  1.38it/s][A
 67%|██████▋   | 149/221 [01:25<00:46,  1.56it/s][A
 68%|██████▊   | 150/221 [01:26<00:42,  1.67it/s][A
 68%|██████▊   | 151/221 [01:26<00:33,  2.07it/s][A
 69%|██████▉   | 152/221 [01:26<00:31,  2.17it/s][A
 69%|██████▉   | 153/221 [01:26<00:24,  2.76it/s][A
 70%|██████▉   | 154/221 [01:27<00:20,  3.21it/s][A
 70%|███████   | 155/221 [01:27<00:17,  3.73it/s][A
 71%|███████   | 156/221 [01:27<00:17,  3.77it/s][A
 71%|███████   | 157/221 [01:32<01:52,  1.75s/it][A
 71%|███████▏  | 158/221 [01:33<01:28,  1.41s/it][A
 72%|███████▏  | 159/221 [01:33<01:03,  1.02s/it][A
 72%|███████▏  | 160/221 [01:33<00:46,  1.31it/s][A
 73%|███████▎  | 162/221 [01:33<00:26,  2.25it/s][A
 74%|███████▍  | 163/221 [01:34<00:23,  2.45it/s][A
 74%|███████▍  | 164/221 [01:34<00:19,  2.96it/s][A
 75%|███████▍  | 165/221 [01:34<00:20,  2.76it/s][A
 75%|███████▌  | 166/221 [01:35<00:24,  2.23it/s][A
 76%|███████▌  | 167/221 [01:35<00:21,  2.48it/s][A
 76%|███████▌  | 168/221 [01:37<00:44,  1.18it/s][A
 76%|███████▋  | 169/221 [01:38<00:38,  1.35it/s][A
 77%|███████▋  | 170/221 [01:38<00:32,  1.59it/s][A
 77%|███████▋  | 171/221 [01:38<00:27,  1.80it/s][A
 78%|███████▊  | 172/221 [01:39<00:23,  2.09it/s][A
 78%|███████▊  | 173/221 [01:39<00:22,  2.17it/s][A
 79%|███████▊  | 174/221 [01:39<00:17,  2.71it/s][A
 79%|███████▉  | 175/221 [01:39<00:16,  2.82it/s][A
 80%|███████▉  | 176/221 [01:40<00:16,  2.66it/s][A
 80%|████████  | 177/221 [01:40<00:15,  2.81it/s][A
 81%|████████  | 178/221 [01:41<00:15,  2.77it/s][A
 81%|████████  | 179/221 [01:41<00:14,  2.94it/s][A
 81%|████████▏ | 180/221 [01:41<00:12,  3.26it/s][A
 82%|████████▏ | 181/221 [01:41<00:11,  3.59it/s][A
 82%|████████▏ | 182/221 [01:42<00:12,  3.09it/s][A
 83%|████████▎ | 183/221 [01:42<00:16,  2.37it/s][A
 83%|████████▎ | 184/221 [01:43<00:16,  2.30it/s][A
 84%|████████▎ | 185/221 [01:43<00:14,  2.48it/s][A
 84%|████████▍ | 186/221 [01:43<00:12,  2.89it/s][A
 85%|████████▍ | 187/221 [01:44<00:11,  2.96it/s][A
 85%|████████▌ | 188/221 [01:44<00:10,  3.19it/s][A
 86%|████████▌ | 189/221 [01:44<00:09,  3.23it/s][A
 86%|████████▌ | 190/221 [01:45<00:10,  2.92it/s][A
 86%|████████▋ | 191/221 [01:45<00:09,  3.30it/s][A
 87%|████████▋ | 192/221 [01:46<00:15,  1.81it/s][A
 87%|████████▋ | 193/221 [01:46<00:12,  2.22it/s][A
 88%|████████▊ | 194/221 [01:47<00:12,  2.16it/s][A
 88%|████████▊ | 195/221 [01:47<00:09,  2.63it/s][A
 89%|████████▊ | 196/221 [01:47<00:10,  2.32it/s][A
 89%|████████▉ | 197/221 [01:48<00:09,  2.55it/s][A
 90%|████████▉ | 198/221 [01:48<00:08,  2.86it/s][A
 90%|█████████ | 199/221 [01:48<00:06,  3.19it/s][A
 90%|█████████ | 200/221 [01:49<00:09,  2.19it/s][A
 91%|█████████ | 201/221 [01:50<00:12,  1.66it/s][A
 91%|█████████▏| 202/221 [01:50<00:09,  1.95it/s][A
 92%|█████████▏| 203/221 [01:51<00:10,  1.68it/s][A
 92%|█████████▏| 204/221 [01:52<00:10,  1.63it/s][A
 93%|█████████▎| 206/221 [01:52<00:06,  2.27it/s][A
 94%|█████████▎| 207/221 [01:52<00:05,  2.78it/s][A
 94%|█████████▍| 208/221 [01:53<00:05,  2.44it/s][A
 95%|█████████▌| 210/221 [01:53<00:03,  3.38it/s][A
 95%|█████████▌| 211/221 [01:54<00:03,  2.91it/s][A
 96%|█████████▌| 212/221 [01:54<00:02,  3.03it/s][A
 96%|█████████▋| 213/221 [01:54<00:02,  3.48it/s][A
 97%|█████████▋| 214/221 [01:55<00:02,  3.12it/s][A
 97%|█████████▋| 215/221 [01:55<00:02,  2.70it/s][A
 98%|█████████▊| 216/221 [01:55<00:01,  2.95it/s][A
 98%|█████████▊| 217/221 [01:59<00:04,  1.17s/it][A
 99%|█████████▊| 218/221 [01:59<00:02,  1.08it/s][A
 99%|█████████▉| 219/221 [01:59<00:01,  1.35it/s][A
100%|█████████▉| 220/221 [02:01<00:01,  1.15s/it][A
100%|██████████| 221/221 [02:01<00:00,  1.16it/s][A100%|██████████| 221/221 [02:01<00:00,  1.81it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:50,  4.32it/s][A
  1%|          | 2/221 [00:01<02:19,  1.57it/s][A
  1%|▏         | 3/221 [00:01<01:36,  2.25it/s][A
  2%|▏         | 4/221 [00:01<01:42,  2.13it/s][A
  2%|▏         | 5/221 [00:02<01:59,  1.81it/s][A
  3%|▎         | 6/221 [00:02<01:44,  2.05it/s][A
  3%|▎         | 7/221 [00:03<01:39,  2.15it/s][A
  4%|▎         | 8/221 [00:04<02:32,  1.39it/s][A
  4%|▍         | 9/221 [00:05<02:40,  1.32it/s][A
  5%|▍         | 10/221 [00:05<02:13,  1.58it/s][A
  5%|▍         | 11/221 [00:06<02:02,  1.72it/s][A
  5%|▌         | 12/221 [00:06<01:46,  1.96it/s][A
  6%|▌         | 13/221 [00:06<01:26,  2.41it/s][A
  6%|▋         | 14/221 [00:06<01:09,  2.99it/s][A
  7%|▋         | 15/221 [00:07<01:10,  2.91it/s][A
  7%|▋         | 16/221 [00:08<01:43,  1.97it/s][A
  8%|▊         | 17/221 [00:09<02:03,  1.65it/s][A
  8%|▊         | 18/221 [00:09<01:39,  2.05it/s][A
  9%|▊         | 19/221 [00:10<02:06,  1.60it/s][A
  9%|▉         | 20/221 [00:10<01:54,  1.76it/s][A
 10%|▉         | 21/221 [00:11<01:53,  1.76it/s][A
 10%|▉         | 22/221 [00:12<02:36,  1.27it/s][A
 10%|█         | 23/221 [00:12<02:06,  1.57it/s][A
 11%|█         | 24/221 [00:13<01:59,  1.65it/s][A
 11%|█▏        | 25/221 [00:14<02:07,  1.54it/s][A
 12%|█▏        | 26/221 [00:14<02:13,  1.46it/s][A
 12%|█▏        | 27/221 [00:15<01:44,  1.86it/s][A
 13%|█▎        | 28/221 [00:16<02:37,  1.22it/s][A
 13%|█▎        | 29/221 [00:16<02:02,  1.57it/s][A
 14%|█▎        | 30/221 [00:17<01:56,  1.64it/s][A
 14%|█▍        | 31/221 [00:17<01:50,  1.71it/s][A
 14%|█▍        | 32/221 [00:18<02:21,  1.34it/s][A
 15%|█▍        | 33/221 [00:19<02:18,  1.36it/s][A
 15%|█▌        | 34/221 [00:20<01:58,  1.58it/s][A
 16%|█▌        | 35/221 [00:20<01:33,  2.00it/s][A
 16%|█▋        | 36/221 [00:20<01:39,  1.87it/s][A
 17%|█▋        | 37/221 [00:21<01:30,  2.03it/s][A
 17%|█▋        | 38/221 [00:22<02:02,  1.49it/s][A
 18%|█▊        | 39/221 [00:22<01:48,  1.67it/s][A
 18%|█▊        | 40/221 [00:23<02:19,  1.30it/s][A
 19%|█▊        | 41/221 [00:24<01:47,  1.68it/s][A
 19%|█▉        | 42/221 [00:24<01:28,  2.02it/s][A
 19%|█▉        | 43/221 [00:24<01:20,  2.22it/s][A
 20%|█▉        | 44/221 [00:25<01:32,  1.92it/s][A
 21%|██        | 46/221 [00:25<01:05,  2.67it/s][A
 21%|██▏       | 47/221 [00:26<01:11,  2.43it/s][A
 22%|██▏       | 48/221 [00:26<00:58,  2.96it/s][A
 22%|██▏       | 49/221 [00:27<01:20,  2.13it/s][A
 23%|██▎       | 50/221 [00:27<01:19,  2.15it/s][A
 23%|██▎       | 51/221 [00:28<01:09,  2.45it/s][A
 24%|██▎       | 52/221 [00:28<01:03,  2.68it/s][A
 24%|██▍       | 53/221 [00:28<01:04,  2.59it/s][A
 24%|██▍       | 54/221 [00:29<01:13,  2.28it/s][A
 25%|██▍       | 55/221 [00:29<00:59,  2.78it/s][A
 25%|██▌       | 56/221 [00:29<01:03,  2.59it/s][A
 26%|██▌       | 57/221 [00:30<00:55,  2.94it/s][A
 26%|██▌       | 58/221 [00:30<00:48,  3.34it/s][A
 27%|██▋       | 59/221 [00:30<00:39,  4.12it/s][A
 27%|██▋       | 60/221 [00:31<00:55,  2.88it/s][A
 28%|██▊       | 61/221 [00:31<01:07,  2.37it/s][A
 28%|██▊       | 62/221 [00:32<01:06,  2.38it/s][A
 29%|██▊       | 63/221 [00:32<01:26,  1.82it/s][A
 29%|██▉       | 64/221 [00:34<02:09,  1.21it/s][A
 29%|██▉       | 65/221 [00:35<02:20,  1.11it/s][A
 30%|██▉       | 66/221 [00:35<02:00,  1.29it/s][A
 30%|███       | 67/221 [00:36<01:36,  1.59it/s][A
 31%|███       | 68/221 [00:36<01:19,  1.92it/s][A
 31%|███       | 69/221 [00:36<01:01,  2.46it/s][A
 32%|███▏      | 70/221 [00:37<01:08,  2.19it/s][A
 32%|███▏      | 71/221 [00:37<01:17,  1.93it/s][A
 33%|███▎      | 72/221 [00:38<01:21,  1.83it/s][A
 33%|███▎      | 73/221 [00:39<01:26,  1.71it/s][A
 33%|███▎      | 74/221 [00:39<01:14,  1.98it/s][A
 34%|███▍      | 75/221 [00:39<01:04,  2.25it/s][A
 34%|███▍      | 76/221 [00:40<01:03,  2.28it/s][A
 35%|███▍      | 77/221 [00:40<00:55,  2.60it/s][A
 35%|███▌      | 78/221 [00:40<00:58,  2.43it/s][A
 36%|███▌      | 79/221 [00:41<01:21,  1.74it/s][A
 36%|███▌      | 80/221 [00:42<01:07,  2.10it/s][A
 37%|███▋      | 81/221 [00:42<01:08,  2.03it/s][A
 37%|███▋      | 82/221 [00:43<01:24,  1.64it/s][A
 38%|███▊      | 83/221 [00:43<01:11,  1.94it/s][A
 38%|███▊      | 84/221 [00:45<01:41,  1.34it/s][A
 38%|███▊      | 85/221 [00:45<01:17,  1.75it/s][A
 39%|███▉      | 86/221 [00:45<01:15,  1.79it/s][A
 39%|███▉      | 87/221 [00:46<01:08,  1.95it/s][A
 40%|███▉      | 88/221 [00:46<01:01,  2.15it/s][A
 40%|████      | 89/221 [00:47<01:12,  1.81it/s][A
 41%|████      | 90/221 [00:47<01:15,  1.74it/s][A
 41%|████      | 91/221 [00:48<01:06,  1.95it/s][A
 42%|████▏     | 92/221 [00:48<01:04,  2.00it/s][A
 43%|████▎     | 94/221 [00:49<00:44,  2.84it/s][A
 43%|████▎     | 95/221 [00:49<00:49,  2.57it/s][A
 43%|████▎     | 96/221 [00:50<00:47,  2.65it/s][A
 44%|████▍     | 97/221 [00:50<00:54,  2.30it/s][A
 44%|████▍     | 98/221 [00:51<01:05,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:11,  1.71it/s][A
 45%|████▌     | 100/221 [00:52<01:20,  1.50it/s][A
 46%|████▌     | 101/221 [00:53<01:26,  1.38it/s][A
 46%|████▌     | 102/221 [00:54<01:37,  1.23it/s][A
 47%|████▋     | 103/221 [00:55<01:20,  1.47it/s][A
 47%|████▋     | 104/221 [00:55<01:22,  1.41it/s][A
 48%|████▊     | 105/221 [00:56<01:09,  1.68it/s][A
 48%|████▊     | 106/221 [00:56<00:56,  2.05it/s][A
 48%|████▊     | 107/221 [00:57<00:54,  2.08it/s][A
 49%|████▉     | 108/221 [00:57<00:58,  1.94it/s][A
 49%|████▉     | 109/221 [00:58<01:02,  1.79it/s][A
 50%|████▉     | 110/221 [00:58<01:06,  1.67it/s][A
 50%|█████     | 111/221 [01:00<01:29,  1.23it/s][A
 51%|█████     | 112/221 [01:00<01:15,  1.45it/s][A
 51%|█████     | 113/221 [01:00<01:00,  1.77it/s][A
 52%|█████▏    | 114/221 [01:01<00:56,  1.90it/s][A
 52%|█████▏    | 115/221 [01:02<01:00,  1.76it/s][A
 52%|█████▏    | 116/221 [01:02<00:49,  2.13it/s][A
 53%|█████▎    | 117/221 [01:02<00:44,  2.32it/s][A
 53%|█████▎    | 118/221 [01:03<00:47,  2.17it/s][A
 54%|█████▍    | 119/221 [01:03<00:48,  2.12it/s][A
 54%|█████▍    | 120/221 [01:04<00:49,  2.06it/s][A
 55%|█████▍    | 121/221 [01:04<00:47,  2.10it/s][A
 55%|█████▌    | 122/221 [01:05<00:51,  1.91it/s][A
 56%|█████▌    | 123/221 [01:05<00:46,  2.09it/s][A
 56%|█████▌    | 124/221 [01:06<01:07,  1.44it/s][A
 57%|█████▋    | 125/221 [01:07<00:58,  1.64it/s][A
 57%|█████▋    | 126/221 [01:07<00:44,  2.12it/s][A
 57%|█████▋    | 127/221 [01:08<01:01,  1.53it/s][A
 58%|█████▊    | 128/221 [01:08<00:55,  1.67it/s][A
 58%|█████▊    | 129/221 [01:09<00:52,  1.74it/s][A
 59%|█████▉    | 130/221 [01:09<00:42,  2.12it/s][A
 59%|█████▉    | 131/221 [01:10<00:43,  2.08it/s][A
 60%|█████▉    | 132/221 [01:11<00:56,  1.58it/s][A
 60%|██████    | 133/221 [01:11<00:57,  1.54it/s][A
 61%|██████    | 134/221 [01:12<00:49,  1.77it/s][A
 61%|██████    | 135/221 [01:12<00:46,  1.84it/s][A
 62%|██████▏   | 136/221 [01:13<00:52,  1.62it/s][A
 62%|██████▏   | 137/221 [01:14<00:53,  1.57it/s][A
 62%|██████▏   | 138/221 [01:15<00:59,  1.40it/s][A
 63%|██████▎   | 139/221 [01:16<01:20,  1.02it/s][A
 63%|██████▎   | 140/221 [01:17<01:14,  1.08it/s][A
 64%|██████▍   | 141/221 [01:17<01:02,  1.28it/s][A
 64%|██████▍   | 142/221 [01:18<00:56,  1.39it/s][A
 65%|██████▍   | 143/221 [01:18<00:43,  1.78it/s][A
 65%|██████▌   | 144/221 [01:19<00:37,  2.05it/s][A
 66%|██████▌   | 145/221 [01:19<00:36,  2.10it/s][A
 66%|██████▌   | 146/221 [01:19<00:28,  2.63it/s][A
 67%|██████▋   | 147/221 [01:20<00:30,  2.40it/s][A
 67%|██████▋   | 148/221 [01:20<00:30,  2.40it/s][A
 68%|██████▊   | 150/221 [01:21<00:23,  3.03it/s][A
 68%|██████▊   | 151/221 [01:21<00:19,  3.65it/s][A
 69%|██████▉   | 152/221 [01:21<00:21,  3.21it/s][A
 69%|██████▉   | 153/221 [01:21<00:20,  3.40it/s][A
 70%|██████▉   | 154/221 [01:22<00:28,  2.34it/s][A
 70%|███████   | 155/221 [01:23<00:42,  1.56it/s][A
 71%|███████   | 156/221 [01:24<00:38,  1.70it/s][A
 71%|███████   | 157/221 [01:24<00:37,  1.69it/s][A
 71%|███████▏  | 158/221 [01:25<00:31,  2.02it/s][A
 72%|███████▏  | 160/221 [01:25<00:25,  2.40it/s][A
 73%|███████▎  | 161/221 [01:25<00:21,  2.82it/s][A
 73%|███████▎  | 162/221 [01:26<00:22,  2.66it/s][A
 74%|███████▍  | 163/221 [01:26<00:19,  3.01it/s][A
 74%|███████▍  | 164/221 [01:26<00:19,  2.85it/s][A
 75%|███████▍  | 165/221 [01:27<00:17,  3.21it/s][A
 75%|███████▌  | 166/221 [01:28<00:27,  2.03it/s][A
 76%|███████▌  | 167/221 [01:28<00:23,  2.34it/s][A
 76%|███████▌  | 168/221 [01:28<00:19,  2.67it/s][A
 76%|███████▋  | 169/221 [01:30<00:36,  1.43it/s][A
 77%|███████▋  | 170/221 [01:30<00:30,  1.66it/s][A
 77%|███████▋  | 171/221 [01:31<00:30,  1.65it/s][A
 78%|███████▊  | 172/221 [01:31<00:24,  2.02it/s][A
 78%|███████▊  | 173/221 [01:31<00:20,  2.34it/s][A
 79%|███████▊  | 174/221 [01:31<00:19,  2.45it/s][A
 79%|███████▉  | 175/221 [01:32<00:19,  2.37it/s][A
 80%|███████▉  | 176/221 [01:32<00:18,  2.45it/s][A
 80%|████████  | 177/221 [01:33<00:19,  2.28it/s][A
 81%|████████  | 178/221 [01:33<00:19,  2.16it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.86it/s][A
 81%|████████▏ | 180/221 [01:34<00:16,  2.45it/s][A
 82%|████████▏ | 181/221 [01:34<00:13,  2.92it/s][A
 82%|████████▏ | 182/221 [01:35<00:15,  2.45it/s][A
 83%|████████▎ | 183/221 [01:35<00:17,  2.15it/s][A
 83%|████████▎ | 184/221 [01:37<00:24,  1.53it/s][A
 84%|████████▎ | 185/221 [01:37<00:23,  1.53it/s][A
 84%|████████▍ | 186/221 [01:37<00:17,  1.95it/s][A
 85%|████████▍ | 187/221 [01:38<00:19,  1.70it/s][A
 85%|████████▌ | 188/221 [01:38<00:15,  2.07it/s][A
 86%|████████▌ | 189/221 [01:39<00:18,  1.77it/s][A
 86%|████████▌ | 190/221 [01:39<00:15,  2.01it/s][A
 86%|████████▋ | 191/221 [01:40<00:16,  1.83it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.85it/s][A
 87%|████████▋ | 193/221 [01:41<00:13,  2.10it/s][A
 88%|████████▊ | 194/221 [01:42<00:15,  1.70it/s][A
 88%|████████▊ | 195/221 [01:43<00:21,  1.23it/s][A
 89%|████████▊ | 196/221 [01:44<00:17,  1.41it/s][A
 89%|████████▉ | 197/221 [01:44<00:17,  1.39it/s][A
 90%|████████▉ | 198/221 [01:45<00:14,  1.54it/s][A
 90%|█████████ | 199/221 [01:45<00:12,  1.79it/s][A
 90%|█████████ | 200/221 [01:46<00:14,  1.43it/s][A
 91%|█████████ | 201/221 [01:47<00:13,  1.51it/s][A
 91%|█████████▏| 202/221 [01:47<00:10,  1.80it/s][A
 92%|█████████▏| 203/221 [01:48<00:09,  1.81it/s][A
 92%|█████████▏| 204/221 [01:48<00:07,  2.14it/s][A
 93%|█████████▎| 205/221 [01:48<00:06,  2.45it/s][A
 93%|█████████▎| 206/221 [01:48<00:05,  2.69it/s][A
 94%|█████████▎| 207/221 [01:49<00:04,  3.31it/s][A
 94%|█████████▍| 208/221 [01:50<00:07,  1.75it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.80it/s][A
 95%|█████████▌| 210/221 [01:51<00:04,  2.21it/s][A
 95%|█████████▌| 211/221 [01:52<00:06,  1.63it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  2.00it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.98it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.79it/s][A
 97%|█████████▋| 215/221 [01:53<00:02,  2.31it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.96it/s][A
 98%|█████████▊| 217/221 [01:55<00:02,  1.58it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.94it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.96it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  2.15it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.99it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]
09/19/2024 11:01:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 6499--===========

09/19/2024 11:01:06 - INFO - __main__ -   {'area_r1': 46.6, 'area_recall': '46.6/75.7/84.6', 'area_ravg': 69.0}
09/19/2024 11:01:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 6499--===========

09/19/2024 11:01:06 - INFO - __main__ -   {'forward_r1': 50.5, 'forward_recall': '50.5/79.0/87.8', 'forward_ravg': 72.4}
09/19/2024 11:01:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 6499--===========

09/19/2024 11:01:06 - INFO - __main__ -   {'area_video_r1': 48.6, 'area_video_recall': '48.6/79.0/88.0', 'area_video_ravg': 71.9}
09/19/2024 11:01:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 11:01:06 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 11:01:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 6499--===========

09/19/2024 11:01:06 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/83.6/89.0', 'area_video_ravg': 78.8, 'area_video_back_r1': 63.8, 'area_video_back_recall': '63.8/85.0/92.2', 'area_video_back_ravg': 80.3}
09/19/2024 11:01:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 6499=======

09/19/2024 11:01:06 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/83.6/89.0', 'area_video_ravg': 78.8, 'area_video_back_r1': 63.8, 'area_video_back_recall': '63.8/85.0/92.2', 'area_video_back_ravg': 80.3}
09/19/2024 11:01:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 6499--===========

09/19/2024 11:01:06 - INFO - __main__ -   {'video_r1': 30.7, 'video_recall': '30.7/56.6/67.2', 'video_ravg': 51.5}
09/19/2024 11:01:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 11:01:06 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 11:01:06 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 6499--===========

09/19/2024 11:01:06 - INFO - __main__ -   {'video_r1': 60.9, 'video_recall': '60.9/80.4/85.2', 'video_ravg': 75.5}
09/19/2024 11:01:06 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 6499=======

09/19/2024 11:01:06 - INFO - __main__ -   {'video_r1': 60.9, 'video_recall': '60.9/80.4/85.2', 'video_ravg': 75.5}
09/19/2024 11:01:38 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.021471470594406128, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0521647930145264, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0736362934112549}
 73%|███████▎  | 6500/8917 [8:47:03<114:12:12, 170.10s/it] 73%|███████▎  | 6501/8917 [8:47:07<80:33:18, 120.03s/it]  73%|███████▎  | 6502/8917 [8:47:10<57:06:17, 85.13s/it]  73%|███████▎  | 6503/8917 [8:47:14<40:40:09, 60.65s/it] 73%|███████▎  | 6504/8917 [8:47:18<29:12:39, 43.58s/it] 73%|███████▎  | 6505/8917 [8:47:21<21:13:25, 31.68s/it] 73%|███████▎  | 6506/8917 [8:47:25<15:35:51, 23.29s/it] 73%|███████▎  | 6507/8917 [8:47:29<11:37:37, 17.37s/it] 73%|███████▎  | 6508/8917 [8:47:32<8:53:12, 13.28s/it]  73%|███████▎  | 6509/8917 [8:47:36<7:00:44, 10.48s/it] 73%|███████▎  | 6510/8917 [8:47:40<5:37:19,  8.41s/it] 73%|███████▎  | 6511/8917 [8:47:44<4:39:21,  6.97s/it] 73%|███████▎  | 6512/8917 [8:47:47<4:01:40,  6.03s/it] 73%|███████▎  | 6513/8917 [8:47:51<3:34:21,  5.35s/it] 73%|███████▎  | 6514/8917 [8:47:55<3:14:25,  4.85s/it] 73%|███████▎  | 6515/8917 [8:47:59<2:59:51,  4.49s/it] 73%|███████▎  | 6516/8917 [8:48:02<2:45:40,  4.14s/it] 73%|███████▎  | 6517/8917 [8:48:06<2:40:26,  4.01s/it] 73%|███████▎  | 6518/8917 [8:48:09<2:38:27,  3.96s/it] 73%|███████▎  | 6519/8917 [8:48:13<2:37:22,  3.94s/it] 73%|███████▎  | 6520/8917 [8:48:17<2:34:51,  3.88s/it] 73%|███████▎  | 6521/8917 [8:48:21<2:38:05,  3.96s/it] 73%|███████▎  | 6522/8917 [8:48:25<2:33:35,  3.85s/it] 73%|███████▎  | 6523/8917 [8:48:28<2:29:23,  3.74s/it] 73%|███████▎  | 6524/8917 [8:48:32<2:27:05,  3.69s/it] 73%|███████▎  | 6525/8917 [8:48:36<2:27:55,  3.71s/it] 73%|███████▎  | 6526/8917 [8:48:39<2:28:10,  3.72s/it] 73%|███████▎  | 6527/8917 [8:48:43<2:25:06,  3.64s/it] 73%|███████▎  | 6528/8917 [8:48:47<2:26:53,  3.69s/it] 73%|███████▎  | 6529/8917 [8:48:51<2:29:38,  3.76s/it] 73%|███████▎  | 6530/8917 [8:48:54<2:30:18,  3.78s/it] 73%|███████▎  | 6531/8917 [8:48:58<2:26:53,  3.69s/it] 73%|███████▎  | 6532/8917 [8:49:02<2:32:17,  3.83s/it] 73%|███████▎  | 6533/8917 [8:49:06<2:28:18,  3.73s/it] 73%|███████▎  | 6534/8917 [8:49:09<2:26:19,  3.68s/it] 73%|███████▎  | 6535/8917 [8:49:13<2:23:40,  3.62s/it] 73%|███████▎  | 6536/8917 [8:49:16<2:23:30,  3.62s/it] 73%|███████▎  | 6537/8917 [8:49:20<2:26:00,  3.68s/it] 73%|███████▎  | 6538/8917 [8:49:24<2:27:29,  3.72s/it] 73%|███████▎  | 6539/8917 [8:49:28<2:27:32,  3.72s/it] 73%|███████▎  | 6540/8917 [8:49:31<2:29:15,  3.77s/it] 73%|███████▎  | 6541/8917 [8:49:35<2:24:38,  3.65s/it] 73%|███████▎  | 6542/8917 [8:49:39<2:27:28,  3.73s/it] 73%|███████▎  | 6543/8917 [8:49:43<2:30:16,  3.80s/it] 73%|███████▎  | 6544/8917 [8:49:46<2:30:26,  3.80s/it] 73%|███████▎  | 6545/8917 [8:49:50<2:32:24,  3.86s/it] 73%|███████▎  | 6546/8917 [8:49:54<2:29:00,  3.77s/it] 73%|███████▎  | 6547/8917 [8:49:58<2:27:14,  3.73s/it] 73%|███████▎  | 6548/8917 [8:50:01<2:28:37,  3.76s/it] 73%|███████▎  | 6549/8917 [8:50:05<2:30:21,  3.81s/it]09/19/2024 11:04:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023100720718503, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3247978687286377, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3478986024856567}
 73%|███████▎  | 6550/8917 [8:50:09<2:27:33,  3.74s/it] 73%|███████▎  | 6551/8917 [8:50:12<2:24:47,  3.67s/it] 73%|███████▎  | 6552/8917 [8:50:16<2:26:03,  3.71s/it] 73%|███████▎  | 6553/8917 [8:50:20<2:27:48,  3.75s/it] 74%|███████▎  | 6554/8917 [8:50:24<2:25:59,  3.71s/it] 74%|███████▎  | 6555/8917 [8:50:27<2:22:46,  3.63s/it] 74%|███████▎  | 6556/8917 [8:50:31<2:27:46,  3.76s/it] 74%|███████▎  | 6557/8917 [8:50:35<2:27:06,  3.74s/it] 74%|███████▎  | 6558/8917 [8:50:39<2:29:57,  3.81s/it] 74%|███████▎  | 6559/8917 [8:50:42<2:26:32,  3.73s/it] 74%|███████▎  | 6560/8917 [8:50:46<2:25:08,  3.69s/it] 74%|███████▎  | 6561/8917 [8:50:50<2:27:35,  3.76s/it] 74%|███████▎  | 6562/8917 [8:50:53<2:24:13,  3.67s/it] 74%|███████▎  | 6563/8917 [8:50:57<2:24:48,  3.69s/it] 74%|███████▎  | 6564/8917 [8:51:01<2:28:06,  3.78s/it] 74%|███████▎  | 6565/8917 [8:51:05<2:30:14,  3.83s/it] 74%|███████▎  | 6566/8917 [8:51:09<2:28:39,  3.79s/it] 74%|███████▎  | 6567/8917 [8:51:13<2:28:03,  3.78s/it] 74%|███████▎  | 6568/8917 [8:51:16<2:25:20,  3.71s/it] 74%|███████▎  | 6569/8917 [8:51:20<2:26:23,  3.74s/it] 74%|███████▎  | 6570/8917 [8:51:24<2:27:12,  3.76s/it] 74%|███████▎  | 6571/8917 [8:51:27<2:24:46,  3.70s/it] 74%|███████▎  | 6572/8917 [8:51:31<2:25:20,  3.72s/it] 74%|███████▎  | 6573/8917 [8:51:35<2:26:19,  3.75s/it] 74%|███████▎  | 6574/8917 [8:51:39<2:25:07,  3.72s/it] 74%|███████▎  | 6575/8917 [8:51:42<2:25:03,  3.72s/it] 74%|███████▎  | 6576/8917 [8:51:46<2:26:34,  3.76s/it] 74%|███████▍  | 6577/8917 [8:51:50<2:25:42,  3.74s/it] 74%|███████▍  | 6578/8917 [8:51:53<2:24:33,  3.71s/it] 74%|███████▍  | 6579/8917 [8:51:57<2:24:25,  3.71s/it] 74%|███████▍  | 6580/8917 [8:52:01<2:25:21,  3.73s/it] 74%|███████▍  | 6581/8917 [8:52:05<2:24:29,  3.71s/it] 74%|███████▍  | 6582/8917 [8:52:09<2:27:18,  3.79s/it] 74%|███████▍  | 6583/8917 [8:52:12<2:26:18,  3.76s/it] 74%|███████▍  | 6584/8917 [8:52:16<2:23:32,  3.69s/it] 74%|███████▍  | 6585/8917 [8:52:19<2:23:16,  3.69s/it] 74%|███████▍  | 6586/8917 [8:52:23<2:22:38,  3.67s/it] 74%|███████▍  | 6587/8917 [8:52:27<2:22:09,  3.66s/it] 74%|███████▍  | 6588/8917 [8:52:30<2:18:55,  3.58s/it] 74%|███████▍  | 6589/8917 [8:52:34<2:24:39,  3.73s/it] 74%|███████▍  | 6590/8917 [8:52:38<2:21:39,  3.65s/it] 74%|███████▍  | 6591/8917 [8:52:41<2:20:42,  3.63s/it] 74%|███████▍  | 6592/8917 [8:52:45<2:25:49,  3.76s/it] 74%|███████▍  | 6593/8917 [8:52:49<2:26:05,  3.77s/it] 74%|███████▍  | 6594/8917 [8:52:53<2:22:17,  3.68s/it] 74%|███████▍  | 6595/8917 [8:52:56<2:23:52,  3.72s/it] 74%|███████▍  | 6596/8917 [8:53:00<2:22:20,  3.68s/it] 74%|███████▍  | 6597/8917 [8:53:03<2:19:29,  3.61s/it] 74%|███████▍  | 6598/8917 [8:53:07<2:18:07,  3.57s/it] 74%|███████▍  | 6599/8917 [8:53:11<2:19:48,  3.62s/it]09/19/2024 11:07:48 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01728088967502117, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0856541395187378, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1029350757598877}
 74%|███████▍  | 6600/8917 [8:53:15<2:23:52,  3.73s/it] 74%|███████▍  | 6601/8917 [8:53:19<2:25:49,  3.78s/it] 74%|███████▍  | 6602/8917 [8:53:22<2:23:54,  3.73s/it] 74%|███████▍  | 6603/8917 [8:53:26<2:25:06,  3.76s/it] 74%|███████▍  | 6604/8917 [8:53:30<2:25:59,  3.79s/it] 74%|███████▍  | 6605/8917 [8:53:34<2:26:21,  3.80s/it] 74%|███████▍  | 6606/8917 [8:53:38<2:27:52,  3.84s/it] 74%|███████▍  | 6607/8917 [8:53:41<2:27:12,  3.82s/it] 74%|███████▍  | 6608/8917 [8:53:45<2:26:33,  3.81s/it] 74%|███████▍  | 6609/8917 [8:53:49<2:25:42,  3.79s/it] 74%|███████▍  | 6610/8917 [8:53:52<2:21:07,  3.67s/it] 74%|███████▍  | 6611/8917 [8:53:56<2:20:35,  3.66s/it] 74%|███████▍  | 6612/8917 [8:54:00<2:21:02,  3.67s/it] 74%|███████▍  | 6613/8917 [8:54:03<2:21:41,  3.69s/it] 74%|███████▍  | 6614/8917 [8:54:08<2:28:11,  3.86s/it] 74%|███████▍  | 6615/8917 [8:54:12<2:28:55,  3.88s/it] 74%|███████▍  | 6616/8917 [8:54:15<2:27:47,  3.85s/it] 74%|███████▍  | 6617/8917 [8:54:19<2:27:27,  3.85s/it] 74%|███████▍  | 6618/8917 [8:54:23<2:24:42,  3.78s/it] 74%|███████▍  | 6619/8917 [8:54:27<2:25:32,  3.80s/it] 74%|███████▍  | 6620/8917 [8:54:30<2:23:24,  3.75s/it] 74%|███████▍  | 6621/8917 [8:54:34<2:24:06,  3.77s/it] 74%|███████▍  | 6622/8917 [8:54:38<2:22:35,  3.73s/it] 74%|███████▍  | 6623/8917 [8:54:42<2:25:41,  3.81s/it] 74%|███████▍  | 6624/8917 [8:54:46<2:25:52,  3.82s/it] 74%|███████▍  | 6625/8917 [8:54:49<2:24:27,  3.78s/it] 74%|███████▍  | 6626/8917 [8:54:53<2:27:03,  3.85s/it] 74%|███████▍  | 6627/8917 [8:54:57<2:21:42,  3.71s/it] 74%|███████▍  | 6628/8917 [8:55:01<2:25:03,  3.80s/it] 74%|███████▍  | 6629/8917 [8:55:04<2:21:35,  3.71s/it] 74%|███████▍  | 6630/8917 [8:55:08<2:23:45,  3.77s/it] 74%|███████▍  | 6631/8917 [8:55:11<2:18:56,  3.65s/it] 74%|███████▍  | 6632/8917 [8:55:16<2:26:14,  3.84s/it] 74%|███████▍  | 6633/8917 [8:55:19<2:25:19,  3.82s/it] 74%|███████▍  | 6634/8917 [8:55:24<2:28:00,  3.89s/it] 74%|███████▍  | 6635/8917 [8:55:27<2:24:31,  3.80s/it] 74%|███████▍  | 6636/8917 [8:55:31<2:23:04,  3.76s/it] 74%|███████▍  | 6637/8917 [8:55:35<2:27:36,  3.88s/it] 74%|███████▍  | 6638/8917 [8:55:39<2:30:31,  3.96s/it] 74%|███████▍  | 6639/8917 [8:55:43<2:30:21,  3.96s/it] 74%|███████▍  | 6640/8917 [8:55:47<2:29:36,  3.94s/it] 74%|███████▍  | 6641/8917 [8:55:51<2:29:41,  3.95s/it] 74%|███████▍  | 6642/8917 [8:55:55<2:27:13,  3.88s/it] 74%|███████▍  | 6643/8917 [8:55:59<2:27:55,  3.90s/it] 75%|███████▍  | 6644/8917 [8:56:03<2:29:56,  3.96s/it] 75%|███████▍  | 6645/8917 [8:56:06<2:25:55,  3.85s/it] 75%|███████▍  | 6646/8917 [8:56:10<2:26:08,  3.86s/it] 75%|███████▍  | 6647/8917 [8:56:14<2:22:42,  3.77s/it] 75%|███████▍  | 6648/8917 [8:56:17<2:21:45,  3.75s/it] 75%|███████▍  | 6649/8917 [8:56:21<2:24:43,  3.83s/it]09/19/2024 11:10:59 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03422984853386879, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3134000301361084, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3476299047470093}
 75%|███████▍  | 6650/8917 [8:56:25<2:26:46,  3.88s/it] 75%|███████▍  | 6651/8917 [8:56:29<2:24:02,  3.81s/it] 75%|███████▍  | 6652/8917 [8:56:33<2:21:07,  3.74s/it] 75%|███████▍  | 6653/8917 [8:56:36<2:20:01,  3.71s/it] 75%|███████▍  | 6654/8917 [8:56:40<2:24:06,  3.82s/it] 75%|███████▍  | 6655/8917 [8:56:44<2:23:47,  3.81s/it] 75%|███████▍  | 6656/8917 [8:56:48<2:20:35,  3.73s/it] 75%|███████▍  | 6657/8917 [8:56:52<2:24:07,  3.83s/it] 75%|███████▍  | 6658/8917 [8:56:55<2:20:32,  3.73s/it] 75%|███████▍  | 6659/8917 [8:56:59<2:22:10,  3.78s/it] 75%|███████▍  | 6660/8917 [8:57:03<2:28:22,  3.94s/it] 75%|███████▍  | 6661/8917 [8:57:07<2:24:29,  3.84s/it] 75%|███████▍  | 6662/8917 [8:57:11<2:25:19,  3.87s/it] 75%|███████▍  | 6663/8917 [8:57:15<2:21:34,  3.77s/it] 75%|███████▍  | 6664/8917 [8:57:19<2:24:35,  3.85s/it] 75%|███████▍  | 6665/8917 [8:57:22<2:24:08,  3.84s/it] 75%|███████▍  | 6666/8917 [8:57:26<2:23:45,  3.83s/it] 75%|███████▍  | 6667/8917 [8:57:30<2:24:26,  3.85s/it] 75%|███████▍  | 6668/8917 [8:57:34<2:21:01,  3.76s/it] 75%|███████▍  | 6669/8917 [8:57:38<2:22:00,  3.79s/it] 75%|███████▍  | 6670/8917 [8:57:41<2:20:51,  3.76s/it] 75%|███████▍  | 6671/8917 [8:57:45<2:19:31,  3.73s/it] 75%|███████▍  | 6672/8917 [8:57:49<2:22:20,  3.80s/it] 75%|███████▍  | 6673/8917 [8:57:52<2:19:09,  3.72s/it] 75%|███████▍  | 6674/8917 [8:57:56<2:23:03,  3.83s/it] 75%|███████▍  | 6675/8917 [8:58:00<2:23:06,  3.83s/it] 75%|███████▍  | 6676/8917 [8:58:05<2:28:49,  3.98s/it] 75%|███████▍  | 6677/8917 [8:58:09<2:29:51,  4.01s/it] 75%|███████▍  | 6678/8917 [8:58:13<2:28:30,  3.98s/it] 75%|███████▍  | 6679/8917 [8:58:17<2:29:22,  4.00s/it] 75%|███████▍  | 6680/8917 [8:58:20<2:25:29,  3.90s/it] 75%|███████▍  | 6681/8917 [8:58:24<2:25:08,  3.89s/it] 75%|███████▍  | 6682/8917 [8:58:28<2:26:54,  3.94s/it] 75%|███████▍  | 6683/8917 [8:58:32<2:28:48,  4.00s/it] 75%|███████▍  | 6684/8917 [8:58:36<2:28:00,  3.98s/it] 75%|███████▍  | 6685/8917 [8:58:40<2:27:02,  3.95s/it] 75%|███████▍  | 6686/8917 [8:58:44<2:23:30,  3.86s/it] 75%|███████▍  | 6687/8917 [8:58:48<2:26:41,  3.95s/it] 75%|███████▌  | 6688/8917 [8:58:52<2:28:05,  3.99s/it] 75%|███████▌  | 6689/8917 [8:58:56<2:24:33,  3.89s/it] 75%|███████▌  | 6690/8917 [8:59:00<2:23:44,  3.87s/it] 75%|███████▌  | 6691/8917 [8:59:04<2:25:07,  3.91s/it] 75%|███████▌  | 6692/8917 [8:59:08<2:27:44,  3.98s/it] 75%|███████▌  | 6693/8917 [8:59:12<2:32:18,  4.11s/it] 75%|███████▌  | 6694/8917 [8:59:16<2:32:10,  4.11s/it] 75%|███████▌  | 6695/8917 [8:59:20<2:30:15,  4.06s/it] 75%|███████▌  | 6696/8917 [8:59:25<2:36:32,  4.23s/it] 75%|███████▌  | 6697/8917 [8:59:29<2:34:57,  4.19s/it] 75%|███████▌  | 6698/8917 [8:59:33<2:29:18,  4.04s/it] 75%|███████▌  | 6699/8917 [8:59:36<2:26:44,  3.97s/it]09/19/2024 11:14:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.037643421441316605, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3045756816864014, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3422191143035889}
 75%|███████▌  | 6700/8917 [8:59:40<2:21:38,  3.83s/it] 75%|███████▌  | 6701/8917 [8:59:44<2:22:44,  3.86s/it] 75%|███████▌  | 6702/8917 [8:59:48<2:28:42,  4.03s/it] 75%|███████▌  | 6703/8917 [8:59:52<2:29:50,  4.06s/it] 75%|███████▌  | 6704/8917 [8:59:56<2:29:28,  4.05s/it] 75%|███████▌  | 6705/8917 [9:00:01<2:30:09,  4.07s/it] 75%|███████▌  | 6706/8917 [9:00:05<2:35:12,  4.21s/it] 75%|███████▌  | 6707/8917 [9:00:09<2:33:50,  4.18s/it] 75%|███████▌  | 6708/8917 [9:00:13<2:34:53,  4.21s/it] 75%|███████▌  | 6709/8917 [9:00:18<2:33:39,  4.18s/it] 75%|███████▌  | 6710/8917 [9:00:22<2:36:54,  4.27s/it] 75%|███████▌  | 6711/8917 [9:00:26<2:36:32,  4.26s/it] 75%|███████▌  | 6712/8917 [9:00:31<2:35:54,  4.24s/it] 75%|███████▌  | 6713/8917 [9:00:34<2:32:23,  4.15s/it] 75%|███████▌  | 6714/8917 [9:00:39<2:31:18,  4.12s/it] 75%|███████▌  | 6715/8917 [9:00:43<2:37:28,  4.29s/it] 75%|███████▌  | 6716/8917 [9:00:47<2:37:02,  4.28s/it] 75%|███████▌  | 6717/8917 [9:00:52<2:37:29,  4.30s/it] 75%|███████▌  | 6718/8917 [9:00:57<2:42:30,  4.43s/it] 75%|███████▌  | 6719/8917 [9:01:01<2:38:17,  4.32s/it] 75%|███████▌  | 6720/8917 [9:01:04<2:30:53,  4.12s/it] 75%|███████▌  | 6721/8917 [9:01:08<2:30:51,  4.12s/it] 75%|███████▌  | 6722/8917 [9:01:13<2:30:53,  4.12s/it] 75%|███████▌  | 6723/8917 [9:01:17<2:29:44,  4.10s/it] 75%|███████▌  | 6724/8917 [9:01:21<2:31:56,  4.16s/it] 75%|███████▌  | 6725/8917 [9:01:25<2:34:48,  4.24s/it] 75%|███████▌  | 6726/8917 [9:01:29<2:32:54,  4.19s/it] 75%|███████▌  | 6727/8917 [9:01:33<2:29:20,  4.09s/it] 75%|███████▌  | 6728/8917 [9:01:37<2:27:33,  4.04s/it] 75%|███████▌  | 6729/8917 [9:01:41<2:24:53,  3.97s/it] 75%|███████▌  | 6730/8917 [9:01:45<2:25:23,  3.99s/it] 75%|███████▌  | 6731/8917 [9:01:50<2:32:01,  4.17s/it] 75%|███████▌  | 6732/8917 [9:01:53<2:28:34,  4.08s/it] 76%|███████▌  | 6733/8917 [9:01:58<2:31:02,  4.15s/it] 76%|███████▌  | 6734/8917 [9:02:02<2:27:39,  4.06s/it] 76%|███████▌  | 6735/8917 [9:02:05<2:23:15,  3.94s/it] 76%|███████▌  | 6736/8917 [9:02:10<2:27:54,  4.07s/it] 76%|███████▌  | 6737/8917 [9:02:14<2:28:52,  4.10s/it] 76%|███████▌  | 6738/8917 [9:02:18<2:28:55,  4.10s/it] 76%|███████▌  | 6739/8917 [9:02:22<2:27:14,  4.06s/it] 76%|███████▌  | 6740/8917 [9:02:26<2:25:12,  4.00s/it] 76%|███████▌  | 6741/8917 [9:02:30<2:30:42,  4.16s/it] 76%|███████▌  | 6742/8917 [9:02:34<2:31:39,  4.18s/it] 76%|███████▌  | 6743/8917 [9:02:39<2:30:22,  4.15s/it] 76%|███████▌  | 6744/8917 [9:02:42<2:27:06,  4.06s/it] 76%|███████▌  | 6745/8917 [9:02:47<2:31:55,  4.20s/it] 76%|███████▌  | 6746/8917 [9:02:51<2:33:49,  4.25s/it] 76%|███████▌  | 6747/8917 [9:02:55<2:29:25,  4.13s/it] 76%|███████▌  | 6748/8917 [9:03:00<2:33:49,  4.26s/it] 76%|███████▌  | 6749/8917 [9:03:04<2:33:20,  4.24s/it]09/19/2024 11:17:42 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029545331373810768, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1612052917480469, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1907505989074707}
 76%|███████▌  | 6750/8917 [9:03:08<2:26:38,  4.06s/it] 76%|███████▌  | 6751/8917 [9:03:11<2:23:41,  3.98s/it] 76%|███████▌  | 6752/8917 [9:03:15<2:22:10,  3.94s/it] 76%|███████▌  | 6753/8917 [9:03:19<2:24:47,  4.01s/it] 76%|███████▌  | 6754/8917 [9:03:24<2:29:27,  4.15s/it] 76%|███████▌  | 6755/8917 [9:03:28<2:28:46,  4.13s/it] 76%|███████▌  | 6756/8917 [9:03:32<2:25:06,  4.03s/it] 76%|███████▌  | 6757/8917 [9:03:36<2:27:33,  4.10s/it] 76%|███████▌  | 6758/8917 [9:03:41<2:32:46,  4.25s/it] 76%|███████▌  | 6759/8917 [9:03:45<2:35:26,  4.32s/it] 76%|███████▌  | 6760/8917 [9:03:49<2:31:15,  4.21s/it] 76%|███████▌  | 6761/8917 [9:03:53<2:29:24,  4.16s/it] 76%|███████▌  | 6762/8917 [9:03:57<2:30:01,  4.18s/it] 76%|███████▌  | 6763/8917 [9:04:01<2:30:12,  4.18s/it] 76%|███████▌  | 6764/8917 [9:04:06<2:33:44,  4.28s/it] 76%|███████▌  | 6765/8917 [9:04:10<2:30:50,  4.21s/it] 76%|███████▌  | 6766/8917 [9:04:14<2:30:32,  4.20s/it] 76%|███████▌  | 6767/8917 [9:04:19<2:33:43,  4.29s/it] 76%|███████▌  | 6768/8917 [9:04:22<2:28:10,  4.14s/it] 76%|███████▌  | 6769/8917 [9:04:26<2:24:48,  4.04s/it] 76%|███████▌  | 6770/8917 [9:04:30<2:23:26,  4.01s/it] 76%|███████▌  | 6771/8917 [9:04:34<2:23:30,  4.01s/it] 76%|███████▌  | 6772/8917 [9:04:38<2:22:09,  3.98s/it] 76%|███████▌  | 6773/8917 [9:04:42<2:22:19,  3.98s/it] 76%|███████▌  | 6774/8917 [9:04:47<2:29:29,  4.19s/it] 76%|███████▌  | 6775/8917 [9:04:51<2:29:40,  4.19s/it] 76%|███████▌  | 6776/8917 [9:04:55<2:23:59,  4.04s/it] 76%|███████▌  | 6777/8917 [9:04:59<2:25:22,  4.08s/it] 76%|███████▌  | 6778/8917 [9:05:03<2:21:17,  3.96s/it] 76%|███████▌  | 6779/8917 [9:05:07<2:22:31,  4.00s/it] 76%|███████▌  | 6780/8917 [9:05:11<2:25:17,  4.08s/it] 76%|███████▌  | 6781/8917 [9:05:15<2:23:32,  4.03s/it] 76%|███████▌  | 6782/8917 [9:05:19<2:21:16,  3.97s/it] 76%|███████▌  | 6783/8917 [9:05:23<2:20:02,  3.94s/it] 76%|███████▌  | 6784/8917 [9:05:27<2:21:17,  3.97s/it] 76%|███████▌  | 6785/8917 [9:05:31<2:21:01,  3.97s/it] 76%|███████▌  | 6786/8917 [9:05:35<2:23:53,  4.05s/it] 76%|███████▌  | 6787/8917 [9:05:39<2:23:17,  4.04s/it] 76%|███████▌  | 6788/8917 [9:05:43<2:22:52,  4.03s/it] 76%|███████▌  | 6789/8917 [9:05:47<2:20:44,  3.97s/it] 76%|███████▌  | 6790/8917 [9:05:51<2:24:35,  4.08s/it] 76%|███████▌  | 6791/8917 [9:05:55<2:21:00,  3.98s/it] 76%|███████▌  | 6792/8917 [9:05:58<2:17:11,  3.87s/it] 76%|███████▌  | 6793/8917 [9:06:02<2:18:11,  3.90s/it] 76%|███████▌  | 6794/8917 [9:06:06<2:21:23,  4.00s/it] 76%|███████▌  | 6795/8917 [9:06:11<2:22:38,  4.03s/it] 76%|███████▌  | 6796/8917 [9:06:15<2:26:22,  4.14s/it] 76%|███████▌  | 6797/8917 [9:06:19<2:24:10,  4.08s/it] 76%|███████▌  | 6798/8917 [9:06:23<2:24:51,  4.10s/it] 76%|███████▌  | 6799/8917 [9:06:27<2:27:51,  4.19s/it]09/19/2024 11:21:05 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018947837874293327, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9739536046981812, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9929014444351196}
 76%|███████▋  | 6800/8917 [9:06:31<2:22:42,  4.04s/it] 76%|███████▋  | 6801/8917 [9:06:35<2:18:36,  3.93s/it] 76%|███████▋  | 6802/8917 [9:06:39<2:20:26,  3.98s/it] 76%|███████▋  | 6803/8917 [9:06:43<2:18:44,  3.94s/it] 76%|███████▋  | 6804/8917 [9:06:47<2:21:25,  4.02s/it] 76%|███████▋  | 6805/8917 [9:06:52<2:29:13,  4.24s/it] 76%|███████▋  | 6806/8917 [9:06:55<2:23:43,  4.09s/it] 76%|███████▋  | 6807/8917 [9:07:00<2:27:38,  4.20s/it] 76%|███████▋  | 6808/8917 [9:07:04<2:22:58,  4.07s/it] 76%|███████▋  | 6809/8917 [9:07:08<2:25:51,  4.15s/it] 76%|███████▋  | 6810/8917 [9:07:12<2:24:15,  4.11s/it] 76%|███████▋  | 6811/8917 [9:07:16<2:20:36,  4.01s/it] 76%|███████▋  | 6812/8917 [9:07:19<2:15:38,  3.87s/it] 76%|███████▋  | 6813/8917 [9:07:24<2:21:54,  4.05s/it] 76%|███████▋  | 6814/8917 [9:07:28<2:25:23,  4.15s/it] 76%|███████▋  | 6815/8917 [9:07:32<2:23:36,  4.10s/it] 76%|███████▋  | 6816/8917 [9:07:36<2:19:11,  3.98s/it] 76%|███████▋  | 6817/8917 [9:07:40<2:22:34,  4.07s/it] 76%|███████▋  | 6818/8917 [9:07:44<2:24:24,  4.13s/it] 76%|███████▋  | 6819/8917 [9:07:48<2:23:27,  4.10s/it] 76%|███████▋  | 6820/8917 [9:07:53<2:26:35,  4.19s/it] 76%|███████▋  | 6821/8917 [9:07:57<2:26:07,  4.18s/it] 77%|███████▋  | 6822/8917 [9:08:01<2:28:37,  4.26s/it] 77%|███████▋  | 6823/8917 [9:08:05<2:24:32,  4.14s/it] 77%|███████▋  | 6824/8917 [9:08:09<2:24:07,  4.13s/it] 77%|███████▋  | 6825/8917 [9:08:13<2:22:49,  4.10s/it] 77%|███████▋  | 6826/8917 [9:08:18<2:22:29,  4.09s/it] 77%|███████▋  | 6827/8917 [9:08:21<2:20:05,  4.02s/it] 77%|███████▋  | 6828/8917 [9:08:25<2:18:18,  3.97s/it] 77%|███████▋  | 6829/8917 [9:08:29<2:20:22,  4.03s/it] 77%|███████▋  | 6830/8917 [9:08:34<2:27:02,  4.23s/it] 77%|███████▋  | 6831/8917 [9:08:38<2:27:20,  4.24s/it] 77%|███████▋  | 6832/8917 [9:08:42<2:22:39,  4.11s/it] 77%|███████▋  | 6833/8917 [9:08:46<2:22:05,  4.09s/it] 77%|███████▋  | 6834/8917 [9:08:50<2:20:59,  4.06s/it] 77%|███████▋  | 6835/8917 [9:08:55<2:26:16,  4.22s/it] 77%|███████▋  | 6836/8917 [9:08:59<2:25:20,  4.19s/it] 77%|███████▋  | 6837/8917 [9:09:03<2:22:29,  4.11s/it] 77%|███████▋  | 6838/8917 [9:09:07<2:19:58,  4.04s/it] 77%|███████▋  | 6839/8917 [9:09:11<2:20:50,  4.07s/it] 77%|███████▋  | 6840/8917 [9:09:15<2:23:09,  4.14s/it] 77%|███████▋  | 6841/8917 [9:09:20<2:25:13,  4.20s/it] 77%|███████▋  | 6842/8917 [9:09:23<2:19:28,  4.03s/it] 77%|███████▋  | 6843/8917 [9:09:27<2:17:36,  3.98s/it] 77%|███████▋  | 6844/8917 [9:09:31<2:16:09,  3.94s/it] 77%|███████▋  | 6845/8917 [9:09:35<2:20:45,  4.08s/it] 77%|███████▋  | 6846/8917 [9:09:40<2:22:32,  4.13s/it] 77%|███████▋  | 6847/8917 [9:09:44<2:21:53,  4.11s/it] 77%|███████▋  | 6848/8917 [9:09:48<2:20:30,  4.07s/it] 77%|███████▋  | 6849/8917 [9:09:52<2:22:45,  4.14s/it]09/19/2024 11:24:29 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04482889547944069, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.5089261531829834, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.5537550449371338}
 77%|███████▋  | 6850/8917 [9:09:56<2:19:09,  4.04s/it] 77%|███████▋  | 6851/8917 [9:09:59<2:12:43,  3.85s/it] 77%|███████▋  | 6852/8917 [9:10:03<2:15:40,  3.94s/it] 77%|███████▋  | 6853/8917 [9:10:07<2:15:48,  3.95s/it] 77%|███████▋  | 6854/8917 [9:10:12<2:23:50,  4.18s/it] 77%|███████▋  | 6855/8917 [9:10:16<2:18:59,  4.04s/it] 77%|███████▋  | 6856/8917 [9:10:20<2:22:25,  4.15s/it] 77%|███████▋  | 6857/8917 [9:10:24<2:18:26,  4.03s/it] 77%|███████▋  | 6858/8917 [9:10:28<2:16:44,  3.98s/it] 77%|███████▋  | 6859/8917 [9:10:32<2:15:07,  3.94s/it] 77%|███████▋  | 6860/8917 [9:10:36<2:16:37,  3.99s/it] 77%|███████▋  | 6861/8917 [9:10:40<2:19:22,  4.07s/it] 77%|███████▋  | 6862/8917 [9:10:44<2:17:01,  4.00s/it] 77%|███████▋  | 6863/8917 [9:10:48<2:16:31,  3.99s/it] 77%|███████▋  | 6864/8917 [9:10:52<2:21:14,  4.13s/it] 77%|███████▋  | 6865/8917 [9:10:56<2:20:19,  4.10s/it] 77%|███████▋  | 6866/8917 [9:11:01<2:24:37,  4.23s/it] 77%|███████▋  | 6867/8917 [9:11:05<2:25:26,  4.26s/it] 77%|███████▋  | 6868/8917 [9:11:09<2:22:31,  4.17s/it] 77%|███████▋  | 6869/8917 [9:11:13<2:17:39,  4.03s/it] 77%|███████▋  | 6870/8917 [9:11:17<2:20:09,  4.11s/it] 77%|███████▋  | 6871/8917 [9:11:22<2:29:09,  4.37s/it] 77%|███████▋  | 6872/8917 [9:11:26<2:25:15,  4.26s/it] 77%|███████▋  | 6873/8917 [9:11:30<2:19:34,  4.10s/it] 77%|███████▋  | 6874/8917 [9:11:34<2:17:18,  4.03s/it] 77%|███████▋  | 6875/8917 [9:11:38<2:18:36,  4.07s/it] 77%|███████▋  | 6876/8917 [9:11:42<2:21:33,  4.16s/it] 77%|███████▋  | 6877/8917 [9:11:46<2:19:56,  4.12s/it] 77%|███████▋  | 6878/8917 [9:11:50<2:18:44,  4.08s/it] 77%|███████▋  | 6879/8917 [9:11:54<2:17:44,  4.06s/it] 77%|███████▋  | 6880/8917 [9:11:58<2:20:11,  4.13s/it] 77%|███████▋  | 6881/8917 [9:12:03<2:19:44,  4.12s/it] 77%|███████▋  | 6882/8917 [9:12:07<2:20:00,  4.13s/it] 77%|███████▋  | 6883/8917 [9:12:11<2:22:16,  4.20s/it] 77%|███████▋  | 6884/8917 [9:12:15<2:17:56,  4.07s/it] 77%|███████▋  | 6885/8917 [9:12:18<2:13:48,  3.95s/it] 77%|███████▋  | 6886/8917 [9:12:22<2:10:59,  3.87s/it] 77%|███████▋  | 6887/8917 [9:12:27<2:20:16,  4.15s/it] 77%|███████▋  | 6888/8917 [9:12:31<2:18:06,  4.08s/it] 77%|███████▋  | 6889/8917 [9:12:35<2:13:35,  3.95s/it] 77%|███████▋  | 6890/8917 [9:12:39<2:18:06,  4.09s/it] 77%|███████▋  | 6891/8917 [9:12:44<2:25:43,  4.32s/it] 77%|███████▋  | 6892/8917 [9:12:48<2:21:08,  4.18s/it] 77%|███████▋  | 6893/8917 [9:12:52<2:18:15,  4.10s/it] 77%|███████▋  | 6894/8917 [9:12:55<2:16:10,  4.04s/it] 77%|███████▋  | 6895/8917 [9:13:00<2:16:39,  4.06s/it] 77%|███████▋  | 6896/8917 [9:13:04<2:16:34,  4.05s/it] 77%|███████▋  | 6897/8917 [9:13:08<2:24:59,  4.31s/it] 77%|███████▋  | 6898/8917 [9:13:13<2:23:33,  4.27s/it] 77%|███████▋  | 6899/8917 [9:13:16<2:17:22,  4.08s/it]09/19/2024 11:27:54 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02387212961912155, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0915355682373047, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1154077053070068}
 77%|███████▋  | 6900/8917 [9:13:20<2:17:08,  4.08s/it] 77%|███████▋  | 6901/8917 [9:13:25<2:19:28,  4.15s/it] 77%|███████▋  | 6902/8917 [9:13:29<2:22:37,  4.25s/it] 77%|███████▋  | 6903/8917 [9:13:33<2:21:59,  4.23s/it] 77%|███████▋  | 6904/8917 [9:13:37<2:16:46,  4.08s/it] 77%|███████▋  | 6905/8917 [9:13:41<2:17:18,  4.09s/it] 77%|███████▋  | 6906/8917 [9:13:45<2:16:05,  4.06s/it] 77%|███████▋  | 6907/8917 [9:13:49<2:12:23,  3.95s/it] 77%|███████▋  | 6908/8917 [9:13:53<2:13:18,  3.98s/it] 77%|███████▋  | 6909/8917 [9:13:57<2:11:55,  3.94s/it] 77%|███████▋  | 6910/8917 [9:14:01<2:14:15,  4.01s/it] 78%|███████▊  | 6911/8917 [9:14:05<2:16:26,  4.08s/it] 78%|███████▊  | 6912/8917 [9:14:09<2:12:40,  3.97s/it] 78%|███████▊  | 6913/8917 [9:14:13<2:13:28,  4.00s/it] 78%|███████▊  | 6914/8917 [9:14:18<2:19:11,  4.17s/it] 78%|███████▊  | 6915/8917 [9:14:22<2:18:08,  4.14s/it] 78%|███████▊  | 6916/8917 [9:14:26<2:17:27,  4.12s/it] 78%|███████▊  | 6917/8917 [9:14:30<2:16:23,  4.09s/it] 78%|███████▊  | 6918/8917 [9:14:34<2:14:36,  4.04s/it] 78%|███████▊  | 6919/8917 [9:14:38<2:13:51,  4.02s/it] 78%|███████▊  | 6920/8917 [9:14:42<2:17:28,  4.13s/it] 78%|███████▊  | 6921/8917 [9:14:46<2:14:33,  4.05s/it] 78%|███████▊  | 6922/8917 [9:14:49<2:09:01,  3.88s/it] 78%|███████▊  | 6923/8917 [9:14:53<2:09:21,  3.89s/it] 78%|███████▊  | 6924/8917 [9:14:57<2:09:17,  3.89s/it] 78%|███████▊  | 6925/8917 [9:15:02<2:13:51,  4.03s/it] 78%|███████▊  | 6926/8917 [9:15:06<2:17:11,  4.13s/it] 78%|███████▊  | 6927/8917 [9:15:10<2:16:15,  4.11s/it] 78%|███████▊  | 6928/8917 [9:15:14<2:10:51,  3.95s/it] 78%|███████▊  | 6929/8917 [9:15:18<2:12:26,  4.00s/it] 78%|███████▊  | 6930/8917 [9:15:22<2:18:13,  4.17s/it] 78%|███████▊  | 6931/8917 [9:15:27<2:21:26,  4.27s/it] 78%|███████▊  | 6932/8917 [9:15:31<2:17:15,  4.15s/it] 78%|███████▊  | 6933/8917 [9:15:35<2:17:51,  4.17s/it] 78%|███████▊  | 6934/8917 [9:15:38<2:12:52,  4.02s/it] 78%|███████▊  | 6935/8917 [9:15:43<2:13:14,  4.03s/it] 78%|███████▊  | 6936/8917 [9:15:47<2:17:21,  4.16s/it] 78%|███████▊  | 6937/8917 [9:15:51<2:19:27,  4.23s/it] 78%|███████▊  | 6938/8917 [9:15:55<2:13:29,  4.05s/it] 78%|███████▊  | 6939/8917 [9:15:59<2:13:01,  4.04s/it] 78%|███████▊  | 6940/8917 [9:16:03<2:16:23,  4.14s/it] 78%|███████▊  | 6941/8917 [9:16:08<2:16:30,  4.14s/it] 78%|███████▊  | 6942/8917 [9:16:11<2:11:58,  4.01s/it] 78%|███████▊  | 6943/8917 [9:16:16<2:15:24,  4.12s/it] 78%|███████▊  | 6944/8917 [9:16:20<2:15:27,  4.12s/it] 78%|███████▊  | 6945/8917 [9:16:23<2:11:45,  4.01s/it] 78%|███████▊  | 6946/8917 [9:16:27<2:10:35,  3.98s/it] 78%|███████▊  | 6947/8917 [9:16:32<2:12:38,  4.04s/it] 78%|███████▊  | 6948/8917 [9:16:36<2:15:36,  4.13s/it] 78%|███████▊  | 6949/8917 [9:16:41<2:22:03,  4.33s/it]09/19/2024 11:31:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02447652630507946, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2031422853469849, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2276188135147095}
 78%|███████▊  | 6950/8917 [9:16:44<2:15:00,  4.12s/it] 78%|███████▊  | 6951/8917 [9:16:48<2:15:15,  4.13s/it] 78%|███████▊  | 6952/8917 [9:16:53<2:14:12,  4.10s/it] 78%|███████▊  | 6953/8917 [9:16:56<2:12:47,  4.06s/it] 78%|███████▊  | 6954/8917 [9:17:01<2:12:54,  4.06s/it] 78%|███████▊  | 6955/8917 [9:17:04<2:09:34,  3.96s/it] 78%|███████▊  | 6956/8917 [9:17:09<2:15:40,  4.15s/it] 78%|███████▊  | 6957/8917 [9:17:14<2:20:22,  4.30s/it] 78%|███████▊  | 6958/8917 [9:17:18<2:18:08,  4.23s/it] 78%|███████▊  | 6959/8917 [9:17:22<2:17:30,  4.21s/it] 78%|███████▊  | 6960/8917 [9:17:26<2:17:43,  4.22s/it] 78%|███████▊  | 6961/8917 [9:17:30<2:14:42,  4.13s/it] 78%|███████▊  | 6962/8917 [9:17:34<2:12:50,  4.08s/it] 78%|███████▊  | 6963/8917 [9:17:38<2:11:58,  4.05s/it] 78%|███████▊  | 6964/8917 [9:17:41<2:07:17,  3.91s/it] 78%|███████▊  | 6965/8917 [9:17:46<2:10:01,  4.00s/it] 78%|███████▊  | 6966/8917 [9:17:50<2:15:00,  4.15s/it] 78%|███████▊  | 6967/8917 [9:17:54<2:15:52,  4.18s/it] 78%|███████▊  | 6968/8917 [9:17:59<2:16:55,  4.22s/it] 78%|███████▊  | 6969/8917 [9:18:03<2:15:05,  4.16s/it] 78%|███████▊  | 6970/8917 [9:18:06<2:09:13,  3.98s/it] 78%|███████▊  | 6971/8917 [9:18:10<2:10:32,  4.03s/it] 78%|███████▊  | 6972/8917 [9:18:14<2:10:07,  4.01s/it] 78%|███████▊  | 6973/8917 [9:18:19<2:11:05,  4.05s/it] 78%|███████▊  | 6974/8917 [9:18:23<2:17:07,  4.23s/it] 78%|███████▊  | 6975/8917 [9:18:28<2:18:31,  4.28s/it] 78%|███████▊  | 6976/8917 [9:18:32<2:18:08,  4.27s/it] 78%|███████▊  | 6977/8917 [9:18:36<2:16:09,  4.21s/it] 78%|███████▊  | 6978/8917 [9:18:40<2:12:27,  4.10s/it] 78%|███████▊  | 6979/8917 [9:18:44<2:12:07,  4.09s/it] 78%|███████▊  | 6980/8917 [9:18:48<2:08:28,  3.98s/it] 78%|███████▊  | 6981/8917 [9:18:52<2:12:39,  4.11s/it] 78%|███████▊  | 6982/8917 [9:18:56<2:10:54,  4.06s/it] 78%|███████▊  | 6983/8917 [9:19:00<2:09:48,  4.03s/it] 78%|███████▊  | 6984/8917 [9:19:04<2:12:05,  4.10s/it] 78%|███████▊  | 6985/8917 [9:19:08<2:12:23,  4.11s/it] 78%|███████▊  | 6986/8917 [9:19:13<2:14:26,  4.18s/it] 78%|███████▊  | 6987/8917 [9:19:16<2:07:32,  3.96s/it] 78%|███████▊  | 6988/8917 [9:19:20<2:04:44,  3.88s/it] 78%|███████▊  | 6989/8917 [9:19:24<2:09:49,  4.04s/it] 78%|███████▊  | 6990/8917 [9:19:29<2:13:59,  4.17s/it] 78%|███████▊  | 6991/8917 [9:19:33<2:11:41,  4.10s/it] 78%|███████▊  | 6992/8917 [9:19:36<2:08:49,  4.02s/it] 78%|███████▊  | 6993/8917 [9:19:41<2:10:12,  4.06s/it] 78%|███████▊  | 6994/8917 [9:19:45<2:11:19,  4.10s/it] 78%|███████▊  | 6995/8917 [9:19:50<2:18:20,  4.32s/it] 78%|███████▊  | 6996/8917 [9:19:53<2:14:01,  4.19s/it] 78%|███████▊  | 6997/8917 [9:19:57<2:08:21,  4.01s/it] 78%|███████▊  | 6998/8917 [9:20:01<2:07:55,  4.00s/it] 78%|███████▊  | 6999/8917 [9:20:05<2:09:10,  4.04s/it]09/19/2024 11:34:42 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 11:34:42 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<04:34,  1.25s/it][A
  1%|          | 2/221 [00:01<02:57,  1.24it/s][A
  1%|▏         | 3/221 [00:02<02:19,  1.56it/s][A
  2%|▏         | 4/221 [00:02<01:45,  2.06it/s][A
  3%|▎         | 6/221 [00:02<00:58,  3.64it/s][A
  3%|▎         | 7/221 [00:02<00:52,  4.05it/s][A
  4%|▎         | 8/221 [00:03<01:05,  3.26it/s][A
  4%|▍         | 9/221 [00:03<01:08,  3.12it/s][A
  5%|▍         | 10/221 [00:04<01:13,  2.86it/s][A
  5%|▌         | 12/221 [00:10<06:00,  1.72s/it][A
  6%|▌         | 13/221 [00:11<04:46,  1.38s/it][A
  7%|▋         | 15/221 [00:11<03:10,  1.08it/s][A
  7%|▋         | 16/221 [00:12<03:03,  1.12it/s][A
  8%|▊         | 17/221 [00:13<03:07,  1.09it/s][A
  8%|▊         | 18/221 [00:13<02:35,  1.31it/s][A
  9%|▊         | 19/221 [00:15<03:28,  1.03s/it][A
  9%|▉         | 20/221 [00:15<02:35,  1.29it/s][A
 10%|▉         | 21/221 [00:15<02:11,  1.53it/s][A
 10%|▉         | 22/221 [00:16<02:22,  1.39it/s][A
 11%|█         | 24/221 [00:17<01:32,  2.13it/s][A
 11%|█▏        | 25/221 [00:17<01:23,  2.35it/s][A
 12%|█▏        | 26/221 [00:17<01:18,  2.50it/s][A
 13%|█▎        | 28/221 [00:18<01:11,  2.69it/s][A
 13%|█▎        | 29/221 [00:18<01:03,  3.02it/s][A
 14%|█▎        | 30/221 [00:19<01:17,  2.46it/s][A
 14%|█▍        | 31/221 [00:19<01:19,  2.39it/s][A
 15%|█▍        | 33/221 [00:20<01:02,  3.00it/s][A
 16%|█▌        | 35/221 [00:20<00:46,  4.00it/s][A
 16%|█▋        | 36/221 [00:20<00:43,  4.22it/s][A
 17%|█▋        | 37/221 [00:21<00:52,  3.49it/s][A
 17%|█▋        | 38/221 [00:21<01:01,  2.96it/s][A
 18%|█▊        | 39/221 [00:22<01:17,  2.34it/s][A
 18%|█▊        | 40/221 [00:22<01:25,  2.12it/s][A
 19%|█▊        | 41/221 [00:23<01:11,  2.52it/s][A
 19%|█▉        | 42/221 [00:23<00:58,  3.07it/s][A
 19%|█▉        | 43/221 [00:23<00:47,  3.78it/s][A
 20%|█▉        | 44/221 [00:23<00:38,  4.60it/s][A
 20%|██        | 45/221 [00:27<04:11,  1.43s/it][A
 21%|██        | 46/221 [00:28<03:13,  1.10s/it][A
 21%|██▏       | 47/221 [00:28<02:40,  1.08it/s][A
 22%|██▏       | 48/221 [00:28<01:57,  1.47it/s][A
 22%|██▏       | 49/221 [00:28<01:39,  1.72it/s][A
 23%|██▎       | 50/221 [00:29<01:32,  1.85it/s][A
 23%|██▎       | 51/221 [00:29<01:14,  2.27it/s][A
 24%|██▎       | 52/221 [00:29<01:02,  2.70it/s][A
 24%|██▍       | 53/221 [00:30<01:08,  2.46it/s][A
 24%|██▍       | 54/221 [00:31<01:28,  1.88it/s][A
 25%|██▍       | 55/221 [00:32<02:12,  1.25it/s][A
 25%|██▌       | 56/221 [00:32<01:40,  1.64it/s][A
 26%|██▌       | 57/221 [00:32<01:19,  2.07it/s][A
 27%|██▋       | 59/221 [00:33<00:52,  3.10it/s][A
 27%|██▋       | 60/221 [00:33<00:51,  3.13it/s][A
 28%|██▊       | 61/221 [00:33<00:47,  3.34it/s][A
 28%|██▊       | 62/221 [00:34<00:48,  3.31it/s][A
 29%|██▊       | 63/221 [00:34<00:42,  3.70it/s][A
 29%|██▉       | 64/221 [00:35<01:19,  1.98it/s][A
 29%|██▉       | 65/221 [00:35<01:03,  2.45it/s][A
 30%|██▉       | 66/221 [00:36<01:07,  2.31it/s][A
 30%|███       | 67/221 [00:36<01:01,  2.50it/s][A
 31%|███       | 68/221 [00:36<00:49,  3.08it/s][A
 31%|███       | 69/221 [00:38<02:18,  1.10it/s][A
 32%|███▏      | 70/221 [00:38<01:42,  1.47it/s][A
 32%|███▏      | 71/221 [00:39<01:24,  1.77it/s][A
 33%|███▎      | 72/221 [00:39<01:17,  1.93it/s][A
 33%|███▎      | 73/221 [00:40<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:40<01:00,  2.45it/s][A
 34%|███▍      | 75/221 [00:40<00:56,  2.59it/s][A
 34%|███▍      | 76/221 [00:40<00:54,  2.67it/s][A
 35%|███▍      | 77/221 [00:42<01:58,  1.21it/s][A
 35%|███▌      | 78/221 [00:43<01:29,  1.60it/s][A
 36%|███▌      | 79/221 [00:43<01:42,  1.38it/s][A
 36%|███▌      | 80/221 [00:44<01:16,  1.85it/s][A
 37%|███▋      | 81/221 [00:44<01:07,  2.06it/s][A
 37%|███▋      | 82/221 [00:47<03:10,  1.37s/it][A
 38%|███▊      | 83/221 [00:48<02:38,  1.15s/it][A
 38%|███▊      | 84/221 [00:48<02:04,  1.10it/s][A
 39%|███▉      | 86/221 [00:49<01:18,  1.73it/s][A
 39%|███▉      | 87/221 [00:49<01:14,  1.81it/s][A
 40%|███▉      | 88/221 [00:49<01:02,  2.13it/s][A
 40%|████      | 89/221 [00:50<00:55,  2.38it/s][A
 41%|████      | 90/221 [00:50<00:50,  2.60it/s][A
 41%|████      | 91/221 [00:50<00:40,  3.18it/s][A
 42%|████▏     | 92/221 [00:50<00:37,  3.44it/s][A
 42%|████▏     | 93/221 [00:51<00:39,  3.22it/s][A
 43%|████▎     | 94/221 [00:51<00:50,  2.51it/s][A
 43%|████▎     | 95/221 [00:52<00:49,  2.56it/s][A
 43%|████▎     | 96/221 [00:53<01:02,  1.99it/s][A
 44%|████▍     | 97/221 [00:53<00:49,  2.51it/s][A
 44%|████▍     | 98/221 [00:53<00:57,  2.15it/s][A
 45%|████▍     | 99/221 [00:54<00:54,  2.22it/s][A
 45%|████▌     | 100/221 [00:55<01:09,  1.74it/s][A
 46%|████▌     | 101/221 [00:55<00:55,  2.17it/s][A
 46%|████▌     | 102/221 [00:56<01:13,  1.63it/s][A
 47%|████▋     | 103/221 [00:56<00:54,  2.17it/s][A
 47%|████▋     | 104/221 [00:56<00:49,  2.34it/s][A
 48%|████▊     | 105/221 [00:57<00:52,  2.20it/s][A
 48%|████▊     | 106/221 [00:58<01:29,  1.28it/s][A
 48%|████▊     | 107/221 [00:59<01:16,  1.50it/s][A
 49%|████▉     | 108/221 [00:59<01:08,  1.64it/s][A
 49%|████▉     | 109/221 [00:59<00:57,  1.94it/s][A
 50%|████▉     | 110/221 [01:00<00:50,  2.22it/s][A
 50%|█████     | 111/221 [01:00<00:46,  2.36it/s][A
 51%|█████     | 112/221 [01:00<00:42,  2.55it/s][A
 51%|█████     | 113/221 [01:01<00:35,  3.02it/s][A
 52%|█████▏    | 115/221 [01:01<00:36,  2.94it/s][A
 52%|█████▏    | 116/221 [01:02<00:35,  2.92it/s][A
 53%|█████▎    | 117/221 [01:02<00:37,  2.78it/s][A
 53%|█████▎    | 118/221 [01:02<00:39,  2.64it/s][A
 54%|█████▍    | 119/221 [01:03<00:34,  2.96it/s][A
 54%|█████▍    | 120/221 [01:03<00:27,  3.66it/s][A
 55%|█████▍    | 121/221 [01:03<00:34,  2.92it/s][A
 55%|█████▌    | 122/221 [01:04<00:37,  2.66it/s][A
 56%|█████▌    | 123/221 [01:05<01:09,  1.41it/s][A
 56%|█████▌    | 124/221 [01:06<00:54,  1.77it/s][A
 57%|█████▋    | 125/221 [01:06<00:59,  1.63it/s][A
 57%|█████▋    | 126/221 [01:15<04:43,  2.98s/it][A
 57%|█████▋    | 127/221 [01:15<03:32,  2.26s/it][A
 58%|█████▊    | 128/221 [01:16<02:38,  1.70s/it][A
 58%|█████▊    | 129/221 [01:16<02:05,  1.37s/it][A
 59%|█████▉    | 130/221 [01:17<01:33,  1.03s/it][A
 59%|█████▉    | 131/221 [01:18<01:31,  1.01s/it][A
 60%|█████▉    | 132/221 [01:19<01:40,  1.13s/it][A
 60%|██████    | 133/221 [01:20<01:24,  1.04it/s][A
 61%|██████    | 134/221 [01:21<01:27,  1.00s/it][A
 61%|██████    | 135/221 [01:21<01:18,  1.09it/s][A
 62%|██████▏   | 136/221 [01:22<01:04,  1.33it/s][A
 62%|██████▏   | 137/221 [01:22<00:52,  1.60it/s][A
 62%|██████▏   | 138/221 [01:22<00:47,  1.76it/s][A
 63%|██████▎   | 139/221 [01:23<00:37,  2.17it/s][A
 63%|██████▎   | 140/221 [01:23<00:38,  2.11it/s][A
 64%|██████▍   | 141/221 [01:24<00:35,  2.27it/s][A
 64%|██████▍   | 142/221 [01:24<00:31,  2.47it/s][A
 65%|██████▍   | 143/221 [01:24<00:29,  2.64it/s][A
 65%|██████▌   | 144/221 [01:24<00:24,  3.20it/s][A
 66%|██████▌   | 145/221 [01:24<00:19,  3.88it/s][A
 66%|██████▌   | 146/221 [01:25<00:15,  4.72it/s][A
 67%|██████▋   | 148/221 [01:27<00:43,  1.69it/s][A
 67%|██████▋   | 149/221 [01:27<00:37,  1.90it/s][A
 68%|██████▊   | 150/221 [01:27<00:35,  1.99it/s][A
 68%|██████▊   | 151/221 [01:28<00:31,  2.22it/s][A
 69%|██████▉   | 152/221 [01:28<00:30,  2.27it/s][A
 69%|██████▉   | 153/221 [01:28<00:23,  2.88it/s][A
 70%|██████▉   | 154/221 [01:28<00:20,  3.27it/s][A
 70%|███████   | 155/221 [01:29<00:17,  3.81it/s][A
 71%|███████   | 156/221 [01:29<00:14,  4.47it/s][A
 71%|███████   | 157/221 [01:36<02:18,  2.16s/it][A
 71%|███████▏  | 158/221 [01:36<01:50,  1.75s/it][A
 72%|███████▏  | 159/221 [01:36<01:18,  1.26s/it][A
 72%|███████▏  | 160/221 [01:37<00:57,  1.06it/s][A
 73%|███████▎  | 161/221 [01:37<00:41,  1.44it/s][A
 74%|███████▍  | 163/221 [01:37<00:25,  2.26it/s][A
 74%|███████▍  | 164/221 [01:37<00:20,  2.77it/s][A
 75%|███████▍  | 165/221 [01:37<00:19,  2.89it/s][A
 75%|███████▌  | 166/221 [01:38<00:23,  2.37it/s][A
 76%|███████▌  | 167/221 [01:38<00:21,  2.52it/s][A
 76%|███████▌  | 168/221 [01:41<00:51,  1.03it/s][A
 76%|███████▋  | 169/221 [01:41<00:43,  1.19it/s][A
 77%|███████▋  | 170/221 [01:42<00:37,  1.37it/s][A
 77%|███████▋  | 171/221 [01:42<00:31,  1.60it/s][A
 78%|███████▊  | 172/221 [01:42<00:25,  1.90it/s][A
 78%|███████▊  | 173/221 [01:43<00:22,  2.12it/s][A
 79%|███████▊  | 174/221 [01:43<00:17,  2.66it/s][A
 79%|███████▉  | 175/221 [01:43<00:16,  2.76it/s][A
 80%|███████▉  | 176/221 [01:44<00:17,  2.62it/s][A
 80%|████████  | 177/221 [01:44<00:15,  2.79it/s][A
 81%|████████  | 178/221 [01:44<00:15,  2.70it/s][A
 81%|████████  | 179/221 [01:45<00:14,  2.81it/s][A
 81%|████████▏ | 180/221 [01:45<00:13,  3.14it/s][A
 82%|████████▏ | 181/221 [01:45<00:10,  3.81it/s][A
 82%|████████▏ | 182/221 [01:46<00:13,  2.99it/s][A
 83%|████████▎ | 183/221 [01:46<00:17,  2.15it/s][A
 83%|████████▎ | 184/221 [01:47<00:17,  2.16it/s][A
 84%|████████▎ | 185/221 [01:47<00:16,  2.21it/s][A
 84%|████████▍ | 186/221 [01:47<00:13,  2.52it/s][A
 85%|████████▍ | 187/221 [01:48<00:12,  2.65it/s][A
 85%|████████▌ | 188/221 [01:48<00:10,  3.12it/s][A
 86%|████████▌ | 189/221 [01:48<00:09,  3.21it/s][A
 86%|████████▌ | 190/221 [01:49<00:09,  3.11it/s][A
 86%|████████▋ | 191/221 [01:49<00:08,  3.45it/s][A
 87%|████████▋ | 192/221 [01:50<00:13,  2.22it/s][A
 87%|████████▋ | 193/221 [01:50<00:10,  2.64it/s][A
 88%|████████▊ | 194/221 [01:50<00:10,  2.61it/s][A
 88%|████████▊ | 195/221 [01:50<00:08,  3.12it/s][A
 89%|████████▊ | 196/221 [01:51<00:09,  2.68it/s][A
 89%|████████▉ | 197/221 [01:51<00:08,  2.96it/s][A
 90%|████████▉ | 198/221 [01:51<00:07,  3.15it/s][A
 90%|█████████ | 199/221 [01:52<00:06,  3.59it/s][A
 90%|█████████ | 200/221 [01:53<00:09,  2.12it/s][A
 91%|█████████ | 201/221 [01:53<00:11,  1.79it/s][A
 91%|█████████▏| 202/221 [01:54<00:09,  2.07it/s][A
 92%|█████████▏| 203/221 [01:54<00:10,  1.79it/s][A
 92%|█████████▏| 204/221 [01:55<00:09,  1.73it/s][A
 93%|█████████▎| 206/221 [01:55<00:05,  2.54it/s][A
 94%|█████████▎| 207/221 [01:56<00:04,  2.96it/s][A
 94%|█████████▍| 208/221 [01:56<00:05,  2.60it/s][A
 95%|█████████▌| 210/221 [01:56<00:03,  3.56it/s][A
 95%|█████████▌| 211/221 [01:57<00:03,  3.03it/s][A
 96%|█████████▌| 212/221 [01:57<00:02,  3.09it/s][A
 96%|█████████▋| 213/221 [01:57<00:02,  3.47it/s][A
 97%|█████████▋| 214/221 [01:58<00:02,  2.97it/s][A
 97%|█████████▋| 215/221 [01:58<00:02,  2.76it/s][A
 98%|█████████▊| 216/221 [01:59<00:01,  2.90it/s][A
 98%|█████████▊| 217/221 [02:02<00:04,  1.16s/it][A
 99%|█████████▊| 218/221 [02:02<00:02,  1.08it/s][A
 99%|█████████▉| 219/221 [02:02<00:01,  1.35it/s][A
100%|█████████▉| 220/221 [02:05<00:01,  1.34s/it][A
100%|██████████| 221/221 [02:05<00:00,  1.01it/s][A100%|██████████| 221/221 [02:05<00:00,  1.76it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:51,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:50,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:09<01:48,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:41,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:40,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:18<01:39,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:33,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:32,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:31,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:27<01:30,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:22,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:36<01:21,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:37<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:45<01:12,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:46<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:05,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:04,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:03<00:54,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:04<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:05<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:37,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:21<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:23<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:30<00:27,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:31<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:32<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:33<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:39<00:18,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:40<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:42<00:15,  1.77it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.81it/s][A
 88%|████████▊ | 195/221 [01:43<00:14,  1.83it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.85it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.86it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.87it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.87it/s][A
 90%|█████████ | 200/221 [01:46<00:11,  1.88it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.88it/s][A
 91%|█████████▏| 202/221 [01:47<00:10,  1.88it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:48<00:09,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:51<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:55<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:56<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:57<00:00,  1.89it/s][A100%|██████████| 221/221 [01:57<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:12,  3.03it/s][A
  1%|          | 2/221 [00:01<02:38,  1.38it/s][A
  1%|▏         | 3/221 [00:01<01:45,  2.06it/s][A
  2%|▏         | 4/221 [00:02<01:46,  2.05it/s][A
  2%|▏         | 5/221 [00:02<01:55,  1.87it/s][A
  3%|▎         | 6/221 [00:02<01:40,  2.15it/s][A
  3%|▎         | 7/221 [00:03<01:30,  2.36it/s][A
  4%|▎         | 8/221 [00:05<02:56,  1.21it/s][A
  4%|▍         | 9/221 [00:05<02:59,  1.18it/s][A
  5%|▍         | 10/221 [00:06<02:16,  1.55it/s][A
  5%|▍         | 11/221 [00:06<02:00,  1.74it/s][A
  5%|▌         | 12/221 [00:06<01:46,  1.96it/s][A
  6%|▌         | 13/221 [00:07<01:25,  2.42it/s][A
  6%|▋         | 14/221 [00:07<01:06,  3.10it/s][A
  7%|▋         | 15/221 [00:07<01:06,  3.08it/s][A
  7%|▋         | 16/221 [00:08<01:41,  2.01it/s][A
  8%|▊         | 17/221 [00:09<02:04,  1.64it/s][A
  8%|▊         | 18/221 [00:09<01:39,  2.04it/s][A
  9%|▊         | 19/221 [00:10<02:01,  1.66it/s][A
  9%|▉         | 20/221 [00:10<01:49,  1.83it/s][A
 10%|▉         | 21/221 [00:11<01:52,  1.77it/s][A
 10%|▉         | 22/221 [00:12<02:35,  1.28it/s][A
 10%|█         | 23/221 [00:12<02:02,  1.61it/s][A
 11%|█         | 24/221 [00:13<01:49,  1.80it/s][A
 11%|█▏        | 25/221 [00:14<02:05,  1.57it/s][A
 12%|█▏        | 26/221 [00:14<02:09,  1.51it/s][A
 12%|█▏        | 27/221 [00:15<01:41,  1.91it/s][A
 13%|█▎        | 28/221 [00:16<02:36,  1.24it/s][A
 13%|█▎        | 29/221 [00:16<02:03,  1.55it/s][A
 14%|█▎        | 30/221 [00:17<02:02,  1.56it/s][A
 14%|█▍        | 31/221 [00:17<01:54,  1.65it/s][A
 14%|█▍        | 32/221 [00:19<02:25,  1.30it/s][A
 15%|█▍        | 33/221 [00:19<02:16,  1.38it/s][A
 15%|█▌        | 34/221 [00:20<01:52,  1.67it/s][A
 16%|█▌        | 35/221 [00:20<01:29,  2.09it/s][A
 16%|█▋        | 36/221 [00:20<01:38,  1.88it/s][A
 17%|█▋        | 37/221 [00:21<01:33,  1.96it/s][A
 17%|█▋        | 38/221 [00:22<02:05,  1.46it/s][A
 18%|█▊        | 39/221 [00:23<02:03,  1.47it/s][A
 18%|█▊        | 40/221 [00:24<02:36,  1.16it/s][A
 19%|█▊        | 41/221 [00:24<02:02,  1.47it/s][A
 19%|█▉        | 42/221 [00:24<01:39,  1.80it/s][A
 19%|█▉        | 43/221 [00:25<01:28,  2.01it/s][A
 20%|█▉        | 44/221 [00:25<01:36,  1.84it/s][A
 21%|██        | 46/221 [00:26<01:08,  2.56it/s][A
 21%|██▏       | 47/221 [00:26<01:16,  2.27it/s][A
 22%|██▏       | 48/221 [00:27<01:03,  2.73it/s][A
 22%|██▏       | 49/221 [00:27<01:19,  2.16it/s][A
 23%|██▎       | 50/221 [00:28<01:22,  2.08it/s][A
 23%|██▎       | 51/221 [00:28<01:13,  2.33it/s][A
 24%|██▎       | 52/221 [00:28<01:07,  2.51it/s][A
 24%|██▍       | 53/221 [00:29<01:09,  2.42it/s][A
 24%|██▍       | 54/221 [00:29<01:13,  2.26it/s][A
 25%|██▍       | 55/221 [00:30<01:03,  2.62it/s][A
 25%|██▌       | 56/221 [00:30<01:05,  2.51it/s][A
 26%|██▌       | 57/221 [00:30<00:58,  2.82it/s][A
 26%|██▌       | 58/221 [00:31<00:51,  3.15it/s][A
 27%|██▋       | 59/221 [00:31<00:41,  3.94it/s][A
 27%|██▋       | 60/221 [00:31<00:55,  2.88it/s][A
 28%|██▊       | 61/221 [00:32<01:12,  2.21it/s][A
 28%|██▊       | 62/221 [00:32<01:10,  2.26it/s][A
 29%|██▊       | 63/221 [00:33<01:32,  1.71it/s][A
 29%|██▉       | 64/221 [00:35<02:19,  1.13it/s][A
 29%|██▉       | 65/221 [00:36<02:37,  1.01s/it][A
 30%|██▉       | 66/221 [00:37<02:12,  1.17it/s][A
 30%|███       | 67/221 [00:37<01:48,  1.41it/s][A
 31%|███       | 68/221 [00:37<01:27,  1.75it/s][A
 31%|███       | 69/221 [00:38<01:10,  2.15it/s][A
 32%|███▏      | 70/221 [00:38<01:14,  2.02it/s][A
 32%|███▏      | 71/221 [00:39<01:20,  1.86it/s][A
 33%|███▎      | 72/221 [00:39<01:22,  1.81it/s][A
 33%|███▎      | 73/221 [00:40<01:27,  1.68it/s][A
 33%|███▎      | 74/221 [00:40<01:16,  1.91it/s][A
 34%|███▍      | 75/221 [00:41<01:06,  2.19it/s][A
 34%|███▍      | 76/221 [00:41<01:04,  2.26it/s][A
 35%|███▍      | 77/221 [00:41<00:55,  2.58it/s][A
 35%|███▌      | 78/221 [00:42<00:59,  2.42it/s][A
 36%|███▌      | 79/221 [00:43<01:22,  1.71it/s][A
 36%|███▌      | 80/221 [00:43<01:05,  2.17it/s][A
 37%|███▋      | 81/221 [00:44<01:08,  2.03it/s][A
 37%|███▋      | 82/221 [00:44<01:27,  1.59it/s][A
 38%|███▊      | 83/221 [00:45<01:17,  1.79it/s][A
 38%|███▊      | 84/221 [00:46<01:43,  1.33it/s][A
 38%|███▊      | 85/221 [00:46<01:19,  1.70it/s][A
 39%|███▉      | 86/221 [00:47<01:15,  1.78it/s][A
 39%|███▉      | 87/221 [00:47<01:12,  1.85it/s][A
 40%|███▉      | 88/221 [00:48<01:05,  2.02it/s][A
 40%|████      | 89/221 [00:48<01:10,  1.88it/s][A
 41%|████      | 90/221 [00:49<01:15,  1.73it/s][A
 41%|████      | 91/221 [00:49<01:02,  2.07it/s][A
 42%|████▏     | 92/221 [00:50<01:01,  2.10it/s][A
 43%|████▎     | 94/221 [00:50<00:43,  2.95it/s][A
 43%|████▎     | 95/221 [00:50<00:43,  2.87it/s][A
 43%|████▎     | 96/221 [00:51<00:43,  2.89it/s][A
 44%|████▍     | 97/221 [00:51<00:50,  2.45it/s][A
 44%|████▍     | 98/221 [00:52<01:04,  1.90it/s][A
 45%|████▍     | 99/221 [00:53<01:07,  1.80it/s][A
 45%|████▌     | 100/221 [00:54<01:21,  1.49it/s][A
 46%|████▌     | 101/221 [00:54<01:23,  1.44it/s][A
 46%|████▌     | 102/221 [00:56<01:35,  1.25it/s][A
 47%|████▋     | 103/221 [00:56<01:15,  1.55it/s][A
 47%|████▋     | 104/221 [00:57<01:16,  1.52it/s][A
 48%|████▊     | 105/221 [00:57<01:07,  1.73it/s][A
 48%|████▊     | 106/221 [00:57<00:53,  2.14it/s][A
 48%|████▊     | 107/221 [00:58<00:53,  2.15it/s][A
 49%|████▉     | 108/221 [00:58<00:57,  1.98it/s][A
 49%|████▉     | 109/221 [00:59<00:56,  1.97it/s][A
 50%|████▉     | 110/221 [00:59<01:05,  1.70it/s][A
 50%|█████     | 111/221 [01:01<01:19,  1.38it/s][A
 51%|█████     | 112/221 [01:01<01:10,  1.55it/s][A
 51%|█████     | 113/221 [01:01<00:58,  1.86it/s][A
 52%|█████▏    | 114/221 [01:02<00:53,  2.00it/s][A
 52%|█████▏    | 115/221 [01:02<00:58,  1.80it/s][A
 52%|█████▏    | 116/221 [01:03<00:46,  2.24it/s][A
 53%|█████▎    | 117/221 [01:03<00:43,  2.40it/s][A
 53%|█████▎    | 118/221 [01:03<00:44,  2.33it/s][A
 54%|█████▍    | 119/221 [01:04<00:48,  2.11it/s][A
 54%|█████▍    | 120/221 [01:04<00:49,  2.05it/s][A
 55%|█████▍    | 121/221 [01:05<00:48,  2.07it/s][A
 55%|█████▌    | 122/221 [01:06<00:51,  1.91it/s][A
 56%|█████▌    | 123/221 [01:06<00:46,  2.10it/s][A
 56%|█████▌    | 124/221 [01:07<01:05,  1.47it/s][A
 57%|█████▋    | 125/221 [01:07<00:55,  1.72it/s][A
 57%|█████▋    | 126/221 [01:08<00:43,  2.20it/s][A
 57%|█████▋    | 127/221 [01:09<00:58,  1.61it/s][A
 58%|█████▊    | 128/221 [01:09<00:55,  1.66it/s][A
 58%|█████▊    | 129/221 [01:10<00:56,  1.64it/s][A
 59%|█████▉    | 130/221 [01:10<00:43,  2.09it/s][A
 59%|█████▉    | 131/221 [01:10<00:41,  2.14it/s][A
 60%|█████▉    | 132/221 [01:11<00:56,  1.58it/s][A
 60%|██████    | 133/221 [01:12<00:57,  1.52it/s][A
 61%|██████    | 134/221 [01:13<00:52,  1.66it/s][A
 61%|██████    | 135/221 [01:13<00:51,  1.68it/s][A
 62%|██████▏   | 136/221 [01:14<00:57,  1.48it/s][A
 62%|██████▏   | 137/221 [01:15<00:58,  1.44it/s][A
 62%|██████▏   | 138/221 [01:16<01:01,  1.34it/s][A
 63%|██████▎   | 139/221 [01:17<01:14,  1.10it/s][A
 63%|██████▎   | 140/221 [01:18<01:11,  1.13it/s][A
 64%|██████▍   | 141/221 [01:18<01:00,  1.33it/s][A
 64%|██████▍   | 142/221 [01:19<00:53,  1.47it/s][A
 65%|██████▍   | 143/221 [01:19<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:19<00:37,  2.05it/s][A
 66%|██████▌   | 145/221 [01:20<00:36,  2.08it/s][A
 66%|██████▌   | 146/221 [01:20<00:28,  2.60it/s][A
 67%|██████▋   | 147/221 [01:20<00:31,  2.37it/s][A
 67%|██████▋   | 148/221 [01:21<00:31,  2.31it/s][A
 68%|██████▊   | 150/221 [01:21<00:23,  3.07it/s][A
 68%|██████▊   | 151/221 [01:21<00:19,  3.67it/s][A
 69%|██████▉   | 152/221 [01:22<00:22,  3.01it/s][A
 69%|██████▉   | 153/221 [01:22<00:21,  3.20it/s][A
 70%|██████▉   | 154/221 [01:23<00:30,  2.18it/s][A
 70%|███████   | 155/221 [01:24<00:43,  1.53it/s][A
 71%|███████   | 156/221 [01:24<00:37,  1.75it/s][A
 71%|███████   | 157/221 [01:25<00:37,  1.72it/s][A
 71%|███████▏  | 158/221 [01:25<00:31,  2.01it/s][A
 72%|███████▏  | 160/221 [01:26<00:26,  2.31it/s][A
 73%|███████▎  | 161/221 [01:26<00:22,  2.72it/s][A
 73%|███████▎  | 162/221 [01:27<00:24,  2.38it/s][A
 74%|███████▍  | 163/221 [01:27<00:20,  2.78it/s][A
 74%|███████▍  | 164/221 [01:27<00:20,  2.77it/s][A
 75%|███████▍  | 165/221 [01:28<00:17,  3.20it/s][A
 75%|███████▌  | 166/221 [01:28<00:25,  2.17it/s][A
 76%|███████▌  | 167/221 [01:29<00:22,  2.38it/s][A
 76%|███████▌  | 168/221 [01:29<00:19,  2.69it/s][A
 76%|███████▋  | 169/221 [01:30<00:35,  1.46it/s][A
 77%|███████▋  | 170/221 [01:31<00:31,  1.64it/s][A
 77%|███████▋  | 171/221 [01:32<00:31,  1.60it/s][A
 78%|███████▊  | 172/221 [01:32<00:25,  1.95it/s][A
 78%|███████▊  | 173/221 [01:32<00:21,  2.27it/s][A
 79%|███████▊  | 174/221 [01:32<00:19,  2.42it/s][A
 79%|███████▉  | 175/221 [01:33<00:18,  2.46it/s][A
 80%|███████▉  | 176/221 [01:33<00:17,  2.57it/s][A
 80%|████████  | 177/221 [01:34<00:17,  2.46it/s][A
 81%|████████  | 178/221 [01:34<00:17,  2.52it/s][A
 81%|████████  | 179/221 [01:35<00:20,  2.01it/s][A
 81%|████████▏ | 180/221 [01:35<00:15,  2.57it/s][A
 82%|████████▏ | 181/221 [01:35<00:12,  3.08it/s][A
 82%|████████▏ | 182/221 [01:36<00:15,  2.49it/s][A
 83%|████████▎ | 183/221 [01:36<00:17,  2.13it/s][A
 83%|████████▎ | 184/221 [01:37<00:22,  1.64it/s][A
 84%|████████▎ | 185/221 [01:38<00:20,  1.73it/s][A
 84%|████████▍ | 186/221 [01:38<00:15,  2.20it/s][A
 85%|████████▍ | 187/221 [01:39<00:18,  1.87it/s][A
 85%|████████▌ | 188/221 [01:39<00:15,  2.15it/s][A
 86%|████████▌ | 189/221 [01:40<00:17,  1.80it/s][A
 86%|████████▌ | 190/221 [01:40<00:14,  2.09it/s][A
 86%|████████▋ | 191/221 [01:41<00:15,  1.92it/s][A
 87%|████████▋ | 192/221 [01:41<00:14,  1.99it/s][A
 87%|████████▋ | 193/221 [01:41<00:12,  2.25it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.80it/s][A
 88%|████████▊ | 195/221 [01:43<00:19,  1.34it/s][A
 89%|████████▊ | 196/221 [01:44<00:16,  1.56it/s][A
 89%|████████▉ | 197/221 [01:44<00:15,  1.51it/s][A
 90%|████████▉ | 198/221 [01:45<00:14,  1.55it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.84it/s][A
 90%|█████████ | 200/221 [01:46<00:13,  1.53it/s][A
 91%|█████████ | 201/221 [01:47<00:12,  1.59it/s][A
 91%|█████████▏| 202/221 [01:47<00:10,  1.85it/s][A
 92%|█████████▏| 203/221 [01:48<00:09,  1.99it/s][A
 92%|█████████▏| 204/221 [01:48<00:07,  2.27it/s][A
 93%|█████████▎| 205/221 [01:48<00:06,  2.58it/s][A
 93%|█████████▎| 206/221 [01:48<00:05,  2.79it/s][A
 94%|█████████▎| 207/221 [01:49<00:04,  3.20it/s][A
 94%|█████████▍| 208/221 [01:50<00:08,  1.55it/s][A
 95%|█████████▍| 209/221 [01:51<00:07,  1.65it/s][A
 95%|█████████▌| 210/221 [01:51<00:05,  2.05it/s][A
 95%|█████████▌| 211/221 [01:52<00:06,  1.60it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.97it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.92it/s][A
 97%|█████████▋| 214/221 [01:53<00:04,  1.68it/s][A
 97%|█████████▋| 215/221 [01:53<00:02,  2.23it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  2.01it/s][A
 98%|█████████▊| 217/221 [01:55<00:02,  1.73it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  2.06it/s][A
 99%|█████████▉| 219/221 [01:56<00:01,  1.95it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  2.13it/s][A
100%|██████████| 221/221 [01:57<00:00,  1.84it/s][A100%|██████████| 221/221 [01:57<00:00,  1.89it/s]
09/19/2024 11:43:40 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 6999--===========

09/19/2024 11:43:40 - INFO - __main__ -   {'area_r1': 45.8, 'area_recall': '45.8/75.6/83.5', 'area_ravg': 68.3}
09/19/2024 11:43:40 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 6999--===========

09/19/2024 11:43:40 - INFO - __main__ -   {'forward_r1': 50.7, 'forward_recall': '50.7/78.2/88.3', 'forward_ravg': 72.4}
09/19/2024 11:43:40 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 6999--===========

09/19/2024 11:43:40 - INFO - __main__ -   {'area_video_r1': 48.3, 'area_video_recall': '48.3/78.5/87.9', 'area_video_ravg': 71.6}
09/19/2024 11:43:40 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 11:43:40 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 11:43:40 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 6999--===========

09/19/2024 11:43:40 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/84.2/89.6', 'area_video_ravg': 79.2, 'area_video_back_r1': 64.1, 'area_video_back_recall': '64.1/85.5/92.1', 'area_video_back_ravg': 80.6}
09/19/2024 11:43:40 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 6999=======

09/19/2024 11:43:40 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/84.2/89.6', 'area_video_ravg': 79.2, 'area_video_back_r1': 64.1, 'area_video_back_recall': '64.1/85.5/92.1', 'area_video_back_ravg': 80.6}
09/19/2024 11:43:40 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 6999--===========

09/19/2024 11:43:40 - INFO - __main__ -   {'video_r1': 30.5, 'video_recall': '30.5/55.8/66.0', 'video_ravg': 50.8}
09/19/2024 11:43:40 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 11:43:40 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 11:43:40 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 6999--===========

09/19/2024 11:43:40 - INFO - __main__ -   {'video_r1': 60.7, 'video_recall': '60.7/81.2/85.1', 'video_ravg': 75.7}
09/19/2024 11:43:40 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 6499=======

09/19/2024 11:43:40 - INFO - __main__ -   {'video_r1': 60.9, 'video_recall': '60.9/80.4/85.2', 'video_ravg': 75.5}
09/19/2024 11:44:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.04088340699672699, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.690575122833252, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.7314585447311401}
 79%|███████▊  | 7000/8917 [9:29:33<92:14:35, 173.23s/it] 79%|███████▊  | 7001/8917 [9:29:37<65:04:57, 122.28s/it] 79%|███████▊  | 7002/8917 [9:29:41<46:11:32, 86.84s/it]  79%|███████▊  | 7003/8917 [9:29:45<32:57:48, 62.00s/it] 79%|███████▊  | 7004/8917 [9:29:49<23:42:33, 44.62s/it] 79%|███████▊  | 7005/8917 [9:29:52<17:09:01, 32.29s/it] 79%|███████▊  | 7006/8917 [9:29:56<12:36:52, 23.76s/it] 79%|███████▊  | 7007/8917 [9:30:00<9:26:40, 17.80s/it]  79%|███████▊  | 7008/8917 [9:30:04<7:14:42, 13.66s/it] 79%|███████▊  | 7009/8917 [9:30:08<5:40:18, 10.70s/it] 79%|███████▊  | 7010/8917 [9:30:12<4:33:14,  8.60s/it] 79%|███████▊  | 7011/8917 [9:30:15<3:46:32,  7.13s/it] 79%|███████▊  | 7012/8917 [9:30:19<3:15:27,  6.16s/it] 79%|███████▊  | 7013/8917 [9:30:23<2:52:36,  5.44s/it] 79%|███████▊  | 7014/8917 [9:30:27<2:35:41,  4.91s/it] 79%|███████▊  | 7015/8917 [9:30:30<2:24:01,  4.54s/it] 79%|███████▊  | 7016/8917 [9:30:34<2:12:41,  4.19s/it] 79%|███████▊  | 7017/8917 [9:30:38<2:10:10,  4.11s/it] 79%|███████▊  | 7018/8917 [9:30:42<2:13:06,  4.21s/it] 79%|███████▊  | 7019/8917 [9:30:46<2:11:06,  4.14s/it] 79%|███████▊  | 7020/8917 [9:30:50<2:06:03,  3.99s/it] 79%|███████▊  | 7021/8917 [9:30:53<2:00:49,  3.82s/it] 79%|███████▊  | 7022/8917 [9:30:57<2:02:42,  3.89s/it] 79%|███████▉  | 7023/8917 [9:31:01<2:02:19,  3.87s/it] 79%|███████▉  | 7024/8917 [9:31:05<2:00:23,  3.82s/it] 79%|███████▉  | 7025/8917 [9:31:09<2:01:16,  3.85s/it] 79%|███████▉  | 7026/8917 [9:31:12<2:00:05,  3.81s/it] 79%|███████▉  | 7027/8917 [9:31:17<2:04:16,  3.95s/it] 79%|███████▉  | 7028/8917 [9:31:20<2:02:01,  3.88s/it] 79%|███████▉  | 7029/8917 [9:31:24<2:02:28,  3.89s/it] 79%|███████▉  | 7030/8917 [9:31:28<2:02:11,  3.89s/it] 79%|███████▉  | 7031/8917 [9:31:32<2:03:08,  3.92s/it] 79%|███████▉  | 7032/8917 [9:31:35<1:57:27,  3.74s/it] 79%|███████▉  | 7033/8917 [9:31:39<1:58:45,  3.78s/it] 79%|███████▉  | 7034/8917 [9:31:43<1:58:12,  3.77s/it] 79%|███████▉  | 7035/8917 [9:31:47<1:59:04,  3.80s/it] 79%|███████▉  | 7036/8917 [9:31:51<2:03:18,  3.93s/it] 79%|███████▉  | 7037/8917 [9:31:55<2:02:09,  3.90s/it] 79%|███████▉  | 7038/8917 [9:31:59<2:01:41,  3.89s/it] 79%|███████▉  | 7039/8917 [9:32:02<1:57:39,  3.76s/it] 79%|███████▉  | 7040/8917 [9:32:06<1:57:37,  3.76s/it] 79%|███████▉  | 7041/8917 [9:32:10<1:59:02,  3.81s/it] 79%|███████▉  | 7042/8917 [9:32:14<2:01:12,  3.88s/it] 79%|███████▉  | 7043/8917 [9:32:18<2:01:08,  3.88s/it] 79%|███████▉  | 7044/8917 [9:32:21<1:57:38,  3.77s/it] 79%|███████▉  | 7045/8917 [9:32:25<1:56:59,  3.75s/it] 79%|███████▉  | 7046/8917 [9:32:29<1:56:55,  3.75s/it] 79%|███████▉  | 7047/8917 [9:32:32<1:54:44,  3.68s/it] 79%|███████▉  | 7048/8917 [9:32:37<1:59:52,  3.85s/it] 79%|███████▉  | 7049/8917 [9:32:40<1:57:40,  3.78s/it]09/19/2024 11:47:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02277047000825405, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0038126707077026, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.026583194732666}
 79%|███████▉  | 7050/8917 [9:32:44<1:54:45,  3.69s/it] 79%|███████▉  | 7051/8917 [9:32:47<1:55:36,  3.72s/it] 79%|███████▉  | 7052/8917 [9:32:51<1:56:57,  3.76s/it] 79%|███████▉  | 7053/8917 [9:32:56<2:00:42,  3.89s/it] 79%|███████▉  | 7054/8917 [9:32:59<1:59:02,  3.83s/it] 79%|███████▉  | 7055/8917 [9:33:03<1:59:41,  3.86s/it] 79%|███████▉  | 7056/8917 [9:33:07<2:00:16,  3.88s/it] 79%|███████▉  | 7057/8917 [9:33:11<1:59:30,  3.85s/it] 79%|███████▉  | 7058/8917 [9:33:15<1:59:59,  3.87s/it] 79%|███████▉  | 7059/8917 [9:33:19<2:02:29,  3.96s/it] 79%|███████▉  | 7060/8917 [9:33:23<2:01:56,  3.94s/it] 79%|███████▉  | 7061/8917 [9:33:27<1:59:54,  3.88s/it] 79%|███████▉  | 7062/8917 [9:33:30<2:00:30,  3.90s/it] 79%|███████▉  | 7063/8917 [9:33:34<1:59:54,  3.88s/it] 79%|███████▉  | 7064/8917 [9:33:38<1:57:11,  3.79s/it] 79%|███████▉  | 7065/8917 [9:33:42<1:57:48,  3.82s/it] 79%|███████▉  | 7066/8917 [9:33:46<1:57:44,  3.82s/it] 79%|███████▉  | 7067/8917 [9:33:50<1:59:22,  3.87s/it] 79%|███████▉  | 7068/8917 [9:33:54<2:01:48,  3.95s/it] 79%|███████▉  | 7069/8917 [9:33:58<2:00:25,  3.91s/it] 79%|███████▉  | 7070/8917 [9:34:01<1:57:48,  3.83s/it] 79%|███████▉  | 7071/8917 [9:34:05<1:57:36,  3.82s/it] 79%|███████▉  | 7072/8917 [9:34:09<1:58:29,  3.85s/it] 79%|███████▉  | 7073/8917 [9:34:13<1:56:14,  3.78s/it] 79%|███████▉  | 7074/8917 [9:34:16<1:55:52,  3.77s/it] 79%|███████▉  | 7075/8917 [9:34:20<1:58:33,  3.86s/it] 79%|███████▉  | 7076/8917 [9:34:24<1:59:35,  3.90s/it] 79%|███████▉  | 7077/8917 [9:34:28<2:00:09,  3.92s/it] 79%|███████▉  | 7078/8917 [9:34:32<1:57:48,  3.84s/it] 79%|███████▉  | 7079/8917 [9:34:36<1:58:14,  3.86s/it] 79%|███████▉  | 7080/8917 [9:34:40<1:58:43,  3.88s/it] 79%|███████▉  | 7081/8917 [9:34:44<1:57:41,  3.85s/it] 79%|███████▉  | 7082/8917 [9:34:48<1:59:14,  3.90s/it] 79%|███████▉  | 7083/8917 [9:34:52<2:01:05,  3.96s/it] 79%|███████▉  | 7084/8917 [9:34:55<1:58:43,  3.89s/it] 79%|███████▉  | 7085/8917 [9:34:59<1:58:07,  3.87s/it] 79%|███████▉  | 7086/8917 [9:35:03<1:57:26,  3.85s/it] 79%|███████▉  | 7087/8917 [9:35:07<1:58:50,  3.90s/it] 79%|███████▉  | 7088/8917 [9:35:11<1:55:52,  3.80s/it] 79%|███████▉  | 7089/8917 [9:35:15<2:00:31,  3.96s/it] 80%|███████▉  | 7090/8917 [9:35:19<1:57:06,  3.85s/it] 80%|███████▉  | 7091/8917 [9:35:22<1:54:21,  3.76s/it] 80%|███████▉  | 7092/8917 [9:35:26<1:54:25,  3.76s/it] 80%|███████▉  | 7093/8917 [9:35:30<1:53:31,  3.73s/it] 80%|███████▉  | 7094/8917 [9:35:33<1:54:11,  3.76s/it] 80%|███████▉  | 7095/8917 [9:35:37<1:54:16,  3.76s/it] 80%|███████▉  | 7096/8917 [9:35:41<1:54:32,  3.77s/it] 80%|███████▉  | 7097/8917 [9:35:44<1:52:37,  3.71s/it] 80%|███████▉  | 7098/8917 [9:35:48<1:53:01,  3.73s/it] 80%|███████▉  | 7099/8917 [9:35:52<1:51:55,  3.69s/it]09/19/2024 11:50:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.024643735960125923, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2606709003448486, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.285314679145813}
 80%|███████▉  | 7100/8917 [9:35:56<1:54:34,  3.78s/it] 80%|███████▉  | 7101/8917 [9:36:00<1:55:02,  3.80s/it] 80%|███████▉  | 7102/8917 [9:36:03<1:53:52,  3.76s/it] 80%|███████▉  | 7103/8917 [9:36:07<1:56:51,  3.87s/it] 80%|███████▉  | 7104/8917 [9:36:11<1:56:05,  3.84s/it] 80%|███████▉  | 7105/8917 [9:36:15<1:57:47,  3.90s/it] 80%|███████▉  | 7106/8917 [9:36:19<1:55:59,  3.84s/it] 80%|███████▉  | 7107/8917 [9:36:23<1:55:21,  3.82s/it] 80%|███████▉  | 7108/8917 [9:36:27<1:56:35,  3.87s/it] 80%|███████▉  | 7109/8917 [9:36:31<1:56:09,  3.85s/it] 80%|███████▉  | 7110/8917 [9:36:35<1:56:58,  3.88s/it] 80%|███████▉  | 7111/8917 [9:36:39<1:57:36,  3.91s/it] 80%|███████▉  | 7112/8917 [9:36:42<1:55:26,  3.84s/it] 80%|███████▉  | 7113/8917 [9:36:46<1:52:42,  3.75s/it] 80%|███████▉  | 7114/8917 [9:36:49<1:52:27,  3.74s/it] 80%|███████▉  | 7115/8917 [9:36:53<1:52:53,  3.76s/it] 80%|███████▉  | 7116/8917 [9:36:57<1:53:12,  3.77s/it] 80%|███████▉  | 7117/8917 [9:37:01<1:51:31,  3.72s/it] 80%|███████▉  | 7118/8917 [9:37:04<1:51:12,  3.71s/it] 80%|███████▉  | 7119/8917 [9:37:08<1:53:34,  3.79s/it] 80%|███████▉  | 7120/8917 [9:37:12<1:54:06,  3.81s/it] 80%|███████▉  | 7121/8917 [9:37:16<1:52:04,  3.74s/it] 80%|███████▉  | 7122/8917 [9:37:20<1:56:35,  3.90s/it] 80%|███████▉  | 7123/8917 [9:37:24<1:56:13,  3.89s/it] 80%|███████▉  | 7124/8917 [9:37:28<1:54:09,  3.82s/it] 80%|███████▉  | 7125/8917 [9:37:31<1:54:14,  3.82s/it] 80%|███████▉  | 7126/8917 [9:37:36<1:57:07,  3.92s/it] 80%|███████▉  | 7127/8917 [9:37:39<1:56:00,  3.89s/it] 80%|███████▉  | 7128/8917 [9:37:43<1:55:10,  3.86s/it] 80%|███████▉  | 7129/8917 [9:37:47<1:50:44,  3.72s/it] 80%|███████▉  | 7130/8917 [9:37:50<1:50:40,  3.72s/it] 80%|███████▉  | 7131/8917 [9:37:54<1:53:33,  3.81s/it] 80%|███████▉  | 7132/8917 [9:37:58<1:53:49,  3.83s/it] 80%|███████▉  | 7133/8917 [9:38:02<1:53:47,  3.83s/it] 80%|████████  | 7134/8917 [9:38:06<1:51:40,  3.76s/it] 80%|████████  | 7135/8917 [9:38:09<1:52:58,  3.80s/it] 80%|████████  | 7136/8917 [9:38:13<1:53:41,  3.83s/it] 80%|████████  | 7137/8917 [9:38:17<1:51:00,  3.74s/it] 80%|████████  | 7138/8917 [9:38:21<1:50:43,  3.73s/it] 80%|████████  | 7139/8917 [9:38:25<1:53:16,  3.82s/it] 80%|████████  | 7140/8917 [9:38:28<1:51:47,  3.77s/it] 80%|████████  | 7141/8917 [9:38:32<1:51:52,  3.78s/it] 80%|████████  | 7142/8917 [9:38:36<1:52:52,  3.82s/it] 80%|████████  | 7143/8917 [9:38:40<1:51:06,  3.76s/it] 80%|████████  | 7144/8917 [9:38:43<1:50:15,  3.73s/it] 80%|████████  | 7145/8917 [9:38:47<1:49:57,  3.72s/it] 80%|████████  | 7146/8917 [9:38:51<1:50:45,  3.75s/it] 80%|████████  | 7147/8917 [9:38:54<1:50:06,  3.73s/it] 80%|████████  | 7148/8917 [9:38:58<1:50:36,  3.75s/it] 80%|████████  | 7149/8917 [9:39:02<1:52:48,  3.83s/it]09/19/2024 11:53:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02481742948293686, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1811864376068115, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2060039043426514}
 80%|████████  | 7150/8917 [9:39:06<1:51:09,  3.77s/it] 80%|████████  | 7151/8917 [9:39:10<1:51:02,  3.77s/it] 80%|████████  | 7152/8917 [9:39:13<1:49:40,  3.73s/it] 80%|████████  | 7153/8917 [9:39:17<1:52:07,  3.81s/it] 80%|████████  | 7154/8917 [9:39:21<1:53:57,  3.88s/it] 80%|████████  | 7155/8917 [9:39:25<1:52:57,  3.85s/it] 80%|████████  | 7156/8917 [9:39:29<1:50:40,  3.77s/it] 80%|████████  | 7157/8917 [9:39:32<1:50:06,  3.75s/it] 80%|████████  | 7158/8917 [9:39:36<1:49:36,  3.74s/it] 80%|████████  | 7159/8917 [9:39:40<1:51:41,  3.81s/it] 80%|████████  | 7160/8917 [9:39:44<1:50:25,  3.77s/it] 80%|████████  | 7161/8917 [9:39:48<1:52:27,  3.84s/it] 80%|████████  | 7162/8917 [9:39:51<1:50:19,  3.77s/it] 80%|████████  | 7163/8917 [9:39:55<1:52:46,  3.86s/it] 80%|████████  | 7164/8917 [9:39:59<1:53:53,  3.90s/it] 80%|████████  | 7165/8917 [9:40:03<1:50:59,  3.80s/it] 80%|████████  | 7166/8917 [9:40:07<1:52:41,  3.86s/it] 80%|████████  | 7167/8917 [9:40:11<1:51:55,  3.84s/it] 80%|████████  | 7168/8917 [9:40:15<1:50:23,  3.79s/it] 80%|████████  | 7169/8917 [9:40:18<1:50:43,  3.80s/it] 80%|████████  | 7170/8917 [9:40:22<1:51:11,  3.82s/it] 80%|████████  | 7171/8917 [9:40:26<1:52:01,  3.85s/it] 80%|████████  | 7172/8917 [9:40:30<1:50:38,  3.80s/it] 80%|████████  | 7173/8917 [9:40:34<1:50:23,  3.80s/it] 80%|████████  | 7174/8917 [9:40:37<1:49:57,  3.79s/it] 80%|████████  | 7175/8917 [9:40:41<1:48:17,  3.73s/it] 80%|████████  | 7176/8917 [9:40:45<1:52:04,  3.86s/it] 80%|████████  | 7177/8917 [9:40:49<1:49:53,  3.79s/it] 80%|████████  | 7178/8917 [9:40:52<1:49:09,  3.77s/it] 81%|████████  | 7179/8917 [9:40:56<1:49:15,  3.77s/it] 81%|████████  | 7180/8917 [9:41:00<1:48:25,  3.75s/it] 81%|████████  | 7181/8917 [9:41:04<1:47:55,  3.73s/it] 81%|████████  | 7182/8917 [9:41:08<1:50:33,  3.82s/it] 81%|████████  | 7183/8917 [9:41:11<1:49:48,  3.80s/it] 81%|████████  | 7184/8917 [9:41:15<1:50:47,  3.84s/it] 81%|████████  | 7185/8917 [9:41:19<1:52:11,  3.89s/it] 81%|████████  | 7186/8917 [9:41:24<1:54:31,  3.97s/it] 81%|████████  | 7187/8917 [9:41:27<1:54:08,  3.96s/it] 81%|████████  | 7188/8917 [9:41:31<1:52:46,  3.91s/it] 81%|████████  | 7189/8917 [9:41:35<1:53:00,  3.92s/it] 81%|████████  | 7190/8917 [9:41:39<1:51:23,  3.87s/it] 81%|████████  | 7191/8917 [9:41:42<1:46:46,  3.71s/it] 81%|████████  | 7192/8917 [9:41:46<1:50:28,  3.84s/it] 81%|████████  | 7193/8917 [9:41:50<1:50:21,  3.84s/it] 81%|████████  | 7194/8917 [9:41:54<1:48:44,  3.79s/it] 81%|████████  | 7195/8917 [9:41:57<1:45:34,  3.68s/it] 81%|████████  | 7196/8917 [9:42:01<1:44:26,  3.64s/it] 81%|████████  | 7197/8917 [9:42:05<1:49:33,  3.82s/it] 81%|████████  | 7198/8917 [9:42:09<1:49:42,  3.83s/it] 81%|████████  | 7199/8917 [9:42:13<1:48:02,  3.77s/it]09/19/2024 11:56:50 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.022231869399547577, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9892433285713196, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0114752054214478}
 81%|████████  | 7200/8917 [9:42:17<1:49:05,  3.81s/it] 81%|████████  | 7201/8917 [9:42:20<1:48:29,  3.79s/it] 81%|████████  | 7202/8917 [9:42:24<1:49:30,  3.83s/it] 81%|████████  | 7203/8917 [9:42:28<1:46:29,  3.73s/it] 81%|████████  | 7204/8917 [9:42:32<1:49:20,  3.83s/it] 81%|████████  | 7205/8917 [9:42:36<1:50:54,  3.89s/it] 81%|████████  | 7206/8917 [9:42:39<1:48:56,  3.82s/it] 81%|████████  | 7207/8917 [9:42:43<1:50:43,  3.89s/it] 81%|████████  | 7208/8917 [9:42:47<1:51:01,  3.90s/it] 81%|████████  | 7209/8917 [9:42:51<1:50:51,  3.89s/it] 81%|████████  | 7210/8917 [9:42:55<1:48:57,  3.83s/it] 81%|████████  | 7211/8917 [9:42:59<1:47:02,  3.76s/it] 81%|████████  | 7212/8917 [9:43:02<1:47:43,  3.79s/it] 81%|████████  | 7213/8917 [9:43:06<1:43:10,  3.63s/it] 81%|████████  | 7214/8917 [9:43:09<1:44:06,  3.67s/it] 81%|████████  | 7215/8917 [9:43:13<1:43:43,  3.66s/it] 81%|████████  | 7216/8917 [9:43:17<1:44:56,  3.70s/it] 81%|████████  | 7217/8917 [9:43:21<1:48:18,  3.82s/it] 81%|████████  | 7218/8917 [9:43:25<1:46:58,  3.78s/it] 81%|████████  | 7219/8917 [9:43:29<1:48:42,  3.84s/it] 81%|████████  | 7220/8917 [9:43:33<1:50:20,  3.90s/it] 81%|████████  | 7221/8917 [9:43:37<1:49:27,  3.87s/it] 81%|████████  | 7222/8917 [9:43:41<1:51:47,  3.96s/it] 81%|████████  | 7223/8917 [9:43:45<1:53:31,  4.02s/it] 81%|████████  | 7224/8917 [9:43:48<1:49:44,  3.89s/it] 81%|████████  | 7225/8917 [9:43:52<1:50:59,  3.94s/it] 81%|████████  | 7226/8917 [9:43:56<1:47:39,  3.82s/it] 81%|████████  | 7227/8917 [9:44:00<1:48:21,  3.85s/it] 81%|████████  | 7228/8917 [9:44:04<1:48:27,  3.85s/it] 81%|████████  | 7229/8917 [9:44:08<1:49:14,  3.88s/it] 81%|████████  | 7230/8917 [9:44:11<1:46:13,  3.78s/it] 81%|████████  | 7231/8917 [9:44:15<1:44:30,  3.72s/it] 81%|████████  | 7232/8917 [9:44:19<1:44:27,  3.72s/it] 81%|████████  | 7233/8917 [9:44:22<1:45:46,  3.77s/it] 81%|████████  | 7234/8917 [9:44:26<1:45:43,  3.77s/it] 81%|████████  | 7235/8917 [9:44:30<1:48:42,  3.88s/it] 81%|████████  | 7236/8917 [9:44:34<1:47:15,  3.83s/it] 81%|████████  | 7237/8917 [9:44:38<1:49:46,  3.92s/it] 81%|████████  | 7238/8917 [9:44:42<1:45:41,  3.78s/it] 81%|████████  | 7239/8917 [9:44:45<1:44:13,  3.73s/it] 81%|████████  | 7240/8917 [9:44:49<1:44:18,  3.73s/it] 81%|████████  | 7241/8917 [9:44:53<1:48:30,  3.88s/it] 81%|████████  | 7242/8917 [9:44:57<1:43:54,  3.72s/it] 81%|████████  | 7243/8917 [9:45:01<1:46:19,  3.81s/it] 81%|████████  | 7244/8917 [9:45:04<1:46:23,  3.82s/it] 81%|████████  | 7245/8917 [9:45:08<1:47:48,  3.87s/it] 81%|████████▏ | 7246/8917 [9:45:12<1:43:33,  3.72s/it] 81%|████████▏ | 7247/8917 [9:45:15<1:42:06,  3.67s/it] 81%|████████▏ | 7248/8917 [9:45:19<1:44:49,  3.77s/it] 81%|████████▏ | 7249/8917 [9:45:24<1:49:17,  3.93s/it]09/19/2024 12:00:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0207302775233984, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2515473365783691, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.272277593612671}
 81%|████████▏ | 7250/8917 [9:45:28<1:48:35,  3.91s/it] 81%|████████▏ | 7251/8917 [9:45:31<1:45:08,  3.79s/it] 81%|████████▏ | 7252/8917 [9:45:35<1:44:40,  3.77s/it] 81%|████████▏ | 7253/8917 [9:45:38<1:43:15,  3.72s/it] 81%|████████▏ | 7254/8917 [9:45:42<1:42:49,  3.71s/it] 81%|████████▏ | 7255/8917 [9:45:46<1:48:03,  3.90s/it] 81%|████████▏ | 7256/8917 [9:45:50<1:45:04,  3.80s/it] 81%|████████▏ | 7257/8917 [9:45:54<1:48:04,  3.91s/it] 81%|████████▏ | 7258/8917 [9:45:58<1:46:18,  3.85s/it] 81%|████████▏ | 7259/8917 [9:46:02<1:47:53,  3.90s/it] 81%|████████▏ | 7260/8917 [9:46:06<1:46:32,  3.86s/it] 81%|████████▏ | 7261/8917 [9:46:09<1:44:37,  3.79s/it] 81%|████████▏ | 7262/8917 [9:46:13<1:47:08,  3.88s/it] 81%|████████▏ | 7263/8917 [9:46:17<1:46:11,  3.85s/it] 81%|████████▏ | 7264/8917 [9:46:21<1:46:35,  3.87s/it] 81%|████████▏ | 7265/8917 [9:46:25<1:46:30,  3.87s/it] 81%|████████▏ | 7266/8917 [9:46:29<1:45:55,  3.85s/it] 81%|████████▏ | 7267/8917 [9:46:33<1:47:55,  3.92s/it] 82%|████████▏ | 7268/8917 [9:46:36<1:45:42,  3.85s/it] 82%|████████▏ | 7269/8917 [9:46:40<1:46:30,  3.88s/it] 82%|████████▏ | 7270/8917 [9:46:44<1:45:51,  3.86s/it] 82%|████████▏ | 7271/8917 [9:46:48<1:44:14,  3.80s/it] 82%|████████▏ | 7272/8917 [9:46:52<1:43:20,  3.77s/it] 82%|████████▏ | 7273/8917 [9:46:56<1:47:16,  3.92s/it] 82%|████████▏ | 7274/8917 [9:47:00<1:46:12,  3.88s/it] 82%|████████▏ | 7275/8917 [9:47:04<1:46:44,  3.90s/it] 82%|████████▏ | 7276/8917 [9:47:07<1:46:17,  3.89s/it] 82%|████████▏ | 7277/8917 [9:47:11<1:43:20,  3.78s/it] 82%|████████▏ | 7278/8917 [9:47:15<1:43:32,  3.79s/it] 82%|████████▏ | 7279/8917 [9:47:19<1:46:52,  3.91s/it] 82%|████████▏ | 7280/8917 [9:47:23<1:46:18,  3.90s/it] 82%|████████▏ | 7281/8917 [9:47:27<1:46:39,  3.91s/it] 82%|████████▏ | 7282/8917 [9:47:31<1:45:56,  3.89s/it] 82%|████████▏ | 7283/8917 [9:47:35<1:48:36,  3.99s/it] 82%|████████▏ | 7284/8917 [9:47:38<1:45:01,  3.86s/it] 82%|████████▏ | 7285/8917 [9:47:42<1:44:39,  3.85s/it] 82%|████████▏ | 7286/8917 [9:47:46<1:44:09,  3.83s/it] 82%|████████▏ | 7287/8917 [9:47:50<1:42:18,  3.77s/it] 82%|████████▏ | 7288/8917 [9:47:54<1:44:10,  3.84s/it] 82%|████████▏ | 7289/8917 [9:47:58<1:44:53,  3.87s/it] 82%|████████▏ | 7290/8917 [9:48:01<1:42:07,  3.77s/it] 82%|████████▏ | 7291/8917 [9:48:05<1:43:45,  3.83s/it] 82%|████████▏ | 7292/8917 [9:48:09<1:42:03,  3.77s/it] 82%|████████▏ | 7293/8917 [9:48:13<1:44:01,  3.84s/it] 82%|████████▏ | 7294/8917 [9:48:16<1:41:43,  3.76s/it] 82%|████████▏ | 7295/8917 [9:48:21<1:46:09,  3.93s/it] 82%|████████▏ | 7296/8917 [9:48:24<1:43:07,  3.82s/it] 82%|████████▏ | 7297/8917 [9:48:28<1:45:53,  3.92s/it] 82%|████████▏ | 7298/8917 [9:48:32<1:45:11,  3.90s/it] 82%|████████▏ | 7299/8917 [9:48:36<1:44:13,  3.86s/it]09/19/2024 12:03:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.019621968269348145, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1833480596542358, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.202970027923584}
 82%|████████▏ | 7300/8917 [9:48:40<1:42:24,  3.80s/it] 82%|████████▏ | 7301/8917 [9:48:43<1:41:39,  3.77s/it] 82%|████████▏ | 7302/8917 [9:48:47<1:43:27,  3.84s/it] 82%|████████▏ | 7303/8917 [9:48:51<1:41:12,  3.76s/it] 82%|████████▏ | 7304/8917 [9:48:55<1:39:56,  3.72s/it] 82%|████████▏ | 7305/8917 [9:48:59<1:42:09,  3.80s/it] 82%|████████▏ | 7306/8917 [9:49:02<1:41:01,  3.76s/it] 82%|████████▏ | 7307/8917 [9:49:07<1:46:37,  3.97s/it] 82%|████████▏ | 7308/8917 [9:49:10<1:44:59,  3.92s/it] 82%|████████▏ | 7309/8917 [9:49:15<1:46:48,  3.99s/it] 82%|████████▏ | 7310/8917 [9:49:18<1:43:11,  3.85s/it] 82%|████████▏ | 7311/8917 [9:49:22<1:45:20,  3.94s/it] 82%|████████▏ | 7312/8917 [9:49:26<1:42:50,  3.84s/it] 82%|████████▏ | 7313/8917 [9:49:30<1:44:11,  3.90s/it] 82%|████████▏ | 7314/8917 [9:49:33<1:41:33,  3.80s/it] 82%|████████▏ | 7315/8917 [9:49:38<1:43:59,  3.89s/it] 82%|████████▏ | 7316/8917 [9:49:41<1:43:34,  3.88s/it] 82%|████████▏ | 7317/8917 [9:49:45<1:43:21,  3.88s/it] 82%|████████▏ | 7318/8917 [9:49:49<1:43:37,  3.89s/it] 82%|████████▏ | 7319/8917 [9:49:53<1:45:55,  3.98s/it] 82%|████████▏ | 7320/8917 [9:49:57<1:45:28,  3.96s/it] 82%|████████▏ | 7321/8917 [9:50:01<1:42:21,  3.85s/it] 82%|████████▏ | 7322/8917 [9:50:04<1:40:08,  3.77s/it] 82%|████████▏ | 7323/8917 [9:50:08<1:41:30,  3.82s/it] 82%|████████▏ | 7324/8917 [9:50:12<1:39:34,  3.75s/it] 82%|████████▏ | 7325/8917 [9:50:16<1:39:52,  3.76s/it] 82%|████████▏ | 7326/8917 [9:50:20<1:42:18,  3.86s/it] 82%|████████▏ | 7327/8917 [9:50:24<1:41:56,  3.85s/it] 82%|████████▏ | 7328/8917 [9:50:28<1:42:02,  3.85s/it] 82%|████████▏ | 7329/8917 [9:50:31<1:40:48,  3.81s/it] 82%|████████▏ | 7330/8917 [9:50:35<1:42:16,  3.87s/it] 82%|████████▏ | 7331/8917 [9:50:39<1:40:28,  3.80s/it] 82%|████████▏ | 7332/8917 [9:50:43<1:39:18,  3.76s/it] 82%|████████▏ | 7333/8917 [9:50:46<1:39:14,  3.76s/it] 82%|████████▏ | 7334/8917 [9:50:51<1:44:07,  3.95s/it] 82%|████████▏ | 7335/8917 [9:50:54<1:41:52,  3.86s/it] 82%|████████▏ | 7336/8917 [9:50:58<1:41:33,  3.85s/it] 82%|████████▏ | 7337/8917 [9:51:02<1:41:51,  3.87s/it] 82%|████████▏ | 7338/8917 [9:51:06<1:44:25,  3.97s/it] 82%|████████▏ | 7339/8917 [9:51:10<1:40:44,  3.83s/it] 82%|████████▏ | 7340/8917 [9:51:13<1:37:21,  3.70s/it] 82%|████████▏ | 7341/8917 [9:51:18<1:41:57,  3.88s/it] 82%|████████▏ | 7342/8917 [9:51:21<1:41:44,  3.88s/it] 82%|████████▏ | 7343/8917 [9:51:25<1:41:10,  3.86s/it] 82%|████████▏ | 7344/8917 [9:51:29<1:40:55,  3.85s/it] 82%|████████▏ | 7345/8917 [9:51:33<1:41:42,  3.88s/it] 82%|████████▏ | 7346/8917 [9:51:37<1:40:21,  3.83s/it] 82%|████████▏ | 7347/8917 [9:51:40<1:39:31,  3.80s/it] 82%|████████▏ | 7348/8917 [9:51:44<1:39:05,  3.79s/it] 82%|████████▏ | 7349/8917 [9:51:48<1:39:40,  3.81s/it]09/19/2024 12:06:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02465931326150894, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2551460266113281, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.279805302619934}
 82%|████████▏ | 7350/8917 [9:51:52<1:39:38,  3.82s/it] 82%|████████▏ | 7351/8917 [9:51:56<1:38:30,  3.77s/it] 82%|████████▏ | 7352/8917 [9:52:00<1:40:07,  3.84s/it] 82%|████████▏ | 7353/8917 [9:52:03<1:38:04,  3.76s/it] 82%|████████▏ | 7354/8917 [9:52:07<1:37:43,  3.75s/it] 82%|████████▏ | 7355/8917 [9:52:11<1:37:35,  3.75s/it] 82%|████████▏ | 7356/8917 [9:52:15<1:39:17,  3.82s/it] 83%|████████▎ | 7357/8917 [9:52:19<1:40:02,  3.85s/it] 83%|████████▎ | 7358/8917 [9:52:22<1:38:21,  3.79s/it] 83%|████████▎ | 7359/8917 [9:52:26<1:37:42,  3.76s/it] 83%|████████▎ | 7360/8917 [9:52:30<1:39:54,  3.85s/it] 83%|████████▎ | 7361/8917 [9:52:34<1:38:56,  3.82s/it] 83%|████████▎ | 7362/8917 [9:52:37<1:37:28,  3.76s/it] 83%|████████▎ | 7363/8917 [9:52:41<1:37:40,  3.77s/it] 83%|████████▎ | 7364/8917 [9:52:45<1:36:31,  3.73s/it] 83%|████████▎ | 7365/8917 [9:52:49<1:37:51,  3.78s/it] 83%|████████▎ | 7366/8917 [9:52:53<1:40:10,  3.88s/it] 83%|████████▎ | 7367/8917 [9:52:56<1:37:52,  3.79s/it] 83%|████████▎ | 7368/8917 [9:53:00<1:37:46,  3.79s/it] 83%|████████▎ | 7369/8917 [9:53:04<1:37:45,  3.79s/it] 83%|████████▎ | 7370/8917 [9:53:08<1:39:22,  3.85s/it] 83%|████████▎ | 7371/8917 [9:53:12<1:39:13,  3.85s/it] 83%|████████▎ | 7372/8917 [9:53:16<1:38:30,  3.83s/it] 83%|████████▎ | 7373/8917 [9:53:19<1:39:11,  3.85s/it] 83%|████████▎ | 7374/8917 [9:53:23<1:37:58,  3.81s/it] 83%|████████▎ | 7375/8917 [9:53:27<1:37:11,  3.78s/it] 83%|████████▎ | 7376/8917 [9:53:31<1:37:35,  3.80s/it] 83%|████████▎ | 7377/8917 [9:53:35<1:40:18,  3.91s/it] 83%|████████▎ | 7378/8917 [9:53:39<1:39:50,  3.89s/it] 83%|████████▎ | 7379/8917 [9:53:42<1:38:39,  3.85s/it] 83%|████████▎ | 7380/8917 [9:53:46<1:35:11,  3.72s/it] 83%|████████▎ | 7381/8917 [9:53:50<1:35:26,  3.73s/it] 83%|████████▎ | 7382/8917 [9:53:54<1:37:55,  3.83s/it] 83%|████████▎ | 7383/8917 [9:53:57<1:37:26,  3.81s/it] 83%|████████▎ | 7384/8917 [9:54:01<1:38:04,  3.84s/it] 83%|████████▎ | 7385/8917 [9:54:05<1:38:06,  3.84s/it] 83%|████████▎ | 7386/8917 [9:54:09<1:38:06,  3.85s/it] 83%|████████▎ | 7387/8917 [9:54:13<1:37:21,  3.82s/it] 83%|████████▎ | 7388/8917 [9:54:17<1:39:21,  3.90s/it] 83%|████████▎ | 7389/8917 [9:54:20<1:36:35,  3.79s/it] 83%|████████▎ | 7390/8917 [9:54:24<1:36:11,  3.78s/it] 83%|████████▎ | 7391/8917 [9:54:28<1:35:00,  3.74s/it] 83%|████████▎ | 7392/8917 [9:54:31<1:33:50,  3.69s/it] 83%|████████▎ | 7393/8917 [9:54:35<1:34:56,  3.74s/it] 83%|████████▎ | 7394/8917 [9:54:39<1:35:18,  3.75s/it] 83%|████████▎ | 7395/8917 [9:54:43<1:37:33,  3.85s/it] 83%|████████▎ | 7396/8917 [9:54:47<1:36:17,  3.80s/it] 83%|████████▎ | 7397/8917 [9:54:50<1:33:44,  3.70s/it] 83%|████████▎ | 7398/8917 [9:54:54<1:34:24,  3.73s/it] 83%|████████▎ | 7399/8917 [9:54:58<1:36:03,  3.80s/it]09/19/2024 12:09:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018603181466460228, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0958962440490723, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1144994497299194}
 83%|████████▎ | 7400/8917 [9:55:02<1:36:52,  3.83s/it] 83%|████████▎ | 7401/8917 [9:55:06<1:35:02,  3.76s/it] 83%|████████▎ | 7402/8917 [9:55:09<1:33:56,  3.72s/it] 83%|████████▎ | 7403/8917 [9:55:13<1:34:08,  3.73s/it] 83%|████████▎ | 7404/8917 [9:55:17<1:35:54,  3.80s/it] 83%|████████▎ | 7405/8917 [9:55:21<1:36:23,  3.83s/it] 83%|████████▎ | 7406/8917 [9:55:25<1:37:06,  3.86s/it] 83%|████████▎ | 7407/8917 [9:55:29<1:38:56,  3.93s/it] 83%|████████▎ | 7408/8917 [9:55:32<1:36:09,  3.82s/it] 83%|████████▎ | 7409/8917 [9:55:36<1:34:00,  3.74s/it] 83%|████████▎ | 7410/8917 [9:55:40<1:36:06,  3.83s/it] 83%|████████▎ | 7411/8917 [9:55:44<1:33:52,  3.74s/it] 83%|████████▎ | 7412/8917 [9:55:48<1:36:50,  3.86s/it] 83%|████████▎ | 7413/8917 [9:55:51<1:36:14,  3.84s/it] 83%|████████▎ | 7414/8917 [9:55:55<1:36:44,  3.86s/it] 83%|████████▎ | 7415/8917 [9:55:59<1:32:33,  3.70s/it] 83%|████████▎ | 7416/8917 [9:56:02<1:26:21,  3.45s/it] 83%|████████▎ | 7417/8917 [9:56:04<1:21:59,  3.28s/it] 83%|████████▎ | 7418/8917 [9:56:07<1:18:55,  3.16s/it] 83%|████████▎ | 7419/8917 [9:56:10<1:16:44,  3.07s/it] 83%|████████▎ | 7420/8917 [9:56:13<1:15:08,  3.01s/it] 83%|████████▎ | 7421/8917 [9:56:16<1:13:58,  2.97s/it] 83%|████████▎ | 7422/8917 [9:56:19<1:13:16,  2.94s/it] 83%|████████▎ | 7423/8917 [9:56:22<1:12:41,  2.92s/it] 83%|████████▎ | 7424/8917 [9:56:25<1:12:19,  2.91s/it] 83%|████████▎ | 7425/8917 [9:56:27<1:12:03,  2.90s/it] 83%|████████▎ | 7426/8917 [9:56:30<1:11:45,  2.89s/it] 83%|████████▎ | 7427/8917 [9:56:33<1:11:38,  2.88s/it] 83%|████████▎ | 7428/8917 [9:56:36<1:11:29,  2.88s/it] 83%|████████▎ | 7429/8917 [9:56:39<1:11:25,  2.88s/it] 83%|████████▎ | 7430/8917 [9:56:42<1:11:18,  2.88s/it]/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
 83%|████████▎ | 7431/8917 [9:57:30<6:50:03, 16.56s/it] 83%|████████▎ | 7432/8917 [9:57:37<5:36:45, 13.61s/it] 83%|████████▎ | 7433/8917 [9:57:45<4:52:29, 11.83s/it] 83%|████████▎ | 7434/8917 [9:57:49<3:58:41,  9.66s/it] 83%|████████▎ | 7435/8917 [9:57:55<3:28:52,  8.46s/it] 83%|████████▎ | 7436/8917 [9:58:01<3:09:25,  7.67s/it] 83%|████████▎ | 7437/8917 [9:58:05<2:47:12,  6.78s/it] 83%|████████▎ | 7438/8917 [9:58:09<2:24:42,  5.87s/it] 83%|████████▎ | 7439/8917 [9:58:13<2:10:50,  5.31s/it] 83%|████████▎ | 7440/8917 [9:58:17<2:00:01,  4.88s/it] 83%|████████▎ | 7441/8917 [9:58:21<1:52:59,  4.59s/it] 83%|████████▎ | 7442/8917 [9:58:24<1:44:48,  4.26s/it] 83%|████████▎ | 7443/8917 [9:58:28<1:42:23,  4.17s/it] 83%|████████▎ | 7444/8917 [9:58:32<1:39:11,  4.04s/it] 83%|████████▎ | 7445/8917 [9:58:36<1:36:39,  3.94s/it] 84%|████████▎ | 7446/8917 [9:58:40<1:37:45,  3.99s/it] 84%|████████▎ | 7447/8917 [9:58:44<1:39:16,  4.05s/it] 84%|████████▎ | 7448/8917 [9:58:49<1:41:32,  4.15s/it] 84%|████████▎ | 7449/8917 [9:58:53<1:40:42,  4.12s/it]09/19/2024 12:13:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.032971542328596115, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3365854024887085, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3695569038391113}
 84%|████████▎ | 7450/8917 [9:58:56<1:35:53,  3.92s/it] 84%|████████▎ | 7451/8917 [9:59:00<1:34:55,  3.89s/it] 84%|████████▎ | 7452/8917 [9:59:03<1:33:08,  3.81s/it] 84%|████████▎ | 7453/8917 [9:59:07<1:32:37,  3.80s/it] 84%|████████▎ | 7454/8917 [9:59:11<1:32:36,  3.80s/it] 84%|████████▎ | 7455/8917 [9:59:15<1:34:27,  3.88s/it] 84%|████████▎ | 7456/8917 [9:59:19<1:34:03,  3.86s/it] 84%|████████▎ | 7457/8917 [9:59:23<1:34:45,  3.89s/it] 84%|████████▎ | 7458/8917 [9:59:27<1:34:41,  3.89s/it] 84%|████████▎ | 7459/8917 [9:59:30<1:33:07,  3.83s/it] 84%|████████▎ | 7460/8917 [9:59:34<1:32:43,  3.82s/it] 84%|████████▎ | 7461/8917 [9:59:38<1:34:26,  3.89s/it] 84%|████████▎ | 7462/8917 [9:59:42<1:32:13,  3.80s/it] 84%|████████▎ | 7463/8917 [9:59:46<1:33:10,  3.84s/it] 84%|████████▎ | 7464/8917 [9:59:50<1:31:36,  3.78s/it] 84%|████████▎ | 7465/8917 [9:59:53<1:31:28,  3.78s/it] 84%|████████▎ | 7466/8917 [9:59:57<1:34:13,  3.90s/it] 84%|████████▎ | 7467/8917 [10:00:01<1:31:52,  3.80s/it] 84%|████████▍ | 7468/8917 [10:00:05<1:31:46,  3.80s/it] 84%|████████▍ | 7469/8917 [10:00:08<1:29:54,  3.73s/it] 84%|████████▍ | 7470/8917 [10:00:12<1:30:38,  3.76s/it] 84%|████████▍ | 7471/8917 [10:00:16<1:30:31,  3.76s/it] 84%|████████▍ | 7472/8917 [10:00:20<1:30:41,  3.77s/it] 84%|████████▍ | 7473/8917 [10:00:24<1:32:00,  3.82s/it] 84%|████████▍ | 7474/8917 [10:00:28<1:31:54,  3.82s/it] 84%|████████▍ | 7475/8917 [10:00:32<1:33:07,  3.88s/it] 84%|████████▍ | 7476/8917 [10:00:36<1:35:28,  3.98s/it] 84%|████████▍ | 7477/8917 [10:00:39<1:32:33,  3.86s/it] 84%|████████▍ | 7478/8917 [10:00:43<1:31:32,  3.82s/it] 84%|████████▍ | 7479/8917 [10:00:48<1:36:59,  4.05s/it] 84%|████████▍ | 7480/8917 [10:00:52<1:35:57,  4.01s/it] 84%|████████▍ | 7481/8917 [10:00:55<1:34:40,  3.96s/it] 84%|████████▍ | 7482/8917 [10:00:59<1:33:14,  3.90s/it] 84%|████████▍ | 7483/8917 [10:01:03<1:30:31,  3.79s/it] 84%|████████▍ | 7484/8917 [10:01:06<1:29:16,  3.74s/it] 84%|████████▍ | 7485/8917 [10:01:10<1:29:56,  3.77s/it] 84%|████████▍ | 7486/8917 [10:01:14<1:30:59,  3.82s/it] 84%|████████▍ | 7487/8917 [10:01:18<1:31:44,  3.85s/it] 84%|████████▍ | 7488/8917 [10:01:22<1:31:35,  3.85s/it] 84%|████████▍ | 7489/8917 [10:01:25<1:30:01,  3.78s/it] 84%|████████▍ | 7490/8917 [10:01:29<1:30:50,  3.82s/it] 84%|████████▍ | 7491/8917 [10:01:33<1:31:28,  3.85s/it] 84%|████████▍ | 7492/8917 [10:01:37<1:32:40,  3.90s/it] 84%|████████▍ | 7493/8917 [10:01:41<1:32:09,  3.88s/it] 84%|████████▍ | 7494/8917 [10:01:45<1:32:12,  3.89s/it] 84%|████████▍ | 7495/8917 [10:01:49<1:31:03,  3.84s/it] 84%|████████▍ | 7496/8917 [10:01:53<1:32:22,  3.90s/it] 84%|████████▍ | 7497/8917 [10:01:57<1:31:07,  3.85s/it] 84%|████████▍ | 7498/8917 [10:02:00<1:31:35,  3.87s/it] 84%|████████▍ | 7499/8917 [10:02:04<1:29:59,  3.81s/it]09/19/2024 12:16:41 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 12:16:41 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<04:00,  1.09s/it][A
  1%|          | 2/221 [00:01<02:30,  1.46it/s][A
  1%|▏         | 3/221 [00:01<02:08,  1.70it/s][A
  2%|▏         | 4/221 [00:02<01:38,  2.21it/s][A
  3%|▎         | 6/221 [00:02<00:58,  3.65it/s][A
  3%|▎         | 7/221 [00:02<00:54,  3.96it/s][A
  4%|▎         | 8/221 [00:03<01:08,  3.10it/s][A
  4%|▍         | 9/221 [00:03<01:11,  2.96it/s][A
  5%|▍         | 10/221 [00:03<01:15,  2.79it/s][A
  5%|▌         | 12/221 [00:09<05:17,  1.52s/it][A
  6%|▌         | 13/221 [00:10<04:11,  1.21s/it][A
  7%|▋         | 15/221 [00:10<02:50,  1.21it/s][A
  7%|▋         | 16/221 [00:11<02:39,  1.29it/s][A
  8%|▊         | 17/221 [00:12<02:41,  1.26it/s][A
  8%|▊         | 18/221 [00:12<02:17,  1.48it/s][A
  9%|▊         | 19/221 [00:13<02:56,  1.14it/s][A
 10%|▉         | 21/221 [00:14<01:57,  1.70it/s][A
 10%|▉         | 22/221 [00:14<02:03,  1.61it/s][A
 11%|█         | 24/221 [00:15<01:25,  2.31it/s][A
 11%|█▏        | 25/221 [00:15<01:22,  2.38it/s][A
 12%|█▏        | 26/221 [00:15<01:18,  2.50it/s][A
 13%|█▎        | 28/221 [00:16<01:09,  2.78it/s][A
 13%|█▎        | 29/221 [00:16<01:03,  3.01it/s][A
 14%|█▎        | 30/221 [00:17<01:16,  2.49it/s][A
 14%|█▍        | 31/221 [00:17<01:18,  2.41it/s][A
 14%|█▍        | 32/221 [00:17<01:02,  3.02it/s][A
 15%|█▍        | 33/221 [00:18<01:00,  3.13it/s][A
 16%|█▌        | 35/221 [00:18<00:47,  3.93it/s][A
 16%|█▋        | 36/221 [00:18<00:43,  4.29it/s][A
 17%|█▋        | 37/221 [00:19<00:52,  3.51it/s][A
 17%|█▋        | 38/221 [00:19<00:59,  3.07it/s][A
 18%|█▊        | 39/221 [00:20<01:08,  2.65it/s][A
 18%|█▊        | 40/221 [00:20<01:18,  2.31it/s][A
 19%|█▊        | 41/221 [00:20<01:07,  2.65it/s][A
 19%|█▉        | 42/221 [00:21<00:53,  3.33it/s][A
 19%|█▉        | 43/221 [00:21<00:43,  4.09it/s][A
 20%|█▉        | 44/221 [00:21<00:36,  4.86it/s][A
 20%|██        | 45/221 [00:24<02:55,  1.00it/s][A
 21%|██        | 46/221 [00:24<02:21,  1.23it/s][A
 21%|██▏       | 47/221 [00:24<02:01,  1.43it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:21,  2.10it/s][A
 23%|██▎       | 50/221 [00:25<01:20,  2.11it/s][A
 23%|██▎       | 51/221 [00:26<01:06,  2.55it/s][A
 24%|██▎       | 52/221 [00:26<00:58,  2.88it/s][A
 24%|██▍       | 53/221 [00:26<01:00,  2.77it/s][A
 24%|██▍       | 54/221 [00:27<01:11,  2.33it/s][A
 25%|██▍       | 55/221 [00:28<01:59,  1.39it/s][A
 25%|██▌       | 56/221 [00:28<01:32,  1.78it/s][A
 26%|██▌       | 57/221 [00:29<01:14,  2.20it/s][A
 27%|██▋       | 59/221 [00:29<00:48,  3.37it/s][A
 27%|██▋       | 60/221 [00:29<00:49,  3.26it/s][A
 28%|██▊       | 61/221 [00:29<00:45,  3.54it/s][A
 28%|██▊       | 62/221 [00:30<00:46,  3.42it/s][A
 29%|██▊       | 63/221 [00:30<00:41,  3.76it/s][A
 29%|██▉       | 64/221 [00:31<01:11,  2.19it/s][A
 29%|██▉       | 65/221 [00:31<00:58,  2.67it/s][A
 30%|██▉       | 66/221 [00:32<01:05,  2.38it/s][A
 30%|███       | 67/221 [00:32<00:59,  2.58it/s][A
 31%|███       | 68/221 [00:32<00:50,  3.02it/s][A
 31%|███       | 69/221 [00:34<01:53,  1.34it/s][A
 32%|███▏      | 70/221 [00:34<01:24,  1.78it/s][A
 32%|███▏      | 71/221 [00:34<01:10,  2.12it/s][A
 33%|███▎      | 72/221 [00:35<01:10,  2.12it/s][A
 33%|███▎      | 73/221 [00:35<01:13,  2.02it/s][A
 33%|███▎      | 74/221 [00:35<00:56,  2.59it/s][A
 34%|███▍      | 75/221 [00:36<00:54,  2.67it/s][A
 34%|███▍      | 76/221 [00:36<00:53,  2.69it/s][A
 35%|███▍      | 77/221 [00:38<01:41,  1.42it/s][A
 35%|███▌      | 78/221 [00:38<01:17,  1.84it/s][A
 36%|███▌      | 79/221 [00:39<01:35,  1.49it/s][A
 37%|███▋      | 81/221 [00:39<01:05,  2.15it/s][A
 37%|███▋      | 82/221 [00:42<02:39,  1.15s/it][A
 38%|███▊      | 83/221 [00:43<02:17,  1.00it/s][A
 38%|███▊      | 84/221 [00:43<01:52,  1.22it/s][A
 39%|███▉      | 86/221 [00:44<01:15,  1.79it/s][A
 39%|███▉      | 87/221 [00:44<01:09,  1.92it/s][A
 40%|███▉      | 88/221 [00:44<00:57,  2.31it/s][A
 40%|████      | 89/221 [00:45<00:52,  2.50it/s][A
 41%|████      | 90/221 [00:45<00:48,  2.68it/s][A
 41%|████      | 91/221 [00:45<00:41,  3.11it/s][A
 42%|████▏     | 92/221 [00:45<00:38,  3.36it/s][A
 42%|████▏     | 93/221 [00:46<00:40,  3.13it/s][A
 43%|████▎     | 94/221 [00:46<00:50,  2.52it/s][A
 43%|████▎     | 95/221 [00:47<00:48,  2.62it/s][A
 43%|████▎     | 96/221 [00:47<00:59,  2.11it/s][A
 44%|████▍     | 97/221 [00:48<00:47,  2.62it/s][A
 44%|████▍     | 98/221 [00:48<00:54,  2.27it/s][A
 45%|████▍     | 99/221 [00:49<00:52,  2.30it/s][A
 45%|████▌     | 100/221 [00:49<01:12,  1.67it/s][A
 46%|████▌     | 101/221 [00:50<00:58,  2.06it/s][A
 46%|████▌     | 102/221 [00:50<01:07,  1.76it/s][A
 47%|████▋     | 103/221 [00:51<00:50,  2.33it/s][A
 47%|████▋     | 104/221 [00:51<00:47,  2.46it/s][A
 48%|████▊     | 105/221 [00:51<00:48,  2.37it/s][A
 48%|████▊     | 106/221 [00:53<01:34,  1.21it/s][A
 48%|████▊     | 107/221 [00:54<01:21,  1.40it/s][A
 49%|████▉     | 108/221 [00:54<01:17,  1.46it/s][A
 49%|████▉     | 109/221 [00:55<01:07,  1.67it/s][A
 50%|████▉     | 110/221 [00:55<00:54,  2.03it/s][A
 50%|█████     | 111/221 [00:55<00:50,  2.19it/s][A
 51%|█████     | 112/221 [00:56<00:46,  2.33it/s][A
 51%|█████     | 113/221 [00:56<00:38,  2.80it/s][A
 52%|█████▏    | 115/221 [00:57<00:38,  2.73it/s][A
 52%|█████▏    | 116/221 [00:57<00:38,  2.73it/s][A
 53%|█████▎    | 117/221 [00:57<00:39,  2.66it/s][A
 53%|█████▎    | 118/221 [00:58<00:39,  2.63it/s][A
 54%|█████▍    | 119/221 [00:58<00:33,  3.02it/s][A
 54%|█████▍    | 120/221 [00:58<00:28,  3.56it/s][A
 55%|█████▍    | 121/221 [00:59<00:37,  2.66it/s][A
 55%|█████▌    | 122/221 [00:59<00:42,  2.34it/s][A
 56%|█████▌    | 123/221 [01:01<01:15,  1.30it/s][A
 56%|█████▌    | 124/221 [01:01<00:58,  1.66it/s][A
 57%|█████▋    | 125/221 [01:02<01:05,  1.46it/s][A
 57%|█████▋    | 126/221 [01:11<04:58,  3.14s/it][A
 57%|█████▋    | 127/221 [01:11<03:39,  2.33s/it][A
 58%|█████▊    | 128/221 [01:12<02:42,  1.75s/it][A
 58%|█████▊    | 129/221 [01:12<02:08,  1.40s/it][A
 59%|█████▉    | 130/221 [01:12<01:36,  1.06s/it][A
 59%|█████▉    | 131/221 [01:14<01:41,  1.13s/it][A
 60%|█████▉    | 132/221 [01:15<01:51,  1.25s/it][A
 60%|██████    | 133/221 [01:16<01:30,  1.03s/it][A
 61%|██████    | 134/221 [01:17<01:26,  1.00it/s][A
 61%|██████    | 135/221 [01:17<01:17,  1.10it/s][A
 62%|██████▏   | 136/221 [01:18<01:03,  1.34it/s][A
 62%|██████▏   | 137/221 [01:18<00:52,  1.58it/s][A
 62%|██████▏   | 138/221 [01:19<00:46,  1.79it/s][A
 63%|██████▎   | 139/221 [01:19<00:36,  2.27it/s][A
 63%|██████▎   | 140/221 [01:19<00:37,  2.16it/s][A
 64%|██████▍   | 141/221 [01:20<00:36,  2.21it/s][A
 64%|██████▍   | 142/221 [01:20<00:32,  2.45it/s][A
 65%|██████▍   | 143/221 [01:20<00:29,  2.64it/s][A
 65%|██████▌   | 144/221 [01:20<00:24,  3.20it/s][A
 66%|██████▌   | 145/221 [01:21<00:19,  3.96it/s][A
 66%|██████▌   | 146/221 [01:21<00:15,  4.81it/s][A
 67%|██████▋   | 148/221 [01:23<00:53,  1.37it/s][A
 67%|██████▋   | 149/221 [01:24<00:46,  1.56it/s][A
 68%|██████▊   | 150/221 [01:24<00:41,  1.70it/s][A
 68%|██████▊   | 151/221 [01:24<00:36,  1.94it/s][A
 69%|██████▉   | 152/221 [01:25<00:31,  2.20it/s][A
 69%|██████▉   | 153/221 [01:25<00:24,  2.79it/s][A
 70%|██████▉   | 154/221 [01:25<00:21,  3.06it/s][A
 70%|███████   | 155/221 [01:25<00:18,  3.57it/s][A
 71%|███████   | 156/221 [01:25<00:16,  4.05it/s][A
 71%|███████   | 157/221 [01:31<01:56,  1.82s/it][A
 71%|███████▏  | 158/221 [01:32<01:38,  1.56s/it][A
 72%|███████▏  | 159/221 [01:32<01:10,  1.13s/it][A
 72%|███████▏  | 160/221 [01:32<00:51,  1.19it/s][A
 73%|███████▎  | 161/221 [01:32<00:37,  1.59it/s][A
 74%|███████▍  | 163/221 [01:33<00:23,  2.44it/s][A
 74%|███████▍  | 164/221 [01:33<00:19,  2.89it/s][A
 75%|███████▍  | 165/221 [01:33<00:18,  3.07it/s][A
 75%|███████▌  | 166/221 [01:34<00:25,  2.15it/s][A
 76%|███████▌  | 167/221 [01:34<00:24,  2.23it/s][A
 76%|███████▌  | 168/221 [01:36<00:47,  1.11it/s][A
 76%|███████▋  | 169/221 [01:37<00:41,  1.25it/s][A
 77%|███████▋  | 170/221 [01:37<00:35,  1.45it/s][A
 77%|███████▋  | 171/221 [01:38<00:29,  1.68it/s][A
 78%|███████▊  | 172/221 [01:38<00:24,  2.02it/s][A
 78%|███████▊  | 173/221 [01:38<00:22,  2.11it/s][A
 79%|███████▊  | 174/221 [01:39<00:17,  2.65it/s][A
 79%|███████▉  | 175/221 [01:39<00:16,  2.78it/s][A
 80%|███████▉  | 176/221 [01:39<00:16,  2.75it/s][A
 80%|████████  | 177/221 [01:40<00:15,  2.89it/s][A
 81%|████████  | 178/221 [01:40<00:14,  2.91it/s][A
 81%|████████  | 179/221 [01:40<00:13,  3.02it/s][A
 81%|████████▏ | 180/221 [01:40<00:12,  3.41it/s][A
 82%|████████▏ | 181/221 [01:41<00:10,  3.91it/s][A
 82%|████████▏ | 182/221 [01:41<00:12,  3.06it/s][A
 83%|████████▎ | 183/221 [01:42<00:16,  2.34it/s][A
 83%|████████▎ | 184/221 [01:42<00:15,  2.43it/s][A
 84%|████████▎ | 185/221 [01:42<00:14,  2.55it/s][A
 84%|████████▍ | 186/221 [01:43<00:12,  2.84it/s][A
 85%|████████▍ | 187/221 [01:43<00:11,  3.00it/s][A
 85%|████████▌ | 188/221 [01:43<00:09,  3.39it/s][A
 86%|████████▌ | 189/221 [01:43<00:09,  3.55it/s][A
 86%|████████▌ | 190/221 [01:44<00:09,  3.26it/s][A
 86%|████████▋ | 191/221 [01:44<00:08,  3.59it/s][A
 87%|████████▋ | 192/221 [01:45<00:14,  2.00it/s][A
 87%|████████▋ | 193/221 [01:45<00:11,  2.43it/s][A
 88%|████████▊ | 194/221 [01:46<00:11,  2.45it/s][A
 88%|████████▊ | 195/221 [01:46<00:08,  2.92it/s][A
 89%|████████▊ | 196/221 [01:46<00:09,  2.51it/s][A
 89%|████████▉ | 197/221 [01:47<00:08,  2.67it/s][A
 90%|████████▉ | 198/221 [01:47<00:08,  2.86it/s][A
 90%|█████████ | 199/221 [01:47<00:06,  3.32it/s][A
 90%|█████████ | 200/221 [01:48<00:09,  2.16it/s][A
 91%|█████████ | 201/221 [01:49<00:12,  1.62it/s][A
 91%|█████████▏| 202/221 [01:49<00:09,  1.90it/s][A
 92%|█████████▏| 203/221 [01:50<00:11,  1.58it/s][A
 92%|█████████▏| 204/221 [01:51<00:09,  1.72it/s][A
 93%|█████████▎| 206/221 [01:51<00:06,  2.39it/s][A
 94%|█████████▎| 207/221 [01:51<00:04,  2.84it/s][A
 94%|█████████▍| 208/221 [01:52<00:05,  2.44it/s][A
 95%|█████████▌| 210/221 [01:52<00:03,  3.39it/s][A
 95%|█████████▌| 211/221 [01:53<00:03,  3.05it/s][A
 96%|█████████▌| 212/221 [01:53<00:02,  3.01it/s][A
 96%|█████████▋| 213/221 [01:53<00:02,  3.46it/s][A
 97%|█████████▋| 214/221 [01:53<00:02,  3.11it/s][A
 97%|█████████▋| 215/221 [01:54<00:02,  2.53it/s][A
 98%|█████████▊| 216/221 [01:54<00:01,  2.68it/s][A
 98%|█████████▊| 217/221 [01:56<00:02,  1.64it/s][A
 99%|█████████▊| 218/221 [01:56<00:01,  1.86it/s][A
 99%|█████████▉| 219/221 [01:56<00:00,  2.08it/s][A
100%|█████████▉| 220/221 [02:01<00:01,  1.69s/it][A
100%|██████████| 221/221 [02:01<00:00,  1.24s/it][A100%|██████████| 221/221 [02:01<00:00,  1.82it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:56,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:51,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:50,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:41,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:40,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:18<01:39,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:33,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:32,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:31,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:27<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:22,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:36<01:21,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:37<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:45<01:12,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:46<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:05,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:04,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:54<01:03,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:56<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:03<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:04<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:05<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:37,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:21<00:36,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:23<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:24<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:28,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:30<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:31<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:32<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:33<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:39<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:40<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:51<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:52<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:57<00:00,  1.89it/s][A100%|██████████| 221/221 [01:57<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<00:56,  3.88it/s][A
  1%|          | 2/221 [00:01<02:29,  1.47it/s][A
  1%|▏         | 3/221 [00:01<01:39,  2.19it/s][A
  2%|▏         | 4/221 [00:01<01:39,  2.17it/s][A
  2%|▏         | 5/221 [00:02<01:50,  1.96it/s][A
  3%|▎         | 6/221 [00:02<01:39,  2.17it/s][A
  3%|▎         | 7/221 [00:03<01:35,  2.24it/s][A
  4%|▎         | 8/221 [00:04<02:56,  1.20it/s][A
  4%|▍         | 9/221 [00:05<03:00,  1.18it/s][A
  5%|▍         | 10/221 [00:06<02:20,  1.50it/s][A
  5%|▍         | 11/221 [00:06<02:02,  1.71it/s][A
  5%|▌         | 12/221 [00:06<01:48,  1.93it/s][A
  6%|▌         | 13/221 [00:07<01:26,  2.41it/s][A
  6%|▋         | 14/221 [00:07<01:08,  3.04it/s][A
  7%|▋         | 15/221 [00:07<01:02,  3.28it/s][A
  7%|▋         | 16/221 [00:08<01:38,  2.07it/s][A
  8%|▊         | 17/221 [00:09<01:58,  1.72it/s][A
  8%|▊         | 18/221 [00:09<01:35,  2.14it/s][A
  9%|▊         | 19/221 [00:10<01:49,  1.85it/s][A
  9%|▉         | 20/221 [00:10<01:40,  1.99it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.90it/s][A
 10%|▉         | 22/221 [00:12<02:28,  1.34it/s][A
 10%|█         | 23/221 [00:12<01:58,  1.67it/s][A
 11%|█         | 24/221 [00:12<01:47,  1.83it/s][A
 11%|█▏        | 25/221 [00:13<01:59,  1.64it/s][A
 12%|█▏        | 26/221 [00:14<02:11,  1.48it/s][A
 12%|█▏        | 27/221 [00:14<01:41,  1.91it/s][A
 13%|█▎        | 28/221 [00:16<02:29,  1.29it/s][A
 13%|█▎        | 29/221 [00:16<02:00,  1.59it/s][A
 14%|█▎        | 30/221 [00:17<02:01,  1.57it/s][A
 14%|█▍        | 31/221 [00:17<01:53,  1.68it/s][A
 14%|█▍        | 32/221 [00:18<02:20,  1.35it/s][A
 15%|█▍        | 33/221 [00:19<02:13,  1.41it/s][A
 15%|█▌        | 34/221 [00:19<01:43,  1.80it/s][A
 16%|█▌        | 35/221 [00:19<01:28,  2.10it/s][A
 16%|█▋        | 36/221 [00:20<01:34,  1.96it/s][A
 17%|█▋        | 37/221 [00:20<01:28,  2.08it/s][A
 17%|█▋        | 38/221 [00:21<01:58,  1.54it/s][A
 18%|█▊        | 39/221 [00:22<02:01,  1.49it/s][A
 18%|█▊        | 40/221 [00:23<02:30,  1.20it/s][A
 19%|█▊        | 41/221 [00:23<01:57,  1.53it/s][A
 19%|█▉        | 42/221 [00:24<01:36,  1.86it/s][A
 19%|█▉        | 43/221 [00:24<01:21,  2.19it/s][A
 20%|█▉        | 44/221 [00:25<01:26,  2.04it/s][A
 20%|██        | 45/221 [00:25<01:05,  2.67it/s][A
 21%|██        | 46/221 [00:25<01:01,  2.87it/s][A
 21%|██▏       | 47/221 [00:25<01:12,  2.39it/s][A
 22%|██▏       | 48/221 [00:26<00:59,  2.89it/s][A
 22%|██▏       | 49/221 [00:26<01:24,  2.04it/s][A
 23%|██▎       | 50/221 [00:27<01:33,  1.84it/s][A
 23%|██▎       | 51/221 [00:27<01:20,  2.11it/s][A
 24%|██▎       | 52/221 [00:28<01:11,  2.37it/s][A
 24%|██▍       | 53/221 [00:28<01:11,  2.36it/s][A
 24%|██▍       | 54/221 [00:29<01:14,  2.25it/s][A
 25%|██▍       | 55/221 [00:29<01:04,  2.59it/s][A
 25%|██▌       | 56/221 [00:29<01:06,  2.49it/s][A
 26%|██▌       | 57/221 [00:30<00:58,  2.79it/s][A
 26%|██▌       | 58/221 [00:30<00:53,  3.04it/s][A
 27%|██▋       | 60/221 [00:31<00:53,  3.03it/s][A
 28%|██▊       | 61/221 [00:31<01:07,  2.38it/s][A
 28%|██▊       | 62/221 [00:32<01:06,  2.39it/s][A
 29%|██▊       | 63/221 [00:33<01:25,  1.85it/s][A
 29%|██▉       | 64/221 [00:34<02:04,  1.26it/s][A
 29%|██▉       | 65/221 [00:35<02:17,  1.13it/s][A
 30%|██▉       | 66/221 [00:36<02:00,  1.29it/s][A
 30%|███       | 67/221 [00:36<01:40,  1.53it/s][A
 31%|███       | 68/221 [00:36<01:23,  1.83it/s][A
 31%|███       | 69/221 [00:37<01:10,  2.15it/s][A
 32%|███▏      | 70/221 [00:37<01:14,  2.04it/s][A
 32%|███▏      | 71/221 [00:38<01:22,  1.83it/s][A
 33%|███▎      | 72/221 [00:38<01:22,  1.80it/s][A
 33%|███▎      | 73/221 [00:39<01:28,  1.67it/s][A
 33%|███▎      | 74/221 [00:40<01:22,  1.78it/s][A
 34%|███▍      | 75/221 [00:40<01:08,  2.12it/s][A
 34%|███▍      | 76/221 [00:40<01:04,  2.23it/s][A
 35%|███▍      | 77/221 [00:40<00:56,  2.56it/s][A
 35%|███▌      | 78/221 [00:41<01:00,  2.36it/s][A
 36%|███▌      | 79/221 [00:42<01:21,  1.74it/s][A
 36%|███▌      | 80/221 [00:42<01:04,  2.19it/s][A
 37%|███▋      | 81/221 [00:43<01:10,  1.97it/s][A
 37%|███▋      | 82/221 [00:44<01:33,  1.49it/s][A
 38%|███▊      | 83/221 [00:44<01:19,  1.74it/s][A
 38%|███▊      | 84/221 [00:45<01:40,  1.36it/s][A
 38%|███▊      | 85/221 [00:45<01:18,  1.73it/s][A
 39%|███▉      | 86/221 [00:46<01:17,  1.74it/s][A
 39%|███▉      | 87/221 [00:46<01:12,  1.85it/s][A
 40%|███▉      | 88/221 [00:47<01:01,  2.15it/s][A
 40%|████      | 89/221 [00:47<01:11,  1.84it/s][A
 41%|████      | 90/221 [00:48<01:15,  1.73it/s][A
 41%|████      | 91/221 [00:48<01:02,  2.08it/s][A
 42%|████▏     | 92/221 [00:49<01:05,  1.98it/s][A
 43%|████▎     | 94/221 [00:49<00:45,  2.76it/s][A
 43%|████▎     | 95/221 [00:50<00:46,  2.71it/s][A
 43%|████▎     | 96/221 [00:50<00:44,  2.78it/s][A
 44%|████▍     | 97/221 [00:51<00:50,  2.47it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.90it/s][A
 45%|████▍     | 99/221 [00:52<01:07,  1.80it/s][A
 45%|████▌     | 100/221 [00:53<01:21,  1.49it/s][A
 46%|████▌     | 101/221 [00:54<01:28,  1.36it/s][A
 46%|████▌     | 102/221 [00:55<01:34,  1.27it/s][A
 47%|████▋     | 103/221 [00:55<01:17,  1.52it/s][A
 47%|████▋     | 104/221 [00:56<01:18,  1.49it/s][A
 48%|████▊     | 105/221 [00:56<01:07,  1.73it/s][A
 48%|████▊     | 106/221 [00:56<00:53,  2.14it/s][A
 48%|████▊     | 107/221 [00:57<00:50,  2.25it/s][A
 49%|████▉     | 108/221 [00:57<00:54,  2.07it/s][A
 49%|████▉     | 109/221 [00:58<00:55,  2.03it/s][A
 50%|████▉     | 110/221 [00:59<01:06,  1.66it/s][A
 50%|█████     | 111/221 [01:00<01:18,  1.40it/s][A
 51%|█████     | 112/221 [01:00<01:07,  1.60it/s][A
 51%|█████     | 113/221 [01:00<00:55,  1.96it/s][A
 52%|█████▏    | 114/221 [01:01<00:51,  2.08it/s][A
 52%|█████▏    | 115/221 [01:01<00:57,  1.84it/s][A
 52%|█████▏    | 116/221 [01:02<00:45,  2.29it/s][A
 53%|█████▎    | 117/221 [01:02<00:41,  2.52it/s][A
 53%|█████▎    | 118/221 [01:02<00:42,  2.40it/s][A
 54%|█████▍    | 119/221 [01:03<00:48,  2.11it/s][A
 54%|█████▍    | 120/221 [01:04<00:51,  1.98it/s][A
 55%|█████▍    | 121/221 [01:04<00:48,  2.05it/s][A
 55%|█████▌    | 122/221 [01:05<00:51,  1.91it/s][A
 56%|█████▌    | 123/221 [01:05<00:46,  2.10it/s][A
 56%|█████▌    | 124/221 [01:06<01:03,  1.52it/s][A
 57%|█████▋    | 125/221 [01:06<00:53,  1.78it/s][A
 57%|█████▋    | 126/221 [01:07<00:42,  2.25it/s][A
 57%|█████▋    | 127/221 [01:08<00:59,  1.57it/s][A
 58%|█████▊    | 128/221 [01:08<00:55,  1.66it/s][A
 58%|█████▊    | 129/221 [01:09<00:56,  1.62it/s][A
 59%|█████▉    | 130/221 [01:09<00:44,  2.06it/s][A
 59%|█████▉    | 131/221 [01:10<00:42,  2.10it/s][A
 60%|█████▉    | 132/221 [01:10<00:52,  1.69it/s][A
 60%|██████    | 133/221 [01:11<00:56,  1.55it/s][A
 61%|██████    | 134/221 [01:12<00:52,  1.66it/s][A
 61%|██████    | 135/221 [01:12<00:49,  1.75it/s][A
 62%|██████▏   | 136/221 [01:13<00:57,  1.47it/s][A
 62%|██████▏   | 137/221 [01:14<00:59,  1.41it/s][A
 62%|██████▏   | 138/221 [01:15<01:01,  1.35it/s][A
 63%|██████▎   | 139/221 [01:16<01:13,  1.12it/s][A
 63%|██████▎   | 140/221 [01:17<01:08,  1.18it/s][A
 64%|██████▍   | 141/221 [01:17<00:58,  1.36it/s][A
 64%|██████▍   | 142/221 [01:18<00:53,  1.46it/s][A
 65%|██████▍   | 143/221 [01:18<00:40,  1.91it/s][A
 65%|██████▌   | 144/221 [01:18<00:38,  2.01it/s][A
 66%|██████▌   | 145/221 [01:19<00:37,  2.02it/s][A
 66%|██████▌   | 146/221 [01:19<00:29,  2.50it/s][A
 67%|██████▋   | 147/221 [01:19<00:31,  2.34it/s][A
 67%|██████▋   | 148/221 [01:20<00:30,  2.43it/s][A
 68%|██████▊   | 150/221 [01:20<00:22,  3.17it/s][A
 68%|██████▊   | 151/221 [01:20<00:19,  3.59it/s][A
 69%|██████▉   | 152/221 [01:21<00:21,  3.19it/s][A
 69%|██████▉   | 153/221 [01:21<00:20,  3.32it/s][A
 70%|██████▉   | 154/221 [01:22<00:32,  2.05it/s][A
 70%|███████   | 155/221 [01:23<00:39,  1.67it/s][A
 71%|███████   | 156/221 [01:23<00:35,  1.85it/s][A
 71%|███████   | 157/221 [01:24<00:35,  1.79it/s][A
 71%|███████▏  | 158/221 [01:24<00:30,  2.07it/s][A
 72%|███████▏  | 159/221 [01:24<00:23,  2.62it/s][A
 72%|███████▏  | 160/221 [01:25<00:26,  2.28it/s][A
 73%|███████▎  | 161/221 [01:25<00:21,  2.75it/s][A
 73%|███████▎  | 162/221 [01:26<00:24,  2.42it/s][A
 74%|███████▍  | 163/221 [01:26<00:20,  2.83it/s][A
 74%|███████▍  | 164/221 [01:26<00:20,  2.82it/s][A
 75%|███████▍  | 165/221 [01:26<00:17,  3.23it/s][A
 75%|███████▌  | 166/221 [01:27<00:26,  2.07it/s][A
 76%|███████▌  | 167/221 [01:28<00:23,  2.29it/s][A
 76%|███████▌  | 168/221 [01:28<00:21,  2.47it/s][A
 76%|███████▋  | 169/221 [01:29<00:32,  1.59it/s][A
 77%|███████▋  | 170/221 [01:29<00:27,  1.82it/s][A
 77%|███████▋  | 171/221 [01:30<00:27,  1.81it/s][A
 78%|███████▊  | 172/221 [01:30<00:22,  2.22it/s][A
 78%|███████▊  | 173/221 [01:31<00:19,  2.46it/s][A
 79%|███████▊  | 174/221 [01:31<00:18,  2.57it/s][A
 79%|███████▉  | 175/221 [01:31<00:18,  2.46it/s][A
 80%|███████▉  | 176/221 [01:32<00:17,  2.63it/s][A
 80%|████████  | 177/221 [01:32<00:17,  2.48it/s][A
 81%|████████  | 178/221 [01:33<00:16,  2.53it/s][A
 81%|████████  | 179/221 [01:33<00:21,  1.99it/s][A
 82%|████████▏ | 181/221 [01:34<00:13,  2.94it/s][A
 82%|████████▏ | 182/221 [01:34<00:15,  2.50it/s][A
 83%|████████▎ | 183/221 [01:35<00:16,  2.31it/s][A
 83%|████████▎ | 184/221 [01:35<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:36<00:19,  1.84it/s][A
 84%|████████▍ | 186/221 [01:36<00:15,  2.30it/s][A
 85%|████████▍ | 187/221 [01:37<00:18,  1.86it/s][A
 85%|████████▌ | 188/221 [01:37<00:15,  2.19it/s][A
 86%|████████▌ | 189/221 [01:38<00:17,  1.85it/s][A
 86%|████████▌ | 190/221 [01:38<00:14,  2.11it/s][A
 86%|████████▋ | 191/221 [01:39<00:15,  1.95it/s][A
 87%|████████▋ | 192/221 [01:39<00:14,  1.97it/s][A
 87%|████████▋ | 193/221 [01:40<00:12,  2.24it/s][A
 88%|████████▊ | 194/221 [01:41<00:15,  1.69it/s][A
 88%|████████▊ | 195/221 [01:42<00:18,  1.37it/s][A
 89%|████████▊ | 196/221 [01:42<00:16,  1.54it/s][A
 89%|████████▉ | 197/221 [01:43<00:15,  1.52it/s][A
 90%|████████▉ | 198/221 [01:43<00:14,  1.55it/s][A
 90%|█████████ | 199/221 [01:44<00:12,  1.82it/s][A
 90%|█████████ | 200/221 [01:45<00:13,  1.57it/s][A
 91%|█████████ | 201/221 [01:45<00:12,  1.61it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.87it/s][A
 92%|█████████▏| 203/221 [01:46<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:46<00:07,  2.26it/s][A
 93%|█████████▎| 205/221 [01:47<00:06,  2.44it/s][A
 93%|█████████▎| 206/221 [01:47<00:05,  2.72it/s][A
 94%|█████████▎| 207/221 [01:47<00:04,  3.19it/s][A
 94%|█████████▍| 208/221 [01:48<00:07,  1.64it/s][A
 95%|█████████▍| 209/221 [01:49<00:06,  1.76it/s][A
 95%|█████████▌| 210/221 [01:49<00:04,  2.25it/s][A
 95%|█████████▌| 211/221 [01:50<00:05,  1.74it/s][A
 96%|█████████▌| 212/221 [01:50<00:04,  2.11it/s][A
 96%|█████████▋| 213/221 [01:51<00:04,  1.99it/s][A
 97%|█████████▋| 214/221 [01:51<00:03,  1.78it/s][A
 97%|█████████▋| 215/221 [01:52<00:02,  2.31it/s][A
 98%|█████████▊| 216/221 [01:52<00:02,  1.97it/s][A
 98%|█████████▊| 217/221 [01:53<00:02,  1.82it/s][A
 99%|█████████▊| 218/221 [01:53<00:01,  2.16it/s][A
 99%|█████████▉| 219/221 [01:54<00:01,  1.96it/s][A
100%|█████████▉| 220/221 [01:54<00:00,  2.16it/s][A
100%|██████████| 221/221 [01:55<00:00,  1.86it/s][A100%|██████████| 221/221 [01:55<00:00,  1.92it/s]
09/19/2024 12:25:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 7499--===========

09/19/2024 12:25:34 - INFO - __main__ -   {'area_r1': 45.5, 'area_recall': '45.5/75.5/83.6', 'area_ravg': 68.2}
09/19/2024 12:25:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 7499--===========

09/19/2024 12:25:34 - INFO - __main__ -   {'forward_r1': 51.5, 'forward_recall': '51.5/79.1/87.6', 'forward_ravg': 72.7}
09/19/2024 12:25:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 7499--===========

09/19/2024 12:25:34 - INFO - __main__ -   {'area_video_r1': 48.8, 'area_video_recall': '48.8/79.0/87.7', 'area_video_ravg': 71.8}
09/19/2024 12:25:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 12:25:34 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 12:25:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 7499--===========

09/19/2024 12:25:34 - INFO - __main__ -   {'area_video_r1': 63.7, 'area_video_recall': '63.7/84.2/89.7', 'area_video_ravg': 79.2, 'area_video_back_r1': 63.1, 'area_video_back_recall': '63.1/85.4/92.3', 'area_video_back_ravg': 80.3}
09/19/2024 12:25:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 6999=======

09/19/2024 12:25:34 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/84.2/89.6', 'area_video_ravg': 79.2, 'area_video_back_r1': 64.1, 'area_video_back_recall': '64.1/85.5/92.1', 'area_video_back_ravg': 80.6}
09/19/2024 12:25:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 7499--===========

09/19/2024 12:25:34 - INFO - __main__ -   {'video_r1': 31.6, 'video_recall': '31.6/56.7/66.0', 'video_ravg': 51.4}
09/19/2024 12:25:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 12:25:34 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 12:25:34 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 7499--===========

09/19/2024 12:25:34 - INFO - __main__ -   {'video_r1': 61.3, 'video_recall': '61.3/81.2/85.5', 'video_ravg': 76.0}
09/19/2024 12:25:34 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 7499=======

09/19/2024 12:25:34 - INFO - __main__ -   {'video_r1': 61.3, 'video_recall': '61.3/81.2/85.5', 'video_ravg': 76.0}
09/19/2024 12:26:03 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015248780138790607, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9934858083724976, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0087345838546753}
 84%|████████▍ | 7500/8917 [10:11:29<67:43:15, 172.05s/it] 84%|████████▍ | 7501/8917 [10:11:32<47:47:03, 121.49s/it] 84%|████████▍ | 7502/8917 [10:11:36<33:51:24, 86.14s/it]  84%|████████▍ | 7503/8917 [10:11:40<24:07:14, 61.41s/it] 84%|████████▍ | 7504/8917 [10:11:43<17:18:34, 44.10s/it] 84%|████████▍ | 7505/8917 [10:11:47<12:35:17, 32.09s/it] 84%|████████▍ | 7506/8917 [10:11:51<9:15:18, 23.61s/it]  84%|████████▍ | 7507/8917 [10:11:55<6:57:32, 17.77s/it] 84%|████████▍ | 7508/8917 [10:11:59<5:17:36, 13.53s/it] 84%|████████▍ | 7509/8917 [10:12:03<4:08:13, 10.58s/it] 84%|████████▍ | 7510/8917 [10:12:07<3:23:05,  8.66s/it] 84%|████████▍ | 7511/8917 [10:12:10<2:46:39,  7.11s/it] 84%|████████▍ | 7512/8917 [10:12:14<2:22:35,  6.09s/it] 84%|████████▍ | 7513/8917 [10:12:17<2:03:00,  5.26s/it] 84%|████████▍ | 7514/8917 [10:12:21<1:52:56,  4.83s/it] 84%|████████▍ | 7515/8917 [10:12:25<1:45:26,  4.51s/it] 84%|████████▍ | 7516/8917 [10:12:30<1:46:40,  4.57s/it] 84%|████████▍ | 7517/8917 [10:12:33<1:37:43,  4.19s/it] 84%|████████▍ | 7518/8917 [10:12:37<1:34:36,  4.06s/it] 84%|████████▍ | 7519/8917 [10:12:41<1:35:17,  4.09s/it] 84%|████████▍ | 7520/8917 [10:12:45<1:32:55,  3.99s/it] 84%|████████▍ | 7521/8917 [10:12:48<1:29:46,  3.86s/it] 84%|████████▍ | 7522/8917 [10:12:52<1:32:07,  3.96s/it] 84%|████████▍ | 7523/8917 [10:12:56<1:31:14,  3.93s/it] 84%|████████▍ | 7524/8917 [10:13:00<1:32:35,  3.99s/it] 84%|████████▍ | 7525/8917 [10:13:04<1:32:15,  3.98s/it] 84%|████████▍ | 7526/8917 [10:13:08<1:30:40,  3.91s/it] 84%|████████▍ | 7527/8917 [10:13:12<1:28:49,  3.83s/it] 84%|████████▍ | 7528/8917 [10:13:15<1:26:42,  3.75s/it] 84%|████████▍ | 7529/8917 [10:13:19<1:26:14,  3.73s/it] 84%|████████▍ | 7530/8917 [10:13:23<1:28:40,  3.84s/it] 84%|████████▍ | 7531/8917 [10:13:27<1:26:12,  3.73s/it] 84%|████████▍ | 7532/8917 [10:13:30<1:25:10,  3.69s/it] 84%|████████▍ | 7533/8917 [10:13:34<1:24:27,  3.66s/it] 84%|████████▍ | 7534/8917 [10:13:38<1:29:00,  3.86s/it] 85%|████████▍ | 7535/8917 [10:13:42<1:28:55,  3.86s/it] 85%|████████▍ | 7536/8917 [10:13:46<1:27:56,  3.82s/it] 85%|████████▍ | 7537/8917 [10:13:50<1:28:17,  3.84s/it] 85%|████████▍ | 7538/8917 [10:13:53<1:28:33,  3.85s/it] 85%|████████▍ | 7539/8917 [10:13:57<1:26:56,  3.79s/it] 85%|████████▍ | 7540/8917 [10:14:01<1:27:53,  3.83s/it] 85%|████████▍ | 7541/8917 [10:14:05<1:27:11,  3.80s/it] 85%|████████▍ | 7542/8917 [10:14:08<1:26:40,  3.78s/it] 85%|████████▍ | 7543/8917 [10:14:12<1:26:22,  3.77s/it] 85%|████████▍ | 7544/8917 [10:14:16<1:25:57,  3.76s/it] 85%|████████▍ | 7545/8917 [10:14:19<1:24:26,  3.69s/it] 85%|████████▍ | 7546/8917 [10:14:23<1:25:08,  3.73s/it] 85%|████████▍ | 7547/8917 [10:14:27<1:26:11,  3.78s/it] 85%|████████▍ | 7548/8917 [10:14:31<1:29:30,  3.92s/it] 85%|████████▍ | 7549/8917 [10:14:35<1:28:11,  3.87s/it]09/19/2024 12:29:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01202976331114769, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.8727208375930786, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.8847506046295166}
 85%|████████▍ | 7550/8917 [10:14:39<1:26:06,  3.78s/it] 85%|████████▍ | 7551/8917 [10:14:43<1:26:31,  3.80s/it] 85%|████████▍ | 7552/8917 [10:14:47<1:27:06,  3.83s/it] 85%|████████▍ | 7553/8917 [10:14:50<1:27:00,  3.83s/it] 85%|████████▍ | 7554/8917 [10:14:54<1:28:29,  3.90s/it] 85%|████████▍ | 7555/8917 [10:14:59<1:30:06,  3.97s/it] 85%|████████▍ | 7556/8917 [10:15:02<1:29:06,  3.93s/it] 85%|████████▍ | 7557/8917 [10:15:06<1:27:20,  3.85s/it] 85%|████████▍ | 7558/8917 [10:15:10<1:26:39,  3.83s/it] 85%|████████▍ | 7559/8917 [10:15:14<1:28:11,  3.90s/it] 85%|████████▍ | 7560/8917 [10:15:18<1:28:26,  3.91s/it] 85%|████████▍ | 7561/8917 [10:15:22<1:26:58,  3.85s/it] 85%|████████▍ | 7562/8917 [10:15:25<1:24:52,  3.76s/it] 85%|████████▍ | 7563/8917 [10:15:29<1:25:01,  3.77s/it] 85%|████████▍ | 7564/8917 [10:15:32<1:23:28,  3.70s/it] 85%|████████▍ | 7565/8917 [10:15:36<1:24:50,  3.77s/it] 85%|████████▍ | 7566/8917 [10:15:40<1:25:51,  3.81s/it] 85%|████████▍ | 7567/8917 [10:15:44<1:25:25,  3.80s/it] 85%|████████▍ | 7568/8917 [10:15:48<1:26:47,  3.86s/it] 85%|████████▍ | 7569/8917 [10:15:52<1:29:06,  3.97s/it] 85%|████████▍ | 7570/8917 [10:15:56<1:25:19,  3.80s/it] 85%|████████▍ | 7571/8917 [10:15:59<1:25:03,  3.79s/it] 85%|████████▍ | 7572/8917 [10:16:03<1:24:16,  3.76s/it] 85%|████████▍ | 7573/8917 [10:16:07<1:23:13,  3.72s/it] 85%|████████▍ | 7574/8917 [10:16:11<1:25:37,  3.83s/it] 85%|████████▍ | 7575/8917 [10:16:14<1:24:44,  3.79s/it] 85%|████████▍ | 7576/8917 [10:16:18<1:25:45,  3.84s/it] 85%|████████▍ | 7577/8917 [10:16:22<1:25:45,  3.84s/it] 85%|████████▍ | 7578/8917 [10:16:26<1:24:49,  3.80s/it] 85%|████████▍ | 7579/8917 [10:16:29<1:22:43,  3.71s/it] 85%|████████▌ | 7580/8917 [10:16:33<1:24:28,  3.79s/it] 85%|████████▌ | 7581/8917 [10:16:38<1:26:51,  3.90s/it] 85%|████████▌ | 7582/8917 [10:16:41<1:24:05,  3.78s/it] 85%|████████▌ | 7583/8917 [10:16:45<1:24:33,  3.80s/it] 85%|████████▌ | 7584/8917 [10:16:49<1:26:38,  3.90s/it] 85%|████████▌ | 7585/8917 [10:16:53<1:24:53,  3.82s/it] 85%|████████▌ | 7586/8917 [10:16:56<1:23:02,  3.74s/it] 85%|████████▌ | 7587/8917 [10:17:00<1:25:10,  3.84s/it] 85%|████████▌ | 7588/8917 [10:17:04<1:24:04,  3.80s/it] 85%|████████▌ | 7589/8917 [10:17:08<1:26:50,  3.92s/it] 85%|████████▌ | 7590/8917 [10:17:12<1:25:16,  3.86s/it] 85%|████████▌ | 7591/8917 [10:17:16<1:25:31,  3.87s/it] 85%|████████▌ | 7592/8917 [10:17:19<1:23:40,  3.79s/it] 85%|████████▌ | 7593/8917 [10:17:23<1:23:13,  3.77s/it] 85%|████████▌ | 7594/8917 [10:17:27<1:21:34,  3.70s/it] 85%|████████▌ | 7595/8917 [10:17:31<1:23:16,  3.78s/it] 85%|████████▌ | 7596/8917 [10:17:35<1:25:05,  3.86s/it] 85%|████████▌ | 7597/8917 [10:17:39<1:24:56,  3.86s/it] 85%|████████▌ | 7598/8917 [10:17:42<1:24:35,  3.85s/it] 85%|████████▌ | 7599/8917 [10:17:46<1:24:40,  3.85s/it]09/19/2024 12:32:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02478116564452648, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0396976470947266, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0644787549972534}
 85%|████████▌ | 7600/8917 [10:17:50<1:23:15,  3.79s/it] 85%|████████▌ | 7601/8917 [10:17:54<1:24:45,  3.86s/it] 85%|████████▌ | 7602/8917 [10:17:58<1:22:30,  3.76s/it] 85%|████████▌ | 7603/8917 [10:18:01<1:22:55,  3.79s/it] 85%|████████▌ | 7604/8917 [10:18:05<1:23:35,  3.82s/it] 85%|████████▌ | 7605/8917 [10:18:09<1:23:13,  3.81s/it] 85%|████████▌ | 7606/8917 [10:18:13<1:23:53,  3.84s/it] 85%|████████▌ | 7607/8917 [10:18:17<1:23:31,  3.83s/it] 85%|████████▌ | 7608/8917 [10:18:21<1:26:36,  3.97s/it] 85%|████████▌ | 7609/8917 [10:18:25<1:26:23,  3.96s/it] 85%|████████▌ | 7610/8917 [10:18:29<1:25:09,  3.91s/it] 85%|████████▌ | 7611/8917 [10:18:33<1:25:18,  3.92s/it] 85%|████████▌ | 7612/8917 [10:18:37<1:25:14,  3.92s/it] 85%|████████▌ | 7613/8917 [10:18:40<1:24:17,  3.88s/it] 85%|████████▌ | 7614/8917 [10:18:45<1:25:26,  3.93s/it] 85%|████████▌ | 7615/8917 [10:18:48<1:23:22,  3.84s/it] 85%|████████▌ | 7616/8917 [10:18:52<1:22:14,  3.79s/it] 85%|████████▌ | 7617/8917 [10:18:56<1:23:30,  3.85s/it] 85%|████████▌ | 7618/8917 [10:19:00<1:23:26,  3.85s/it] 85%|████████▌ | 7619/8917 [10:19:03<1:21:58,  3.79s/it] 85%|████████▌ | 7620/8917 [10:19:07<1:21:53,  3.79s/it] 85%|████████▌ | 7621/8917 [10:19:11<1:21:38,  3.78s/it] 85%|████████▌ | 7622/8917 [10:19:15<1:22:52,  3.84s/it] 85%|████████▌ | 7623/8917 [10:19:18<1:21:16,  3.77s/it] 85%|████████▌ | 7624/8917 [10:19:22<1:21:30,  3.78s/it] 86%|████████▌ | 7625/8917 [10:19:26<1:22:41,  3.84s/it] 86%|████████▌ | 7626/8917 [10:19:30<1:22:13,  3.82s/it] 86%|████████▌ | 7627/8917 [10:19:34<1:22:03,  3.82s/it] 86%|████████▌ | 7628/8917 [10:19:38<1:25:49,  4.00s/it] 86%|████████▌ | 7629/8917 [10:19:42<1:24:39,  3.94s/it] 86%|████████▌ | 7630/8917 [10:19:46<1:22:13,  3.83s/it] 86%|████████▌ | 7631/8917 [10:19:49<1:21:10,  3.79s/it] 86%|████████▌ | 7632/8917 [10:19:53<1:19:00,  3.69s/it] 86%|████████▌ | 7633/8917 [10:19:57<1:19:34,  3.72s/it] 86%|████████▌ | 7634/8917 [10:20:00<1:20:49,  3.78s/it] 86%|████████▌ | 7635/8917 [10:20:05<1:23:05,  3.89s/it] 86%|████████▌ | 7636/8917 [10:20:08<1:22:34,  3.87s/it] 86%|████████▌ | 7637/8917 [10:20:13<1:25:40,  4.02s/it] 86%|████████▌ | 7638/8917 [10:20:16<1:23:02,  3.90s/it] 86%|████████▌ | 7639/8917 [10:20:20<1:21:20,  3.82s/it] 86%|████████▌ | 7640/8917 [10:20:24<1:22:09,  3.86s/it] 86%|████████▌ | 7641/8917 [10:20:28<1:20:40,  3.79s/it] 86%|████████▌ | 7642/8917 [10:20:31<1:18:10,  3.68s/it] 86%|████████▌ | 7643/8917 [10:20:35<1:17:06,  3.63s/it] 86%|████████▌ | 7644/8917 [10:20:39<1:20:30,  3.79s/it] 86%|████████▌ | 7645/8917 [10:20:42<1:20:04,  3.78s/it] 86%|████████▌ | 7646/8917 [10:20:47<1:22:32,  3.90s/it] 86%|████████▌ | 7647/8917 [10:20:51<1:24:13,  3.98s/it] 86%|████████▌ | 7648/8917 [10:20:55<1:23:33,  3.95s/it] 86%|████████▌ | 7649/8917 [10:20:58<1:21:31,  3.86s/it]09/19/2024 12:35:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020684855058789253, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1738471984863281, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.194532036781311}
 86%|████████▌ | 7650/8917 [10:21:02<1:20:36,  3.82s/it] 86%|████████▌ | 7651/8917 [10:21:06<1:20:51,  3.83s/it] 86%|████████▌ | 7652/8917 [10:21:10<1:21:24,  3.86s/it] 86%|████████▌ | 7653/8917 [10:21:14<1:22:55,  3.94s/it] 86%|████████▌ | 7654/8917 [10:21:18<1:22:21,  3.91s/it] 86%|████████▌ | 7655/8917 [10:21:22<1:21:29,  3.87s/it] 86%|████████▌ | 7656/8917 [10:21:25<1:20:34,  3.83s/it] 86%|████████▌ | 7657/8917 [10:21:29<1:17:25,  3.69s/it] 86%|████████▌ | 7658/8917 [10:21:33<1:18:42,  3.75s/it] 86%|████████▌ | 7659/8917 [10:21:36<1:19:04,  3.77s/it] 86%|████████▌ | 7660/8917 [10:21:40<1:19:24,  3.79s/it] 86%|████████▌ | 7661/8917 [10:21:44<1:20:29,  3.85s/it] 86%|████████▌ | 7662/8917 [10:21:48<1:21:18,  3.89s/it] 86%|████████▌ | 7663/8917 [10:21:52<1:21:48,  3.91s/it] 86%|████████▌ | 7664/8917 [10:21:56<1:18:56,  3.78s/it] 86%|████████▌ | 7665/8917 [10:21:59<1:18:55,  3.78s/it] 86%|████████▌ | 7666/8917 [10:22:03<1:18:16,  3.75s/it] 86%|████████▌ | 7667/8917 [10:22:07<1:18:08,  3.75s/it] 86%|████████▌ | 7668/8917 [10:22:10<1:17:06,  3.70s/it] 86%|████████▌ | 7669/8917 [10:22:15<1:19:08,  3.81s/it] 86%|████████▌ | 7670/8917 [10:22:19<1:20:25,  3.87s/it] 86%|████████▌ | 7671/8917 [10:22:22<1:18:27,  3.78s/it] 86%|████████▌ | 7672/8917 [10:22:26<1:19:42,  3.84s/it] 86%|████████▌ | 7673/8917 [10:22:30<1:20:49,  3.90s/it] 86%|████████▌ | 7674/8917 [10:22:34<1:20:22,  3.88s/it] 86%|████████▌ | 7675/8917 [10:22:38<1:19:02,  3.82s/it] 86%|████████▌ | 7676/8917 [10:22:42<1:20:24,  3.89s/it] 86%|████████▌ | 7677/8917 [10:22:45<1:18:33,  3.80s/it] 86%|████████▌ | 7678/8917 [10:22:49<1:17:23,  3.75s/it] 86%|████████▌ | 7679/8917 [10:22:53<1:18:01,  3.78s/it] 86%|████████▌ | 7680/8917 [10:22:56<1:17:08,  3.74s/it] 86%|████████▌ | 7681/8917 [10:23:01<1:19:27,  3.86s/it] 86%|████████▌ | 7682/8917 [10:23:04<1:17:49,  3.78s/it] 86%|████████▌ | 7683/8917 [10:23:08<1:18:15,  3.80s/it] 86%|████████▌ | 7684/8917 [10:23:12<1:17:59,  3.79s/it] 86%|████████▌ | 7685/8917 [10:23:16<1:18:30,  3.82s/it] 86%|████████▌ | 7686/8917 [10:23:19<1:17:11,  3.76s/it] 86%|████████▌ | 7687/8917 [10:23:23<1:18:26,  3.83s/it] 86%|████████▌ | 7688/8917 [10:23:27<1:18:52,  3.85s/it] 86%|████████▌ | 7689/8917 [10:23:31<1:17:56,  3.81s/it] 86%|████████▌ | 7690/8917 [10:23:35<1:18:17,  3.83s/it] 86%|████████▋ | 7691/8917 [10:23:38<1:16:41,  3.75s/it] 86%|████████▋ | 7692/8917 [10:23:42<1:14:54,  3.67s/it] 86%|████████▋ | 7693/8917 [10:23:46<1:16:45,  3.76s/it] 86%|████████▋ | 7694/8917 [10:23:50<1:18:25,  3.85s/it] 86%|████████▋ | 7695/8917 [10:23:54<1:17:18,  3.80s/it] 86%|████████▋ | 7696/8917 [10:23:57<1:16:42,  3.77s/it] 86%|████████▋ | 7697/8917 [10:24:01<1:17:26,  3.81s/it] 86%|████████▋ | 7698/8917 [10:24:05<1:18:58,  3.89s/it] 86%|████████▋ | 7699/8917 [10:24:09<1:17:24,  3.81s/it]09/19/2024 12:38:46 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0187581367790699, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1863934993743896, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2051516771316528}
 86%|████████▋ | 7700/8917 [10:24:12<1:15:11,  3.71s/it] 86%|████████▋ | 7701/8917 [10:24:16<1:15:11,  3.71s/it] 86%|████████▋ | 7702/8917 [10:24:20<1:16:51,  3.80s/it] 86%|████████▋ | 7703/8917 [10:24:24<1:15:44,  3.74s/it] 86%|████████▋ | 7704/8917 [10:24:28<1:16:52,  3.80s/it] 86%|████████▋ | 7705/8917 [10:24:32<1:19:23,  3.93s/it] 86%|████████▋ | 7706/8917 [10:24:35<1:16:34,  3.79s/it] 86%|████████▋ | 7707/8917 [10:24:40<1:19:54,  3.96s/it] 86%|████████▋ | 7708/8917 [10:24:44<1:19:27,  3.94s/it] 86%|████████▋ | 7709/8917 [10:24:47<1:17:55,  3.87s/it] 86%|████████▋ | 7710/8917 [10:24:51<1:19:41,  3.96s/it] 86%|████████▋ | 7711/8917 [10:24:55<1:18:19,  3.90s/it] 86%|████████▋ | 7712/8917 [10:24:59<1:18:20,  3.90s/it] 86%|████████▋ | 7713/8917 [10:25:03<1:15:57,  3.79s/it] 87%|████████▋ | 7714/8917 [10:25:06<1:15:57,  3.79s/it] 87%|████████▋ | 7715/8917 [10:25:11<1:18:02,  3.90s/it] 87%|████████▋ | 7716/8917 [10:25:14<1:16:31,  3.82s/it] 87%|████████▋ | 7717/8917 [10:25:18<1:14:36,  3.73s/it] 87%|████████▋ | 7718/8917 [10:25:21<1:13:22,  3.67s/it] 87%|████████▋ | 7719/8917 [10:25:25<1:12:41,  3.64s/it] 87%|████████▋ | 7720/8917 [10:25:29<1:16:19,  3.83s/it] 87%|████████▋ | 7721/8917 [10:25:33<1:16:52,  3.86s/it] 87%|████████▋ | 7722/8917 [10:25:37<1:15:29,  3.79s/it] 87%|████████▋ | 7723/8917 [10:25:40<1:15:42,  3.80s/it] 87%|████████▋ | 7724/8917 [10:25:44<1:15:28,  3.80s/it] 87%|████████▋ | 7725/8917 [10:25:48<1:14:24,  3.75s/it] 87%|████████▋ | 7726/8917 [10:25:51<1:13:34,  3.71s/it] 87%|████████▋ | 7727/8917 [10:25:55<1:14:45,  3.77s/it] 87%|████████▋ | 7728/8917 [10:25:59<1:15:15,  3.80s/it] 87%|████████▋ | 7729/8917 [10:26:03<1:15:45,  3.83s/it] 87%|████████▋ | 7730/8917 [10:26:07<1:15:22,  3.81s/it] 87%|████████▋ | 7731/8917 [10:26:11<1:15:15,  3.81s/it] 87%|████████▋ | 7732/8917 [10:26:15<1:15:39,  3.83s/it] 87%|████████▋ | 7733/8917 [10:26:18<1:15:50,  3.84s/it] 87%|████████▋ | 7734/8917 [10:26:23<1:17:10,  3.91s/it] 87%|████████▋ | 7735/8917 [10:26:26<1:15:42,  3.84s/it] 87%|████████▋ | 7736/8917 [10:26:30<1:16:33,  3.89s/it] 87%|████████▋ | 7737/8917 [10:26:34<1:17:33,  3.94s/it] 87%|████████▋ | 7738/8917 [10:26:38<1:16:12,  3.88s/it] 87%|████████▋ | 7739/8917 [10:26:42<1:16:07,  3.88s/it] 87%|████████▋ | 7740/8917 [10:26:46<1:16:46,  3.91s/it] 87%|████████▋ | 7741/8917 [10:26:50<1:15:46,  3.87s/it] 87%|████████▋ | 7742/8917 [10:26:53<1:14:09,  3.79s/it] 87%|████████▋ | 7743/8917 [10:26:57<1:16:48,  3.93s/it] 87%|████████▋ | 7744/8917 [10:27:01<1:16:06,  3.89s/it] 87%|████████▋ | 7745/8917 [10:27:05<1:15:01,  3.84s/it] 87%|████████▋ | 7746/8917 [10:27:09<1:16:40,  3.93s/it] 87%|████████▋ | 7747/8917 [10:27:13<1:14:44,  3.83s/it] 87%|████████▋ | 7748/8917 [10:27:16<1:13:31,  3.77s/it] 87%|████████▋ | 7749/8917 [10:27:21<1:15:53,  3.90s/it]09/19/2024 12:41:58 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03285105153918266, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.255476474761963, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2883275747299194}
 87%|████████▋ | 7750/8917 [10:27:24<1:15:18,  3.87s/it] 87%|████████▋ | 7751/8917 [10:27:28<1:15:52,  3.90s/it] 87%|████████▋ | 7752/8917 [10:27:32<1:15:19,  3.88s/it] 87%|████████▋ | 7753/8917 [10:27:36<1:15:26,  3.89s/it] 87%|████████▋ | 7754/8917 [10:27:40<1:16:18,  3.94s/it] 87%|████████▋ | 7755/8917 [10:27:44<1:15:32,  3.90s/it] 87%|████████▋ | 7756/8917 [10:27:48<1:14:08,  3.83s/it] 87%|████████▋ | 7757/8917 [10:27:51<1:11:27,  3.70s/it] 87%|████████▋ | 7758/8917 [10:27:55<1:14:24,  3.85s/it] 87%|████████▋ | 7759/8917 [10:27:59<1:15:11,  3.90s/it] 87%|████████▋ | 7760/8917 [10:28:03<1:14:02,  3.84s/it] 87%|████████▋ | 7761/8917 [10:28:07<1:12:51,  3.78s/it] 87%|████████▋ | 7762/8917 [10:28:11<1:14:13,  3.86s/it] 87%|████████▋ | 7763/8917 [10:28:15<1:14:59,  3.90s/it] 87%|████████▋ | 7764/8917 [10:28:18<1:13:42,  3.84s/it] 87%|████████▋ | 7765/8917 [10:28:23<1:17:25,  4.03s/it] 87%|████████▋ | 7766/8917 [10:28:27<1:15:27,  3.93s/it] 87%|████████▋ | 7767/8917 [10:28:30<1:14:28,  3.89s/it] 87%|████████▋ | 7768/8917 [10:28:34<1:13:46,  3.85s/it] 87%|████████▋ | 7769/8917 [10:28:38<1:12:46,  3.80s/it] 87%|████████▋ | 7770/8917 [10:28:42<1:12:48,  3.81s/it] 87%|████████▋ | 7771/8917 [10:28:45<1:12:41,  3.81s/it] 87%|████████▋ | 7772/8917 [10:28:49<1:13:11,  3.84s/it] 87%|████████▋ | 7773/8917 [10:28:53<1:12:39,  3.81s/it] 87%|████████▋ | 7774/8917 [10:28:57<1:11:44,  3.77s/it] 87%|████████▋ | 7775/8917 [10:29:00<1:11:26,  3.75s/it] 87%|████████▋ | 7776/8917 [10:29:04<1:12:19,  3.80s/it] 87%|████████▋ | 7777/8917 [10:29:08<1:10:36,  3.72s/it] 87%|████████▋ | 7778/8917 [10:29:12<1:10:46,  3.73s/it] 87%|████████▋ | 7779/8917 [10:29:15<1:07:52,  3.58s/it] 87%|████████▋ | 7780/8917 [10:29:19<1:10:27,  3.72s/it] 87%|████████▋ | 7781/8917 [10:29:22<1:09:34,  3.67s/it] 87%|████████▋ | 7782/8917 [10:29:26<1:11:36,  3.79s/it] 87%|████████▋ | 7783/8917 [10:29:30<1:11:56,  3.81s/it] 87%|████████▋ | 7784/8917 [10:29:34<1:12:17,  3.83s/it] 87%|████████▋ | 7785/8917 [10:29:38<1:11:02,  3.77s/it] 87%|████████▋ | 7786/8917 [10:29:42<1:11:24,  3.79s/it] 87%|████████▋ | 7787/8917 [10:29:46<1:13:38,  3.91s/it] 87%|████████▋ | 7788/8917 [10:29:50<1:12:20,  3.84s/it] 87%|████████▋ | 7789/8917 [10:29:53<1:12:05,  3.83s/it] 87%|████████▋ | 7790/8917 [10:29:57<1:10:48,  3.77s/it] 87%|████████▋ | 7791/8917 [10:30:01<1:11:34,  3.81s/it] 87%|████████▋ | 7792/8917 [10:30:05<1:12:02,  3.84s/it] 87%|████████▋ | 7793/8917 [10:30:09<1:13:04,  3.90s/it] 87%|████████▋ | 7794/8917 [10:30:12<1:10:12,  3.75s/it] 87%|████████▋ | 7795/8917 [10:30:16<1:09:22,  3.71s/it] 87%|████████▋ | 7796/8917 [10:30:20<1:10:12,  3.76s/it] 87%|████████▋ | 7797/8917 [10:30:24<1:11:07,  3.81s/it] 87%|████████▋ | 7798/8917 [10:30:27<1:09:40,  3.74s/it] 87%|████████▋ | 7799/8917 [10:30:31<1:10:05,  3.76s/it]09/19/2024 12:45:09 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.019908206537365913, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1940398216247559, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2139480113983154}
 87%|████████▋ | 7800/8917 [10:30:35<1:11:41,  3.85s/it] 87%|████████▋ | 7801/8917 [10:30:39<1:11:34,  3.85s/it] 87%|████████▋ | 7802/8917 [10:30:43<1:11:13,  3.83s/it] 88%|████████▊ | 7803/8917 [10:30:46<1:09:50,  3.76s/it] 88%|████████▊ | 7804/8917 [10:30:50<1:10:45,  3.81s/it] 88%|████████▊ | 7805/8917 [10:30:54<1:09:38,  3.76s/it] 88%|████████▊ | 7806/8917 [10:30:58<1:08:32,  3.70s/it] 88%|████████▊ | 7807/8917 [10:31:01<1:08:16,  3.69s/it] 88%|████████▊ | 7808/8917 [10:31:05<1:08:03,  3.68s/it] 88%|████████▊ | 7809/8917 [10:31:09<1:09:03,  3.74s/it] 88%|████████▊ | 7810/8917 [10:31:12<1:08:41,  3.72s/it] 88%|████████▊ | 7811/8917 [10:31:16<1:09:26,  3.77s/it] 88%|████████▊ | 7812/8917 [10:31:20<1:06:58,  3.64s/it] 88%|████████▊ | 7813/8917 [10:31:23<1:07:39,  3.68s/it] 88%|████████▊ | 7814/8917 [10:31:27<1:07:05,  3.65s/it] 88%|████████▊ | 7815/8917 [10:31:31<1:07:10,  3.66s/it] 88%|████████▊ | 7816/8917 [10:31:34<1:06:19,  3.61s/it] 88%|████████▊ | 7817/8917 [10:31:38<1:07:18,  3.67s/it] 88%|████████▊ | 7818/8917 [10:31:42<1:07:48,  3.70s/it] 88%|████████▊ | 7819/8917 [10:31:45<1:06:41,  3.64s/it] 88%|████████▊ | 7820/8917 [10:31:49<1:08:22,  3.74s/it] 88%|████████▊ | 7821/8917 [10:31:53<1:07:14,  3.68s/it] 88%|████████▊ | 7822/8917 [10:31:57<1:08:48,  3.77s/it] 88%|████████▊ | 7823/8917 [10:32:01<1:09:30,  3.81s/it] 88%|████████▊ | 7824/8917 [10:32:04<1:06:38,  3.66s/it] 88%|████████▊ | 7825/8917 [10:32:08<1:07:17,  3.70s/it] 88%|████████▊ | 7826/8917 [10:32:11<1:07:02,  3.69s/it] 88%|████████▊ | 7827/8917 [10:32:16<1:09:50,  3.84s/it] 88%|████████▊ | 7828/8917 [10:32:20<1:10:46,  3.90s/it] 88%|████████▊ | 7829/8917 [10:32:23<1:08:58,  3.80s/it] 88%|████████▊ | 7830/8917 [10:32:27<1:07:51,  3.75s/it] 88%|████████▊ | 7831/8917 [10:32:30<1:05:11,  3.60s/it] 88%|████████▊ | 7832/8917 [10:32:34<1:05:32,  3.62s/it] 88%|████████▊ | 7833/8917 [10:32:38<1:06:14,  3.67s/it] 88%|████████▊ | 7834/8917 [10:32:41<1:06:24,  3.68s/it] 88%|████████▊ | 7835/8917 [10:32:45<1:05:31,  3.63s/it] 88%|████████▊ | 7836/8917 [10:32:48<1:05:34,  3.64s/it] 88%|████████▊ | 7837/8917 [10:32:52<1:05:45,  3.65s/it] 88%|████████▊ | 7838/8917 [10:32:56<1:05:44,  3.66s/it] 88%|████████▊ | 7839/8917 [10:33:00<1:07:34,  3.76s/it] 88%|████████▊ | 7840/8917 [10:33:03<1:05:04,  3.63s/it] 88%|████████▊ | 7841/8917 [10:33:07<1:04:16,  3.58s/it] 88%|████████▊ | 7842/8917 [10:33:11<1:06:16,  3.70s/it] 88%|████████▊ | 7843/8917 [10:33:14<1:05:06,  3.64s/it] 88%|████████▊ | 7844/8917 [10:33:18<1:06:24,  3.71s/it] 88%|████████▊ | 7845/8917 [10:33:22<1:08:43,  3.85s/it] 88%|████████▊ | 7846/8917 [10:33:26<1:07:28,  3.78s/it] 88%|████████▊ | 7847/8917 [10:33:29<1:06:02,  3.70s/it] 88%|████████▊ | 7848/8917 [10:33:33<1:05:11,  3.66s/it] 88%|████████▊ | 7849/8917 [10:33:36<1:05:24,  3.67s/it]09/19/2024 12:48:14 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.018199535086750984, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2119379043579102, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2301374673843384}
 88%|████████▊ | 7850/8917 [10:33:41<1:07:36,  3.80s/it] 88%|████████▊ | 7851/8917 [10:33:45<1:08:55,  3.88s/it] 88%|████████▊ | 7852/8917 [10:33:48<1:06:04,  3.72s/it] 88%|████████▊ | 7853/8917 [10:33:52<1:05:13,  3.68s/it] 88%|████████▊ | 7854/8917 [10:33:55<1:04:36,  3.65s/it] 88%|████████▊ | 7855/8917 [10:33:59<1:03:56,  3.61s/it] 88%|████████▊ | 7856/8917 [10:34:03<1:05:27,  3.70s/it] 88%|████████▊ | 7857/8917 [10:34:06<1:04:31,  3.65s/it] 88%|████████▊ | 7858/8917 [10:34:10<1:06:28,  3.77s/it] 88%|████████▊ | 7859/8917 [10:34:14<1:05:02,  3.69s/it] 88%|████████▊ | 7860/8917 [10:34:17<1:03:55,  3.63s/it] 88%|████████▊ | 7861/8917 [10:34:21<1:04:29,  3.66s/it] 88%|████████▊ | 7862/8917 [10:34:25<1:05:31,  3.73s/it] 88%|████████▊ | 7863/8917 [10:34:28<1:05:15,  3.71s/it] 88%|████████▊ | 7864/8917 [10:34:32<1:04:47,  3.69s/it] 88%|████████▊ | 7865/8917 [10:34:36<1:06:08,  3.77s/it] 88%|████████▊ | 7866/8917 [10:34:40<1:04:27,  3.68s/it] 88%|████████▊ | 7867/8917 [10:34:43<1:05:08,  3.72s/it] 88%|████████▊ | 7868/8917 [10:34:47<1:04:25,  3.68s/it] 88%|████████▊ | 7869/8917 [10:34:51<1:06:35,  3.81s/it] 88%|████████▊ | 7870/8917 [10:34:54<1:04:12,  3.68s/it] 88%|████████▊ | 7871/8917 [10:34:58<1:03:11,  3.62s/it] 88%|████████▊ | 7872/8917 [10:35:02<1:03:36,  3.65s/it] 88%|████████▊ | 7873/8917 [10:35:05<1:03:46,  3.67s/it] 88%|████████▊ | 7874/8917 [10:35:09<1:04:02,  3.68s/it] 88%|████████▊ | 7875/8917 [10:35:13<1:04:16,  3.70s/it] 88%|████████▊ | 7876/8917 [10:35:17<1:07:15,  3.88s/it] 88%|████████▊ | 7877/8917 [10:35:21<1:06:58,  3.86s/it] 88%|████████▊ | 7878/8917 [10:35:24<1:04:36,  3.73s/it] 88%|████████▊ | 7879/8917 [10:35:28<1:04:47,  3.74s/it] 88%|████████▊ | 7880/8917 [10:35:31<1:01:59,  3.59s/it] 88%|████████▊ | 7881/8917 [10:35:35<1:01:25,  3.56s/it] 88%|████████▊ | 7882/8917 [10:35:39<1:02:57,  3.65s/it] 88%|████████▊ | 7883/8917 [10:35:43<1:04:19,  3.73s/it] 88%|████████▊ | 7884/8917 [10:35:46<1:04:32,  3.75s/it] 88%|████████▊ | 7885/8917 [10:35:50<1:03:46,  3.71s/it] 88%|████████▊ | 7886/8917 [10:35:54<1:04:48,  3.77s/it] 88%|████████▊ | 7887/8917 [10:35:58<1:04:08,  3.74s/it] 88%|████████▊ | 7888/8917 [10:36:01<1:02:41,  3.66s/it] 88%|████████▊ | 7889/8917 [10:36:04<1:01:13,  3.57s/it] 88%|████████▊ | 7890/8917 [10:36:08<1:02:11,  3.63s/it] 88%|████████▊ | 7891/8917 [10:36:12<1:02:58,  3.68s/it] 89%|████████▊ | 7892/8917 [10:36:16<1:03:32,  3.72s/it] 89%|████████▊ | 7893/8917 [10:36:20<1:03:47,  3.74s/it] 89%|████████▊ | 7894/8917 [10:36:23<1:02:29,  3.67s/it] 89%|████████▊ | 7895/8917 [10:36:27<1:03:03,  3.70s/it] 89%|████████▊ | 7896/8917 [10:36:31<1:04:35,  3.80s/it] 89%|████████▊ | 7897/8917 [10:36:35<1:04:09,  3.77s/it] 89%|████████▊ | 7898/8917 [10:36:38<1:04:28,  3.80s/it] 89%|████████▊ | 7899/8917 [10:36:42<1:03:22,  3.74s/it]09/19/2024 12:51:20 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.020351840183138847, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1506288051605225, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.17098069190979}
 89%|████████▊ | 7900/8917 [10:36:46<1:03:31,  3.75s/it] 89%|████████▊ | 7901/8917 [10:36:49<1:01:56,  3.66s/it] 89%|████████▊ | 7902/8917 [10:36:53<1:02:15,  3.68s/it] 89%|████████▊ | 7903/8917 [10:36:57<1:01:39,  3.65s/it] 89%|████████▊ | 7904/8917 [10:37:00<1:02:00,  3.67s/it] 89%|████████▊ | 7905/8917 [10:37:04<1:03:34,  3.77s/it] 89%|████████▊ | 7906/8917 [10:37:08<1:03:20,  3.76s/it] 89%|████████▊ | 7907/8917 [10:37:12<1:03:01,  3.74s/it] 89%|████████▊ | 7908/8917 [10:37:15<1:01:58,  3.69s/it] 89%|████████▊ | 7909/8917 [10:37:20<1:04:36,  3.85s/it] 89%|████████▊ | 7910/8917 [10:37:23<1:02:01,  3.70s/it] 89%|████████▊ | 7911/8917 [10:37:26<1:01:28,  3.67s/it] 89%|████████▊ | 7912/8917 [10:37:30<1:00:39,  3.62s/it] 89%|████████▊ | 7913/8917 [10:37:34<1:01:33,  3.68s/it] 89%|████████▉ | 7914/8917 [10:37:37<58:56,  3.53s/it]   89%|████████▉ | 7915/8917 [10:37:41<1:00:30,  3.62s/it] 89%|████████▉ | 7916/8917 [10:37:44<59:18,  3.56s/it]   89%|████████▉ | 7917/8917 [10:37:48<1:00:27,  3.63s/it] 89%|████████▉ | 7918/8917 [10:37:52<1:01:39,  3.70s/it] 89%|████████▉ | 7919/8917 [10:37:56<1:01:41,  3.71s/it] 89%|████████▉ | 7920/8917 [10:37:59<1:01:38,  3.71s/it] 89%|████████▉ | 7921/8917 [10:38:03<1:02:35,  3.77s/it] 89%|████████▉ | 7922/8917 [10:38:07<1:02:04,  3.74s/it] 89%|████████▉ | 7923/8917 [10:38:11<1:02:10,  3.75s/it] 89%|████████▉ | 7924/8917 [10:38:14<1:01:04,  3.69s/it] 89%|████████▉ | 7925/8917 [10:38:18<1:00:37,  3.67s/it] 89%|████████▉ | 7926/8917 [10:38:22<1:01:27,  3.72s/it] 89%|████████▉ | 7927/8917 [10:38:26<1:03:20,  3.84s/it] 89%|████████▉ | 7928/8917 [10:38:29<1:01:32,  3.73s/it] 89%|████████▉ | 7929/8917 [10:38:33<59:51,  3.64s/it]   89%|████████▉ | 7930/8917 [10:38:36<59:02,  3.59s/it] 89%|████████▉ | 7931/8917 [10:38:40<1:00:58,  3.71s/it] 89%|████████▉ | 7932/8917 [10:38:44<1:01:15,  3.73s/it] 89%|████████▉ | 7933/8917 [10:38:47<59:28,  3.63s/it]   89%|████████▉ | 7934/8917 [10:38:51<58:43,  3.58s/it] 89%|████████▉ | 7935/8917 [10:38:55<59:36,  3.64s/it] 89%|████████▉ | 7936/8917 [10:38:58<1:00:30,  3.70s/it] 89%|████████▉ | 7937/8917 [10:39:02<59:16,  3.63s/it]   89%|████████▉ | 7938/8917 [10:39:06<59:41,  3.66s/it] 89%|████████▉ | 7939/8917 [10:39:09<1:00:39,  3.72s/it] 89%|████████▉ | 7940/8917 [10:39:13<1:00:43,  3.73s/it] 89%|████████▉ | 7941/8917 [10:39:17<1:01:13,  3.76s/it] 89%|████████▉ | 7942/8917 [10:39:20<58:32,  3.60s/it]   89%|████████▉ | 7943/8917 [10:39:24<59:07,  3.64s/it] 89%|████████▉ | 7944/8917 [10:39:28<1:01:44,  3.81s/it] 89%|████████▉ | 7945/8917 [10:39:32<1:00:07,  3.71s/it] 89%|████████▉ | 7946/8917 [10:39:35<59:18,  3.66s/it]   89%|████████▉ | 7947/8917 [10:39:39<59:58,  3.71s/it] 89%|████████▉ | 7948/8917 [10:39:43<1:00:15,  3.73s/it] 89%|████████▉ | 7949/8917 [10:39:47<1:00:59,  3.78s/it]09/19/2024 12:54:24 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015274673700332642, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9695720672607422, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9848467111587524}
 89%|████████▉ | 7950/8917 [10:39:51<1:00:50,  3.78s/it] 89%|████████▉ | 7951/8917 [10:39:54<59:33,  3.70s/it]   89%|████████▉ | 7952/8917 [10:39:57<57:15,  3.56s/it] 89%|████████▉ | 7953/8917 [10:40:01<1:00:15,  3.75s/it] 89%|████████▉ | 7954/8917 [10:40:05<1:01:19,  3.82s/it] 89%|████████▉ | 7955/8917 [10:40:09<59:19,  3.70s/it]   89%|████████▉ | 7956/8917 [10:40:12<58:30,  3.65s/it] 89%|████████▉ | 7957/8917 [10:40:16<1:00:07,  3.76s/it] 89%|████████▉ | 7958/8917 [10:40:20<59:34,  3.73s/it]   89%|████████▉ | 7959/8917 [10:40:24<59:50,  3.75s/it] 89%|████████▉ | 7960/8917 [10:40:28<1:00:33,  3.80s/it] 89%|████████▉ | 7961/8917 [10:40:32<1:00:53,  3.82s/it] 89%|████████▉ | 7962/8917 [10:40:35<1:00:22,  3.79s/it] 89%|████████▉ | 7963/8917 [10:40:39<59:41,  3.75s/it]   89%|████████▉ | 7964/8917 [10:40:42<57:57,  3.65s/it] 89%|████████▉ | 7965/8917 [10:40:46<56:28,  3.56s/it] 89%|████████▉ | 7966/8917 [10:40:50<58:19,  3.68s/it] 89%|████████▉ | 7967/8917 [10:40:53<57:45,  3.65s/it] 89%|████████▉ | 7968/8917 [10:40:57<57:32,  3.64s/it] 89%|████████▉ | 7969/8917 [10:41:01<58:21,  3.69s/it] 89%|████████▉ | 7970/8917 [10:41:05<59:56,  3.80s/it] 89%|████████▉ | 7971/8917 [10:41:08<58:14,  3.69s/it] 89%|████████▉ | 7972/8917 [10:41:12<58:44,  3.73s/it] 89%|████████▉ | 7973/8917 [10:41:16<59:51,  3.80s/it] 89%|████████▉ | 7974/8917 [10:41:20<59:16,  3.77s/it] 89%|████████▉ | 7975/8917 [10:41:23<57:21,  3.65s/it] 89%|████████▉ | 7976/8917 [10:41:27<58:12,  3.71s/it] 89%|████████▉ | 7977/8917 [10:41:31<57:15,  3.65s/it] 89%|████████▉ | 7978/8917 [10:41:35<59:12,  3.78s/it] 89%|████████▉ | 7979/8917 [10:41:38<58:35,  3.75s/it] 89%|████████▉ | 7980/8917 [10:41:42<58:15,  3.73s/it] 90%|████████▉ | 7981/8917 [10:41:46<58:08,  3.73s/it] 90%|████████▉ | 7982/8917 [10:41:49<58:28,  3.75s/it] 90%|████████▉ | 7983/8917 [10:41:53<57:59,  3.73s/it] 90%|████████▉ | 7984/8917 [10:41:57<59:52,  3.85s/it] 90%|████████▉ | 7985/8917 [10:42:01<58:14,  3.75s/it] 90%|████████▉ | 7986/8917 [10:42:04<57:33,  3.71s/it] 90%|████████▉ | 7987/8917 [10:42:08<58:04,  3.75s/it] 90%|████████▉ | 7988/8917 [10:42:12<56:36,  3.66s/it] 90%|████████▉ | 7989/8917 [10:42:15<55:12,  3.57s/it] 90%|████████▉ | 7990/8917 [10:42:19<57:38,  3.73s/it] 90%|████████▉ | 7991/8917 [10:42:23<59:15,  3.84s/it] 90%|████████▉ | 7992/8917 [10:42:27<57:07,  3.71s/it] 90%|████████▉ | 7993/8917 [10:42:30<56:18,  3.66s/it] 90%|████████▉ | 7994/8917 [10:42:34<56:18,  3.66s/it] 90%|████████▉ | 7995/8917 [10:42:38<57:10,  3.72s/it] 90%|████████▉ | 7996/8917 [10:42:41<55:01,  3.59s/it] 90%|████████▉ | 7997/8917 [10:42:45<56:03,  3.66s/it] 90%|████████▉ | 7998/8917 [10:42:48<55:58,  3.65s/it] 90%|████████▉ | 7999/8917 [10:42:52<55:19,  3.62s/it]09/19/2024 12:57:28 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 12:57:28 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<04:26,  1.21s/it][A
  1%|          | 2/221 [00:01<02:39,  1.37it/s][A
  1%|▏         | 3/221 [00:02<02:13,  1.63it/s][A
  2%|▏         | 4/221 [00:02<01:41,  2.13it/s][A
  3%|▎         | 6/221 [00:02<00:58,  3.67it/s][A
  3%|▎         | 7/221 [00:02<00:54,  3.95it/s][A
  4%|▎         | 8/221 [00:03<01:09,  3.05it/s][A
  4%|▍         | 9/221 [00:03<01:08,  3.10it/s][A
  5%|▍         | 10/221 [00:03<01:13,  2.85it/s][A
  5%|▌         | 12/221 [00:11<06:15,  1.79s/it][A
  6%|▌         | 13/221 [00:11<04:55,  1.42s/it][A
  6%|▋         | 14/221 [00:11<03:43,  1.08s/it][A
  7%|▋         | 15/221 [00:11<03:04,  1.12it/s][A
  7%|▋         | 16/221 [00:12<02:50,  1.20it/s][A
  8%|▊         | 17/221 [00:13<02:57,  1.15it/s][A
  8%|▊         | 18/221 [00:13<02:24,  1.41it/s][A
  9%|▊         | 19/221 [00:15<03:32,  1.05s/it][A
  9%|▉         | 20/221 [00:15<02:35,  1.30it/s][A
 10%|▉         | 21/221 [00:16<02:09,  1.54it/s][A
 10%|▉         | 22/221 [00:16<02:23,  1.39it/s][A
 10%|█         | 23/221 [00:17<01:46,  1.87it/s][A
 11%|█         | 24/221 [00:17<01:27,  2.25it/s][A
 11%|█▏        | 25/221 [00:17<01:19,  2.47it/s][A
 12%|█▏        | 26/221 [00:18<01:18,  2.48it/s][A
 13%|█▎        | 28/221 [00:18<01:09,  2.78it/s][A
 13%|█▎        | 29/221 [00:18<01:02,  3.09it/s][A
 14%|█▎        | 30/221 [00:19<01:11,  2.67it/s][A
 14%|█▍        | 31/221 [00:19<01:16,  2.49it/s][A
 14%|█▍        | 32/221 [00:19<01:00,  3.14it/s][A
 15%|█▍        | 33/221 [00:20<00:59,  3.17it/s][A
 16%|█▌        | 35/221 [00:20<00:45,  4.08it/s][A
 16%|█▋        | 36/221 [00:20<00:42,  4.39it/s][A
 17%|█▋        | 37/221 [00:21<00:52,  3.52it/s][A
 17%|█▋        | 38/221 [00:21<01:00,  3.04it/s][A
 18%|█▊        | 39/221 [00:22<01:10,  2.56it/s][A
 18%|█▊        | 40/221 [00:22<01:16,  2.36it/s][A
 19%|█▊        | 41/221 [00:22<01:02,  2.88it/s][A
 19%|█▉        | 42/221 [00:22<00:50,  3.53it/s][A
 19%|█▉        | 43/221 [00:23<00:41,  4.30it/s][A
 20%|█▉        | 44/221 [00:23<00:34,  5.08it/s][A
 20%|██        | 45/221 [00:26<03:23,  1.16s/it][A
 21%|██        | 46/221 [00:27<02:40,  1.09it/s][A
 21%|██▏       | 47/221 [00:27<02:16,  1.28it/s][A
 22%|██▏       | 48/221 [00:27<01:41,  1.70it/s][A
 22%|██▏       | 49/221 [00:27<01:30,  1.91it/s][A
 23%|██▎       | 50/221 [00:28<01:26,  1.98it/s][A
 23%|██▎       | 51/221 [00:28<01:09,  2.46it/s][A
 24%|██▎       | 52/221 [00:28<00:58,  2.87it/s][A
 24%|██▍       | 53/221 [00:29<01:01,  2.75it/s][A
 24%|██▍       | 54/221 [00:29<01:16,  2.19it/s][A
 25%|██▍       | 55/221 [00:31<01:59,  1.39it/s][A
 25%|██▌       | 56/221 [00:31<01:32,  1.78it/s][A
 26%|██▌       | 57/221 [00:31<01:14,  2.20it/s][A
 27%|██▋       | 59/221 [00:31<00:49,  3.30it/s][A
 27%|██▋       | 60/221 [00:32<00:49,  3.23it/s][A
 28%|██▊       | 61/221 [00:32<00:45,  3.52it/s][A
 28%|██▊       | 62/221 [00:32<00:46,  3.43it/s][A
 29%|██▊       | 63/221 [00:32<00:42,  3.71it/s][A
 29%|██▉       | 64/221 [00:33<01:13,  2.15it/s][A
 29%|██▉       | 65/221 [00:34<00:59,  2.64it/s][A
 30%|██▉       | 66/221 [00:34<01:01,  2.51it/s][A
 30%|███       | 67/221 [00:34<00:57,  2.68it/s][A
 31%|███       | 68/221 [00:35<00:49,  3.10it/s][A
 31%|███       | 69/221 [00:36<02:01,  1.25it/s][A
 32%|███▏      | 70/221 [00:37<01:30,  1.66it/s][A
 32%|███▏      | 71/221 [00:37<01:16,  1.95it/s][A
 33%|███▎      | 72/221 [00:37<01:09,  2.14it/s][A
 33%|███▎      | 73/221 [00:38<01:12,  2.03it/s][A
 33%|███▎      | 74/221 [00:38<00:55,  2.64it/s][A
 34%|███▍      | 75/221 [00:38<00:53,  2.73it/s][A
 34%|███▍      | 76/221 [00:39<00:51,  2.80it/s][A
 35%|███▍      | 77/221 [00:40<01:35,  1.51it/s][A
 35%|███▌      | 78/221 [00:40<01:14,  1.92it/s][A
 36%|███▌      | 79/221 [00:41<01:29,  1.59it/s][A
 36%|███▌      | 80/221 [00:41<01:07,  2.09it/s][A
 37%|███▋      | 81/221 [00:42<01:01,  2.29it/s][A
 37%|███▋      | 82/221 [00:45<02:52,  1.24s/it][A
 38%|███▊      | 83/221 [00:45<02:27,  1.07s/it][A
 38%|███▊      | 84/221 [00:46<01:53,  1.21it/s][A
 39%|███▉      | 86/221 [00:46<01:12,  1.85it/s][A
 39%|███▉      | 87/221 [00:46<01:08,  1.96it/s][A
 40%|███▉      | 88/221 [00:47<00:57,  2.33it/s][A
 40%|████      | 89/221 [00:47<00:51,  2.56it/s][A
 41%|████      | 90/221 [00:47<00:47,  2.76it/s][A
 41%|████      | 91/221 [00:47<00:38,  3.38it/s][A
 42%|████▏     | 92/221 [00:47<00:34,  3.75it/s][A
 42%|████▏     | 93/221 [00:48<00:39,  3.27it/s][A
 43%|████▎     | 94/221 [00:48<00:49,  2.59it/s][A
 43%|████▎     | 95/221 [00:49<00:47,  2.65it/s][A
 43%|████▎     | 96/221 [00:49<00:58,  2.15it/s][A
 44%|████▍     | 97/221 [00:50<00:46,  2.68it/s][A
 44%|████▍     | 98/221 [00:50<00:53,  2.32it/s][A
 45%|████▍     | 99/221 [00:51<00:51,  2.38it/s][A
 45%|████▌     | 100/221 [00:52<01:09,  1.75it/s][A
 46%|████▌     | 101/221 [00:52<00:55,  2.16it/s][A
 46%|████▌     | 102/221 [00:53<01:07,  1.76it/s][A
 47%|████▋     | 103/221 [00:53<00:50,  2.34it/s][A
 47%|████▋     | 104/221 [00:53<00:46,  2.51it/s][A
 48%|████▊     | 105/221 [00:53<00:48,  2.39it/s][A
 48%|████▊     | 106/221 [00:55<01:26,  1.32it/s][A
 48%|████▊     | 107/221 [00:55<01:13,  1.55it/s][A
 49%|████▉     | 108/221 [00:56<01:10,  1.61it/s][A
 49%|████▉     | 109/221 [00:56<01:01,  1.83it/s][A
 50%|████▉     | 110/221 [00:57<00:50,  2.21it/s][A
 50%|█████     | 111/221 [00:57<00:46,  2.38it/s][A
 51%|█████     | 112/221 [00:57<00:42,  2.56it/s][A
 51%|█████     | 113/221 [00:57<00:35,  3.04it/s][A
 52%|█████▏    | 115/221 [00:58<00:36,  2.92it/s][A
 52%|█████▏    | 116/221 [00:58<00:36,  2.91it/s][A
 53%|█████▎    | 117/221 [00:59<00:37,  2.77it/s][A
 53%|█████▎    | 118/221 [00:59<00:39,  2.64it/s][A
 54%|█████▍    | 119/221 [00:59<00:32,  3.11it/s][A
 54%|█████▍    | 120/221 [01:00<00:27,  3.65it/s][A
 55%|█████▍    | 121/221 [01:00<00:33,  2.94it/s][A
 55%|█████▌    | 122/221 [01:01<00:38,  2.59it/s][A
 56%|█████▌    | 123/221 [01:02<01:09,  1.41it/s][A
 56%|█████▌    | 124/221 [01:02<00:55,  1.75it/s][A
 57%|█████▋    | 125/221 [01:03<00:59,  1.62it/s][A
 57%|█████▋    | 126/221 [01:12<04:43,  2.99s/it][A
 57%|█████▋    | 127/221 [01:12<03:28,  2.22s/it][A
 58%|█████▊    | 128/221 [01:12<02:34,  1.66s/it][A
 58%|█████▊    | 129/221 [01:13<02:07,  1.39s/it][A
 59%|█████▉    | 130/221 [01:13<01:34,  1.04s/it][A
 59%|█████▉    | 131/221 [01:14<01:34,  1.05s/it][A
 60%|█████▉    | 132/221 [01:16<01:47,  1.21s/it][A
 60%|██████    | 133/221 [01:17<01:28,  1.01s/it][A
 61%|██████    | 134/221 [01:17<01:24,  1.03it/s][A
 61%|██████    | 135/221 [01:18<01:15,  1.15it/s][A
 62%|██████▏   | 136/221 [01:19<01:02,  1.37it/s][A
 62%|██████▏   | 137/221 [01:19<00:50,  1.65it/s][A
 62%|██████▏   | 138/221 [01:19<00:44,  1.87it/s][A
 63%|██████▎   | 139/221 [01:19<00:34,  2.36it/s][A
 63%|██████▎   | 140/221 [01:20<00:35,  2.29it/s][A
 64%|██████▍   | 141/221 [01:20<00:35,  2.26it/s][A
 64%|██████▍   | 142/221 [01:21<00:30,  2.59it/s][A
 65%|██████▍   | 143/221 [01:21<00:28,  2.75it/s][A
 65%|██████▌   | 144/221 [01:21<00:23,  3.32it/s][A
 66%|██████▌   | 145/221 [01:21<00:19,  3.91it/s][A
 66%|██████▌   | 146/221 [01:21<00:15,  4.71it/s][A
 67%|██████▋   | 148/221 [01:23<00:39,  1.85it/s][A
 67%|██████▋   | 149/221 [01:23<00:34,  2.09it/s][A
 68%|██████▊   | 150/221 [01:24<00:33,  2.14it/s][A
 68%|██████▊   | 151/221 [01:24<00:29,  2.39it/s][A
 69%|██████▉   | 152/221 [01:25<00:27,  2.47it/s][A
 69%|██████▉   | 153/221 [01:25<00:22,  3.06it/s][A
 70%|██████▉   | 154/221 [01:25<00:20,  3.34it/s][A
 70%|███████   | 155/221 [01:25<00:17,  3.72it/s][A
 71%|███████   | 156/221 [01:25<00:15,  4.25it/s][A
 71%|███████   | 157/221 [01:32<02:15,  2.12s/it][A
 71%|███████▏  | 158/221 [01:33<01:50,  1.76s/it][A
 72%|███████▏  | 159/221 [01:33<01:18,  1.27s/it][A
 72%|███████▏  | 160/221 [01:33<00:57,  1.07it/s][A
 73%|███████▎  | 161/221 [01:33<00:41,  1.45it/s][A
 74%|███████▍  | 163/221 [01:33<00:25,  2.25it/s][A
 74%|███████▍  | 164/221 [01:34<00:21,  2.71it/s][A
 75%|███████▍  | 165/221 [01:34<00:18,  3.04it/s][A
 75%|███████▌  | 166/221 [01:35<00:24,  2.23it/s][A
 76%|███████▌  | 167/221 [01:35<00:22,  2.35it/s][A
 76%|███████▌  | 168/221 [01:37<00:46,  1.13it/s][A
 76%|███████▋  | 169/221 [01:38<00:42,  1.21it/s][A
 77%|███████▋  | 170/221 [01:38<00:36,  1.40it/s][A
 77%|███████▋  | 171/221 [01:38<00:30,  1.65it/s][A
 78%|███████▊  | 172/221 [01:39<00:25,  1.96it/s][A
 78%|███████▊  | 173/221 [01:39<00:22,  2.18it/s][A
 79%|███████▊  | 174/221 [01:39<00:16,  2.81it/s][A
 79%|███████▉  | 175/221 [01:39<00:15,  2.98it/s][A
 80%|███████▉  | 176/221 [01:40<00:15,  2.90it/s][A
 80%|████████  | 177/221 [01:40<00:14,  2.99it/s][A
 81%|████████  | 178/221 [01:40<00:14,  3.06it/s][A
 81%|████████  | 179/221 [01:41<00:13,  3.01it/s][A
 81%|████████▏ | 180/221 [01:41<00:12,  3.37it/s][A
 82%|████████▏ | 181/221 [01:41<00:09,  4.06it/s][A
 82%|████████▏ | 182/221 [01:42<00:11,  3.39it/s][A
 83%|████████▎ | 183/221 [01:42<00:15,  2.47it/s][A
 83%|████████▎ | 184/221 [01:43<00:15,  2.39it/s][A
 84%|████████▎ | 185/221 [01:43<00:13,  2.59it/s][A
 84%|████████▍ | 186/221 [01:43<00:12,  2.90it/s][A
 85%|████████▍ | 187/221 [01:43<00:10,  3.14it/s][A
 85%|████████▌ | 188/221 [01:44<00:09,  3.58it/s][A
 86%|████████▌ | 189/221 [01:44<00:08,  3.70it/s][A
 86%|████████▌ | 190/221 [01:44<00:09,  3.35it/s][A
 86%|████████▋ | 191/221 [01:44<00:08,  3.71it/s][A
 87%|████████▋ | 192/221 [01:45<00:14,  2.03it/s][A
 87%|████████▋ | 193/221 [01:46<00:11,  2.40it/s][A
 88%|████████▊ | 194/221 [01:46<00:10,  2.51it/s][A
 88%|████████▊ | 195/221 [01:46<00:08,  2.96it/s][A
 89%|████████▊ | 196/221 [01:47<00:10,  2.37it/s][A
 89%|████████▉ | 197/221 [01:47<00:09,  2.62it/s][A
 90%|████████▉ | 198/221 [01:47<00:07,  2.90it/s][A
 90%|█████████ | 199/221 [01:48<00:06,  3.31it/s][A
 90%|█████████ | 200/221 [01:48<00:09,  2.14it/s][A
 91%|█████████ | 201/221 [01:50<00:12,  1.54it/s][A
 91%|█████████▏| 202/221 [01:50<00:10,  1.85it/s][A
 92%|█████████▏| 203/221 [01:51<00:11,  1.61it/s][A
 92%|█████████▏| 204/221 [01:51<00:10,  1.65it/s][A
 93%|█████████▎| 206/221 [01:52<00:06,  2.29it/s][A
 94%|█████████▎| 207/221 [01:52<00:05,  2.74it/s][A
 94%|█████████▍| 208/221 [01:52<00:05,  2.30it/s][A
 95%|█████████▍| 209/221 [01:53<00:04,  2.90it/s][A
 95%|█████████▌| 210/221 [01:53<00:03,  3.30it/s][A
 95%|█████████▌| 211/221 [01:53<00:03,  2.94it/s][A
 96%|█████████▌| 212/221 [01:54<00:03,  2.99it/s][A
 96%|█████████▋| 213/221 [01:54<00:02,  3.38it/s][A
 97%|█████████▋| 214/221 [01:54<00:02,  3.04it/s][A
 97%|█████████▋| 215/221 [01:55<00:02,  2.41it/s][A
 98%|█████████▊| 216/221 [01:55<00:01,  2.70it/s][A
 98%|█████████▊| 217/221 [01:57<00:03,  1.05it/s][A
 99%|█████████▊| 218/221 [01:58<00:02,  1.31it/s][A
 99%|█████████▉| 219/221 [01:58<00:01,  1.57it/s][A
100%|█████████▉| 220/221 [02:03<00:01,  1.95s/it][A
100%|██████████| 221/221 [02:03<00:00,  1.42s/it][A100%|██████████| 221/221 [02:03<00:00,  1.79it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:51,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:27<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:23,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:55<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:55,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:23<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:51<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:08,  3.22it/s][A
  1%|          | 2/221 [00:01<02:35,  1.41it/s][A
  1%|▏         | 3/221 [00:01<01:43,  2.11it/s][A
  2%|▏         | 4/221 [00:02<01:58,  1.84it/s][A
  2%|▏         | 5/221 [00:02<02:06,  1.70it/s][A
  3%|▎         | 6/221 [00:03<01:48,  1.98it/s][A
  3%|▎         | 7/221 [00:03<01:41,  2.10it/s][A
  4%|▎         | 8/221 [00:05<02:46,  1.28it/s][A
  4%|▍         | 9/221 [00:05<02:50,  1.24it/s][A
  5%|▍         | 10/221 [00:06<02:16,  1.55it/s][A
  5%|▍         | 11/221 [00:06<01:55,  1.82it/s][A
  5%|▌         | 12/221 [00:06<01:43,  2.01it/s][A
  6%|▌         | 13/221 [00:07<01:22,  2.51it/s][A
  6%|▋         | 14/221 [00:07<01:06,  3.12it/s][A
  7%|▋         | 15/221 [00:07<00:59,  3.47it/s][A
  7%|▋         | 16/221 [00:08<01:33,  2.18it/s][A
  8%|▊         | 17/221 [00:09<02:05,  1.62it/s][A
  8%|▊         | 18/221 [00:09<01:39,  2.03it/s][A
  9%|▊         | 19/221 [00:10<01:53,  1.79it/s][A
  9%|▉         | 20/221 [00:10<01:44,  1.92it/s][A
 10%|▉         | 21/221 [00:11<01:49,  1.82it/s][A
 10%|▉         | 22/221 [00:12<02:36,  1.27it/s][A
 10%|█         | 23/221 [00:12<02:04,  1.59it/s][A
 11%|█         | 24/221 [00:13<01:54,  1.72it/s][A
 11%|█▏        | 25/221 [00:14<02:05,  1.56it/s][A
 12%|█▏        | 26/221 [00:14<02:12,  1.47it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.90it/s][A
 13%|█▎        | 28/221 [00:16<02:29,  1.29it/s][A
 13%|█▎        | 29/221 [00:16<01:57,  1.63it/s][A
 14%|█▎        | 30/221 [00:17<01:59,  1.60it/s][A
 14%|█▍        | 31/221 [00:17<01:52,  1.68it/s][A
 14%|█▍        | 32/221 [00:18<02:27,  1.28it/s][A
 15%|█▍        | 33/221 [00:19<02:14,  1.40it/s][A
 15%|█▌        | 34/221 [00:19<01:46,  1.75it/s][A
 16%|█▌        | 35/221 [00:20<01:32,  2.02it/s][A
 16%|█▋        | 36/221 [00:20<01:38,  1.88it/s][A
 17%|█▋        | 37/221 [00:21<01:31,  2.02it/s][A
 17%|█▋        | 38/221 [00:22<02:00,  1.52it/s][A
 18%|█▊        | 39/221 [00:22<02:09,  1.41it/s][A
 18%|█▊        | 40/221 [00:24<02:31,  1.19it/s][A
 19%|█▊        | 41/221 [00:24<01:57,  1.53it/s][A
 19%|█▉        | 42/221 [00:24<01:37,  1.84it/s][A
 19%|█▉        | 43/221 [00:24<01:17,  2.29it/s][A
 20%|█▉        | 44/221 [00:25<01:24,  2.10it/s][A
 20%|██        | 45/221 [00:25<01:04,  2.72it/s][A
 21%|██        | 46/221 [00:25<01:00,  2.88it/s][A
 21%|██▏       | 47/221 [00:26<01:12,  2.40it/s][A
 22%|██▏       | 48/221 [00:26<01:00,  2.85it/s][A
 22%|██▏       | 49/221 [00:27<01:27,  1.96it/s][A
 23%|██▎       | 50/221 [00:27<01:29,  1.92it/s][A
 23%|██▎       | 51/221 [00:28<01:16,  2.21it/s][A
 24%|██▎       | 52/221 [00:28<01:10,  2.41it/s][A
 24%|██▍       | 53/221 [00:29<01:11,  2.35it/s][A
 24%|██▍       | 54/221 [00:29<01:17,  2.14it/s][A
 25%|██▍       | 55/221 [00:29<01:09,  2.38it/s][A
 25%|██▌       | 56/221 [00:30<01:11,  2.29it/s][A
 26%|██▌       | 57/221 [00:30<01:02,  2.63it/s][A
 26%|██▌       | 58/221 [00:30<00:55,  2.91it/s][A
 27%|██▋       | 59/221 [00:31<00:44,  3.65it/s][A
 27%|██▋       | 60/221 [00:31<00:59,  2.71it/s][A
 28%|██▊       | 61/221 [00:32<01:13,  2.17it/s][A
 28%|██▊       | 62/221 [00:32<01:11,  2.22it/s][A
 29%|██▊       | 63/221 [00:33<01:35,  1.66it/s][A
 29%|██▉       | 64/221 [00:35<02:10,  1.21it/s][A
 29%|██▉       | 65/221 [00:36<02:25,  1.07it/s][A
 30%|██▉       | 66/221 [00:36<02:04,  1.24it/s][A
 30%|███       | 67/221 [00:37<01:44,  1.47it/s][A
 31%|███       | 68/221 [00:37<01:26,  1.78it/s][A
 31%|███       | 69/221 [00:37<01:14,  2.05it/s][A
 32%|███▏      | 70/221 [00:38<01:16,  1.96it/s][A
 32%|███▏      | 71/221 [00:38<01:25,  1.76it/s][A
 33%|███▎      | 72/221 [00:39<01:22,  1.80it/s][A
 33%|███▎      | 73/221 [00:40<01:27,  1.70it/s][A
 33%|███▎      | 74/221 [00:40<01:22,  1.78it/s][A
 34%|███▍      | 75/221 [00:40<01:10,  2.06it/s][A
 34%|███▍      | 76/221 [00:41<01:05,  2.22it/s][A
 35%|███▍      | 77/221 [00:41<00:54,  2.64it/s][A
 35%|███▌      | 78/221 [00:41<00:56,  2.54it/s][A
 36%|███▌      | 79/221 [00:42<01:17,  1.84it/s][A
 36%|███▌      | 80/221 [00:43<01:01,  2.29it/s][A
 37%|███▋      | 81/221 [00:43<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:44<01:34,  1.47it/s][A
 38%|███▊      | 83/221 [00:45<01:18,  1.75it/s][A
 38%|███▊      | 84/221 [00:46<01:30,  1.52it/s][A
 38%|███▊      | 85/221 [00:46<01:10,  1.93it/s][A
 39%|███▉      | 86/221 [00:46<01:10,  1.93it/s][A
 39%|███▉      | 87/221 [00:47<01:07,  1.99it/s][A
 40%|███▉      | 88/221 [00:47<00:57,  2.33it/s][A
 40%|████      | 89/221 [00:48<01:04,  2.04it/s][A
 41%|████      | 90/221 [00:48<01:12,  1.81it/s][A
 41%|████      | 91/221 [00:49<01:01,  2.11it/s][A
 42%|████▏     | 92/221 [00:49<01:02,  2.05it/s][A
 43%|████▎     | 94/221 [00:49<00:44,  2.86it/s][A
 43%|████▎     | 95/221 [00:50<00:43,  2.92it/s][A
 43%|████▎     | 96/221 [00:50<00:42,  2.95it/s][A
 44%|████▍     | 97/221 [00:51<00:46,  2.68it/s][A
 44%|████▍     | 98/221 [00:51<01:01,  1.99it/s][A
 45%|████▍     | 99/221 [00:52<01:06,  1.82it/s][A
 45%|████▌     | 100/221 [00:53<01:15,  1.60it/s][A
 46%|████▌     | 101/221 [00:54<01:18,  1.53it/s][A
 46%|████▌     | 102/221 [00:55<01:26,  1.37it/s][A
 47%|████▋     | 103/221 [00:55<01:14,  1.57it/s][A
 47%|████▋     | 104/221 [00:56<01:17,  1.52it/s][A
 48%|████▊     | 105/221 [00:56<01:05,  1.77it/s][A
 48%|████▊     | 106/221 [00:56<00:52,  2.17it/s][A
 48%|████▊     | 107/221 [00:57<00:50,  2.27it/s][A
 49%|████▉     | 108/221 [00:57<00:49,  2.29it/s][A
 49%|████▉     | 109/221 [00:58<00:53,  2.07it/s][A
 50%|████▉     | 110/221 [00:58<01:03,  1.75it/s][A
 50%|█████     | 111/221 [00:59<01:18,  1.40it/s][A
 51%|█████     | 112/221 [01:00<01:07,  1.60it/s][A
 51%|█████     | 113/221 [01:00<00:55,  1.94it/s][A
 52%|█████▏    | 114/221 [01:01<00:53,  2.01it/s][A
 52%|█████▏    | 115/221 [01:01<00:57,  1.86it/s][A
 52%|█████▏    | 116/221 [01:01<00:45,  2.31it/s][A
 53%|█████▎    | 117/221 [01:02<00:42,  2.42it/s][A
 53%|█████▎    | 118/221 [01:02<00:44,  2.34it/s][A
 54%|█████▍    | 119/221 [01:03<00:49,  2.05it/s][A
 54%|█████▍    | 120/221 [01:03<00:51,  1.95it/s][A
 55%|█████▍    | 121/221 [01:04<00:48,  2.07it/s][A
 55%|█████▌    | 122/221 [01:04<00:50,  1.95it/s][A
 56%|█████▌    | 123/221 [01:05<00:45,  2.15it/s][A
 56%|█████▌    | 124/221 [01:06<01:04,  1.50it/s][A
 57%|█████▋    | 125/221 [01:06<00:54,  1.78it/s][A
 57%|█████▋    | 126/221 [01:06<00:42,  2.22it/s][A
 57%|█████▋    | 127/221 [01:07<00:58,  1.60it/s][A
 58%|█████▊    | 128/221 [01:08<00:54,  1.71it/s][A
 58%|█████▊    | 129/221 [01:09<00:57,  1.59it/s][A
 59%|█████▉    | 130/221 [01:09<00:45,  2.00it/s][A
 59%|█████▉    | 131/221 [01:09<00:43,  2.06it/s][A
 60%|█████▉    | 132/221 [01:10<00:53,  1.65it/s][A
 60%|██████    | 133/221 [01:11<00:53,  1.63it/s][A
 61%|██████    | 134/221 [01:11<00:52,  1.67it/s][A
 61%|██████    | 135/221 [01:12<00:46,  1.84it/s][A
 62%|██████▏   | 136/221 [01:13<00:58,  1.46it/s][A
 62%|██████▏   | 137/221 [01:14<00:59,  1.42it/s][A
 62%|██████▏   | 138/221 [01:14<01:00,  1.37it/s][A
 63%|██████▎   | 139/221 [01:16<01:11,  1.14it/s][A
 63%|██████▎   | 140/221 [01:16<01:09,  1.16it/s][A
 64%|██████▍   | 141/221 [01:17<01:01,  1.30it/s][A
 64%|██████▍   | 142/221 [01:18<00:56,  1.41it/s][A
 65%|██████▍   | 143/221 [01:18<00:44,  1.77it/s][A
 65%|██████▌   | 144/221 [01:18<00:41,  1.87it/s][A
 66%|██████▌   | 145/221 [01:19<00:38,  1.95it/s][A
 66%|██████▌   | 146/221 [01:19<00:29,  2.50it/s][A
 67%|██████▋   | 147/221 [01:19<00:33,  2.20it/s][A
 67%|██████▋   | 148/221 [01:20<00:30,  2.36it/s][A
 68%|██████▊   | 150/221 [01:20<00:23,  3.06it/s][A
 68%|██████▊   | 151/221 [01:20<00:19,  3.54it/s][A
 69%|██████▉   | 152/221 [01:21<00:20,  3.37it/s][A
 69%|██████▉   | 153/221 [01:21<00:19,  3.56it/s][A
 70%|██████▉   | 154/221 [01:22<00:31,  2.15it/s][A
 70%|███████   | 155/221 [01:23<00:39,  1.65it/s][A
 71%|███████   | 156/221 [01:23<00:36,  1.80it/s][A
 71%|███████   | 157/221 [01:24<00:37,  1.70it/s][A
 71%|███████▏  | 158/221 [01:24<00:31,  1.99it/s][A
 72%|███████▏  | 159/221 [01:24<00:24,  2.53it/s][A
 72%|███████▏  | 160/221 [01:25<00:26,  2.30it/s][A
 73%|███████▎  | 161/221 [01:25<00:21,  2.77it/s][A
 73%|███████▎  | 162/221 [01:26<00:25,  2.35it/s][A
 74%|███████▍  | 163/221 [01:26<00:21,  2.70it/s][A
 74%|███████▍  | 164/221 [01:26<00:21,  2.70it/s][A
 75%|███████▍  | 165/221 [01:26<00:18,  3.09it/s][A
 75%|███████▌  | 166/221 [01:27<00:27,  2.03it/s][A
 76%|███████▌  | 167/221 [01:28<00:23,  2.27it/s][A
 76%|███████▌  | 168/221 [01:28<00:21,  2.48it/s][A
 76%|███████▋  | 169/221 [01:29<00:36,  1.43it/s][A
 77%|███████▋  | 170/221 [01:30<00:30,  1.65it/s][A
 77%|███████▋  | 171/221 [01:30<00:30,  1.66it/s][A
 78%|███████▊  | 172/221 [01:31<00:24,  2.03it/s][A
 78%|███████▊  | 173/221 [01:31<00:20,  2.36it/s][A
 79%|███████▊  | 174/221 [01:31<00:18,  2.51it/s][A
 79%|███████▉  | 175/221 [01:32<00:18,  2.48it/s][A
 80%|███████▉  | 176/221 [01:32<00:16,  2.66it/s][A
 80%|████████  | 177/221 [01:32<00:17,  2.49it/s][A
 81%|████████  | 178/221 [01:33<00:16,  2.54it/s][A
 81%|████████  | 179/221 [01:34<00:20,  2.02it/s][A
 81%|████████▏ | 180/221 [01:34<00:15,  2.64it/s][A
 82%|████████▏ | 181/221 [01:34<00:12,  3.09it/s][A
 82%|████████▏ | 182/221 [01:34<00:13,  2.82it/s][A
 83%|████████▎ | 183/221 [01:35<00:15,  2.49it/s][A
 83%|████████▎ | 184/221 [01:36<00:18,  1.96it/s][A
 84%|████████▎ | 185/221 [01:36<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:36<00:15,  2.32it/s][A
 85%|████████▍ | 187/221 [01:37<00:18,  1.84it/s][A
 85%|████████▌ | 188/221 [01:37<00:14,  2.21it/s][A
 86%|████████▌ | 189/221 [01:38<00:17,  1.81it/s][A
 86%|████████▌ | 190/221 [01:38<00:15,  2.06it/s][A
 86%|████████▋ | 191/221 [01:39<00:14,  2.02it/s][A
 87%|████████▋ | 192/221 [01:40<00:15,  1.93it/s][A
 87%|████████▋ | 193/221 [01:40<00:12,  2.21it/s][A
 88%|████████▊ | 194/221 [01:41<00:16,  1.63it/s][A
 88%|████████▊ | 195/221 [01:42<00:19,  1.36it/s][A
 89%|████████▊ | 196/221 [01:42<00:16,  1.51it/s][A
 89%|████████▉ | 197/221 [01:43<00:15,  1.53it/s][A
 90%|████████▉ | 198/221 [01:44<00:15,  1.51it/s][A
 90%|█████████ | 199/221 [01:44<00:12,  1.79it/s][A
 90%|█████████ | 200/221 [01:45<00:13,  1.61it/s][A
 91%|█████████ | 201/221 [01:45<00:12,  1.65it/s][A
 91%|█████████▏| 202/221 [01:46<00:09,  1.92it/s][A
 92%|█████████▏| 203/221 [01:46<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:46<00:07,  2.30it/s][A
 93%|█████████▎| 205/221 [01:47<00:06,  2.47it/s][A
 93%|█████████▎| 206/221 [01:47<00:05,  2.64it/s][A
 94%|█████████▎| 207/221 [01:47<00:04,  2.99it/s][A
 94%|█████████▍| 208/221 [01:49<00:07,  1.65it/s][A
 95%|█████████▍| 209/221 [01:49<00:06,  1.72it/s][A
 95%|█████████▌| 210/221 [01:49<00:05,  2.15it/s][A
 95%|█████████▌| 211/221 [01:50<00:06,  1.66it/s][A
 96%|█████████▌| 212/221 [01:50<00:04,  2.04it/s][A
 96%|█████████▋| 213/221 [01:51<00:04,  1.93it/s][A
 97%|█████████▋| 214/221 [01:52<00:04,  1.69it/s][A
 97%|█████████▋| 215/221 [01:52<00:02,  2.20it/s][A
 98%|█████████▊| 216/221 [01:52<00:02,  1.98it/s][A
 98%|█████████▊| 217/221 [01:53<00:02,  1.82it/s][A
 99%|█████████▊| 218/221 [01:53<00:01,  2.19it/s][A
 99%|█████████▉| 219/221 [01:54<00:00,  2.04it/s][A
100%|█████████▉| 220/221 [01:54<00:00,  2.21it/s][A
100%|██████████| 221/221 [01:55<00:00,  1.97it/s][A100%|██████████| 221/221 [01:55<00:00,  1.91it/s]
09/19/2024 13:06:16 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 7999--===========

09/19/2024 13:06:16 - INFO - __main__ -   {'area_r1': 45.5, 'area_recall': '45.5/75.3/83.5', 'area_ravg': 68.1}
09/19/2024 13:06:16 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 7999--===========

09/19/2024 13:06:16 - INFO - __main__ -   {'forward_r1': 51.2, 'forward_recall': '51.2/78.7/88.1', 'forward_ravg': 72.7}
09/19/2024 13:06:16 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 7999--===========

09/19/2024 13:06:16 - INFO - __main__ -   {'area_video_r1': 49.4, 'area_video_recall': '49.4/78.7/87.8', 'area_video_ravg': 72.0}
09/19/2024 13:06:16 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 13:06:16 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 13:06:16 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 7999--===========

09/19/2024 13:06:16 - INFO - __main__ -   {'area_video_r1': 63.5, 'area_video_recall': '63.5/83.9/89.4', 'area_video_ravg': 78.9, 'area_video_back_r1': 63.5, 'area_video_back_recall': '63.5/85.7/92.4', 'area_video_back_ravg': 80.5}
09/19/2024 13:06:16 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 6999=======

09/19/2024 13:06:16 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/84.2/89.6', 'area_video_ravg': 79.2, 'area_video_back_r1': 64.1, 'area_video_back_recall': '64.1/85.5/92.1', 'area_video_back_ravg': 80.6}
09/19/2024 13:06:16 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 7999--===========

09/19/2024 13:06:16 - INFO - __main__ -   {'video_r1': 31.2, 'video_recall': '31.2/56.3/66.6', 'video_ravg': 51.4}
09/19/2024 13:06:16 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 13:06:16 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 13:06:16 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 7999--===========

09/19/2024 13:06:16 - INFO - __main__ -   {'video_r1': 61.2, 'video_recall': '61.2/81.4/85.5', 'video_ravg': 76.1}
09/19/2024 13:06:16 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 7499=======

09/19/2024 13:06:16 - INFO - __main__ -   {'video_r1': 61.3, 'video_recall': '61.3/81.2/85.5', 'video_ravg': 76.0}
09/19/2024 13:06:36 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01386195421218872, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0382883548736572, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0521502494812012}
 90%|████████▉ | 8000/8917 [10:52:02<42:39:14, 167.45s/it] 90%|████████▉ | 8001/8917 [10:52:05<30:04:11, 118.18s/it] 90%|████████▉ | 8002/8917 [10:52:08<21:17:45, 83.79s/it]  90%|████████▉ | 8003/8917 [10:52:12<15:09:45, 59.72s/it] 90%|████████▉ | 8004/8917 [10:52:16<10:52:01, 42.85s/it] 90%|████████▉ | 8005/8917 [10:52:19<7:53:01, 31.12s/it]  90%|████████▉ | 8006/8917 [10:52:23<5:47:01, 22.86s/it] 90%|████████▉ | 8007/8917 [10:52:27<4:20:29, 17.18s/it] 90%|████████▉ | 8008/8917 [10:52:30<3:18:53, 13.13s/it] 90%|████████▉ | 8009/8917 [10:52:34<2:36:48, 10.36s/it] 90%|████████▉ | 8010/8917 [10:52:38<2:05:33,  8.31s/it] 90%|████████▉ | 8011/8917 [10:52:42<1:45:05,  6.96s/it] 90%|████████▉ | 8012/8917 [10:52:46<1:31:21,  6.06s/it] 90%|████████▉ | 8013/8917 [10:52:49<1:19:59,  5.31s/it] 90%|████████▉ | 8014/8917 [10:52:53<1:12:27,  4.81s/it] 90%|████████▉ | 8015/8917 [10:52:56<1:05:44,  4.37s/it] 90%|████████▉ | 8016/8917 [10:53:00<1:03:17,  4.21s/it] 90%|████████▉ | 8017/8917 [10:53:04<1:00:29,  4.03s/it] 90%|████████▉ | 8018/8917 [10:53:07<58:54,  3.93s/it]   90%|████████▉ | 8019/8917 [10:53:11<57:19,  3.83s/it] 90%|████████▉ | 8020/8917 [10:53:14<55:34,  3.72s/it] 90%|████████▉ | 8021/8917 [10:53:18<55:49,  3.74s/it] 90%|████████▉ | 8022/8917 [10:53:22<54:43,  3.67s/it] 90%|████████▉ | 8023/8917 [10:53:25<54:21,  3.65s/it] 90%|████████▉ | 8024/8917 [10:53:29<54:58,  3.69s/it] 90%|████████▉ | 8025/8917 [10:53:33<54:23,  3.66s/it] 90%|█████████ | 8026/8917 [10:53:36<54:33,  3.67s/it] 90%|█████████ | 8027/8917 [10:53:40<54:56,  3.70s/it] 90%|█████████ | 8028/8917 [10:53:44<54:47,  3.70s/it] 90%|█████████ | 8029/8917 [10:53:48<55:12,  3.73s/it] 90%|█████████ | 8030/8917 [10:53:51<54:45,  3.70s/it] 90%|█████████ | 8031/8917 [10:53:55<56:09,  3.80s/it] 90%|█████████ | 8032/8917 [10:53:59<54:23,  3.69s/it] 90%|█████████ | 8033/8917 [10:54:02<53:27,  3.63s/it] 90%|█████████ | 8034/8917 [10:54:06<54:46,  3.72s/it] 90%|█████████ | 8035/8917 [10:54:10<54:12,  3.69s/it] 90%|█████████ | 8036/8917 [10:54:14<57:21,  3.91s/it] 90%|█████████ | 8037/8917 [10:54:18<55:53,  3.81s/it] 90%|█████████ | 8038/8917 [10:54:21<54:20,  3.71s/it] 90%|█████████ | 8039/8917 [10:54:25<53:24,  3.65s/it] 90%|█████████ | 8040/8917 [10:54:28<53:34,  3.67s/it] 90%|█████████ | 8041/8917 [10:54:32<54:55,  3.76s/it] 90%|█████████ | 8042/8917 [10:54:36<54:40,  3.75s/it] 90%|█████████ | 8043/8917 [10:54:40<53:29,  3.67s/it] 90%|█████████ | 8044/8917 [10:54:43<52:57,  3.64s/it] 90%|█████████ | 8045/8917 [10:54:47<51:48,  3.56s/it] 90%|█████████ | 8046/8917 [10:54:51<53:01,  3.65s/it] 90%|█████████ | 8047/8917 [10:54:54<54:24,  3.75s/it] 90%|█████████ | 8048/8917 [10:54:58<54:29,  3.76s/it] 90%|█████████ | 8049/8917 [10:55:02<53:43,  3.71s/it]09/19/2024 13:09:40 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.023079359903931618, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9856939911842346, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0087733268737793}
 90%|█████████ | 8050/8917 [10:55:06<53:45,  3.72s/it] 90%|█████████ | 8051/8917 [10:55:09<54:22,  3.77s/it] 90%|█████████ | 8052/8917 [10:55:13<54:30,  3.78s/it] 90%|█████████ | 8053/8917 [10:55:17<52:43,  3.66s/it] 90%|█████████ | 8054/8917 [10:55:20<52:47,  3.67s/it] 90%|█████████ | 8055/8917 [10:55:24<52:00,  3.62s/it] 90%|█████████ | 8056/8917 [10:55:28<53:37,  3.74s/it] 90%|█████████ | 8057/8917 [10:55:31<52:11,  3.64s/it] 90%|█████████ | 8058/8917 [10:55:35<51:49,  3.62s/it] 90%|█████████ | 8059/8917 [10:55:38<51:48,  3.62s/it] 90%|█████████ | 8060/8917 [10:55:42<51:04,  3.58s/it] 90%|█████████ | 8061/8917 [10:55:46<52:42,  3.69s/it] 90%|█████████ | 8062/8917 [10:55:50<54:05,  3.80s/it] 90%|█████████ | 8063/8917 [10:55:53<52:41,  3.70s/it] 90%|█████████ | 8064/8917 [10:55:57<53:24,  3.76s/it] 90%|█████████ | 8065/8917 [10:56:01<52:37,  3.71s/it] 90%|█████████ | 8066/8917 [10:56:05<53:08,  3.75s/it] 90%|█████████ | 8067/8917 [10:56:09<53:18,  3.76s/it] 90%|█████████ | 8068/8917 [10:56:12<52:35,  3.72s/it] 90%|█████████ | 8069/8917 [10:56:16<52:05,  3.69s/it] 91%|█████████ | 8070/8917 [10:56:20<52:15,  3.70s/it] 91%|█████████ | 8071/8917 [10:56:23<52:23,  3.72s/it] 91%|█████████ | 8072/8917 [10:56:27<54:27,  3.87s/it] 91%|█████████ | 8073/8917 [10:56:31<53:05,  3.77s/it] 91%|█████████ | 8074/8917 [10:56:35<52:39,  3.75s/it] 91%|█████████ | 8075/8917 [10:56:38<52:24,  3.73s/it] 91%|█████████ | 8076/8917 [10:56:42<52:28,  3.74s/it] 91%|█████████ | 8077/8917 [10:56:46<51:43,  3.69s/it] 91%|█████████ | 8078/8917 [10:56:49<51:15,  3.67s/it] 91%|█████████ | 8079/8917 [10:56:53<52:38,  3.77s/it] 91%|█████████ | 8080/8917 [10:56:57<51:24,  3.69s/it] 91%|█████████ | 8081/8917 [10:57:01<51:41,  3.71s/it] 91%|█████████ | 8082/8917 [10:57:04<50:39,  3.64s/it] 91%|█████████ | 8083/8917 [10:57:08<51:30,  3.71s/it] 91%|█████████ | 8084/8917 [10:57:12<51:50,  3.73s/it] 91%|█████████ | 8085/8917 [10:57:16<52:27,  3.78s/it] 91%|█████████ | 8086/8917 [10:57:19<52:01,  3.76s/it] 91%|█████████ | 8087/8917 [10:57:23<51:43,  3.74s/it] 91%|█████████ | 8088/8917 [10:57:27<50:38,  3.66s/it] 91%|█████████ | 8089/8917 [10:57:30<49:48,  3.61s/it] 91%|█████████ | 8090/8917 [10:57:34<50:41,  3.68s/it] 91%|█████████ | 8091/8917 [10:57:38<52:03,  3.78s/it] 91%|█████████ | 8092/8917 [10:57:41<50:04,  3.64s/it] 91%|█████████ | 8093/8917 [10:57:45<50:45,  3.70s/it] 91%|█████████ | 8094/8917 [10:57:48<49:25,  3.60s/it] 91%|█████████ | 8095/8917 [10:57:52<49:51,  3.64s/it] 91%|█████████ | 8096/8917 [10:57:56<48:55,  3.58s/it] 91%|█████████ | 8097/8917 [10:57:59<50:04,  3.66s/it] 91%|█████████ | 8098/8917 [10:58:03<49:02,  3.59s/it] 91%|█████████ | 8099/8917 [10:58:07<49:08,  3.60s/it]09/19/2024 13:12:44 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01574805937707424, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2516381740570068, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2673861980438232}
 91%|█████████ | 8100/8917 [10:58:10<49:09,  3.61s/it] 91%|█████████ | 8101/8917 [10:58:14<50:23,  3.71s/it] 91%|█████████ | 8102/8917 [10:58:18<49:15,  3.63s/it] 91%|█████████ | 8103/8917 [10:58:21<49:28,  3.65s/it] 91%|█████████ | 8104/8917 [10:58:25<49:00,  3.62s/it] 91%|█████████ | 8105/8917 [10:58:29<49:32,  3.66s/it] 91%|█████████ | 8106/8917 [10:58:32<50:36,  3.74s/it] 91%|█████████ | 8107/8917 [10:58:36<50:48,  3.76s/it] 91%|█████████ | 8108/8917 [10:58:40<50:37,  3.75s/it] 91%|█████████ | 8109/8917 [10:58:44<50:22,  3.74s/it] 91%|█████████ | 8110/8917 [10:58:47<49:35,  3.69s/it] 91%|█████████ | 8111/8917 [10:58:51<49:57,  3.72s/it] 91%|█████████ | 8112/8917 [10:58:55<51:01,  3.80s/it] 91%|█████████ | 8113/8917 [10:58:58<49:25,  3.69s/it] 91%|█████████ | 8114/8917 [10:59:02<48:34,  3.63s/it] 91%|█████████ | 8115/8917 [10:59:06<49:01,  3.67s/it] 91%|█████████ | 8116/8917 [10:59:09<49:09,  3.68s/it] 91%|█████████ | 8117/8917 [10:59:13<49:10,  3.69s/it] 91%|█████████ | 8118/8917 [10:59:17<48:49,  3.67s/it] 91%|█████████ | 8119/8917 [10:59:21<49:28,  3.72s/it] 91%|█████████ | 8120/8917 [10:59:24<49:29,  3.73s/it] 91%|█████████ | 8121/8917 [10:59:28<49:50,  3.76s/it] 91%|█████████ | 8122/8917 [10:59:32<50:06,  3.78s/it] 91%|█████████ | 8123/8917 [10:59:35<48:50,  3.69s/it] 91%|█████████ | 8124/8917 [10:59:39<49:32,  3.75s/it] 91%|█████████ | 8125/8917 [10:59:43<50:30,  3.83s/it] 91%|█████████ | 8126/8917 [10:59:47<49:58,  3.79s/it] 91%|█████████ | 8127/8917 [10:59:51<49:57,  3.79s/it] 91%|█████████ | 8128/8917 [10:59:54<49:05,  3.73s/it] 91%|█████████ | 8129/8917 [10:59:58<48:10,  3.67s/it] 91%|█████████ | 8130/8917 [11:00:02<47:51,  3.65s/it] 91%|█████████ | 8131/8917 [11:00:05<48:22,  3.69s/it] 91%|█████████ | 8132/8917 [11:00:10<49:56,  3.82s/it] 91%|█████████ | 8133/8917 [11:00:13<49:18,  3.77s/it] 91%|█████████ | 8134/8917 [11:00:17<49:09,  3.77s/it] 91%|█████████ | 8135/8917 [11:00:20<48:14,  3.70s/it] 91%|█████████ | 8136/8917 [11:00:25<49:29,  3.80s/it] 91%|█████████▏| 8137/8917 [11:00:28<49:15,  3.79s/it] 91%|█████████▏| 8138/8917 [11:00:32<49:05,  3.78s/it] 91%|█████████▏| 8139/8917 [11:00:36<48:43,  3.76s/it] 91%|█████████▏| 8140/8917 [11:00:39<47:32,  3.67s/it] 91%|█████████▏| 8141/8917 [11:00:43<47:52,  3.70s/it] 91%|█████████▏| 8142/8917 [11:00:47<48:04,  3.72s/it] 91%|█████████▏| 8143/8917 [11:00:50<47:34,  3.69s/it] 91%|█████████▏| 8144/8917 [11:00:54<48:16,  3.75s/it] 91%|█████████▏| 8145/8917 [11:00:58<48:47,  3.79s/it] 91%|█████████▏| 8146/8917 [11:01:02<48:47,  3.80s/it] 91%|█████████▏| 8147/8917 [11:01:06<48:08,  3.75s/it] 91%|█████████▏| 8148/8917 [11:01:09<47:28,  3.70s/it] 91%|█████████▏| 8149/8917 [11:01:13<48:32,  3.79s/it]09/19/2024 13:15:51 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01388601865619421, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.133857250213623, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1477432250976562}
 91%|█████████▏| 8150/8917 [11:01:17<48:59,  3.83s/it] 91%|█████████▏| 8151/8917 [11:01:21<47:26,  3.72s/it] 91%|█████████▏| 8152/8917 [11:01:24<47:02,  3.69s/it] 91%|█████████▏| 8153/8917 [11:01:28<46:08,  3.62s/it] 91%|█████████▏| 8154/8917 [11:01:31<45:40,  3.59s/it] 91%|█████████▏| 8155/8917 [11:01:35<46:50,  3.69s/it] 91%|█████████▏| 8156/8917 [11:01:39<47:09,  3.72s/it] 91%|█████████▏| 8157/8917 [11:01:43<47:22,  3.74s/it] 91%|█████████▏| 8158/8917 [11:01:46<46:50,  3.70s/it] 91%|█████████▏| 8159/8917 [11:01:50<47:17,  3.74s/it] 92%|█████████▏| 8160/8917 [11:01:54<47:11,  3.74s/it] 92%|█████████▏| 8161/8917 [11:01:57<45:52,  3.64s/it] 92%|█████████▏| 8162/8917 [11:02:01<46:16,  3.68s/it] 92%|█████████▏| 8163/8917 [11:02:05<46:37,  3.71s/it] 92%|█████████▏| 8164/8917 [11:02:09<47:12,  3.76s/it] 92%|█████████▏| 8165/8917 [11:02:13<47:18,  3.77s/it] 92%|█████████▏| 8166/8917 [11:02:16<46:46,  3.74s/it] 92%|█████████▏| 8167/8917 [11:02:20<46:09,  3.69s/it] 92%|█████████▏| 8168/8917 [11:02:23<45:20,  3.63s/it] 92%|█████████▏| 8169/8917 [11:02:27<45:03,  3.61s/it] 92%|█████████▏| 8170/8917 [11:02:30<45:15,  3.63s/it] 92%|█████████▏| 8171/8917 [11:02:34<44:40,  3.59s/it] 92%|█████████▏| 8172/8917 [11:02:38<44:26,  3.58s/it] 92%|█████████▏| 8173/8917 [11:02:42<46:03,  3.71s/it] 92%|█████████▏| 8174/8917 [11:02:46<47:44,  3.86s/it] 92%|█████████▏| 8175/8917 [11:02:49<45:44,  3.70s/it] 92%|█████████▏| 8176/8917 [11:02:53<46:21,  3.75s/it] 92%|█████████▏| 8177/8917 [11:02:57<47:11,  3.83s/it] 92%|█████████▏| 8178/8917 [11:03:00<45:59,  3.73s/it] 92%|█████████▏| 8179/8917 [11:03:04<46:04,  3.75s/it] 92%|█████████▏| 8180/8917 [11:03:08<45:20,  3.69s/it] 92%|█████████▏| 8181/8917 [11:03:12<45:39,  3.72s/it] 92%|█████████▏| 8182/8917 [11:03:15<45:26,  3.71s/it] 92%|█████████▏| 8183/8917 [11:03:19<44:58,  3.68s/it] 92%|█████████▏| 8184/8917 [11:03:23<45:27,  3.72s/it] 92%|█████████▏| 8185/8917 [11:03:26<44:43,  3.67s/it] 92%|█████████▏| 8186/8917 [11:03:30<45:15,  3.71s/it] 92%|█████████▏| 8187/8917 [11:03:34<44:59,  3.70s/it] 92%|█████████▏| 8188/8917 [11:03:37<44:23,  3.65s/it] 92%|█████████▏| 8189/8917 [11:03:41<44:55,  3.70s/it] 92%|█████████▏| 8190/8917 [11:03:45<44:19,  3.66s/it] 92%|█████████▏| 8191/8917 [11:03:49<46:32,  3.85s/it] 92%|█████████▏| 8192/8917 [11:03:52<45:01,  3.73s/it] 92%|█████████▏| 8193/8917 [11:03:56<43:39,  3.62s/it] 92%|█████████▏| 8194/8917 [11:03:59<44:03,  3.66s/it] 92%|█████████▏| 8195/8917 [11:04:03<44:08,  3.67s/it] 92%|█████████▏| 8196/8917 [11:04:07<43:19,  3.61s/it] 92%|█████████▏| 8197/8917 [11:04:11<44:14,  3.69s/it] 92%|█████████▏| 8198/8917 [11:04:14<43:06,  3.60s/it] 92%|█████████▏| 8199/8917 [11:04:18<43:01,  3.60s/it]09/19/2024 13:18:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.015906644985079765, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.047080636024475, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0629873275756836}
 92%|█████████▏| 8200/8917 [11:04:21<43:13,  3.62s/it] 92%|█████████▏| 8201/8917 [11:04:25<42:52,  3.59s/it] 92%|█████████▏| 8202/8917 [11:04:28<43:00,  3.61s/it] 92%|█████████▏| 8203/8917 [11:04:32<43:51,  3.69s/it] 92%|█████████▏| 8204/8917 [11:04:36<44:24,  3.74s/it] 92%|█████████▏| 8205/8917 [11:04:40<44:17,  3.73s/it] 92%|█████████▏| 8206/8917 [11:04:44<44:14,  3.73s/it] 92%|█████████▏| 8207/8917 [11:04:47<43:43,  3.70s/it] 92%|█████████▏| 8208/8917 [11:04:51<43:31,  3.68s/it] 92%|█████████▏| 8209/8917 [11:04:54<42:34,  3.61s/it] 92%|█████████▏| 8210/8917 [11:04:58<42:52,  3.64s/it] 92%|█████████▏| 8211/8917 [11:05:02<44:43,  3.80s/it] 92%|█████████▏| 8212/8917 [11:05:06<43:48,  3.73s/it] 92%|█████████▏| 8213/8917 [11:05:09<43:10,  3.68s/it] 92%|█████████▏| 8214/8917 [11:05:13<43:34,  3.72s/it] 92%|█████████▏| 8215/8917 [11:05:17<44:42,  3.82s/it] 92%|█████████▏| 8216/8917 [11:05:21<43:54,  3.76s/it] 92%|█████████▏| 8217/8917 [11:05:24<43:37,  3.74s/it] 92%|█████████▏| 8218/8917 [11:05:28<42:58,  3.69s/it] 92%|█████████▏| 8219/8917 [11:05:32<42:24,  3.65s/it] 92%|█████████▏| 8220/8917 [11:05:36<43:33,  3.75s/it] 92%|█████████▏| 8221/8917 [11:05:40<44:35,  3.84s/it] 92%|█████████▏| 8222/8917 [11:05:43<43:18,  3.74s/it] 92%|█████████▏| 8223/8917 [11:05:47<42:50,  3.70s/it] 92%|█████████▏| 8224/8917 [11:05:50<43:01,  3.73s/it] 92%|█████████▏| 8225/8917 [11:05:54<42:01,  3.64s/it] 92%|█████████▏| 8226/8917 [11:05:58<43:42,  3.79s/it] 92%|█████████▏| 8227/8917 [11:06:02<43:53,  3.82s/it] 92%|█████████▏| 8228/8917 [11:06:06<43:03,  3.75s/it] 92%|█████████▏| 8229/8917 [11:06:09<42:57,  3.75s/it] 92%|█████████▏| 8230/8917 [11:06:13<42:45,  3.73s/it] 92%|█████████▏| 8231/8917 [11:06:17<42:23,  3.71s/it] 92%|█████████▏| 8232/8917 [11:06:20<41:34,  3.64s/it] 92%|█████████▏| 8233/8917 [11:06:24<41:39,  3.65s/it] 92%|█████████▏| 8234/8917 [11:06:28<42:17,  3.72s/it] 92%|█████████▏| 8235/8917 [11:06:31<42:10,  3.71s/it] 92%|█████████▏| 8236/8917 [11:06:36<44:02,  3.88s/it] 92%|█████████▏| 8237/8917 [11:06:39<43:46,  3.86s/it] 92%|█████████▏| 8238/8917 [11:06:43<42:17,  3.74s/it] 92%|█████████▏| 8239/8917 [11:06:47<41:57,  3.71s/it] 92%|█████████▏| 8240/8917 [11:06:50<41:46,  3.70s/it] 92%|█████████▏| 8241/8917 [11:06:54<41:13,  3.66s/it] 92%|█████████▏| 8242/8917 [11:06:58<41:54,  3.72s/it] 92%|█████████▏| 8243/8917 [11:07:02<42:32,  3.79s/it] 92%|█████████▏| 8244/8917 [11:07:06<42:59,  3.83s/it] 92%|█████████▏| 8245/8917 [11:07:09<41:58,  3.75s/it] 92%|█████████▏| 8246/8917 [11:07:13<40:45,  3.64s/it] 92%|█████████▏| 8247/8917 [11:07:16<40:48,  3.65s/it] 92%|█████████▏| 8248/8917 [11:07:20<40:54,  3.67s/it] 93%|█████████▎| 8249/8917 [11:07:23<40:37,  3.65s/it]09/19/2024 13:22:01 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.027869630604982376, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2037376165390015, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.23160719871521}
 93%|█████████▎| 8250/8917 [11:07:27<40:47,  3.67s/it] 93%|█████████▎| 8251/8917 [11:07:31<40:13,  3.62s/it] 93%|█████████▎| 8252/8917 [11:07:35<40:59,  3.70s/it] 93%|█████████▎| 8253/8917 [11:07:38<41:31,  3.75s/it] 93%|█████████▎| 8254/8917 [11:07:42<40:52,  3.70s/it] 93%|█████████▎| 8255/8917 [11:07:46<40:16,  3.65s/it] 93%|█████████▎| 8256/8917 [11:07:49<41:04,  3.73s/it] 93%|█████████▎| 8257/8917 [11:07:53<40:50,  3.71s/it] 93%|█████████▎| 8258/8917 [11:07:57<41:12,  3.75s/it] 93%|█████████▎| 8259/8917 [11:08:01<40:48,  3.72s/it] 93%|█████████▎| 8260/8917 [11:08:05<41:32,  3.79s/it] 93%|█████████▎| 8261/8917 [11:08:08<41:12,  3.77s/it] 93%|█████████▎| 8262/8917 [11:08:12<40:28,  3.71s/it] 93%|█████████▎| 8263/8917 [11:08:16<40:13,  3.69s/it] 93%|█████████▎| 8264/8917 [11:08:19<40:56,  3.76s/it] 93%|█████████▎| 8265/8917 [11:08:23<40:56,  3.77s/it] 93%|█████████▎| 8266/8917 [11:08:27<41:11,  3.80s/it] 93%|█████████▎| 8267/8917 [11:08:31<40:38,  3.75s/it] 93%|█████████▎| 8268/8917 [11:08:34<40:24,  3.74s/it] 93%|█████████▎| 8269/8917 [11:08:38<40:58,  3.79s/it] 93%|█████████▎| 8270/8917 [11:08:42<38:58,  3.62s/it] 93%|█████████▎| 8271/8917 [11:08:45<38:58,  3.62s/it] 93%|█████████▎| 8272/8917 [11:08:49<38:27,  3.58s/it] 93%|█████████▎| 8273/8917 [11:08:52<38:27,  3.58s/it] 93%|█████████▎| 8274/8917 [11:08:56<39:20,  3.67s/it] 93%|█████████▎| 8275/8917 [11:09:00<40:53,  3.82s/it] 93%|█████████▎| 8276/8917 [11:09:04<41:25,  3.88s/it] 93%|█████████▎| 8277/8917 [11:09:08<40:02,  3.75s/it] 93%|█████████▎| 8278/8917 [11:09:11<39:25,  3.70s/it] 93%|█████████▎| 8279/8917 [11:09:15<40:01,  3.76s/it] 93%|█████████▎| 8280/8917 [11:09:19<39:45,  3.75s/it] 93%|█████████▎| 8281/8917 [11:09:23<39:23,  3.72s/it] 93%|█████████▎| 8282/8917 [11:09:26<38:32,  3.64s/it] 93%|█████████▎| 8283/8917 [11:09:30<37:51,  3.58s/it] 93%|█████████▎| 8284/8917 [11:09:33<38:24,  3.64s/it] 93%|█████████▎| 8285/8917 [11:09:37<38:47,  3.68s/it] 93%|█████████▎| 8286/8917 [11:09:41<37:57,  3.61s/it] 93%|█████████▎| 8287/8917 [11:09:45<39:00,  3.72s/it] 93%|█████████▎| 8288/8917 [11:09:48<37:49,  3.61s/it] 93%|█████████▎| 8289/8917 [11:09:52<39:01,  3.73s/it] 93%|█████████▎| 8290/8917 [11:09:55<38:29,  3.68s/it] 93%|█████████▎| 8291/8917 [11:09:59<38:34,  3.70s/it] 93%|█████████▎| 8292/8917 [11:10:03<37:40,  3.62s/it] 93%|█████████▎| 8293/8917 [11:10:06<37:50,  3.64s/it] 93%|█████████▎| 8294/8917 [11:10:10<37:44,  3.63s/it] 93%|█████████▎| 8295/8917 [11:10:14<38:32,  3.72s/it] 93%|█████████▎| 8296/8917 [11:10:18<38:57,  3.76s/it] 93%|█████████▎| 8297/8917 [11:10:21<37:18,  3.61s/it] 93%|█████████▎| 8298/8917 [11:10:25<38:20,  3.72s/it] 93%|█████████▎| 8299/8917 [11:10:28<37:31,  3.64s/it]09/19/2024 13:25:06 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.01573588140308857, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.9174709320068359, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.9332067966461182}
 93%|█████████▎| 8300/8917 [11:10:32<37:25,  3.64s/it] 93%|█████████▎| 8301/8917 [11:10:36<37:49,  3.68s/it] 93%|█████████▎| 8302/8917 [11:10:40<37:43,  3.68s/it] 93%|█████████▎| 8303/8917 [11:10:43<38:16,  3.74s/it] 93%|█████████▎| 8304/8917 [11:10:47<37:43,  3.69s/it] 93%|█████████▎| 8305/8917 [11:10:51<37:23,  3.67s/it] 93%|█████████▎| 8306/8917 [11:10:54<37:48,  3.71s/it] 93%|█████████▎| 8307/8917 [11:10:58<37:01,  3.64s/it] 93%|█████████▎| 8308/8917 [11:11:02<37:04,  3.65s/it] 93%|█████████▎| 8309/8917 [11:11:05<36:59,  3.65s/it] 93%|█████████▎| 8310/8917 [11:11:09<37:12,  3.68s/it] 93%|█████████▎| 8311/8917 [11:11:13<38:34,  3.82s/it] 93%|█████████▎| 8312/8917 [11:11:17<37:15,  3.69s/it] 93%|█████████▎| 8313/8917 [11:11:20<37:39,  3.74s/it] 93%|█████████▎| 8314/8917 [11:11:24<37:32,  3.74s/it] 93%|█████████▎| 8315/8917 [11:11:28<37:30,  3.74s/it] 93%|█████████▎| 8316/8917 [11:11:31<37:16,  3.72s/it] 93%|█████████▎| 8317/8917 [11:11:35<37:20,  3.73s/it] 93%|█████████▎| 8318/8917 [11:11:39<37:19,  3.74s/it] 93%|█████████▎| 8319/8917 [11:11:43<37:02,  3.72s/it] 93%|█████████▎| 8320/8917 [11:11:46<36:09,  3.63s/it] 93%|█████████▎| 8321/8917 [11:11:50<37:16,  3.75s/it] 93%|█████████▎| 8322/8917 [11:11:54<37:44,  3.81s/it] 93%|█████████▎| 8323/8917 [11:11:58<37:47,  3.82s/it] 93%|█████████▎| 8324/8917 [11:12:01<36:08,  3.66s/it] 93%|█████████▎| 8325/8917 [11:12:05<36:18,  3.68s/it] 93%|█████████▎| 8326/8917 [11:12:09<36:56,  3.75s/it] 93%|█████████▎| 8327/8917 [11:12:12<35:53,  3.65s/it] 93%|█████████▎| 8328/8917 [11:12:16<36:21,  3.70s/it] 93%|█████████▎| 8329/8917 [11:12:20<36:37,  3.74s/it] 93%|█████████▎| 8330/8917 [11:12:23<36:06,  3.69s/it] 93%|█████████▎| 8331/8917 [11:12:27<36:48,  3.77s/it] 93%|█████████▎| 8332/8917 [11:12:31<35:54,  3.68s/it] 93%|█████████▎| 8333/8917 [11:12:35<36:20,  3.73s/it] 93%|█████████▎| 8334/8917 [11:12:38<36:13,  3.73s/it] 93%|█████████▎| 8335/8917 [11:12:42<36:16,  3.74s/it] 93%|█████████▎| 8336/8917 [11:12:46<37:31,  3.88s/it] 93%|█████████▎| 8337/8917 [11:12:50<36:19,  3.76s/it] 94%|█████████▎| 8338/8917 [11:12:54<36:51,  3.82s/it] 94%|█████████▎| 8339/8917 [11:12:57<35:31,  3.69s/it] 94%|█████████▎| 8340/8917 [11:13:01<34:37,  3.60s/it] 94%|█████████▎| 8341/8917 [11:13:04<34:51,  3.63s/it] 94%|█████████▎| 8342/8917 [11:13:08<36:05,  3.77s/it] 94%|█████████▎| 8343/8917 [11:13:12<35:36,  3.72s/it] 94%|█████████▎| 8344/8917 [11:13:16<36:09,  3.79s/it] 94%|█████████▎| 8345/8917 [11:13:20<36:40,  3.85s/it] 94%|█████████▎| 8346/8917 [11:13:23<34:43,  3.65s/it] 94%|█████████▎| 8347/8917 [11:13:27<35:49,  3.77s/it] 94%|█████████▎| 8348/8917 [11:13:31<35:54,  3.79s/it] 94%|█████████▎| 8349/8917 [11:13:35<34:52,  3.68s/it]09/19/2024 13:28:12 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.027670947834849358, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0647313594818115, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0924023389816284}
 94%|█████████▎| 8350/8917 [11:13:38<34:56,  3.70s/it] 94%|█████████▎| 8351/8917 [11:13:43<36:44,  3.90s/it] 94%|█████████▎| 8352/8917 [11:13:46<35:08,  3.73s/it] 94%|█████████▎| 8353/8917 [11:13:50<34:51,  3.71s/it] 94%|█████████▎| 8354/8917 [11:13:53<34:12,  3.64s/it] 94%|█████████▎| 8355/8917 [11:13:57<34:17,  3.66s/it] 94%|█████████▎| 8356/8917 [11:14:01<34:27,  3.68s/it] 94%|█████████▎| 8357/8917 [11:14:04<33:52,  3.63s/it] 94%|█████████▎| 8358/8917 [11:14:08<34:10,  3.67s/it] 94%|█████████▎| 8359/8917 [11:14:12<34:43,  3.73s/it] 94%|█████████▍| 8360/8917 [11:14:15<34:42,  3.74s/it] 94%|█████████▍| 8361/8917 [11:14:19<33:57,  3.66s/it] 94%|█████████▍| 8362/8917 [11:14:23<34:42,  3.75s/it] 94%|█████████▍| 8363/8917 [11:14:27<35:10,  3.81s/it] 94%|█████████▍| 8364/8917 [11:14:30<34:02,  3.69s/it] 94%|█████████▍| 8365/8917 [11:14:34<33:57,  3.69s/it] 94%|█████████▍| 8366/8917 [11:14:37<33:23,  3.64s/it] 94%|█████████▍| 8367/8917 [11:14:42<34:46,  3.79s/it] 94%|█████████▍| 8368/8917 [11:14:45<34:55,  3.82s/it] 94%|█████████▍| 8369/8917 [11:14:49<34:38,  3.79s/it] 94%|█████████▍| 8370/8917 [11:14:53<34:14,  3.76s/it] 94%|█████████▍| 8371/8917 [11:14:56<33:17,  3.66s/it] 94%|█████████▍| 8372/8917 [11:15:00<32:57,  3.63s/it] 94%|█████████▍| 8373/8917 [11:15:04<33:04,  3.65s/it] 94%|█████████▍| 8374/8917 [11:15:07<33:12,  3.67s/it] 94%|█████████▍| 8375/8917 [11:15:11<33:37,  3.72s/it] 94%|█████████▍| 8376/8917 [11:15:15<33:27,  3.71s/it] 94%|█████████▍| 8377/8917 [11:15:19<33:22,  3.71s/it] 94%|█████████▍| 8378/8917 [11:15:22<33:44,  3.76s/it] 94%|█████████▍| 8379/8917 [11:15:26<33:16,  3.71s/it] 94%|█████████▍| 8380/8917 [11:15:30<33:34,  3.75s/it] 94%|█████████▍| 8381/8917 [11:15:33<33:11,  3.71s/it] 94%|█████████▍| 8382/8917 [11:15:37<32:41,  3.67s/it] 94%|█████████▍| 8383/8917 [11:15:41<32:52,  3.69s/it] 94%|█████████▍| 8384/8917 [11:15:45<33:26,  3.76s/it] 94%|█████████▍| 8385/8917 [11:15:48<32:29,  3.66s/it] 94%|█████████▍| 8386/8917 [11:15:52<31:59,  3.62s/it] 94%|█████████▍| 8387/8917 [11:15:56<33:14,  3.76s/it] 94%|█████████▍| 8388/8917 [11:15:59<32:42,  3.71s/it] 94%|█████████▍| 8389/8917 [11:16:03<32:21,  3.68s/it] 94%|█████████▍| 8390/8917 [11:16:06<31:58,  3.64s/it] 94%|█████████▍| 8391/8917 [11:16:11<32:59,  3.76s/it] 94%|█████████▍| 8392/8917 [11:16:14<33:00,  3.77s/it] 94%|█████████▍| 8393/8917 [11:16:18<32:14,  3.69s/it] 94%|█████████▍| 8394/8917 [11:16:22<32:15,  3.70s/it] 94%|█████████▍| 8395/8917 [11:16:25<32:28,  3.73s/it] 94%|█████████▍| 8396/8917 [11:16:29<33:25,  3.85s/it] 94%|█████████▍| 8397/8917 [11:16:33<32:53,  3.80s/it] 94%|█████████▍| 8398/8917 [11:16:37<32:32,  3.76s/it] 94%|█████████▍| 8399/8917 [11:16:40<31:55,  3.70s/it]09/19/2024 13:31:18 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.011430492624640465, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 0.8993514776229858, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 0.910781979560852}
 94%|█████████▍| 8400/8917 [11:16:44<30:58,  3.59s/it] 94%|█████████▍| 8401/8917 [11:16:48<31:30,  3.66s/it] 94%|█████████▍| 8402/8917 [11:16:51<32:01,  3.73s/it] 94%|█████████▍| 8403/8917 [11:16:55<31:27,  3.67s/it] 94%|█████████▍| 8404/8917 [11:16:59<31:23,  3.67s/it] 94%|█████████▍| 8405/8917 [11:17:02<31:29,  3.69s/it] 94%|█████████▍| 8406/8917 [11:17:06<31:06,  3.65s/it] 94%|█████████▍| 8407/8917 [11:17:10<32:11,  3.79s/it] 94%|█████████▍| 8408/8917 [11:17:14<31:39,  3.73s/it] 94%|█████████▍| 8409/8917 [11:17:18<32:12,  3.81s/it] 94%|█████████▍| 8410/8917 [11:17:22<32:18,  3.82s/it] 94%|█████████▍| 8411/8917 [11:17:25<31:07,  3.69s/it] 94%|█████████▍| 8412/8917 [11:17:29<31:10,  3.70s/it] 94%|█████████▍| 8413/8917 [11:17:32<31:27,  3.74s/it] 94%|█████████▍| 8414/8917 [11:17:36<31:07,  3.71s/it] 94%|█████████▍| 8415/8917 [11:17:40<31:10,  3.73s/it] 94%|█████████▍| 8416/8917 [11:17:44<31:04,  3.72s/it] 94%|█████████▍| 8417/8917 [11:17:47<30:36,  3.67s/it] 94%|█████████▍| 8418/8917 [11:17:51<30:14,  3.64s/it] 94%|█████████▍| 8419/8917 [11:17:55<30:46,  3.71s/it] 94%|█████████▍| 8420/8917 [11:17:58<30:29,  3.68s/it] 94%|█████████▍| 8421/8917 [11:18:02<31:00,  3.75s/it] 94%|█████████▍| 8422/8917 [11:18:06<31:42,  3.84s/it] 94%|█████████▍| 8423/8917 [11:18:10<32:01,  3.89s/it] 94%|█████████▍| 8424/8917 [11:18:14<30:48,  3.75s/it] 94%|█████████▍| 8425/8917 [11:18:17<30:30,  3.72s/it] 94%|█████████▍| 8426/8917 [11:18:21<31:22,  3.83s/it] 95%|█████████▍| 8427/8917 [11:18:25<30:55,  3.79s/it] 95%|█████████▍| 8428/8917 [11:18:29<30:57,  3.80s/it] 95%|█████████▍| 8429/8917 [11:18:33<31:11,  3.84s/it] 95%|█████████▍| 8430/8917 [11:18:36<29:39,  3.65s/it] 95%|█████████▍| 8431/8917 [11:18:40<30:50,  3.81s/it] 95%|█████████▍| 8432/8917 [11:18:44<30:41,  3.80s/it] 95%|█████████▍| 8433/8917 [11:18:48<30:50,  3.82s/it] 95%|█████████▍| 8434/8917 [11:18:51<30:22,  3.77s/it] 95%|█████████▍| 8435/8917 [11:18:55<29:44,  3.70s/it] 95%|█████████▍| 8436/8917 [11:18:59<29:39,  3.70s/it] 95%|█████████▍| 8437/8917 [11:19:02<29:30,  3.69s/it] 95%|█████████▍| 8438/8917 [11:19:06<29:47,  3.73s/it] 95%|█████████▍| 8439/8917 [11:19:10<31:04,  3.90s/it] 95%|█████████▍| 8440/8917 [11:19:14<29:58,  3.77s/it] 95%|█████████▍| 8441/8917 [11:19:18<30:15,  3.81s/it] 95%|█████████▍| 8442/8917 [11:19:22<30:48,  3.89s/it] 95%|█████████▍| 8443/8917 [11:19:26<30:14,  3.83s/it] 95%|█████████▍| 8444/8917 [11:19:29<29:01,  3.68s/it] 95%|█████████▍| 8445/8917 [11:19:33<29:32,  3.75s/it] 95%|█████████▍| 8446/8917 [11:19:37<29:21,  3.74s/it] 95%|█████████▍| 8447/8917 [11:19:40<28:32,  3.64s/it] 95%|█████████▍| 8448/8917 [11:19:44<28:46,  3.68s/it] 95%|█████████▍| 8449/8917 [11:19:48<28:55,  3.71s/it]09/19/2024 13:34:25 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.016320904716849327, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0453474521636963, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.0616683959960938}
 95%|█████████▍| 8450/8917 [11:19:52<29:33,  3.80s/it] 95%|█████████▍| 8451/8917 [11:19:55<29:12,  3.76s/it] 95%|█████████▍| 8452/8917 [11:19:59<28:29,  3.68s/it] 95%|█████████▍| 8453/8917 [11:20:02<28:28,  3.68s/it] 95%|█████████▍| 8454/8917 [11:20:06<27:42,  3.59s/it] 95%|█████████▍| 8455/8917 [11:20:09<27:37,  3.59s/it] 95%|█████████▍| 8456/8917 [11:20:13<27:59,  3.64s/it] 95%|█████████▍| 8457/8917 [11:20:17<28:21,  3.70s/it] 95%|█████████▍| 8458/8917 [11:20:21<28:06,  3.67s/it] 95%|█████████▍| 8459/8917 [11:20:24<27:54,  3.66s/it] 95%|█████████▍| 8460/8917 [11:20:28<27:23,  3.60s/it] 95%|█████████▍| 8461/8917 [11:20:32<28:02,  3.69s/it] 95%|█████████▍| 8462/8917 [11:20:35<28:04,  3.70s/it] 95%|█████████▍| 8463/8917 [11:20:39<27:49,  3.68s/it] 95%|█████████▍| 8464/8917 [11:20:42<27:24,  3.63s/it] 95%|█████████▍| 8465/8917 [11:20:46<27:53,  3.70s/it] 95%|█████████▍| 8466/8917 [11:20:50<28:31,  3.80s/it] 95%|█████████▍| 8467/8917 [11:20:54<27:37,  3.68s/it] 95%|█████████▍| 8468/8917 [11:20:58<28:04,  3.75s/it] 95%|█████████▍| 8469/8917 [11:21:01<27:56,  3.74s/it] 95%|█████████▍| 8470/8917 [11:21:05<27:50,  3.74s/it] 95%|█████████▍| 8471/8917 [11:21:09<27:06,  3.65s/it] 95%|█████████▌| 8472/8917 [11:21:12<27:29,  3.71s/it] 95%|█████████▌| 8473/8917 [11:21:16<27:55,  3.77s/it] 95%|█████████▌| 8474/8917 [11:21:20<28:12,  3.82s/it] 95%|█████████▌| 8475/8917 [11:21:24<27:52,  3.78s/it] 95%|█████████▌| 8476/8917 [11:21:28<27:32,  3.75s/it] 95%|█████████▌| 8477/8917 [11:21:31<27:36,  3.77s/it] 95%|█████████▌| 8478/8917 [11:21:35<27:15,  3.73s/it] 95%|█████████▌| 8479/8917 [11:21:39<27:10,  3.72s/it] 95%|█████████▌| 8480/8917 [11:21:42<27:07,  3.72s/it] 95%|█████████▌| 8481/8917 [11:21:46<27:38,  3.80s/it] 95%|█████████▌| 8482/8917 [11:21:50<27:19,  3.77s/it] 95%|█████████▌| 8483/8917 [11:21:54<27:14,  3.77s/it] 95%|█████████▌| 8484/8917 [11:21:57<26:12,  3.63s/it] 95%|█████████▌| 8485/8917 [11:22:01<27:12,  3.78s/it] 95%|█████████▌| 8486/8917 [11:22:05<26:56,  3.75s/it] 95%|█████████▌| 8487/8917 [11:22:09<26:58,  3.76s/it] 95%|█████████▌| 8488/8917 [11:22:13<27:08,  3.80s/it] 95%|█████████▌| 8489/8917 [11:22:16<26:36,  3.73s/it] 95%|█████████▌| 8490/8917 [11:22:20<26:20,  3.70s/it] 95%|█████████▌| 8491/8917 [11:22:23<25:48,  3.64s/it] 95%|█████████▌| 8492/8917 [11:22:27<26:14,  3.70s/it] 95%|█████████▌| 8493/8917 [11:22:31<26:43,  3.78s/it] 95%|█████████▌| 8494/8917 [11:22:35<26:37,  3.78s/it] 95%|█████████▌| 8495/8917 [11:22:39<26:33,  3.78s/it] 95%|█████████▌| 8496/8917 [11:22:43<26:40,  3.80s/it] 95%|█████████▌| 8497/8917 [11:22:46<25:54,  3.70s/it] 95%|█████████▌| 8498/8917 [11:22:50<25:52,  3.71s/it] 95%|█████████▌| 8499/8917 [11:22:53<25:45,  3.70s/it]09/19/2024 13:37:30 - INFO - __main__ -   evaluate on ret%tvas--msrvtt_ret task
09/19/2024 13:37:30 - INFO - __main__ -   start running ret%tvas validation...
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(
/leonardo/home/userexternal/gcicchet/.conda/envs/vast/lib/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  warnings.warn(

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:01<04:32,  1.24s/it][A
  1%|          | 2/221 [00:01<02:43,  1.34it/s][A
  1%|▏         | 3/221 [00:02<02:13,  1.64it/s][A
  2%|▏         | 4/221 [00:02<01:43,  2.10it/s][A
  3%|▎         | 6/221 [00:02<00:59,  3.63it/s][A
  3%|▎         | 7/221 [00:02<00:56,  3.77it/s][A
  4%|▎         | 8/221 [00:03<01:12,  2.94it/s][A
  4%|▍         | 9/221 [00:03<01:10,  3.02it/s][A
  5%|▍         | 10/221 [00:04<01:14,  2.85it/s][A
  5%|▌         | 12/221 [00:10<05:46,  1.66s/it][A
  6%|▌         | 13/221 [00:10<04:35,  1.32s/it][A
  6%|▋         | 14/221 [00:10<03:30,  1.01s/it][A
  7%|▋         | 15/221 [00:11<02:56,  1.17it/s][A
  7%|▋         | 16/221 [00:11<02:42,  1.26it/s][A
  8%|▊         | 17/221 [00:12<02:52,  1.18it/s][A
  8%|▊         | 18/221 [00:13<02:19,  1.45it/s][A
  9%|▊         | 19/221 [00:15<03:34,  1.06s/it][A
  9%|▉         | 20/221 [00:15<02:36,  1.29it/s][A
 10%|▉         | 21/221 [00:15<02:09,  1.54it/s][A
 10%|▉         | 22/221 [00:16<02:21,  1.40it/s][A
 10%|█         | 23/221 [00:16<01:45,  1.87it/s][A
 11%|█         | 24/221 [00:16<01:28,  2.23it/s][A
 11%|█▏        | 25/221 [00:17<01:19,  2.45it/s][A
 12%|█▏        | 26/221 [00:17<01:18,  2.49it/s][A
 13%|█▎        | 28/221 [00:18<01:06,  2.89it/s][A
 13%|█▎        | 29/221 [00:18<01:00,  3.19it/s][A
 14%|█▎        | 30/221 [00:18<01:12,  2.63it/s][A
 14%|█▍        | 31/221 [00:19<01:18,  2.43it/s][A
 14%|█▍        | 32/221 [00:19<01:02,  3.01it/s][A
 15%|█▍        | 33/221 [00:19<01:01,  3.07it/s][A
 16%|█▌        | 35/221 [00:20<00:44,  4.19it/s][A
 16%|█▋        | 36/221 [00:20<00:42,  4.35it/s][A
 17%|█▋        | 37/221 [00:20<00:52,  3.49it/s][A
 17%|█▋        | 38/221 [00:21<01:00,  3.03it/s][A
 18%|█▊        | 39/221 [00:21<01:08,  2.65it/s][A
 18%|█▊        | 40/221 [00:22<01:13,  2.47it/s][A
 19%|█▊        | 41/221 [00:22<01:02,  2.86it/s][A
 19%|█▉        | 42/221 [00:22<00:51,  3.51it/s][A
 19%|█▉        | 43/221 [00:22<00:42,  4.18it/s][A
 20%|█▉        | 44/221 [00:22<00:35,  4.96it/s][A
 20%|██        | 45/221 [00:26<03:22,  1.15s/it][A
 21%|██        | 46/221 [00:26<02:39,  1.10it/s][A
 21%|██▏       | 47/221 [00:26<02:14,  1.29it/s][A
 22%|██▏       | 48/221 [00:27<01:40,  1.72it/s][A
 22%|██▏       | 49/221 [00:27<01:29,  1.93it/s][A
 23%|██▎       | 50/221 [00:27<01:25,  1.99it/s][A
 23%|██▎       | 51/221 [00:28<01:08,  2.48it/s][A
 24%|██▎       | 52/221 [00:28<01:00,  2.78it/s][A
 24%|██▍       | 53/221 [00:28<01:01,  2.75it/s][A
 24%|██▍       | 54/221 [00:29<01:16,  2.17it/s][A
 25%|██▍       | 55/221 [00:30<01:57,  1.41it/s][A
 25%|██▌       | 56/221 [00:30<01:31,  1.81it/s][A
 26%|██▌       | 57/221 [00:31<01:13,  2.23it/s][A
 27%|██▋       | 59/221 [00:31<00:48,  3.31it/s][A
 27%|██▋       | 60/221 [00:31<00:47,  3.41it/s][A
 28%|██▊       | 61/221 [00:31<00:43,  3.67it/s][A
 28%|██▊       | 62/221 [00:32<00:44,  3.54it/s][A
 29%|██▊       | 63/221 [00:32<00:41,  3.83it/s][A
 29%|██▉       | 64/221 [00:33<01:11,  2.21it/s][A
 29%|██▉       | 65/221 [00:33<00:57,  2.71it/s][A
 30%|██▉       | 66/221 [00:33<01:00,  2.55it/s][A
 30%|███       | 67/221 [00:34<00:54,  2.81it/s][A
 31%|███       | 68/221 [00:34<00:46,  3.27it/s][A
 31%|███       | 69/221 [00:36<01:58,  1.28it/s][A
 32%|███▏      | 70/221 [00:36<01:28,  1.71it/s][A
 32%|███▏      | 71/221 [00:36<01:15,  1.99it/s][A
 33%|███▎      | 72/221 [00:37<01:10,  2.12it/s][A
 33%|███▎      | 73/221 [00:37<01:13,  2.02it/s][A
 33%|███▎      | 74/221 [00:37<00:57,  2.57it/s][A
 34%|███▍      | 75/221 [00:38<00:54,  2.68it/s][A
 34%|███▍      | 76/221 [00:38<00:52,  2.76it/s][A
 35%|███▍      | 77/221 [00:39<01:37,  1.48it/s][A
 35%|███▌      | 78/221 [00:40<01:16,  1.87it/s][A
 36%|███▌      | 79/221 [00:40<01:30,  1.56it/s][A
 36%|███▌      | 80/221 [00:41<01:09,  2.04it/s][A
 37%|███▋      | 81/221 [00:41<01:01,  2.26it/s][A
 37%|███▋      | 82/221 [00:44<02:53,  1.25s/it][A
 38%|███▊      | 83/221 [00:45<02:25,  1.05s/it][A
 38%|███▊      | 84/221 [00:45<01:51,  1.23it/s][A
 39%|███▉      | 86/221 [00:45<01:12,  1.87it/s][A
 39%|███▉      | 87/221 [00:46<01:08,  1.96it/s][A
 40%|███▉      | 88/221 [00:46<00:58,  2.28it/s][A
 40%|████      | 89/221 [00:46<00:53,  2.48it/s][A
 41%|████      | 90/221 [00:47<00:47,  2.74it/s][A
 41%|████      | 91/221 [00:47<00:39,  3.30it/s][A
 42%|████▏     | 92/221 [00:47<00:34,  3.71it/s][A
 42%|████▏     | 93/221 [00:47<00:38,  3.34it/s][A
 43%|████▎     | 94/221 [00:48<00:48,  2.63it/s][A
 43%|████▎     | 95/221 [00:48<00:46,  2.70it/s][A
 43%|████▎     | 96/221 [00:49<00:57,  2.17it/s][A
 44%|████▍     | 97/221 [00:49<00:45,  2.70it/s][A
 44%|████▍     | 98/221 [00:50<00:51,  2.39it/s][A
 45%|████▍     | 99/221 [00:50<00:49,  2.47it/s][A
 45%|████▌     | 100/221 [00:51<01:06,  1.82it/s][A
 46%|████▌     | 101/221 [00:51<00:52,  2.29it/s][A
 46%|████▌     | 102/221 [00:52<01:02,  1.90it/s][A
 47%|████▋     | 103/221 [00:52<00:47,  2.50it/s][A
 47%|████▋     | 104/221 [00:52<00:43,  2.70it/s][A
 48%|████▊     | 105/221 [00:53<00:47,  2.46it/s][A
 48%|████▊     | 106/221 [00:54<01:25,  1.35it/s][A
 48%|████▊     | 107/221 [00:55<01:13,  1.56it/s][A
 49%|████▉     | 108/221 [00:55<01:09,  1.62it/s][A
 49%|████▉     | 109/221 [00:56<01:00,  1.84it/s][A
 50%|████▉     | 110/221 [00:56<00:48,  2.28it/s][A
 50%|█████     | 111/221 [00:56<00:45,  2.41it/s][A
 51%|█████     | 112/221 [00:56<00:42,  2.56it/s][A
 51%|█████     | 113/221 [00:57<00:36,  2.99it/s][A
 52%|█████▏    | 115/221 [00:57<00:36,  2.92it/s][A
 52%|█████▏    | 116/221 [00:58<00:36,  2.91it/s][A
 53%|█████▎    | 117/221 [00:58<00:37,  2.76it/s][A
 53%|█████▎    | 118/221 [00:59<00:39,  2.62it/s][A
 54%|█████▍    | 119/221 [00:59<00:32,  3.11it/s][A
 54%|█████▍    | 120/221 [00:59<00:27,  3.68it/s][A
 55%|█████▍    | 121/221 [00:59<00:35,  2.84it/s][A
 55%|█████▌    | 122/221 [01:00<00:38,  2.58it/s][A
 56%|█████▌    | 123/221 [01:01<01:09,  1.42it/s][A
 56%|█████▌    | 124/221 [01:02<00:54,  1.79it/s][A
 57%|█████▋    | 125/221 [01:02<00:58,  1.64it/s][A
 57%|█████▋    | 126/221 [01:11<04:52,  3.08s/it][A
 57%|█████▋    | 127/221 [01:12<03:35,  2.29s/it][A
 58%|█████▊    | 128/221 [01:12<02:39,  1.72s/it][A
 58%|█████▊    | 129/221 [01:13<02:11,  1.43s/it][A
 59%|█████▉    | 130/221 [01:13<01:37,  1.07s/it][A
 59%|█████▉    | 131/221 [01:14<01:37,  1.08s/it][A
 60%|█████▉    | 132/221 [01:15<01:44,  1.18s/it][A
 60%|██████    | 133/221 [01:16<01:25,  1.03it/s][A
 61%|██████    | 134/221 [01:17<01:18,  1.11it/s][A
 61%|██████    | 135/221 [01:17<01:10,  1.22it/s][A
 62%|██████▏   | 136/221 [01:18<00:58,  1.44it/s][A
 62%|██████▏   | 137/221 [01:18<00:48,  1.73it/s][A
 62%|██████▏   | 138/221 [01:18<00:42,  1.95it/s][A
 63%|██████▎   | 139/221 [01:19<00:33,  2.44it/s][A
 63%|██████▎   | 140/221 [01:19<00:33,  2.40it/s][A
 64%|██████▍   | 141/221 [01:19<00:34,  2.32it/s][A
 64%|██████▍   | 142/221 [01:20<00:29,  2.64it/s][A
 65%|██████▍   | 143/221 [01:20<00:28,  2.75it/s][A
 65%|██████▌   | 144/221 [01:20<00:23,  3.34it/s][A
 66%|██████▌   | 145/221 [01:20<00:18,  4.03it/s][A
 66%|██████▌   | 146/221 [01:20<00:15,  4.88it/s][A
 67%|██████▋   | 148/221 [01:22<00:42,  1.71it/s][A
 67%|██████▋   | 149/221 [01:23<00:37,  1.93it/s][A
 68%|██████▊   | 150/221 [01:23<00:35,  2.00it/s][A
 68%|██████▊   | 151/221 [01:23<00:30,  2.29it/s][A
 69%|██████▉   | 152/221 [01:24<00:29,  2.33it/s][A
 69%|██████▉   | 153/221 [01:24<00:23,  2.89it/s][A
 70%|██████▉   | 154/221 [01:24<00:20,  3.20it/s][A
 70%|███████   | 155/221 [01:24<00:17,  3.67it/s][A
 71%|███████   | 156/221 [01:25<00:15,  4.24it/s][A
 71%|███████   | 157/221 [01:31<02:09,  2.03s/it][A
 71%|███████▏  | 158/221 [01:32<01:47,  1.70s/it][A
 72%|███████▏  | 159/221 [01:32<01:15,  1.22s/it][A
 72%|███████▏  | 160/221 [01:32<00:55,  1.10it/s][A
 73%|███████▎  | 161/221 [01:32<00:39,  1.50it/s][A
 74%|███████▍  | 163/221 [01:32<00:24,  2.34it/s][A
 74%|███████▍  | 164/221 [01:33<00:20,  2.81it/s][A
 75%|███████▍  | 165/221 [01:33<00:17,  3.14it/s][A
 75%|███████▌  | 166/221 [01:34<00:23,  2.34it/s][A
 76%|███████▌  | 167/221 [01:34<00:21,  2.46it/s][A
 76%|███████▌  | 168/221 [01:35<00:38,  1.39it/s][A
 76%|███████▋  | 169/221 [01:36<00:35,  1.44it/s][A
 77%|███████▋  | 170/221 [01:36<00:31,  1.61it/s][A
 77%|███████▋  | 171/221 [01:37<00:26,  1.86it/s][A
 78%|███████▊  | 172/221 [01:37<00:22,  2.14it/s][A
 78%|███████▊  | 173/221 [01:37<00:20,  2.35it/s][A
 79%|███████▊  | 174/221 [01:38<00:15,  3.01it/s][A
 79%|███████▉  | 175/221 [01:38<00:15,  3.05it/s][A
 80%|███████▉  | 176/221 [01:38<00:15,  2.88it/s][A
 80%|████████  | 177/221 [01:39<00:14,  3.03it/s][A
 81%|████████  | 178/221 [01:39<00:14,  2.96it/s][A
 81%|████████  | 179/221 [01:39<00:13,  3.04it/s][A
 81%|████████▏ | 180/221 [01:39<00:11,  3.43it/s][A
 82%|████████▏ | 182/221 [01:40<00:10,  3.75it/s][A
 83%|████████▎ | 183/221 [01:41<00:15,  2.49it/s][A
 83%|████████▎ | 184/221 [01:41<00:15,  2.42it/s][A
 84%|████████▎ | 185/221 [01:41<00:14,  2.56it/s][A
 84%|████████▍ | 186/221 [01:42<00:12,  2.85it/s][A
 85%|████████▍ | 187/221 [01:42<00:11,  3.00it/s][A
 85%|████████▌ | 188/221 [01:42<00:09,  3.48it/s][A
 86%|████████▌ | 189/221 [01:42<00:09,  3.55it/s][A
 86%|████████▌ | 190/221 [01:43<00:09,  3.38it/s][A
 86%|████████▋ | 191/221 [01:43<00:08,  3.69it/s][A
 87%|████████▋ | 192/221 [01:44<00:14,  2.03it/s][A
 87%|████████▋ | 193/221 [01:44<00:11,  2.40it/s][A
 88%|████████▊ | 194/221 [01:45<00:10,  2.58it/s][A
 88%|████████▊ | 195/221 [01:45<00:08,  3.03it/s][A
 89%|████████▊ | 196/221 [01:45<00:09,  2.50it/s][A
 89%|████████▉ | 197/221 [01:46<00:08,  2.68it/s][A
 90%|████████▉ | 198/221 [01:46<00:07,  2.98it/s][A
 90%|█████████ | 199/221 [01:46<00:06,  3.35it/s][A
 90%|█████████ | 200/221 [01:47<00:09,  2.18it/s][A
 91%|█████████ | 201/221 [01:48<00:12,  1.58it/s][A
 91%|█████████▏| 202/221 [01:48<00:10,  1.87it/s][A
 92%|█████████▏| 203/221 [01:49<00:10,  1.68it/s][A
 92%|█████████▏| 204/221 [01:50<00:09,  1.71it/s][A
 93%|█████████▎| 206/221 [01:50<00:06,  2.32it/s][A
 94%|█████████▎| 207/221 [01:50<00:05,  2.76it/s][A
 94%|█████████▍| 208/221 [01:51<00:05,  2.36it/s][A
 95%|█████████▌| 210/221 [01:51<00:03,  3.32it/s][A
 95%|█████████▌| 211/221 [01:52<00:03,  3.01it/s][A
 96%|█████████▌| 212/221 [01:52<00:02,  3.02it/s][A
 96%|█████████▋| 213/221 [01:52<00:02,  3.45it/s][A
 97%|█████████▋| 214/221 [01:52<00:02,  3.05it/s][A
 97%|█████████▋| 215/221 [01:53<00:02,  2.48it/s][A
 98%|█████████▊| 216/221 [01:53<00:01,  2.67it/s][A
 98%|█████████▊| 217/221 [01:55<00:03,  1.17it/s][A
 99%|█████████▊| 218/221 [01:56<00:02,  1.45it/s][A
 99%|█████████▉| 219/221 [01:56<00:01,  1.70it/s][A
100%|█████████▉| 220/221 [02:01<00:01,  1.91s/it][A
100%|██████████| 221/221 [02:01<00:00,  1.39s/it][A100%|██████████| 221/221 [02:01<00:00,  1.81it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:56,  1.89it/s][A
  1%|          | 2/221 [00:01<01:55,  1.89it/s][A
  1%|▏         | 3/221 [00:01<01:55,  1.89it/s][A
  2%|▏         | 4/221 [00:02<01:54,  1.89it/s][A
  2%|▏         | 5/221 [00:02<01:54,  1.89it/s][A
  3%|▎         | 6/221 [00:03<01:53,  1.89it/s][A
  3%|▎         | 7/221 [00:03<01:53,  1.89it/s][A
  4%|▎         | 8/221 [00:04<01:52,  1.89it/s][A
  4%|▍         | 9/221 [00:04<01:52,  1.89it/s][A
  5%|▍         | 10/221 [00:05<01:51,  1.89it/s][A
  5%|▍         | 11/221 [00:05<01:50,  1.89it/s][A
  5%|▌         | 12/221 [00:06<01:50,  1.89it/s][A
  6%|▌         | 13/221 [00:06<01:49,  1.89it/s][A
  6%|▋         | 14/221 [00:07<01:49,  1.89it/s][A
  7%|▋         | 15/221 [00:07<01:48,  1.89it/s][A
  7%|▋         | 16/221 [00:08<01:48,  1.89it/s][A
  8%|▊         | 17/221 [00:08<01:47,  1.89it/s][A
  8%|▊         | 18/221 [00:09<01:47,  1.89it/s][A
  9%|▊         | 19/221 [00:10<01:46,  1.89it/s][A
  9%|▉         | 20/221 [00:10<01:46,  1.89it/s][A
 10%|▉         | 21/221 [00:11<01:45,  1.89it/s][A
 10%|▉         | 22/221 [00:11<01:45,  1.89it/s][A
 10%|█         | 23/221 [00:12<01:44,  1.89it/s][A
 11%|█         | 24/221 [00:12<01:44,  1.89it/s][A
 11%|█▏        | 25/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 26/221 [00:13<01:43,  1.89it/s][A
 12%|█▏        | 27/221 [00:14<01:42,  1.89it/s][A
 13%|█▎        | 28/221 [00:14<01:41,  1.89it/s][A
 13%|█▎        | 29/221 [00:15<01:41,  1.89it/s][A
 14%|█▎        | 30/221 [00:15<01:40,  1.89it/s][A
 14%|█▍        | 31/221 [00:16<01:40,  1.89it/s][A
 14%|█▍        | 32/221 [00:16<01:39,  1.89it/s][A
 15%|█▍        | 33/221 [00:17<01:39,  1.89it/s][A
 15%|█▌        | 34/221 [00:17<01:38,  1.89it/s][A
 16%|█▌        | 35/221 [00:18<01:38,  1.89it/s][A
 16%|█▋        | 36/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 37/221 [00:19<01:37,  1.89it/s][A
 17%|█▋        | 38/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 39/221 [00:20<01:36,  1.89it/s][A
 18%|█▊        | 40/221 [00:21<01:35,  1.89it/s][A
 19%|█▊        | 41/221 [00:21<01:35,  1.89it/s][A
 19%|█▉        | 42/221 [00:22<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:22<01:34,  1.89it/s][A
 20%|█▉        | 44/221 [00:23<01:33,  1.89it/s][A
 20%|██        | 45/221 [00:23<01:32,  1.89it/s][A
 21%|██        | 46/221 [00:24<01:32,  1.89it/s][A
 21%|██▏       | 47/221 [00:24<01:31,  1.89it/s][A
 22%|██▏       | 48/221 [00:25<01:31,  1.89it/s][A
 22%|██▏       | 49/221 [00:25<01:30,  1.89it/s][A
 23%|██▎       | 50/221 [00:26<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:26<01:29,  1.89it/s][A
 24%|██▎       | 52/221 [00:27<01:29,  1.89it/s][A
 24%|██▍       | 53/221 [00:28<01:28,  1.89it/s][A
 24%|██▍       | 54/221 [00:28<01:28,  1.89it/s][A
 25%|██▍       | 55/221 [00:29<01:27,  1.89it/s][A
 25%|██▌       | 56/221 [00:29<01:27,  1.89it/s][A
 26%|██▌       | 57/221 [00:30<01:26,  1.89it/s][A
 26%|██▌       | 58/221 [00:30<01:26,  1.89it/s][A
 27%|██▋       | 59/221 [00:31<01:25,  1.89it/s][A
 27%|██▋       | 60/221 [00:31<01:25,  1.89it/s][A
 28%|██▊       | 61/221 [00:32<01:24,  1.89it/s][A
 28%|██▊       | 62/221 [00:32<01:24,  1.89it/s][A
 29%|██▊       | 63/221 [00:33<01:23,  1.89it/s][A
 29%|██▉       | 64/221 [00:33<01:22,  1.89it/s][A
 29%|██▉       | 65/221 [00:34<01:22,  1.89it/s][A
 30%|██▉       | 66/221 [00:34<01:21,  1.89it/s][A
 30%|███       | 67/221 [00:35<01:21,  1.89it/s][A
 31%|███       | 68/221 [00:35<01:20,  1.89it/s][A
 31%|███       | 69/221 [00:36<01:20,  1.89it/s][A
 32%|███▏      | 70/221 [00:36<01:19,  1.89it/s][A
 32%|███▏      | 71/221 [00:37<01:19,  1.89it/s][A
 33%|███▎      | 72/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 73/221 [00:38<01:18,  1.89it/s][A
 33%|███▎      | 74/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 75/221 [00:39<01:17,  1.89it/s][A
 34%|███▍      | 76/221 [00:40<01:16,  1.89it/s][A
 35%|███▍      | 77/221 [00:40<01:16,  1.89it/s][A
 35%|███▌      | 78/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 79/221 [00:41<01:15,  1.89it/s][A
 36%|███▌      | 80/221 [00:42<01:14,  1.89it/s][A
 37%|███▋      | 81/221 [00:42<01:13,  1.89it/s][A
 37%|███▋      | 82/221 [00:43<01:13,  1.89it/s][A
 38%|███▊      | 83/221 [00:43<01:12,  1.89it/s][A
 38%|███▊      | 84/221 [00:44<01:12,  1.89it/s][A
 38%|███▊      | 85/221 [00:44<01:11,  1.89it/s][A
 39%|███▉      | 86/221 [00:45<01:11,  1.89it/s][A
 39%|███▉      | 87/221 [00:45<01:10,  1.89it/s][A
 40%|███▉      | 88/221 [00:46<01:10,  1.89it/s][A
 40%|████      | 89/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 90/221 [00:47<01:09,  1.89it/s][A
 41%|████      | 91/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 92/221 [00:48<01:08,  1.89it/s][A
 42%|████▏     | 93/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 94/221 [00:49<01:07,  1.89it/s][A
 43%|████▎     | 95/221 [00:50<01:06,  1.89it/s][A
 43%|████▎     | 96/221 [00:50<01:06,  1.89it/s][A
 44%|████▍     | 97/221 [00:51<01:05,  1.89it/s][A
 44%|████▍     | 98/221 [00:51<01:04,  1.89it/s][A
 45%|████▍     | 99/221 [00:52<01:04,  1.89it/s][A
 45%|████▌     | 100/221 [00:52<01:03,  1.89it/s][A
 46%|████▌     | 101/221 [00:53<01:03,  1.89it/s][A
 46%|████▌     | 102/221 [00:53<01:02,  1.89it/s][A
 47%|████▋     | 103/221 [00:54<01:02,  1.89it/s][A
 47%|████▋     | 104/221 [00:54<01:01,  1.89it/s][A
 48%|████▊     | 105/221 [00:55<01:01,  1.89it/s][A
 48%|████▊     | 106/221 [00:55<01:00,  1.89it/s][A
 48%|████▊     | 107/221 [00:56<01:00,  1.89it/s][A
 49%|████▉     | 108/221 [00:57<00:59,  1.89it/s][A
 49%|████▉     | 109/221 [00:57<00:59,  1.89it/s][A
 50%|████▉     | 110/221 [00:58<00:58,  1.89it/s][A
 50%|█████     | 111/221 [00:58<00:58,  1.89it/s][A
 51%|█████     | 112/221 [00:59<00:57,  1.89it/s][A
 51%|█████     | 113/221 [00:59<00:57,  1.89it/s][A
 52%|█████▏    | 114/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 115/221 [01:00<00:56,  1.89it/s][A
 52%|█████▏    | 116/221 [01:01<00:55,  1.89it/s][A
 53%|█████▎    | 117/221 [01:01<00:54,  1.89it/s][A
 53%|█████▎    | 118/221 [01:02<00:54,  1.89it/s][A
 54%|█████▍    | 119/221 [01:02<00:53,  1.89it/s][A
 54%|█████▍    | 120/221 [01:03<00:53,  1.89it/s][A
 55%|█████▍    | 121/221 [01:03<00:52,  1.89it/s][A
 55%|█████▌    | 122/221 [01:04<00:52,  1.89it/s][A
 56%|█████▌    | 123/221 [01:04<00:51,  1.89it/s][A
 56%|█████▌    | 124/221 [01:05<00:51,  1.89it/s][A
 57%|█████▋    | 125/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 126/221 [01:06<00:50,  1.89it/s][A
 57%|█████▋    | 127/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 128/221 [01:07<00:49,  1.89it/s][A
 58%|█████▊    | 129/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 130/221 [01:08<00:48,  1.89it/s][A
 59%|█████▉    | 131/221 [01:09<00:47,  1.89it/s][A
 60%|█████▉    | 132/221 [01:09<00:47,  1.89it/s][A
 60%|██████    | 133/221 [01:10<00:46,  1.89it/s][A
 61%|██████    | 134/221 [01:10<00:45,  1.89it/s][A
 61%|██████    | 135/221 [01:11<00:45,  1.89it/s][A
 62%|██████▏   | 136/221 [01:11<00:44,  1.89it/s][A
 62%|██████▏   | 137/221 [01:12<00:44,  1.89it/s][A
 62%|██████▏   | 138/221 [01:12<00:43,  1.89it/s][A
 63%|██████▎   | 139/221 [01:13<00:43,  1.89it/s][A
 63%|██████▎   | 140/221 [01:13<00:42,  1.89it/s][A
 64%|██████▍   | 141/221 [01:14<00:42,  1.89it/s][A
 64%|██████▍   | 142/221 [01:15<00:41,  1.89it/s][A
 65%|██████▍   | 143/221 [01:15<00:41,  1.89it/s][A
 65%|██████▌   | 144/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 145/221 [01:16<00:40,  1.89it/s][A
 66%|██████▌   | 146/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 147/221 [01:17<00:39,  1.89it/s][A
 67%|██████▋   | 148/221 [01:18<00:38,  1.89it/s][A
 67%|██████▋   | 149/221 [01:18<00:38,  1.89it/s][A
 68%|██████▊   | 150/221 [01:19<00:37,  1.89it/s][A
 68%|██████▊   | 151/221 [01:19<00:36,  1.89it/s][A
 69%|██████▉   | 152/221 [01:20<00:36,  1.89it/s][A
 69%|██████▉   | 153/221 [01:20<00:35,  1.89it/s][A
 70%|██████▉   | 154/221 [01:21<00:35,  1.89it/s][A
 70%|███████   | 155/221 [01:21<00:34,  1.89it/s][A
 71%|███████   | 156/221 [01:22<00:34,  1.89it/s][A
 71%|███████   | 157/221 [01:22<00:33,  1.89it/s][A
 71%|███████▏  | 158/221 [01:23<00:33,  1.89it/s][A
 72%|███████▏  | 159/221 [01:23<00:32,  1.89it/s][A
 72%|███████▏  | 160/221 [01:24<00:32,  1.89it/s][A
 73%|███████▎  | 161/221 [01:25<00:31,  1.89it/s][A
 73%|███████▎  | 162/221 [01:25<00:31,  1.89it/s][A
 74%|███████▍  | 163/221 [01:26<00:30,  1.89it/s][A
 74%|███████▍  | 164/221 [01:26<00:30,  1.89it/s][A
 75%|███████▍  | 165/221 [01:27<00:29,  1.89it/s][A
 75%|███████▌  | 166/221 [01:27<00:29,  1.89it/s][A
 76%|███████▌  | 167/221 [01:28<00:28,  1.89it/s][A
 76%|███████▌  | 168/221 [01:28<00:27,  1.89it/s][A
 76%|███████▋  | 169/221 [01:29<00:27,  1.89it/s][A
 77%|███████▋  | 170/221 [01:29<00:26,  1.89it/s][A
 77%|███████▋  | 171/221 [01:30<00:26,  1.89it/s][A
 78%|███████▊  | 172/221 [01:30<00:25,  1.89it/s][A
 78%|███████▊  | 173/221 [01:31<00:25,  1.89it/s][A
 79%|███████▊  | 174/221 [01:31<00:24,  1.89it/s][A
 79%|███████▉  | 175/221 [01:32<00:24,  1.89it/s][A
 80%|███████▉  | 176/221 [01:32<00:23,  1.89it/s][A
 80%|████████  | 177/221 [01:33<00:23,  1.89it/s][A
 81%|████████  | 178/221 [01:34<00:22,  1.89it/s][A
 81%|████████  | 179/221 [01:34<00:22,  1.89it/s][A
 81%|████████▏ | 180/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 181/221 [01:35<00:21,  1.89it/s][A
 82%|████████▏ | 182/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 183/221 [01:36<00:20,  1.89it/s][A
 83%|████████▎ | 184/221 [01:37<00:19,  1.89it/s][A
 84%|████████▎ | 185/221 [01:37<00:19,  1.89it/s][A
 84%|████████▍ | 186/221 [01:38<00:18,  1.89it/s][A
 85%|████████▍ | 187/221 [01:38<00:17,  1.89it/s][A
 85%|████████▌ | 188/221 [01:39<00:17,  1.89it/s][A
 86%|████████▌ | 189/221 [01:39<00:16,  1.89it/s][A
 86%|████████▌ | 190/221 [01:40<00:16,  1.89it/s][A
 86%|████████▋ | 191/221 [01:40<00:15,  1.89it/s][A
 87%|████████▋ | 192/221 [01:41<00:15,  1.89it/s][A
 87%|████████▋ | 193/221 [01:41<00:14,  1.89it/s][A
 88%|████████▊ | 194/221 [01:42<00:14,  1.89it/s][A
 88%|████████▊ | 195/221 [01:43<00:13,  1.89it/s][A
 89%|████████▊ | 196/221 [01:43<00:13,  1.89it/s][A
 89%|████████▉ | 197/221 [01:44<00:12,  1.89it/s][A
 90%|████████▉ | 198/221 [01:44<00:12,  1.89it/s][A
 90%|█████████ | 199/221 [01:45<00:11,  1.89it/s][A
 90%|█████████ | 200/221 [01:45<00:11,  1.89it/s][A
 91%|█████████ | 201/221 [01:46<00:10,  1.89it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.89it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.89it/s][A
 92%|█████████▏| 204/221 [01:47<00:08,  1.89it/s][A
 93%|█████████▎| 205/221 [01:48<00:08,  1.89it/s][A
 93%|█████████▎| 206/221 [01:48<00:07,  1.89it/s][A
 94%|█████████▎| 207/221 [01:49<00:07,  1.89it/s][A
 94%|█████████▍| 208/221 [01:49<00:06,  1.89it/s][A
 95%|█████████▍| 209/221 [01:50<00:06,  1.89it/s][A
 95%|█████████▌| 210/221 [01:50<00:05,  1.89it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.89it/s][A
 96%|█████████▌| 212/221 [01:51<00:04,  1.89it/s][A
 96%|█████████▋| 213/221 [01:52<00:04,  1.89it/s][A
 97%|█████████▋| 214/221 [01:53<00:03,  1.89it/s][A
 97%|█████████▋| 215/221 [01:53<00:03,  1.89it/s][A
 98%|█████████▊| 216/221 [01:54<00:02,  1.89it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.89it/s][A
 99%|█████████▊| 218/221 [01:55<00:01,  1.89it/s][A
 99%|█████████▉| 219/221 [01:55<00:01,  1.89it/s][A
100%|█████████▉| 220/221 [01:56<00:00,  1.89it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.89it/s][A100%|██████████| 221/221 [01:56<00:00,  1.89it/s]

  0%|          | 0/221 [00:00<?, ?it/s][A
  0%|          | 1/221 [00:00<01:10,  3.14it/s][A
  1%|          | 2/221 [00:01<02:34,  1.41it/s][A
  1%|▏         | 3/221 [00:01<01:43,  2.10it/s][A
  2%|▏         | 4/221 [00:02<01:56,  1.86it/s][A
  2%|▏         | 5/221 [00:02<02:06,  1.71it/s][A
  3%|▎         | 6/221 [00:03<01:47,  1.99it/s][A
  3%|▎         | 7/221 [00:03<01:41,  2.11it/s][A
  4%|▎         | 8/221 [00:04<02:43,  1.30it/s][A
  4%|▍         | 9/221 [00:05<02:45,  1.28it/s][A
  5%|▍         | 10/221 [00:06<02:11,  1.60it/s][A
  5%|▍         | 11/221 [00:06<01:52,  1.87it/s][A
  5%|▌         | 12/221 [00:06<01:40,  2.08it/s][A
  6%|▌         | 13/221 [00:06<01:20,  2.58it/s][A
  6%|▋         | 14/221 [00:07<01:04,  3.23it/s][A
  7%|▋         | 15/221 [00:07<00:58,  3.49it/s][A
  7%|▋         | 16/221 [00:08<01:36,  2.13it/s][A
  8%|▊         | 17/221 [00:09<02:12,  1.54it/s][A
  8%|▊         | 18/221 [00:09<01:44,  1.94it/s][A
  9%|▊         | 19/221 [00:10<02:00,  1.67it/s][A
  9%|▉         | 20/221 [00:10<01:48,  1.85it/s][A
 10%|▉         | 21/221 [00:11<01:52,  1.78it/s][A
 10%|▉         | 22/221 [00:12<02:37,  1.26it/s][A
 10%|█         | 23/221 [00:12<02:05,  1.57it/s][A
 11%|█         | 24/221 [00:13<01:55,  1.70it/s][A
 11%|█▏        | 25/221 [00:14<02:02,  1.60it/s][A
 12%|█▏        | 26/221 [00:14<02:12,  1.47it/s][A
 12%|█▏        | 27/221 [00:15<01:43,  1.87it/s][A
 13%|█▎        | 28/221 [00:16<02:28,  1.30it/s][A
 13%|█▎        | 29/221 [00:16<01:56,  1.64it/s][A
 14%|█▎        | 30/221 [00:17<01:59,  1.59it/s][A
 14%|█▍        | 31/221 [00:17<01:54,  1.65it/s][A
 14%|█▍        | 32/221 [00:19<02:29,  1.26it/s][A
 15%|█▍        | 33/221 [00:19<02:16,  1.38it/s][A
 15%|█▌        | 34/221 [00:19<01:46,  1.76it/s][A
 16%|█▌        | 35/221 [00:20<01:30,  2.05it/s][A
 16%|█▋        | 36/221 [00:20<01:38,  1.88it/s][A
 17%|█▋        | 37/221 [00:21<01:30,  2.04it/s][A
 17%|█▋        | 38/221 [00:22<01:53,  1.61it/s][A
 18%|█▊        | 39/221 [00:22<01:58,  1.53it/s][A
 18%|█▊        | 40/221 [00:23<02:22,  1.27it/s][A
 19%|█▊        | 41/221 [00:24<01:52,  1.60it/s][A
 19%|█▉        | 42/221 [00:24<01:34,  1.89it/s][A
 19%|█▉        | 43/221 [00:24<01:15,  2.35it/s][A
 20%|█▉        | 44/221 [00:25<01:24,  2.09it/s][A
 20%|██        | 45/221 [00:25<01:04,  2.74it/s][A
 21%|██        | 46/221 [00:25<01:01,  2.85it/s][A
 21%|██▏       | 47/221 [00:26<01:13,  2.36it/s][A
 22%|██▏       | 48/221 [00:26<01:01,  2.82it/s][A
 22%|██▏       | 49/221 [00:27<01:28,  1.94it/s][A
 23%|██▎       | 50/221 [00:27<01:30,  1.89it/s][A
 23%|██▎       | 51/221 [00:28<01:19,  2.13it/s][A
 24%|██▎       | 52/221 [00:28<01:12,  2.32it/s][A
 24%|██▍       | 53/221 [00:29<01:13,  2.28it/s][A
 24%|██▍       | 54/221 [00:29<01:20,  2.08it/s][A
 25%|██▍       | 55/221 [00:29<01:11,  2.31it/s][A
 25%|██▌       | 56/221 [00:30<01:12,  2.29it/s][A
 26%|██▌       | 57/221 [00:30<01:02,  2.61it/s][A
 26%|██▌       | 58/221 [00:30<00:58,  2.79it/s][A
 27%|██▋       | 59/221 [00:31<00:46,  3.51it/s][A
 27%|██▋       | 60/221 [00:31<01:02,  2.59it/s][A
 28%|██▊       | 61/221 [00:32<01:15,  2.11it/s][A
 28%|██▊       | 62/221 [00:32<01:12,  2.19it/s][A
 29%|██▊       | 63/221 [00:33<01:37,  1.62it/s][A
 29%|██▉       | 64/221 [00:35<02:08,  1.23it/s][A
 29%|██▉       | 65/221 [00:36<02:24,  1.08it/s][A
 30%|██▉       | 66/221 [00:36<02:04,  1.25it/s][A
 30%|███       | 67/221 [00:37<01:43,  1.49it/s][A
 31%|███       | 68/221 [00:37<01:25,  1.79it/s][A
 31%|███       | 69/221 [00:37<01:13,  2.06it/s][A
 32%|███▏      | 70/221 [00:38<01:16,  1.98it/s][A
 32%|███▏      | 71/221 [00:39<01:30,  1.67it/s][A
 33%|███▎      | 72/221 [00:39<01:26,  1.73it/s][A
 33%|███▎      | 73/221 [00:40<01:30,  1.64it/s][A
 33%|███▎      | 74/221 [00:40<01:22,  1.78it/s][A
 34%|███▍      | 75/221 [00:40<01:09,  2.11it/s][A
 34%|███▍      | 76/221 [00:41<01:04,  2.25it/s][A
 35%|███▍      | 77/221 [00:41<00:55,  2.62it/s][A
 35%|███▌      | 78/221 [00:42<00:56,  2.53it/s][A
 36%|███▌      | 79/221 [00:42<01:14,  1.91it/s][A
 36%|███▌      | 80/221 [00:43<00:59,  2.36it/s][A
 37%|███▋      | 81/221 [00:43<01:07,  2.07it/s][A
 37%|███▋      | 82/221 [00:44<01:32,  1.51it/s][A
 38%|███▊      | 83/221 [00:45<01:16,  1.81it/s][A
 38%|███▊      | 84/221 [00:45<01:28,  1.55it/s][A
 38%|███▊      | 85/221 [00:46<01:08,  1.97it/s][A
 39%|███▉      | 86/221 [00:46<01:08,  1.97it/s][A
 39%|███▉      | 87/221 [00:47<01:07,  1.99it/s][A
 40%|███▉      | 88/221 [00:47<00:58,  2.26it/s][A
 40%|████      | 89/221 [00:48<01:08,  1.93it/s][A
 41%|████      | 90/221 [00:48<01:14,  1.76it/s][A
 41%|████      | 91/221 [00:49<01:03,  2.06it/s][A
 42%|████▏     | 92/221 [00:49<01:04,  2.01it/s][A
 43%|████▎     | 94/221 [00:49<00:45,  2.78it/s][A
 43%|████▎     | 95/221 [00:50<00:44,  2.82it/s][A
 43%|████▎     | 96/221 [00:50<00:44,  2.84it/s][A
 44%|████▍     | 97/221 [00:51<00:49,  2.53it/s][A
 44%|████▍     | 98/221 [00:52<01:04,  1.90it/s][A
 45%|████▍     | 99/221 [00:52<01:07,  1.81it/s][A
 45%|████▌     | 100/221 [00:53<01:14,  1.63it/s][A
 46%|████▌     | 101/221 [00:54<01:17,  1.54it/s][A
 46%|████▌     | 102/221 [00:55<01:25,  1.40it/s][A
 47%|████▋     | 103/221 [00:55<01:14,  1.58it/s][A
 47%|████▋     | 104/221 [00:56<01:16,  1.53it/s][A
 48%|████▊     | 105/221 [00:56<01:07,  1.72it/s][A
 48%|████▊     | 106/221 [00:56<00:54,  2.12it/s][A
 48%|████▊     | 107/221 [00:57<00:51,  2.23it/s][A
 49%|████▉     | 108/221 [00:57<00:50,  2.22it/s][A
 49%|████▉     | 109/221 [00:58<00:53,  2.11it/s][A
 50%|████▉     | 110/221 [00:58<01:00,  1.84it/s][A
 50%|█████     | 111/221 [00:59<01:16,  1.44it/s][A
 51%|█████     | 112/221 [01:00<01:06,  1.65it/s][A
 51%|█████     | 113/221 [01:00<00:56,  1.92it/s][A
 52%|█████▏    | 114/221 [01:01<00:53,  1.99it/s][A
 52%|█████▏    | 115/221 [01:01<00:56,  1.87it/s][A
 52%|█████▏    | 116/221 [01:01<00:46,  2.28it/s][A
 53%|█████▎    | 117/221 [01:02<00:42,  2.43it/s][A
 53%|█████▎    | 118/221 [01:02<00:43,  2.37it/s][A
 54%|█████▍    | 119/221 [01:03<00:50,  2.02it/s][A
 54%|█████▍    | 120/221 [01:03<00:52,  1.92it/s][A
 55%|█████▍    | 121/221 [01:04<00:48,  2.07it/s][A
 55%|█████▌    | 122/221 [01:04<00:51,  1.94it/s][A
 56%|█████▌    | 123/221 [01:05<00:45,  2.15it/s][A
 56%|█████▌    | 124/221 [01:06<01:03,  1.53it/s][A
 57%|█████▋    | 125/221 [01:06<00:53,  1.78it/s][A
 57%|█████▋    | 126/221 [01:06<00:42,  2.23it/s][A
 57%|█████▋    | 127/221 [01:08<00:59,  1.57it/s][A
 58%|█████▊    | 128/221 [01:08<00:55,  1.69it/s][A
 58%|█████▊    | 129/221 [01:09<00:58,  1.56it/s][A
 59%|█████▉    | 130/221 [01:09<00:46,  1.96it/s][A
 59%|█████▉    | 131/221 [01:09<00:43,  2.06it/s][A
 60%|█████▉    | 132/221 [01:10<00:52,  1.68it/s][A
 60%|██████    | 133/221 [01:11<00:54,  1.63it/s][A
 61%|██████    | 134/221 [01:11<00:50,  1.71it/s][A
 61%|██████    | 135/221 [01:12<00:46,  1.84it/s][A
 62%|██████▏   | 136/221 [01:13<00:55,  1.53it/s][A
 62%|██████▏   | 137/221 [01:14<00:58,  1.44it/s][A
 62%|██████▏   | 138/221 [01:14<01:00,  1.37it/s][A
 63%|██████▎   | 139/221 [01:16<01:12,  1.13it/s][A
 63%|██████▎   | 140/221 [01:16<01:10,  1.15it/s][A
 64%|██████▍   | 141/221 [01:17<01:02,  1.28it/s][A
 64%|██████▍   | 142/221 [01:18<00:56,  1.39it/s][A
 65%|██████▍   | 143/221 [01:18<00:44,  1.75it/s][A
 65%|██████▌   | 144/221 [01:18<00:41,  1.87it/s][A
 66%|██████▌   | 145/221 [01:19<00:39,  1.93it/s][A
 66%|██████▌   | 146/221 [01:19<00:30,  2.50it/s][A
 67%|██████▋   | 147/221 [01:19<00:32,  2.28it/s][A
 67%|██████▋   | 148/221 [01:20<00:30,  2.40it/s][A
 68%|██████▊   | 150/221 [01:20<00:22,  3.13it/s][A
 68%|██████▊   | 151/221 [01:20<00:19,  3.61it/s][A
 69%|██████▉   | 152/221 [01:21<00:21,  3.14it/s][A
 69%|██████▉   | 153/221 [01:21<00:20,  3.37it/s][A
 70%|██████▉   | 154/221 [01:22<00:31,  2.11it/s][A
 70%|███████   | 155/221 [01:23<00:40,  1.62it/s][A
 71%|███████   | 156/221 [01:23<00:36,  1.77it/s][A
 71%|███████   | 157/221 [01:24<00:36,  1.76it/s][A
 71%|███████▏  | 158/221 [01:24<00:29,  2.10it/s][A
 72%|███████▏  | 159/221 [01:24<00:23,  2.62it/s][A
 72%|███████▏  | 160/221 [01:25<00:25,  2.35it/s][A
 73%|███████▎  | 161/221 [01:25<00:21,  2.83it/s][A
 73%|███████▎  | 162/221 [01:26<00:25,  2.31it/s][A
 74%|███████▍  | 163/221 [01:26<00:22,  2.61it/s][A
 74%|███████▍  | 164/221 [01:26<00:21,  2.65it/s][A
 75%|███████▍  | 165/221 [01:27<00:18,  3.05it/s][A
 75%|███████▌  | 166/221 [01:27<00:26,  2.09it/s][A
 76%|███████▌  | 167/221 [01:28<00:23,  2.30it/s][A
 76%|███████▌  | 168/221 [01:28<00:21,  2.48it/s][A
 76%|███████▋  | 169/221 [01:29<00:37,  1.39it/s][A
 77%|███████▋  | 170/221 [01:30<00:31,  1.60it/s][A
 77%|███████▋  | 171/221 [01:30<00:30,  1.62it/s][A
 78%|███████▊  | 172/221 [01:31<00:24,  1.99it/s][A
 78%|███████▊  | 173/221 [01:31<00:21,  2.28it/s][A
 79%|███████▊  | 174/221 [01:31<00:20,  2.34it/s][A
 79%|███████▉  | 175/221 [01:32<00:19,  2.33it/s][A
 80%|███████▉  | 176/221 [01:32<00:17,  2.50it/s][A
 80%|████████  | 177/221 [01:33<00:18,  2.37it/s][A
 81%|████████  | 178/221 [01:33<00:17,  2.42it/s][A
 81%|████████  | 179/221 [01:34<00:20,  2.03it/s][A
 82%|████████▏ | 181/221 [01:34<00:13,  3.03it/s][A
 82%|████████▏ | 182/221 [01:34<00:14,  2.77it/s][A
 83%|████████▎ | 183/221 [01:35<00:16,  2.37it/s][A
 83%|████████▎ | 184/221 [01:36<00:19,  1.88it/s][A
 84%|████████▎ | 185/221 [01:36<00:19,  1.81it/s][A
 84%|████████▍ | 186/221 [01:37<00:15,  2.22it/s][A
 85%|████████▍ | 187/221 [01:37<00:18,  1.82it/s][A
 85%|████████▌ | 188/221 [01:38<00:15,  2.18it/s][A
 86%|████████▌ | 189/221 [01:38<00:17,  1.81it/s][A
 86%|████████▌ | 190/221 [01:39<00:15,  2.06it/s][A
 86%|████████▋ | 191/221 [01:39<00:15,  1.93it/s][A
 87%|████████▋ | 192/221 [01:40<00:14,  1.96it/s][A
 87%|████████▋ | 193/221 [01:40<00:12,  2.23it/s][A
 88%|████████▊ | 194/221 [01:41<00:15,  1.70it/s][A
 88%|████████▊ | 195/221 [01:42<00:19,  1.36it/s][A
 89%|████████▊ | 196/221 [01:43<00:16,  1.50it/s][A
 89%|████████▉ | 197/221 [01:43<00:15,  1.54it/s][A
 90%|████████▉ | 198/221 [01:44<00:15,  1.51it/s][A
 90%|█████████ | 199/221 [01:44<00:12,  1.76it/s][A
 90%|█████████ | 200/221 [01:45<00:13,  1.55it/s][A
 91%|█████████ | 201/221 [01:46<00:12,  1.61it/s][A
 91%|█████████▏| 202/221 [01:46<00:10,  1.88it/s][A
 92%|█████████▏| 203/221 [01:47<00:09,  1.85it/s][A
 92%|█████████▏| 204/221 [01:47<00:07,  2.19it/s][A
 93%|█████████▎| 205/221 [01:47<00:06,  2.38it/s][A
 93%|█████████▎| 206/221 [01:48<00:05,  2.53it/s][A
 94%|█████████▎| 207/221 [01:48<00:04,  2.87it/s][A
 94%|█████████▍| 208/221 [01:49<00:07,  1.64it/s][A
 95%|█████████▍| 209/221 [01:50<00:07,  1.71it/s][A
 95%|█████████▌| 210/221 [01:50<00:04,  2.20it/s][A
 95%|█████████▌| 211/221 [01:51<00:05,  1.71it/s][A
 96%|█████████▌| 212/221 [01:51<00:04,  2.07it/s][A
 96%|█████████▋| 213/221 [01:51<00:04,  1.96it/s][A
 97%|█████████▋| 214/221 [01:52<00:04,  1.69it/s][A
 97%|█████████▋| 215/221 [01:52<00:02,  2.21it/s][A
 98%|█████████▊| 216/221 [01:53<00:02,  1.93it/s][A
 98%|█████████▊| 217/221 [01:54<00:02,  1.78it/s][A
 99%|█████████▊| 218/221 [01:54<00:01,  2.15it/s][A
 99%|█████████▉| 219/221 [01:54<00:00,  2.01it/s][A
100%|█████████▉| 220/221 [01:55<00:00,  2.12it/s][A
100%|██████████| 221/221 [01:56<00:00,  1.90it/s][A100%|██████████| 221/221 [01:56<00:00,  1.90it/s]
09/19/2024 13:46:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_forward=====step 8499--===========

09/19/2024 13:46:18 - INFO - __main__ -   {'area_r1': 45.9, 'area_recall': '45.9/75.8/84.4', 'area_ravg': 68.7}
09/19/2024 13:46:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_backard=====step 8499--===========

09/19/2024 13:46:18 - INFO - __main__ -   {'forward_r1': 50.7, 'forward_recall': '50.7/78.7/87.4', 'forward_ravg': 72.3}
09/19/2024 13:46:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video=====step 8499--===========

09/19/2024 13:46:18 - INFO - __main__ -   {'area_video_r1': 49.4, 'area_video_recall': '49.4/79.0/88.0', 'area_video_ravg': 72.1}
09/19/2024 13:46:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_area_back_with_video====history best step: 3499=======

09/19/2024 13:46:18 - INFO - __main__ -   {'area_video_r1': 50.1, 'area_video_recall': '50.1/79.0/87.4', 'area_video_ravg': 72.2}
09/19/2024 13:46:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_area=====step 8499--===========

09/19/2024 13:46:18 - INFO - __main__ -   {'area_video_r1': 63.5, 'area_video_recall': '63.5/83.6/89.3', 'area_video_ravg': 78.8, 'area_video_back_r1': 63.7, 'area_video_back_recall': '63.7/85.2/92.3', 'area_video_back_ravg': 80.4}
09/19/2024 13:46:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_area====history best step: 6999=======

09/19/2024 13:46:18 - INFO - __main__ -   {'area_video_r1': 63.8, 'area_video_recall': '63.8/84.2/89.6', 'area_video_ravg': 79.2, 'area_video_back_r1': 64.1, 'area_video_back_recall': '64.1/85.5/92.1', 'area_video_back_ravg': 80.6}
09/19/2024 13:46:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas=====step 8499--===========

09/19/2024 13:46:18 - INFO - __main__ -   {'video_r1': 31.3, 'video_recall': '31.3/55.9/66.7', 'video_ravg': 51.3}
09/19/2024 13:46:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itc_tvas====history best step: 999=======

09/19/2024 13:46:18 - INFO - __main__ -   {'video_r1': 36.4, 'video_recall': '36.4/61.7/73.5', 'video_ravg': 57.2}
09/19/2024 13:46:18 - INFO - __main__ -   ====-evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas=====step 8499--===========

09/19/2024 13:46:18 - INFO - __main__ -   {'video_r1': 61.4, 'video_recall': '61.4/80.7/85.1', 'video_ravg': 75.7}
09/19/2024 13:46:18 - INFO - __main__ -   ======evaluation--ret%tvas--msrvtt_ret_ret_itm_tvas====history best step: 8499=======

09/19/2024 13:46:18 - INFO - __main__ -   {'video_r1': 61.4, 'video_recall': '61.4/80.7/85.1', 'video_ravg': 75.7}
09/19/2024 13:46:43 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02608315646648407, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.0690317153930664, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.095114827156067}
 95%|█████████▌| 8500/8917 [11:32:09<19:36:34, 169.29s/it] 95%|█████████▌| 8501/8917 [11:32:12<13:48:16, 119.46s/it] 95%|█████████▌| 8502/8917 [11:32:16<9:45:32, 84.66s/it]   95%|█████████▌| 8503/8917 [11:32:20<6:58:07, 60.60s/it] 95%|█████████▌| 8504/8917 [11:32:24<4:59:27, 43.51s/it] 95%|█████████▌| 8505/8917 [11:32:27<3:36:00, 31.46s/it] 95%|█████████▌| 8506/8917 [11:32:31<2:38:15, 23.10s/it] 95%|█████████▌| 8507/8917 [11:32:34<1:57:49, 17.24s/it] 95%|█████████▌| 8508/8917 [11:32:38<1:30:03, 13.21s/it] 95%|█████████▌| 8509/8917 [11:32:42<1:11:16, 10.48s/it] 95%|█████████▌| 8510/8917 [11:32:46<57:10,  8.43s/it]   95%|█████████▌| 8511/8917 [11:32:50<47:36,  7.04s/it] 95%|█████████▌| 8512/8917 [11:32:53<40:25,  5.99s/it] 95%|█████████▌| 8513/8917 [11:32:57<35:57,  5.34s/it] 95%|█████████▌| 8514/8917 [11:33:01<33:52,  5.04s/it] 95%|█████████▌| 8515/8917 [11:33:05<31:07,  4.64s/it] 96%|█████████▌| 8516/8917 [11:33:08<28:19,  4.24s/it] 96%|█████████▌| 8517/8917 [11:33:13<28:03,  4.21s/it] 96%|█████████▌| 8518/8917 [11:33:17<27:33,  4.14s/it] 96%|█████████▌| 8519/8917 [11:33:20<26:03,  3.93s/it] 96%|█████████▌| 8520/8917 [11:33:24<25:28,  3.85s/it] 96%|█████████▌| 8521/8917 [11:33:28<25:31,  3.87s/it] 96%|█████████▌| 8522/8917 [11:33:31<25:14,  3.83s/it] 96%|█████████▌| 8523/8917 [11:33:35<25:16,  3.85s/it] 96%|█████████▌| 8524/8917 [11:33:39<24:33,  3.75s/it] 96%|█████████▌| 8525/8917 [11:33:42<24:19,  3.72s/it] 96%|█████████▌| 8526/8917 [11:33:46<24:04,  3.69s/it] 96%|█████████▌| 8527/8917 [11:33:50<24:00,  3.69s/it] 96%|█████████▌| 8528/8917 [11:33:54<24:19,  3.75s/it] 96%|█████████▌| 8529/8917 [11:33:57<24:05,  3.73s/it] 96%|█████████▌| 8530/8917 [11:34:01<23:43,  3.68s/it] 96%|█████████▌| 8531/8917 [11:34:05<23:43,  3.69s/it] 96%|█████████▌| 8532/8917 [11:34:08<23:31,  3.67s/it] 96%|█████████▌| 8533/8917 [11:34:12<23:58,  3.75s/it] 96%|█████████▌| 8534/8917 [11:34:16<23:42,  3.71s/it] 96%|█████████▌| 8535/8917 [11:34:19<23:14,  3.65s/it] 96%|█████████▌| 8536/8917 [11:34:23<23:05,  3.64s/it] 96%|█████████▌| 8537/8917 [11:34:27<23:21,  3.69s/it] 96%|█████████▌| 8538/8917 [11:34:30<23:08,  3.66s/it] 96%|█████████▌| 8539/8917 [11:34:35<24:18,  3.86s/it] 96%|█████████▌| 8540/8917 [11:34:38<23:40,  3.77s/it] 96%|█████████▌| 8541/8917 [11:34:42<23:30,  3.75s/it] 96%|█████████▌| 8542/8917 [11:34:45<23:01,  3.68s/it] 96%|█████████▌| 8543/8917 [11:34:49<23:22,  3.75s/it] 96%|█████████▌| 8544/8917 [11:34:53<22:55,  3.69s/it] 96%|█████████▌| 8545/8917 [11:34:57<23:05,  3.72s/it] 96%|█████████▌| 8546/8917 [11:35:01<23:40,  3.83s/it] 96%|█████████▌| 8547/8917 [11:35:04<23:23,  3.79s/it] 96%|█████████▌| 8548/8917 [11:35:08<23:19,  3.79s/it] 96%|█████████▌| 8549/8917 [11:35:12<23:14,  3.79s/it]09/19/2024 13:49:49 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0398966483771801, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.3458311557769775, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.3857277631759644}
 96%|█████████▌| 8550/8917 [11:35:16<22:44,  3.72s/it] 96%|█████████▌| 8551/8917 [11:35:19<22:09,  3.63s/it] 96%|█████████▌| 8552/8917 [11:35:22<21:44,  3.57s/it] 96%|█████████▌| 8553/8917 [11:35:26<22:36,  3.73s/it] 96%|█████████▌| 8554/8917 [11:35:30<22:00,  3.64s/it] 96%|█████████▌| 8555/8917 [11:35:34<22:12,  3.68s/it] 96%|█████████▌| 8556/8917 [11:35:37<22:18,  3.71s/it] 96%|█████████▌| 8557/8917 [11:35:41<22:23,  3.73s/it] 96%|█████████▌| 8558/8917 [11:35:45<22:22,  3.74s/it] 96%|█████████▌| 8559/8917 [11:35:48<21:20,  3.58s/it] 96%|█████████▌| 8560/8917 [11:35:52<21:20,  3.59s/it] 96%|█████████▌| 8561/8917 [11:35:56<21:38,  3.65s/it] 96%|█████████▌| 8562/8917 [11:35:59<21:31,  3.64s/it] 96%|█████████▌| 8563/8917 [11:36:03<22:26,  3.80s/it] 96%|█████████▌| 8564/8917 [11:36:07<22:26,  3.81s/it] 96%|█████████▌| 8565/8917 [11:36:11<21:40,  3.69s/it] 96%|█████████▌| 8566/8917 [11:36:15<22:18,  3.81s/it] 96%|█████████▌| 8567/8917 [11:36:19<22:06,  3.79s/it] 96%|█████████▌| 8568/8917 [11:36:22<21:41,  3.73s/it] 96%|█████████▌| 8569/8917 [11:36:26<21:17,  3.67s/it] 96%|█████████▌| 8570/8917 [11:36:30<21:48,  3.77s/it] 96%|█████████▌| 8571/8917 [11:36:33<21:28,  3.72s/it] 96%|█████████▌| 8572/8917 [11:36:37<21:51,  3.80s/it] 96%|█████████▌| 8573/8917 [11:36:41<21:26,  3.74s/it] 96%|█████████▌| 8574/8917 [11:36:44<20:45,  3.63s/it] 96%|█████████▌| 8575/8917 [11:36:48<20:38,  3.62s/it] 96%|█████████▌| 8576/8917 [11:36:51<20:43,  3.65s/it] 96%|█████████▌| 8577/8917 [11:36:55<20:46,  3.67s/it] 96%|█████████▌| 8578/8917 [11:36:59<21:13,  3.76s/it] 96%|█████████▌| 8579/8917 [11:37:03<21:19,  3.79s/it] 96%|█████████▌| 8580/8917 [11:37:07<20:53,  3.72s/it] 96%|█████████▌| 8581/8917 [11:37:11<21:14,  3.79s/it] 96%|█████████▌| 8582/8917 [11:37:14<20:50,  3.73s/it] 96%|█████████▋| 8583/8917 [11:37:17<20:00,  3.60s/it] 96%|█████████▋| 8584/8917 [11:37:21<20:13,  3.64s/it] 96%|█████████▋| 8585/8917 [11:37:25<20:56,  3.78s/it] 96%|█████████▋| 8586/8917 [11:37:29<20:45,  3.76s/it] 96%|█████████▋| 8587/8917 [11:37:33<20:38,  3.75s/it] 96%|█████████▋| 8588/8917 [11:37:36<20:16,  3.70s/it] 96%|█████████▋| 8589/8917 [11:37:40<20:18,  3.72s/it] 96%|█████████▋| 8590/8917 [11:37:44<20:48,  3.82s/it] 96%|█████████▋| 8591/8917 [11:37:48<20:38,  3.80s/it] 96%|█████████▋| 8592/8917 [11:37:52<21:07,  3.90s/it] 96%|█████████▋| 8593/8917 [11:37:55<20:02,  3.71s/it] 96%|█████████▋| 8594/8917 [11:37:59<20:07,  3.74s/it] 96%|█████████▋| 8595/8917 [11:38:03<19:58,  3.72s/it] 96%|█████████▋| 8596/8917 [11:38:06<19:49,  3.71s/it] 96%|█████████▋| 8597/8917 [11:38:10<20:04,  3.76s/it] 96%|█████████▋| 8598/8917 [11:38:14<19:55,  3.75s/it] 96%|█████████▋| 8599/8917 [11:38:18<19:52,  3.75s/it]09/19/2024 13:52:55 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0194964911788702, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1623320579528809, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.181828498840332}
 96%|█████████▋| 8600/8917 [11:38:22<20:09,  3.82s/it] 96%|█████████▋| 8601/8917 [11:38:26<20:11,  3.83s/it] 96%|█████████▋| 8602/8917 [11:38:29<19:44,  3.76s/it] 96%|█████████▋| 8603/8917 [11:38:33<19:52,  3.80s/it] 96%|█████████▋| 8604/8917 [11:38:37<19:18,  3.70s/it] 97%|█████████▋| 8605/8917 [11:38:40<19:08,  3.68s/it] 97%|█████████▋| 8606/8917 [11:38:44<18:58,  3.66s/it] 97%|█████████▋| 8607/8917 [11:38:47<18:44,  3.63s/it] 97%|█████████▋| 8608/8917 [11:38:51<19:14,  3.74s/it] 97%|█████████▋| 8609/8917 [11:38:55<18:37,  3.63s/it] 97%|█████████▋| 8610/8917 [11:38:58<18:39,  3.65s/it] 97%|█████████▋| 8611/8917 [11:39:02<18:53,  3.70s/it] 97%|█████████▋| 8612/8917 [11:39:06<18:41,  3.68s/it] 97%|█████████▋| 8613/8917 [11:39:10<18:44,  3.70s/it] 97%|█████████▋| 8614/8917 [11:39:13<18:20,  3.63s/it] 97%|█████████▋| 8615/8917 [11:39:17<18:29,  3.68s/it] 97%|█████████▋| 8616/8917 [11:39:20<17:50,  3.56s/it] 97%|█████████▋| 8617/8917 [11:39:24<18:13,  3.65s/it] 97%|█████████▋| 8618/8917 [11:39:28<18:03,  3.62s/it] 97%|█████████▋| 8619/8917 [11:39:31<17:47,  3.58s/it] 97%|█████████▋| 8620/8917 [11:39:35<17:59,  3.64s/it] 97%|█████████▋| 8621/8917 [11:39:38<17:41,  3.59s/it] 97%|█████████▋| 8622/8917 [11:39:42<18:13,  3.71s/it] 97%|█████████▋| 8623/8917 [11:39:46<18:07,  3.70s/it] 97%|█████████▋| 8624/8917 [11:39:50<18:00,  3.69s/it] 97%|█████████▋| 8625/8917 [11:39:53<18:01,  3.70s/it] 97%|█████████▋| 8626/8917 [11:39:57<18:07,  3.74s/it] 97%|█████████▋| 8627/8917 [11:40:01<17:44,  3.67s/it] 97%|█████████▋| 8628/8917 [11:40:04<17:16,  3.58s/it] 97%|█████████▋| 8629/8917 [11:40:08<17:45,  3.70s/it] 97%|█████████▋| 8630/8917 [11:40:12<17:59,  3.76s/it] 97%|█████████▋| 8631/8917 [11:40:15<17:12,  3.61s/it] 97%|█████████▋| 8632/8917 [11:40:19<17:20,  3.65s/it] 97%|█████████▋| 8633/8917 [11:40:23<17:40,  3.73s/it] 97%|█████████▋| 8634/8917 [11:40:27<17:25,  3.70s/it] 97%|█████████▋| 8635/8917 [11:40:31<17:52,  3.80s/it] 97%|█████████▋| 8636/8917 [11:40:35<18:12,  3.89s/it] 97%|█████████▋| 8637/8917 [11:40:38<17:54,  3.84s/it] 97%|█████████▋| 8638/8917 [11:40:42<17:27,  3.75s/it] 97%|█████████▋| 8639/8917 [11:40:46<17:42,  3.82s/it] 97%|█████████▋| 8640/8917 [11:40:50<17:40,  3.83s/it] 97%|█████████▋| 8641/8917 [11:40:53<17:15,  3.75s/it] 97%|█████████▋| 8642/8917 [11:40:57<17:21,  3.79s/it] 97%|█████████▋| 8643/8917 [11:41:01<16:40,  3.65s/it] 97%|█████████▋| 8644/8917 [11:41:04<16:50,  3.70s/it] 97%|█████████▋| 8645/8917 [11:41:08<16:20,  3.61s/it] 97%|█████████▋| 8646/8917 [11:41:12<17:00,  3.76s/it] 97%|█████████▋| 8647/8917 [11:41:15<16:32,  3.68s/it] 97%|█████████▋| 8648/8917 [11:41:19<16:24,  3.66s/it] 97%|█████████▋| 8649/8917 [11:41:23<16:21,  3.66s/it]09/19/2024 13:56:00 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.025033432990312576, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.232266902923584, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2573003768920898}
 97%|█████████▋| 8650/8917 [11:41:26<16:29,  3.71s/it] 97%|█████████▋| 8651/8917 [11:41:30<16:31,  3.73s/it] 97%|█████████▋| 8652/8917 [11:41:34<16:17,  3.69s/it] 97%|█████████▋| 8653/8917 [11:41:38<16:24,  3.73s/it] 97%|█████████▋| 8654/8917 [11:41:41<16:01,  3.65s/it] 97%|█████████▋| 8655/8917 [11:41:45<16:10,  3.70s/it] 97%|█████████▋| 8656/8917 [11:41:49<16:16,  3.74s/it] 97%|█████████▋| 8657/8917 [11:41:53<16:17,  3.76s/it] 97%|█████████▋| 8658/8917 [11:41:56<15:42,  3.64s/it] 97%|█████████▋| 8659/8917 [11:42:00<16:01,  3.73s/it] 97%|█████████▋| 8660/8917 [11:42:04<16:08,  3.77s/it] 97%|█████████▋| 8661/8917 [11:42:07<15:24,  3.61s/it] 97%|█████████▋| 8662/8917 [11:42:11<15:44,  3.70s/it] 97%|█████████▋| 8663/8917 [11:42:15<15:43,  3.71s/it] 97%|█████████▋| 8664/8917 [11:42:18<15:27,  3.66s/it] 97%|█████████▋| 8665/8917 [11:42:22<15:18,  3.65s/it] 97%|█████████▋| 8666/8917 [11:42:26<15:47,  3.77s/it] 97%|█████████▋| 8667/8917 [11:42:30<15:38,  3.75s/it] 97%|█████████▋| 8668/8917 [11:42:33<15:05,  3.64s/it] 97%|█████████▋| 8669/8917 [11:42:37<15:30,  3.75s/it] 97%|█████████▋| 8670/8917 [11:42:41<15:36,  3.79s/it] 97%|█████████▋| 8671/8917 [11:42:44<15:16,  3.72s/it] 97%|█████████▋| 8672/8917 [11:42:48<15:09,  3.71s/it] 97%|█████████▋| 8673/8917 [11:42:52<15:14,  3.75s/it] 97%|█████████▋| 8674/8917 [11:42:56<15:06,  3.73s/it] 97%|█████████▋| 8675/8917 [11:42:59<15:01,  3.73s/it] 97%|█████████▋| 8676/8917 [11:43:03<14:49,  3.69s/it] 97%|█████████▋| 8677/8917 [11:43:06<14:26,  3.61s/it] 97%|█████████▋| 8678/8917 [11:43:10<14:23,  3.61s/it] 97%|█████████▋| 8679/8917 [11:43:14<14:40,  3.70s/it] 97%|█████████▋| 8680/8917 [11:43:18<14:45,  3.74s/it] 97%|█████████▋| 8681/8917 [11:43:22<14:46,  3.76s/it] 97%|█████████▋| 8682/8917 [11:43:25<14:47,  3.78s/it] 97%|█████████▋| 8683/8917 [11:43:29<14:20,  3.68s/it] 97%|█████████▋| 8684/8917 [11:43:32<14:13,  3.66s/it] 97%|█████████▋| 8685/8917 [11:43:36<14:16,  3.69s/it] 97%|█████████▋| 8686/8917 [11:43:40<14:26,  3.75s/it] 97%|█████████▋| 8687/8917 [11:43:44<14:56,  3.90s/it] 97%|█████████▋| 8688/8917 [11:43:48<15:04,  3.95s/it] 97%|█████████▋| 8689/8917 [11:43:52<14:19,  3.77s/it] 97%|█████████▋| 8690/8917 [11:43:55<14:11,  3.75s/it] 97%|█████████▋| 8691/8917 [11:43:59<13:59,  3.72s/it] 97%|█████████▋| 8692/8917 [11:44:03<13:52,  3.70s/it] 97%|█████████▋| 8693/8917 [11:44:06<13:43,  3.68s/it] 97%|█████████▋| 8694/8917 [11:44:10<13:59,  3.77s/it] 98%|█████████▊| 8695/8917 [11:44:14<13:52,  3.75s/it] 98%|█████████▊| 8696/8917 [11:44:18<13:38,  3.70s/it] 98%|█████████▊| 8697/8917 [11:44:21<13:40,  3.73s/it] 98%|█████████▊| 8698/8917 [11:44:25<13:35,  3.72s/it] 98%|█████████▊| 8699/8917 [11:44:29<13:38,  3.75s/it]09/19/2024 13:59:07 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.0265555027872324, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1321107149124146, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1586662530899048}
 98%|█████████▊| 8700/8917 [11:44:33<13:45,  3.81s/it] 98%|█████████▊| 8701/8917 [11:44:36<13:15,  3.68s/it] 98%|█████████▊| 8702/8917 [11:44:40<13:38,  3.80s/it] 98%|█████████▊| 8703/8917 [11:44:44<13:31,  3.79s/it] 98%|█████████▊| 8704/8917 [11:44:48<13:22,  3.77s/it] 98%|█████████▊| 8705/8917 [11:44:52<13:13,  3.74s/it] 98%|█████████▊| 8706/8917 [11:44:55<13:12,  3.76s/it] 98%|█████████▊| 8707/8917 [11:44:59<12:53,  3.68s/it] 98%|█████████▊| 8708/8917 [11:45:02<12:41,  3.64s/it] 98%|█████████▊| 8709/8917 [11:45:06<13:02,  3.76s/it] 98%|█████████▊| 8710/8917 [11:45:10<12:44,  3.69s/it] 98%|█████████▊| 8711/8917 [11:45:14<12:50,  3.74s/it] 98%|█████████▊| 8712/8917 [11:45:18<12:51,  3.76s/it] 98%|█████████▊| 8713/8917 [11:45:21<12:48,  3.77s/it] 98%|█████████▊| 8714/8917 [11:45:25<12:36,  3.72s/it] 98%|█████████▊| 8715/8917 [11:45:29<12:32,  3.73s/it] 98%|█████████▊| 8716/8917 [11:45:33<12:46,  3.81s/it] 98%|█████████▊| 8717/8917 [11:45:36<12:32,  3.76s/it] 98%|█████████▊| 8718/8917 [11:45:40<12:19,  3.71s/it] 98%|█████████▊| 8719/8917 [11:45:44<12:03,  3.65s/it] 98%|█████████▊| 8720/8917 [11:45:47<12:14,  3.73s/it] 98%|█████████▊| 8721/8917 [11:45:51<12:08,  3.72s/it] 98%|█████████▊| 8722/8917 [11:45:55<11:59,  3.69s/it] 98%|█████████▊| 8723/8917 [11:45:58<11:45,  3.64s/it] 98%|█████████▊| 8724/8917 [11:46:02<12:08,  3.77s/it] 98%|█████████▊| 8725/8917 [11:46:06<12:05,  3.78s/it] 98%|█████████▊| 8726/8917 [11:46:10<11:43,  3.68s/it] 98%|█████████▊| 8727/8917 [11:46:13<11:43,  3.70s/it] 98%|█████████▊| 8728/8917 [11:46:17<11:21,  3.61s/it] 98%|█████████▊| 8729/8917 [11:46:21<11:47,  3.76s/it] 98%|█████████▊| 8730/8917 [11:46:24<11:27,  3.68s/it] 98%|█████████▊| 8731/8917 [11:46:28<11:14,  3.63s/it] 98%|█████████▊| 8732/8917 [11:46:32<11:27,  3.71s/it] 98%|█████████▊| 8733/8917 [11:46:35<11:16,  3.67s/it] 98%|█████████▊| 8734/8917 [11:46:39<11:35,  3.80s/it] 98%|█████████▊| 8735/8917 [11:46:43<11:11,  3.69s/it] 98%|█████████▊| 8736/8917 [11:46:47<11:14,  3.73s/it] 98%|█████████▊| 8737/8917 [11:46:50<11:13,  3.74s/it] 98%|█████████▊| 8738/8917 [11:46:54<11:24,  3.83s/it] 98%|█████████▊| 8739/8917 [11:46:58<11:21,  3.83s/it] 98%|█████████▊| 8740/8917 [11:47:02<10:50,  3.67s/it] 98%|█████████▊| 8741/8917 [11:47:05<10:48,  3.69s/it] 98%|█████████▊| 8742/8917 [11:47:09<11:02,  3.79s/it] 98%|█████████▊| 8743/8917 [11:47:13<10:53,  3.76s/it] 98%|█████████▊| 8744/8917 [11:47:17<10:47,  3.74s/it] 98%|█████████▊| 8745/8917 [11:47:20<10:43,  3.74s/it] 98%|█████████▊| 8746/8917 [11:47:24<10:44,  3.77s/it] 98%|█████████▊| 8747/8917 [11:47:28<10:35,  3.74s/it] 98%|█████████▊| 8748/8917 [11:47:32<10:28,  3.72s/it] 98%|█████████▊| 8749/8917 [11:47:36<10:37,  3.80s/it]09/19/2024 14:02:13 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02012159489095211, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2323291301727295, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.252450704574585}
 98%|█████████▊| 8750/8917 [11:47:39<10:19,  3.71s/it] 98%|█████████▊| 8751/8917 [11:47:43<10:12,  3.69s/it] 98%|█████████▊| 8752/8917 [11:47:47<10:17,  3.74s/it] 98%|█████████▊| 8753/8917 [11:47:51<10:27,  3.82s/it] 98%|█████████▊| 8754/8917 [11:47:54<10:05,  3.71s/it] 98%|█████████▊| 8755/8917 [11:47:58<10:08,  3.75s/it] 98%|█████████▊| 8756/8917 [11:48:02<09:57,  3.71s/it] 98%|█████████▊| 8757/8917 [11:48:06<10:18,  3.86s/it] 98%|█████████▊| 8758/8917 [11:48:09<10:01,  3.78s/it] 98%|█████████▊| 8759/8917 [11:48:13<10:05,  3.83s/it] 98%|█████████▊| 8760/8917 [11:48:17<09:55,  3.80s/it] 98%|█████████▊| 8761/8917 [11:48:21<09:44,  3.75s/it] 98%|█████████▊| 8762/8917 [11:48:24<09:32,  3.69s/it] 98%|█████████▊| 8763/8917 [11:48:28<09:37,  3.75s/it] 98%|█████████▊| 8764/8917 [11:48:32<09:33,  3.75s/it] 98%|█████████▊| 8765/8917 [11:48:36<09:24,  3.72s/it] 98%|█████████▊| 8766/8917 [11:48:40<09:44,  3.87s/it] 98%|█████████▊| 8767/8917 [11:48:43<09:29,  3.80s/it] 98%|█████████▊| 8768/8917 [11:48:47<09:29,  3.82s/it] 98%|█████████▊| 8769/8917 [11:48:51<09:21,  3.80s/it] 98%|█████████▊| 8770/8917 [11:48:55<09:12,  3.76s/it] 98%|█████████▊| 8771/8917 [11:48:58<08:51,  3.64s/it] 98%|█████████▊| 8772/8917 [11:49:02<08:43,  3.61s/it] 98%|█████████▊| 8773/8917 [11:49:05<08:49,  3.67s/it] 98%|█████████▊| 8774/8917 [11:49:09<08:54,  3.74s/it] 98%|█████████▊| 8775/8917 [11:49:13<08:51,  3.74s/it] 98%|█████████▊| 8776/8917 [11:49:17<08:41,  3.70s/it] 98%|█████████▊| 8777/8917 [11:49:20<08:27,  3.62s/it] 98%|█████████▊| 8778/8917 [11:49:24<08:25,  3.64s/it] 98%|█████████▊| 8779/8917 [11:49:28<08:41,  3.78s/it] 98%|█████████▊| 8780/8917 [11:49:31<08:25,  3.69s/it] 98%|█████████▊| 8781/8917 [11:49:35<08:17,  3.66s/it] 98%|█████████▊| 8782/8917 [11:49:39<08:16,  3.68s/it] 98%|█████████▊| 8783/8917 [11:49:42<08:04,  3.61s/it] 99%|█████████▊| 8784/8917 [11:49:46<08:16,  3.73s/it] 99%|█████████▊| 8785/8917 [11:49:50<08:28,  3.85s/it] 99%|█████████▊| 8786/8917 [11:49:54<08:12,  3.76s/it] 99%|█████████▊| 8787/8917 [11:49:58<08:08,  3.76s/it] 99%|█████████▊| 8788/8917 [11:50:01<07:53,  3.67s/it] 99%|█████████▊| 8789/8917 [11:50:05<07:49,  3.67s/it] 99%|█████████▊| 8790/8917 [11:50:08<07:46,  3.67s/it] 99%|█████████▊| 8791/8917 [11:50:12<07:51,  3.74s/it] 99%|█████████▊| 8792/8917 [11:50:16<07:55,  3.81s/it] 99%|█████████▊| 8793/8917 [11:50:20<07:43,  3.73s/it] 99%|█████████▊| 8794/8917 [11:50:23<07:36,  3.71s/it] 99%|█████████▊| 8795/8917 [11:50:27<07:27,  3.66s/it] 99%|█████████▊| 8796/8917 [11:50:31<07:18,  3.62s/it] 99%|█████████▊| 8797/8917 [11:50:34<07:23,  3.69s/it] 99%|█████████▊| 8798/8917 [11:50:38<07:24,  3.73s/it] 99%|█████████▊| 8799/8917 [11:50:42<07:27,  3.79s/it]09/19/2024 14:05:19 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.03469548001885414, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2412011623382568, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.275896668434143}
 99%|█████████▊| 8800/8917 [11:50:46<07:11,  3.69s/it] 99%|█████████▊| 8801/8917 [11:50:49<07:00,  3.62s/it] 99%|█████████▊| 8802/8917 [11:50:53<07:07,  3.72s/it] 99%|█████████▊| 8803/8917 [11:50:57<07:16,  3.83s/it] 99%|█████████▊| 8804/8917 [11:51:01<07:05,  3.77s/it] 99%|█████████▊| 8805/8917 [11:51:04<06:54,  3.70s/it] 99%|█████████▉| 8806/8917 [11:51:08<06:53,  3.73s/it] 99%|█████████▉| 8807/8917 [11:51:12<06:56,  3.79s/it] 99%|█████████▉| 8808/8917 [11:51:16<06:46,  3.73s/it] 99%|█████████▉| 8809/8917 [11:51:20<06:56,  3.85s/it] 99%|█████████▉| 8810/8917 [11:51:23<06:43,  3.77s/it] 99%|█████████▉| 8811/8917 [11:51:27<06:29,  3.68s/it] 99%|█████████▉| 8812/8917 [11:51:31<06:34,  3.75s/it] 99%|█████████▉| 8813/8917 [11:51:34<06:19,  3.65s/it] 99%|█████████▉| 8814/8917 [11:51:38<06:13,  3.62s/it] 99%|█████████▉| 8815/8917 [11:51:41<06:14,  3.67s/it] 99%|█████████▉| 8816/8917 [11:51:45<06:18,  3.75s/it] 99%|█████████▉| 8817/8917 [11:51:49<06:09,  3.69s/it] 99%|█████████▉| 8818/8917 [11:51:52<06:01,  3.66s/it] 99%|█████████▉| 8819/8917 [11:51:56<05:56,  3.64s/it] 99%|█████████▉| 8820/8917 [11:52:00<05:57,  3.69s/it] 99%|█████████▉| 8821/8917 [11:52:04<05:53,  3.69s/it] 99%|█████████▉| 8822/8917 [11:52:07<05:48,  3.67s/it] 99%|█████████▉| 8823/8917 [11:52:11<05:39,  3.61s/it] 99%|█████████▉| 8824/8917 [11:52:14<05:38,  3.64s/it] 99%|█████████▉| 8825/8917 [11:52:18<05:38,  3.68s/it] 99%|█████████▉| 8826/8917 [11:52:22<05:37,  3.71s/it] 99%|█████████▉| 8827/8917 [11:52:26<05:38,  3.76s/it] 99%|█████████▉| 8828/8917 [11:52:29<05:30,  3.72s/it] 99%|█████████▉| 8829/8917 [11:52:33<05:28,  3.73s/it] 99%|█████████▉| 8830/8917 [11:52:37<05:15,  3.63s/it] 99%|█████████▉| 8831/8917 [11:52:41<05:27,  3.81s/it] 99%|█████████▉| 8832/8917 [11:52:45<05:23,  3.80s/it] 99%|█████████▉| 8833/8917 [11:52:48<05:12,  3.72s/it] 99%|█████████▉| 8834/8917 [11:52:52<05:12,  3.76s/it] 99%|█████████▉| 8835/8917 [11:52:56<05:07,  3.75s/it] 99%|█████████▉| 8836/8917 [11:52:59<05:04,  3.75s/it] 99%|█████████▉| 8837/8917 [11:53:03<05:02,  3.78s/it] 99%|█████████▉| 8838/8917 [11:53:07<05:07,  3.89s/it] 99%|█████████▉| 8839/8917 [11:53:11<04:57,  3.81s/it] 99%|█████████▉| 8840/8917 [11:53:15<04:50,  3.77s/it] 99%|█████████▉| 8841/8917 [11:53:18<04:39,  3.67s/it] 99%|█████████▉| 8842/8917 [11:53:22<04:37,  3.70s/it] 99%|█████████▉| 8843/8917 [11:53:26<04:42,  3.82s/it] 99%|█████████▉| 8844/8917 [11:53:30<04:36,  3.79s/it] 99%|█████████▉| 8845/8917 [11:53:33<04:27,  3.72s/it] 99%|█████████▉| 8846/8917 [11:53:37<04:15,  3.60s/it] 99%|█████████▉| 8847/8917 [11:53:40<04:14,  3.63s/it] 99%|█████████▉| 8848/8917 [11:53:44<04:10,  3.63s/it] 99%|█████████▉| 8849/8917 [11:53:48<04:14,  3.74s/it]09/19/2024 14:08:26 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.029312560334801674, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.2585179805755615, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.2878305912017822}
 99%|█████████▉| 8850/8917 [11:53:52<04:15,  3.82s/it] 99%|█████████▉| 8851/8917 [11:53:55<04:05,  3.73s/it] 99%|█████████▉| 8852/8917 [11:53:59<03:57,  3.65s/it] 99%|█████████▉| 8853/8917 [11:54:03<03:58,  3.73s/it] 99%|█████████▉| 8854/8917 [11:54:07<03:53,  3.71s/it] 99%|█████████▉| 8855/8917 [11:54:10<03:48,  3.69s/it] 99%|█████████▉| 8856/8917 [11:54:14<03:45,  3.70s/it] 99%|█████████▉| 8857/8917 [11:54:17<03:39,  3.66s/it] 99%|█████████▉| 8858/8917 [11:54:21<03:32,  3.60s/it] 99%|█████████▉| 8859/8917 [11:54:24<03:25,  3.54s/it] 99%|█████████▉| 8860/8917 [11:54:28<03:23,  3.57s/it] 99%|█████████▉| 8861/8917 [11:54:31<03:18,  3.54s/it] 99%|█████████▉| 8862/8917 [11:54:35<03:14,  3.54s/it] 99%|█████████▉| 8863/8917 [11:54:39<03:13,  3.58s/it] 99%|█████████▉| 8864/8917 [11:54:42<03:12,  3.64s/it] 99%|█████████▉| 8865/8917 [11:54:46<03:13,  3.73s/it] 99%|█████████▉| 8866/8917 [11:54:50<03:16,  3.85s/it] 99%|█████████▉| 8867/8917 [11:54:54<03:10,  3.81s/it] 99%|█████████▉| 8868/8917 [11:54:57<02:57,  3.62s/it] 99%|█████████▉| 8869/8917 [11:55:01<02:53,  3.62s/it] 99%|█████████▉| 8870/8917 [11:55:05<02:51,  3.64s/it] 99%|█████████▉| 8871/8917 [11:55:08<02:47,  3.63s/it] 99%|█████████▉| 8872/8917 [11:55:12<02:48,  3.73s/it]100%|█████████▉| 8873/8917 [11:55:16<02:43,  3.72s/it]100%|█████████▉| 8874/8917 [11:55:19<02:37,  3.66s/it]100%|█████████▉| 8875/8917 [11:55:23<02:30,  3.58s/it]100%|█████████▉| 8876/8917 [11:55:27<02:29,  3.64s/it]100%|█████████▉| 8877/8917 [11:55:31<02:34,  3.86s/it]100%|█████████▉| 8878/8917 [11:55:35<02:26,  3.76s/it]100%|█████████▉| 8879/8917 [11:55:38<02:18,  3.64s/it]100%|█████████▉| 8880/8917 [11:55:42<02:16,  3.68s/it]100%|█████████▉| 8881/8917 [11:55:45<02:12,  3.68s/it]100%|█████████▉| 8882/8917 [11:55:49<02:09,  3.70s/it]100%|█████████▉| 8883/8917 [11:55:53<02:03,  3.64s/it]100%|█████████▉| 8884/8917 [11:55:56<02:00,  3.64s/it]100%|█████████▉| 8885/8917 [11:56:00<01:55,  3.61s/it]100%|█████████▉| 8886/8917 [11:56:04<01:53,  3.66s/it]100%|█████████▉| 8887/8917 [11:56:07<01:51,  3.71s/it]100%|█████████▉| 8888/8917 [11:56:11<01:48,  3.75s/it]100%|█████████▉| 8889/8917 [11:56:15<01:43,  3.70s/it]100%|█████████▉| 8890/8917 [11:56:19<01:41,  3.75s/it]100%|█████████▉| 8891/8917 [11:56:22<01:37,  3.74s/it]100%|█████████▉| 8892/8917 [11:56:26<01:32,  3.70s/it]100%|█████████▉| 8893/8917 [11:56:30<01:31,  3.82s/it]100%|█████████▉| 8894/8917 [11:56:34<01:25,  3.73s/it]100%|█████████▉| 8895/8917 [11:56:37<01:20,  3.66s/it]100%|█████████▉| 8896/8917 [11:56:41<01:18,  3.74s/it]100%|█████████▉| 8897/8917 [11:56:45<01:15,  3.79s/it]100%|█████████▉| 8898/8917 [11:56:49<01:10,  3.73s/it]100%|█████████▉| 8899/8917 [11:56:52<01:07,  3.73s/it]09/19/2024 14:11:30 - INFO - __main__ -   {'loss_ret%tv%ta--msrvtt_ret/loss_itc': 0.0, 'loss_ret%tv%ta--msrvtt_ret/loss_itm': 0.02153952606022358, 'loss_ret%tv%ta--msrvtt_ret/loss_area': 1.1596897840499878, 'loss_ret%tv%ta--msrvtt_ret/total_loss': 1.1812293529510498}
100%|█████████▉| 8900/8917 [11:56:56<01:04,  3.78s/it]100%|█████████▉| 8901/8917 [11:57:00<01:01,  3.82s/it]100%|█████████▉| 8902/8917 [11:57:04<00:56,  3.77s/it]100%|█████████▉| 8903/8917 [11:57:07<00:51,  3.65s/it]100%|█████████▉| 8904/8917 [11:57:11<00:46,  3.59s/it]100%|█████████▉| 8905/8917 [11:57:15<00:44,  3.70s/it]100%|█████████▉| 8906/8917 [11:57:18<00:40,  3.71s/it]100%|█████████▉| 8907/8917 [11:57:22<00:36,  3.65s/it]100%|█████████▉| 8908/8917 [11:57:26<00:33,  3.71s/it]100%|█████████▉| 8909/8917 [11:57:29<00:29,  3.73s/it]100%|█████████▉| 8910/8917 [11:57:34<00:27,  3.87s/it]100%|█████████▉| 8911/8917 [11:57:37<00:22,  3.73s/it]100%|█████████▉| 8912/8917 [11:57:41<00:18,  3.71s/it]100%|█████████▉| 8913/8917 [11:57:45<00:15,  3.83s/it]100%|█████████▉| 8914/8917 [11:57:49<00:11,  3.84s/it]100%|█████████▉| 8915/8917 [11:57:52<00:07,  3.80s/it]100%|█████████▉| 8916/8917 [11:57:56<00:03,  3.84s/it]100%|██████████| 8917/8917 [11:58:00<00:00,  3.79s/it]100%|██████████| 8917/8917 [11:58:00<00:00,  4.83s/it]
wandb: 
wandb: Run history:
wandb:  loss_area ▄▅█▃▄▄▄▆▅▂▂▃▁▂▅▃▂▂▅▂▄▃▂▂▁▂▃▃▂▃▃▂▃▂▃▄▂▃▁▃
wandb:   loss_itc ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb:   loss_itm ▇█▇▄█▅▅▅█▄▂▆▁▆▇▅▄▃▇▂▄▃▁▁▃▂▆▄▁▂▄▄▅▃▆█▃▅▁▂
wandb: total_loss ▄▅█▃▅▄▄▆▅▂▂▃▁▃▅▃▂▂▅▂▄▂▂▁▁▂▃▃▂▃▃▂▃▂▃▄▂▃▁▃
wandb: 
wandb: Run summary:
wandb:  loss_area 1.20328
wandb:   loss_itc 0.0
wandb:   loss_itm 0.02412
wandb: total_loss 1.22739
wandb: 
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /leonardo_scratch/fast/IscrC_GenOpt/giordano/VAST/wandb/offline-run-20240919_020404-1px89gt4
wandb: Find logs at: ./wandb/offline-run-20240919_020404-1px89gt4/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
09/19/2024 14:12:42 - WARNING - urllib3.connectionpool -   Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x148bf7e500a0>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
09/19/2024 14:12:42 - WARNING - urllib3.connectionpool -   Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x148bf7e51b20>: Failed to establish a new connection: [Errno 101] Network is unreachable')': /api/4504800232407040/envelope/
